CLASS_ORDER: [827, 290, 387, 646, 659, 812, 507, 364, 826, 368, 656, 779, 311, 314, 715, 140, 636, 434, 111, 730, 783, 625, 602, 23, 266, 258, 600, 146, 781, 815, 130, 576, 709, 403, 349, 674, 645, 58, 149, 861, 556, 3, 555, 159, 375, 142, 751, 723, 818, 156, 8, 834, 285, 867, 166, 160, 335, 326, 820, 302, 582, 695, 707, 683, 796, 267, 508, 524, 502, 745, 209, 877, 808, 727, 559, 837, 331, 82, 108, 532, 872, 329, 511, 155, 12, 172, 737, 797, 844, 126, 163, 346, 657, 879, 535, 848, 276, 814, 80, 859, 845, 627, 878, 2, 211, 238, 293, 568, 754, 252, 529, 868, 419, 165, 633, 184, 251, 35, 430, 777, 31, 109, 21, 342, 793, 377, 4, 595, 843, 697, 201, 512, 681, 603, 84, 829, 611, 333, 26, 397, 182, 351, 55, 61, 405, 803, 102, 452, 892, 226, 463, 135, 77, 566, 353, 217, 32, 653, 339, 362, 461, 448, 699, 68, 534, 853, 528, 425, 870, 599, 426, 421, 578, 857, 663, 671, 124, 225, 496, 52, 110, 567, 746, 222, 614, 680, 0, 490, 484, 499, 287, 116, 199, 791, 458, 444, 78, 9, 612, 280, 457, 764, 53, 323, 449, 396, 852, 594, 409, 693, 765, 1, 30, 192, 296, 164, 800, 301, 691, 722, 785, 215, 234, 75, 718, 766, 347, 259, 394, 530, 101, 88, 17, 128, 119, 799, 809, 196, 224, 807, 728, 446, 178, 583, 413, 736, 197, 415, 204, 884, 869, 520, 388, 543, 437, 549, 828, 676, 759, 390, 181, 541, 562, 42, 174, 313, 230, 418, 317, 356, 662, 806, 533, 855, 819, 596, 289, 158, 720, 847, 640, 849, 618, 873, 733, 644, 773, 510, 219, 115, 658, 588, 384, 207, 890, 90, 404, 725, 833, 64, 220, 790, 321, 735, 711, 203, 281, 563, 250, 367, 228, 105, 863, 161, 345, 891, 22, 606, 443, 118, 123, 303, 690, 306, 264, 202, 739, 504, 619, 731, 632, 60, 14, 531, 194, 103, 670, 93, 411, 881, 223, 624, 6, 416, 360, 183, 81, 328, 851, 574, 553, 340, 665, 760, 127, 527, 177, 519, 424, 85, 876, 771, 497, 320, 558, 180, 41, 382, 37, 575, 888, 33, 265, 584, 651, 830, 604, 120, 445, 571, 107, 282, 294, 838, 752, 210, 778, 675, 694, 205, 50, 36, 148, 688, 679, 776, 344, 439, 15, 539, 423, 338, 273, 592, 185, 780, 661, 436, 641, 114, 772, 408, 491, 242, 455, 350, 136, 133, 883, 456, 742, 889, 354, 609, 565, 536, 488, 150, 113, 398, 11, 145, 372, 417, 24, 811, 885, 380, 607, 112, 621, 309, 241, 171, 794, 76, 38, 308, 341, 451, 880, 69, 835, 460, 634, 432, 601, 514, 318, 724, 291, 191, 631, 117, 494, 493, 214, 89, 887, 440, 756, 395, 782, 18, 271, 572, 138, 705, 561, 810, 560, 483, 151, 513, 477, 652, 506, 716, 431, 286, 586, 334, 104, 231, 669, 552, 710, 719, 237, 471, 544, 248, 186, 548, 79, 100, 244, 467, 763, 10, 700, 312, 410, 865, 153, 260, 635, 232, 703, 179, 277, 420, 386, 72, 593, 740, 698, 755, 581, 299, 668, 99, 465, 212, 324, 401, 685, 262, 193, 257, 363, 481, 801, 147, 325, 369, 310, 660, 589, 249, 721, 638, 525, 770, 348, 371, 630, 702, 462, 580, 590, 195, 805, 476, 442, 523, 381, 125, 272, 315, 288, 824, 537, 616, 538, 726, 227, 13, 213, 769, 587, 152, 821, 629, 866, 743, 501, 121, 498, 841, 47, 860, 642, 438, 831, 206, 495, 173, 235, 613, 255, 570, 864, 34, 7, 218, 706, 40, 792, 373, 407, 487, 453, 253, 54, 427, 505, 802, 485, 489, 83, 208, 422, 482, 391, 708, 643, 284, 701, 836, 44, 480, 216, 788, 157, 307, 550, 503, 29, 637, 261, 188, 330, 198, 402, 132, 49, 73, 813, 486, 696, 279, 854, 768, 664, 59, 19, 767, 358, 750, 187, 667, 319, 274, 758, 71, 229, 786, 569, 893, 564, 617, 666, 385, 134, 585, 305, 51, 97, 16, 361, 673, 874, 393, 744, 300, 243, 370, 686, 378, 221, 92, 25, 620, 823, 650, 295, 546, 154, 376, 175, 278, 478, 56, 39, 143, 94, 131, 714, 516, 454, 825, 734, 741, 677, 95, 70, 327, 762, 239, 858, 517, 412, 86, 598, 464, 298, 169, 615, 684, 554, 846, 399, 428, 233, 597, 761, 579, 882, 704, 144, 626, 875, 240, 190, 816, 557, 468, 682, 91, 655, 775, 729, 245, 732, 355, 162, 577, 139, 106, 832, 748, 469, 784, 447, 406, 441, 509, 62, 322, 850, 608, 856, 254, 435, 96, 787, 473, 389, 551, 46, 297, 649, 542, 316, 687, 713, 366, 712, 622, 521, 886, 433, 137, 628, 839, 789, 466, 545, 246, 43, 798, 129, 459, 374, 871, 472, 817, 359, 639, 352, 774, 337, 122, 518, 304, 717, 63, 283, 383, 74, 28, 67, 692, 515, 379, 414, 170, 357, 167, 236, 689, 5, 804, 678, 189, 862, 57, 65, 753, 98, 540, 269, 672, 479, 747, 610, 275, 500, 365, 400, 176, 263, 268, 332, 474, 247, 475, 48, 168, 141, 623, 45, 522, 270, 840, 429, 392, 757, 343, 795, 547, 526, 66, 591, 450, 647, 256, 336, 654, 573, 200, 842, 20, 492, 648, 470, 292, 738, 822, 27, 749, 87, 605]
class_group: [(827, 290, 387, 646, 659, 812, 507, 364, 826, 368, 656, 779, 311, 314, 715, 140, 636, 434, 111, 730, 783, 625, 602, 23, 266, 258, 600, 146, 781, 815, 130, 576, 709, 403), (349, 674, 645, 58, 149, 861, 556, 3, 555, 159, 375, 142, 751, 723, 818, 156, 8, 834, 285, 867), (166, 160, 335, 326, 820, 302, 582, 695, 707, 683, 796, 267, 508, 524, 502, 745, 209, 877, 808, 727), (559, 837, 331, 82, 108, 532, 872, 329, 511, 155, 12, 172, 737, 797, 844, 126, 163, 346, 657, 879), (535, 848, 276, 814, 80, 859, 845, 627, 878, 2, 211, 238, 293, 568, 754, 252, 529, 868, 419, 165), (633, 184, 251, 35, 430, 777, 31, 109, 21, 342, 793, 377, 4, 595, 843, 697, 201, 512, 681, 603), (84, 829, 611, 333, 26, 397, 182, 351, 55, 61, 405, 803, 102, 452, 892, 226, 463, 135, 77, 566), (353, 217, 32, 653, 339, 362, 461, 448, 699, 68, 534, 853, 528, 425, 870, 599, 426, 421, 578, 857), (663, 671, 124, 225, 496, 52, 110, 567, 746, 222, 614, 680, 0, 490, 484, 499, 287, 116, 199, 791), (458, 444, 78, 9, 612, 280, 457, 764, 53, 323, 449, 396, 852, 594, 409, 693, 765, 1, 30, 192), (296, 164, 800, 301, 691, 722, 785, 215, 234, 75, 718, 766, 347, 259, 394, 530, 101, 88, 17, 128), (119, 799, 809, 196, 224, 807, 728, 446, 178, 583, 413, 736, 197, 415, 204, 884, 869, 520, 388, 543), (437, 549, 828, 676, 759, 390, 181, 541, 562, 42, 174, 313, 230, 418, 317, 356, 662, 806, 533, 855), (819, 596, 289, 158, 720, 847, 640, 849, 618, 873, 733, 644, 773, 510, 219, 115, 658, 588, 384, 207), (890, 90, 404, 725, 833, 64, 220, 790, 321, 735, 711, 203, 281, 563, 250, 367, 228, 105, 863, 161), (345, 891, 22, 606, 443, 118, 123, 303, 690, 306, 264, 202, 739, 504, 619, 731, 632, 60, 14, 531), (194, 103, 670, 93, 411, 881, 223, 624, 6, 416, 360, 183, 81, 328, 851, 574, 553, 340, 665, 760), (127, 527, 177, 519, 424, 85, 876, 771, 497, 320, 558, 180, 41, 382, 37, 575, 888, 33, 265, 584), (651, 830, 604, 120, 445, 571, 107, 282, 294, 838, 752, 210, 778, 675, 694, 205, 50, 36, 148, 688), (679, 776, 344, 439, 15, 539, 423, 338, 273, 592, 185, 780, 661, 436, 641, 114, 772, 408, 491, 242), (455, 350, 136, 133, 883, 456, 742, 889, 354, 609, 565, 536, 488, 150, 113, 398, 11, 145, 372, 417), (24, 811, 885, 380, 607, 112, 621, 309, 241, 171, 794, 76, 38, 308, 341, 451, 880, 69, 835, 460), (634, 432, 601, 514, 318, 724, 291, 191, 631, 117, 494, 493, 214, 89, 887, 440, 756, 395, 782, 18), (271, 572, 138, 705, 561, 810, 560, 483, 151, 513, 477, 652, 506, 716, 431, 286, 586, 334, 104, 231), (669, 552, 710, 719, 237, 471, 544, 248, 186, 548, 79, 100, 244, 467, 763, 10, 700, 312, 410, 865), (153, 260, 635, 232, 703, 179, 277, 420, 386, 72, 593, 740, 698, 755, 581, 299, 668, 99, 465, 212), (324, 401, 685, 262, 193, 257, 363, 481, 801, 147, 325, 369, 310, 660, 589, 249, 721, 638, 525, 770), (348, 371, 630, 702, 462, 580, 590, 195, 805, 476, 442, 523, 381, 125, 272, 315, 288, 824, 537, 616), (538, 726, 227, 13, 213, 769, 587, 152, 821, 629, 866, 743, 501, 121, 498, 841, 47, 860, 642, 438), (831, 206, 495, 173, 235, 613, 255, 570, 864, 34, 7, 218, 706, 40, 792, 373, 407, 487, 453, 253), (54, 427, 505, 802, 485, 489, 83, 208, 422, 482, 391, 708, 643, 284, 701, 836, 44, 480, 216, 788), (157, 307, 550, 503, 29, 637, 261, 188, 330, 198, 402, 132, 49, 73, 813, 486, 696, 279, 854, 768), (664, 59, 19, 767, 358, 750, 187, 667, 319, 274, 758, 71, 229, 786, 569, 893, 564, 617, 666, 385), (134, 585, 305, 51, 97, 16, 361, 673, 874, 393, 744, 300, 243, 370, 686, 378, 221, 92, 25, 620), (823, 650, 295, 546, 154, 376, 175, 278, 478, 56, 39, 143, 94, 131, 714, 516, 454, 825, 734, 741), (677, 95, 70, 327, 762, 239, 858, 517, 412, 86, 598, 464, 298, 169, 615, 684, 554, 846, 399, 428), (233, 597, 761, 579, 882, 704, 144, 626, 875, 240, 190, 816, 557, 468, 682, 91, 655, 775, 729, 245), (732, 355, 162, 577, 139, 106, 832, 748, 469, 784, 447, 406, 441, 509, 62, 322, 850, 608, 856, 254), (435, 96, 787, 473, 389, 551, 46, 297, 649, 542, 316, 687, 713, 366, 712, 622, 521, 886, 433, 137), (628, 839, 789, 466, 545, 246, 43, 798, 129, 459, 374, 871, 472, 817, 359, 639, 352, 774, 337, 122), (518, 304, 717, 63, 283, 383, 74, 28, 67, 692, 515, 379, 414, 170, 357, 167, 236, 689, 5, 804), (678, 189, 862, 57, 65, 753, 98, 540, 269, 672, 479, 747, 610, 275, 500, 365, 400, 176, 263, 268), (332, 474, 247, 475, 48, 168, 141, 623, 45, 522, 270, 840, 429, 392, 757, 343, 795, 547, 526, 66), (591, 450, 647, 256, 336, 654, 573, 200, 842, 20, 492, 648, 470, 292, 738, 822, 27, 749, 87, 605)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.8929	step1_train_accuracy: 50.9728
STEP-1	Epoch: 20/50	loss: 1.1103	step1_train_accuracy: 77.8210
STEP-1	Epoch: 30/50	loss: 0.7532	step1_train_accuracy: 84.2412
STEP-1	Epoch: 40/50	loss: 0.5692	step1_train_accuracy: 86.3813
STEP-1	Epoch: 50/50	loss: 0.4484	step1_train_accuracy: 90.0778
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.2191	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.2067	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.2001	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.1960	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.1924	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.1891	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.1859	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.1833	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.1811	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.1793	gate_loss: 0.0000	step2_classification_accuracy: 90.1961	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.5194	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.5758	step1_train_accuracy: 59.7561
STEP-1	Epoch: 20/50	loss: 0.7841	step1_train_accuracy: 86.5854
STEP-1	Epoch: 30/50	loss: 0.4385	step1_train_accuracy: 94.2073
STEP-1	Epoch: 40/50	loss: 0.2789	step1_train_accuracy: 94.2073
STEP-1	Epoch: 50/50	loss: 0.1996	step1_train_accuracy: 96.0366
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.2183	gate_loss: 0.1025	step2_classification_accuracy: 91.0301	step_2_gate_accuracy: 97.6852
STEP-2	Epoch: 40/200	classification_loss: 0.1877	gate_loss: 0.0418	step2_classification_accuracy: 91.4931	step_2_gate_accuracy: 99.0741
STEP-2	Epoch: 60/200	classification_loss: 0.1561	gate_loss: 0.0189	step2_classification_accuracy: 91.8403	step_2_gate_accuracy: 99.8843
STEP-2	Epoch: 80/200	classification_loss: 0.1534	gate_loss: 0.0133	step2_classification_accuracy: 91.7824	step_2_gate_accuracy: 99.8843
STEP-2	Epoch: 100/200	classification_loss: 0.1465	gate_loss: 0.0085	step2_classification_accuracy: 91.8981	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.1820	gate_loss: 0.0192	step2_classification_accuracy: 91.7824	step_2_gate_accuracy: 99.7685
STEP-2	Epoch: 140/200	classification_loss: 0.1547	gate_loss: 0.0110	step2_classification_accuracy: 91.7245	step_2_gate_accuracy: 99.7106
STEP-2	Epoch: 160/200	classification_loss: 0.1385	gate_loss: 0.0050	step2_classification_accuracy: 91.8981	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.1384	gate_loss: 0.0046	step2_classification_accuracy: 91.8981	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.1353	gate_loss: 0.0043	step2_classification_accuracy: 91.8981	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.7442	gate_accuracy: 99.2248
	Task-1	val_accuracy: 89.0244	gate_accuracy: 96.3415
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 98.1043


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 2.3081	step1_train_accuracy: 50.1845
STEP-1	Epoch: 20/50	loss: 1.1537	step1_train_accuracy: 81.1808
STEP-1	Epoch: 30/50	loss: 0.6451	step1_train_accuracy: 94.8339
STEP-1	Epoch: 40/50	loss: 0.4033	step1_train_accuracy: 95.5720
STEP-1	Epoch: 50/50	loss: 0.2796	step1_train_accuracy: 96.3100
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.2661	gate_loss: 0.2361	step2_classification_accuracy: 88.8288	step_2_gate_accuracy: 92.8378
STEP-2	Epoch: 40/200	classification_loss: 0.2256	gate_loss: 0.1045	step2_classification_accuracy: 89.5495	step_2_gate_accuracy: 95.9459
STEP-2	Epoch: 60/200	classification_loss: 0.2059	gate_loss: 0.0747	step2_classification_accuracy: 90.4054	step_2_gate_accuracy: 96.8919
STEP-2	Epoch: 80/200	classification_loss: 0.1935	gate_loss: 0.0658	step2_classification_accuracy: 90.3153	step_2_gate_accuracy: 96.8919
STEP-2	Epoch: 100/200	classification_loss: 0.1862	gate_loss: 0.0584	step2_classification_accuracy: 90.5856	step_2_gate_accuracy: 97.0721
STEP-2	Epoch: 120/200	classification_loss: 0.1790	gate_loss: 0.0535	step2_classification_accuracy: 90.6306	step_2_gate_accuracy: 97.6126
STEP-2	Epoch: 140/200	classification_loss: 0.1725	gate_loss: 0.0508	step2_classification_accuracy: 90.8559	step_2_gate_accuracy: 97.7477
STEP-2	Epoch: 160/200	classification_loss: 0.1699	gate_loss: 0.0476	step2_classification_accuracy: 91.2162	step_2_gate_accuracy: 97.7027
STEP-2	Epoch: 180/200	classification_loss: 0.1661	gate_loss: 0.0462	step2_classification_accuracy: 91.0360	step_2_gate_accuracy: 97.8829
STEP-2	Epoch: 200/200	classification_loss: 0.1614	gate_loss: 0.0445	step2_classification_accuracy: 91.0360	step_2_gate_accuracy: 97.9279
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.5426	gate_accuracy: 89.9225
	Task-1	val_accuracy: 85.3659	gate_accuracy: 96.3415
	Task-2	val_accuracy: 79.4118	gate_accuracy: 70.5882
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 87.0968


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 1.8221	step1_train_accuracy: 65.5689
STEP-1	Epoch: 20/50	loss: 0.7745	step1_train_accuracy: 88.0239
STEP-1	Epoch: 30/50	loss: 0.4041	step1_train_accuracy: 96.1078
STEP-1	Epoch: 40/50	loss: 0.2623	step1_train_accuracy: 97.3054
STEP-1	Epoch: 50/50	loss: 0.1935	step1_train_accuracy: 97.9042
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.3578	gate_loss: 0.3404	step2_classification_accuracy: 85.8710	step_2_gate_accuracy: 88.9628
STEP-2	Epoch: 40/200	classification_loss: 0.2947	gate_loss: 0.1832	step2_classification_accuracy: 87.8657	step_2_gate_accuracy: 92.0213
STEP-2	Epoch: 60/200	classification_loss: 0.2646	gate_loss: 0.1481	step2_classification_accuracy: 88.3644	step_2_gate_accuracy: 93.5838
STEP-2	Epoch: 80/200	classification_loss: 0.2548	gate_loss: 0.1353	step2_classification_accuracy: 88.4641	step_2_gate_accuracy: 93.2513
STEP-2	Epoch: 100/200	classification_loss: 0.2454	gate_loss: 0.1270	step2_classification_accuracy: 88.9628	step_2_gate_accuracy: 94.0824
STEP-2	Epoch: 120/200	classification_loss: 0.2323	gate_loss: 0.1206	step2_classification_accuracy: 89.6277	step_2_gate_accuracy: 94.5479
STEP-2	Epoch: 140/200	classification_loss: 0.2948	gate_loss: 0.1554	step2_classification_accuracy: 88.9960	step_2_gate_accuracy: 93.8165
STEP-2	Epoch: 160/200	classification_loss: 0.2182	gate_loss: 0.1097	step2_classification_accuracy: 90.4920	step_2_gate_accuracy: 94.8138
STEP-2	Epoch: 180/200	classification_loss: 0.2135	gate_loss: 0.1065	step2_classification_accuracy: 90.1596	step_2_gate_accuracy: 94.8138
STEP-2	Epoch: 200/200	classification_loss: 0.2118	gate_loss: 0.1037	step2_classification_accuracy: 90.0598	step_2_gate_accuracy: 95.3125
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.7674	gate_accuracy: 89.1473
	Task-1	val_accuracy: 85.3659	gate_accuracy: 93.9024
	Task-2	val_accuracy: 69.1176	gate_accuracy: 69.1176
	Task-3	val_accuracy: 84.3373	gate_accuracy: 79.5181
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 84.2541


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.0056	step1_train_accuracy: 62.1875
STEP-1	Epoch: 20/50	loss: 0.7917	step1_train_accuracy: 94.0625
STEP-1	Epoch: 30/50	loss: 0.4017	step1_train_accuracy: 95.3125
STEP-1	Epoch: 40/50	loss: 0.2606	step1_train_accuracy: 95.9375
STEP-1	Epoch: 50/50	loss: 0.1866	step1_train_accuracy: 98.1250
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.3507	gate_loss: 0.3969	step2_classification_accuracy: 86.1630	step_2_gate_accuracy: 87.0968
STEP-2	Epoch: 40/200	classification_loss: 0.2798	gate_loss: 0.1986	step2_classification_accuracy: 88.2569	step_2_gate_accuracy: 92.2184
STEP-2	Epoch: 60/200	classification_loss: 0.2578	gate_loss: 0.1588	step2_classification_accuracy: 88.8795	step_2_gate_accuracy: 93.4352
STEP-2	Epoch: 80/200	classification_loss: 0.2353	gate_loss: 0.1387	step2_classification_accuracy: 89.6435	step_2_gate_accuracy: 94.1992
STEP-2	Epoch: 100/200	classification_loss: 0.2313	gate_loss: 0.1287	step2_classification_accuracy: 89.9547	step_2_gate_accuracy: 93.9728
STEP-2	Epoch: 120/200	classification_loss: 0.2159	gate_loss: 0.1184	step2_classification_accuracy: 90.3792	step_2_gate_accuracy: 94.6803
STEP-2	Epoch: 140/200	classification_loss: 0.2163	gate_loss: 0.1140	step2_classification_accuracy: 90.4641	step_2_gate_accuracy: 94.8783
STEP-2	Epoch: 160/200	classification_loss: 0.2022	gate_loss: 0.1059	step2_classification_accuracy: 90.6904	step_2_gate_accuracy: 95.5009
STEP-2	Epoch: 180/200	classification_loss: 0.1919	gate_loss: 0.1004	step2_classification_accuracy: 91.3413	step_2_gate_accuracy: 95.6140
STEP-2	Epoch: 200/200	classification_loss: 0.1843	gate_loss: 0.0929	step2_classification_accuracy: 91.3130	step_2_gate_accuracy: 95.8687
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.6667	gate_accuracy: 83.7209
	Task-1	val_accuracy: 81.7073	gate_accuracy: 89.0244
	Task-2	val_accuracy: 66.1765	gate_accuracy: 66.1765
	Task-3	val_accuracy: 79.5181	gate_accuracy: 75.9036
	Task-4	val_accuracy: 95.0000	gate_accuracy: 95.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.5792


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.0373	step1_train_accuracy: 59.0604
STEP-1	Epoch: 20/50	loss: 0.9273	step1_train_accuracy: 87.2483
STEP-1	Epoch: 30/50	loss: 0.5339	step1_train_accuracy: 95.6376
STEP-1	Epoch: 40/50	loss: 0.3495	step1_train_accuracy: 97.6510
STEP-1	Epoch: 50/50	loss: 0.2438	step1_train_accuracy: 97.9866
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.3706	gate_loss: 0.3720	step2_classification_accuracy: 86.2053	step_2_gate_accuracy: 87.7657
STEP-2	Epoch: 40/200	classification_loss: 0.2839	gate_loss: 0.2084	step2_classification_accuracy: 88.4215	step_2_gate_accuracy: 91.7910
STEP-2	Epoch: 60/200	classification_loss: 0.2689	gate_loss: 0.1684	step2_classification_accuracy: 89.1904	step_2_gate_accuracy: 92.9444
STEP-2	Epoch: 80/200	classification_loss: 0.2324	gate_loss: 0.1395	step2_classification_accuracy: 90.0271	step_2_gate_accuracy: 93.9620
STEP-2	Epoch: 100/200	classification_loss: 0.2107	gate_loss: 0.1206	step2_classification_accuracy: 90.5473	step_2_gate_accuracy: 94.6630
STEP-2	Epoch: 120/200	classification_loss: 0.2023	gate_loss: 0.1139	step2_classification_accuracy: 91.1126	step_2_gate_accuracy: 95.0475
STEP-2	Epoch: 140/200	classification_loss: 0.1929	gate_loss: 0.1045	step2_classification_accuracy: 91.3840	step_2_gate_accuracy: 95.2284
STEP-2	Epoch: 160/200	classification_loss: 0.1929	gate_loss: 0.1032	step2_classification_accuracy: 91.4066	step_2_gate_accuracy: 95.7711
STEP-2	Epoch: 180/200	classification_loss: 0.1817	gate_loss: 0.0957	step2_classification_accuracy: 91.7006	step_2_gate_accuracy: 95.7485
STEP-2	Epoch: 200/200	classification_loss: 0.1753	gate_loss: 0.0892	step2_classification_accuracy: 92.0850	step_2_gate_accuracy: 96.4722
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.0155	gate_accuracy: 80.6202
	Task-1	val_accuracy: 81.7073	gate_accuracy: 90.2439
	Task-2	val_accuracy: 79.4118	gate_accuracy: 80.8824
	Task-3	val_accuracy: 72.2892	gate_accuracy: 68.6747
	Task-4	val_accuracy: 93.7500	gate_accuracy: 95.0000
	Task-5	val_accuracy: 86.6667	gate_accuracy: 85.3333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 83.1721


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 2.3437	step1_train_accuracy: 48.4321
STEP-1	Epoch: 20/50	loss: 1.1603	step1_train_accuracy: 80.1394
STEP-1	Epoch: 30/50	loss: 0.6343	step1_train_accuracy: 90.5923
STEP-1	Epoch: 40/50	loss: 0.4105	step1_train_accuracy: 96.1672
STEP-1	Epoch: 50/50	loss: 0.2780	step1_train_accuracy: 97.2125
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.3579	gate_loss: 0.3723	step2_classification_accuracy: 86.5801	step_2_gate_accuracy: 88.0756
STEP-2	Epoch: 40/200	classification_loss: 0.2809	gate_loss: 0.2005	step2_classification_accuracy: 89.2168	step_2_gate_accuracy: 92.1881
STEP-2	Epoch: 60/200	classification_loss: 0.2421	gate_loss: 0.1511	step2_classification_accuracy: 90.2007	step_2_gate_accuracy: 93.7033
STEP-2	Epoch: 80/200	classification_loss: 0.2191	gate_loss: 0.1288	step2_classification_accuracy: 90.8894	step_2_gate_accuracy: 94.5887
STEP-2	Epoch: 100/200	classification_loss: 0.2110	gate_loss: 0.1182	step2_classification_accuracy: 91.3420	step_2_gate_accuracy: 95.2184
STEP-2	Epoch: 120/200	classification_loss: 0.1965	gate_loss: 0.1037	step2_classification_accuracy: 91.9323	step_2_gate_accuracy: 96.0055
STEP-2	Epoch: 140/200	classification_loss: 0.1884	gate_loss: 0.0980	step2_classification_accuracy: 92.0701	step_2_gate_accuracy: 95.6907
STEP-2	Epoch: 160/200	classification_loss: 0.1915	gate_loss: 0.0971	step2_classification_accuracy: 91.9520	step_2_gate_accuracy: 95.9662
STEP-2	Epoch: 180/200	classification_loss: 0.1890	gate_loss: 0.0945	step2_classification_accuracy: 91.6765	step_2_gate_accuracy: 95.9662
STEP-2	Epoch: 200/200	classification_loss: 0.1726	gate_loss: 0.0831	step2_classification_accuracy: 92.4242	step_2_gate_accuracy: 96.4187
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.7907	gate_accuracy: 79.0698
	Task-1	val_accuracy: 82.9268	gate_accuracy: 87.8049
	Task-2	val_accuracy: 67.6471	gate_accuracy: 70.5882
	Task-3	val_accuracy: 74.6988	gate_accuracy: 79.5181
	Task-4	val_accuracy: 90.0000	gate_accuracy: 83.7500
	Task-5	val_accuracy: 86.6667	gate_accuracy: 88.0000
	Task-6	val_accuracy: 84.7222	gate_accuracy: 86.1111
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.0034


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 2.0376	step1_train_accuracy: 59.0778
STEP-1	Epoch: 20/50	loss: 0.7701	step1_train_accuracy: 86.1671
STEP-1	Epoch: 30/50	loss: 0.3692	step1_train_accuracy: 95.3890
STEP-1	Epoch: 40/50	loss: 0.2311	step1_train_accuracy: 97.6945
STEP-1	Epoch: 50/50	loss: 0.1665	step1_train_accuracy: 98.2709
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.3633	gate_loss: 0.3810	step2_classification_accuracy: 86.9732	step_2_gate_accuracy: 87.4608
STEP-2	Epoch: 40/200	classification_loss: 0.2676	gate_loss: 0.1983	step2_classification_accuracy: 89.9512	step_2_gate_accuracy: 92.6506
STEP-2	Epoch: 60/200	classification_loss: 0.2292	gate_loss: 0.1468	step2_classification_accuracy: 90.8220	step_2_gate_accuracy: 94.3051
STEP-2	Epoch: 80/200	classification_loss: 0.5490	gate_loss: 0.2356	step2_classification_accuracy: 89.7423	step_2_gate_accuracy: 92.7551
STEP-2	Epoch: 100/200	classification_loss: 0.1901	gate_loss: 0.1080	step2_classification_accuracy: 91.9714	step_2_gate_accuracy: 95.8551
STEP-2	Epoch: 120/200	classification_loss: 0.1761	gate_loss: 0.0981	step2_classification_accuracy: 92.2849	step_2_gate_accuracy: 95.9944
STEP-2	Epoch: 140/200	classification_loss: 0.1657	gate_loss: 0.0886	step2_classification_accuracy: 92.7203	step_2_gate_accuracy: 96.4298
STEP-2	Epoch: 160/200	classification_loss: 0.1615	gate_loss: 0.0840	step2_classification_accuracy: 92.7377	step_2_gate_accuracy: 96.3427
STEP-2	Epoch: 180/200	classification_loss: 0.1530	gate_loss: 0.0787	step2_classification_accuracy: 93.1557	step_2_gate_accuracy: 96.7607
STEP-2	Epoch: 200/200	classification_loss: 0.1627	gate_loss: 0.0833	step2_classification_accuracy: 92.9467	step_2_gate_accuracy: 96.4298
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.6899	gate_accuracy: 75.1938
	Task-1	val_accuracy: 78.0488	gate_accuracy: 79.2683
	Task-2	val_accuracy: 67.6471	gate_accuracy: 75.0000
	Task-3	val_accuracy: 78.3133	gate_accuracy: 77.1084
	Task-4	val_accuracy: 87.5000	gate_accuracy: 90.0000
	Task-5	val_accuracy: 82.6667	gate_accuracy: 85.3333
	Task-6	val_accuracy: 88.8889	gate_accuracy: 90.2778
	Task-7	val_accuracy: 89.6552	gate_accuracy: 89.6552
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.2485


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.4475	step1_train_accuracy: 56.0284
STEP-1	Epoch: 20/50	loss: 1.0981	step1_train_accuracy: 78.3688
STEP-1	Epoch: 30/50	loss: 0.5759	step1_train_accuracy: 94.3262
STEP-1	Epoch: 40/50	loss: 0.3820	step1_train_accuracy: 97.8723
STEP-1	Epoch: 50/50	loss: 0.2554	step1_train_accuracy: 98.5816
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.3904	gate_loss: 0.4387	step2_classification_accuracy: 86.6312	step_2_gate_accuracy: 86.9139
STEP-2	Epoch: 40/200	classification_loss: 0.2755	gate_loss: 0.2143	step2_classification_accuracy: 89.9401	step_2_gate_accuracy: 93.0828
STEP-2	Epoch: 60/200	classification_loss: 0.2332	gate_loss: 0.1584	step2_classification_accuracy: 91.2205	step_2_gate_accuracy: 94.1636
STEP-2	Epoch: 80/200	classification_loss: 0.2057	gate_loss: 0.1294	step2_classification_accuracy: 91.7692	step_2_gate_accuracy: 95.1114
STEP-2	Epoch: 100/200	classification_loss: 0.1970	gate_loss: 0.1163	step2_classification_accuracy: 91.9687	step_2_gate_accuracy: 95.1945
STEP-2	Epoch: 120/200	classification_loss: 0.1775	gate_loss: 0.1018	step2_classification_accuracy: 92.4177	step_2_gate_accuracy: 95.7765
STEP-2	Epoch: 140/200	classification_loss: 0.1707	gate_loss: 0.0941	step2_classification_accuracy: 92.6172	step_2_gate_accuracy: 96.2088
STEP-2	Epoch: 160/200	classification_loss: 0.1644	gate_loss: 0.0880	step2_classification_accuracy: 93.0828	step_2_gate_accuracy: 96.5248
STEP-2	Epoch: 180/200	classification_loss: 0.1573	gate_loss: 0.0828	step2_classification_accuracy: 93.1161	step_2_gate_accuracy: 96.7077
STEP-2	Epoch: 200/200	classification_loss: 0.1571	gate_loss: 0.0789	step2_classification_accuracy: 93.3655	step_2_gate_accuracy: 96.8573
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.4651	gate_accuracy: 79.8450
	Task-1	val_accuracy: 82.9268	gate_accuracy: 84.1463
	Task-2	val_accuracy: 67.6471	gate_accuracy: 66.1765
	Task-3	val_accuracy: 77.1084	gate_accuracy: 75.9036
	Task-4	val_accuracy: 87.5000	gate_accuracy: 85.0000
	Task-5	val_accuracy: 82.6667	gate_accuracy: 82.6667
	Task-6	val_accuracy: 87.5000	gate_accuracy: 88.8889
	Task-7	val_accuracy: 86.2069	gate_accuracy: 83.9080
	Task-8	val_accuracy: 85.7143	gate_accuracy: 82.8571
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.0992


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 1.8123	step1_train_accuracy: 52.7950
STEP-1	Epoch: 20/50	loss: 0.8410	step1_train_accuracy: 83.2298
STEP-1	Epoch: 30/50	loss: 0.4900	step1_train_accuracy: 93.4783
STEP-1	Epoch: 40/50	loss: 0.3487	step1_train_accuracy: 95.3416
STEP-1	Epoch: 50/50	loss: 0.2362	step1_train_accuracy: 95.9627
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.3855	gate_loss: 0.4127	step2_classification_accuracy: 86.6893	step_2_gate_accuracy: 87.1566
STEP-2	Epoch: 40/200	classification_loss: 0.2802	gate_loss: 0.2156	step2_classification_accuracy: 89.5639	step_2_gate_accuracy: 92.3676
STEP-2	Epoch: 60/200	classification_loss: 0.2311	gate_loss: 0.1582	step2_classification_accuracy: 90.7675	step_2_gate_accuracy: 93.7695
STEP-2	Epoch: 80/200	classification_loss: 0.2166	gate_loss: 0.1328	step2_classification_accuracy: 91.2914	step_2_gate_accuracy: 94.3217
STEP-2	Epoch: 100/200	classification_loss: 0.2021	gate_loss: 0.1183	step2_classification_accuracy: 91.6596	step_2_gate_accuracy: 95.0722
STEP-2	Epoch: 120/200	classification_loss: 0.1957	gate_loss: 0.1110	step2_classification_accuracy: 92.2685	step_2_gate_accuracy: 95.4404
STEP-2	Epoch: 140/200	classification_loss: 0.1947	gate_loss: 0.1040	step2_classification_accuracy: 92.1835	step_2_gate_accuracy: 95.5112
STEP-2	Epoch: 160/200	classification_loss: 0.1676	gate_loss: 0.0880	step2_classification_accuracy: 92.8915	step_2_gate_accuracy: 96.4316
STEP-2	Epoch: 180/200	classification_loss: 0.1680	gate_loss: 0.0852	step2_classification_accuracy: 93.1039	step_2_gate_accuracy: 96.5449
STEP-2	Epoch: 200/200	classification_loss: 0.1633	gate_loss: 0.0824	step2_classification_accuracy: 93.1181	step_2_gate_accuracy: 96.4741
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.6899	gate_accuracy: 79.8450
	Task-1	val_accuracy: 76.8293	gate_accuracy: 84.1463
	Task-2	val_accuracy: 69.1176	gate_accuracy: 75.0000
	Task-3	val_accuracy: 78.3133	gate_accuracy: 77.1084
	Task-4	val_accuracy: 88.7500	gate_accuracy: 83.7500
	Task-5	val_accuracy: 85.3333	gate_accuracy: 85.3333
	Task-6	val_accuracy: 86.1111	gate_accuracy: 86.1111
	Task-7	val_accuracy: 85.0575	gate_accuracy: 79.3103
	Task-8	val_accuracy: 81.4286	gate_accuracy: 82.8571
	Task-9	val_accuracy: 85.1852	gate_accuracy: 90.1235
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.2249


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.1471	step1_train_accuracy: 51.9878
STEP-1	Epoch: 20/50	loss: 0.8160	step1_train_accuracy: 88.0734
STEP-1	Epoch: 30/50	loss: 0.4065	step1_train_accuracy: 96.9419
STEP-1	Epoch: 40/50	loss: 0.2796	step1_train_accuracy: 97.5535
STEP-1	Epoch: 50/50	loss: 0.1950	step1_train_accuracy: 98.1651
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.3985	gate_loss: 0.4522	step2_classification_accuracy: 86.4450	step_2_gate_accuracy: 86.7655
STEP-2	Epoch: 40/200	classification_loss: 0.2905	gate_loss: 0.2264	step2_classification_accuracy: 89.3830	step_2_gate_accuracy: 92.0139
STEP-2	Epoch: 60/200	classification_loss: 0.2457	gate_loss: 0.1703	step2_classification_accuracy: 90.6918	step_2_gate_accuracy: 93.3894
STEP-2	Epoch: 80/200	classification_loss: 0.2225	gate_loss: 0.1429	step2_classification_accuracy: 91.5198	step_2_gate_accuracy: 94.2842
STEP-2	Epoch: 100/200	classification_loss: 0.2055	gate_loss: 0.1274	step2_classification_accuracy: 91.8269	step_2_gate_accuracy: 94.9252
STEP-2	Epoch: 120/200	classification_loss: 0.2002	gate_loss: 0.1192	step2_classification_accuracy: 91.9605	step_2_gate_accuracy: 95.1656
STEP-2	Epoch: 140/200	classification_loss: 0.1812	gate_loss: 0.1033	step2_classification_accuracy: 92.5748	step_2_gate_accuracy: 95.7399
STEP-2	Epoch: 160/200	classification_loss: 0.1679	gate_loss: 0.0944	step2_classification_accuracy: 93.1624	step_2_gate_accuracy: 96.2740
STEP-2	Epoch: 180/200	classification_loss: 0.1692	gate_loss: 0.0929	step2_classification_accuracy: 92.8419	step_2_gate_accuracy: 96.1405
STEP-2	Epoch: 200/200	classification_loss: 0.1610	gate_loss: 0.0875	step2_classification_accuracy: 93.2826	step_2_gate_accuracy: 96.3542
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.9147	gate_accuracy: 76.7442
	Task-1	val_accuracy: 79.2683	gate_accuracy: 85.3659
	Task-2	val_accuracy: 63.2353	gate_accuracy: 61.7647
	Task-3	val_accuracy: 77.1084	gate_accuracy: 78.3133
	Task-4	val_accuracy: 87.5000	gate_accuracy: 87.5000
	Task-5	val_accuracy: 85.3333	gate_accuracy: 86.6667
	Task-6	val_accuracy: 81.9444	gate_accuracy: 84.7222
	Task-7	val_accuracy: 85.0575	gate_accuracy: 87.3563
	Task-8	val_accuracy: 82.8571	gate_accuracy: 81.4286
	Task-9	val_accuracy: 86.4198	gate_accuracy: 88.8889
	Task-10	val_accuracy: 85.3659	gate_accuracy: 81.7073
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.8482


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 1.9220	step1_train_accuracy: 66.3743
STEP-1	Epoch: 20/50	loss: 0.7546	step1_train_accuracy: 90.3509
STEP-1	Epoch: 30/50	loss: 0.3878	step1_train_accuracy: 92.9825
STEP-1	Epoch: 40/50	loss: 0.2452	step1_train_accuracy: 93.2749
STEP-1	Epoch: 50/50	loss: 0.1800	step1_train_accuracy: 95.6140
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.4240	gate_loss: 0.5214	step2_classification_accuracy: 85.6430	step_2_gate_accuracy: 84.8032
STEP-2	Epoch: 40/200	classification_loss: 0.3072	gate_loss: 0.2454	step2_classification_accuracy: 89.3176	step_2_gate_accuracy: 91.8898
STEP-2	Epoch: 60/200	classification_loss: 0.2671	gate_loss: 0.1808	step2_classification_accuracy: 90.2756	step_2_gate_accuracy: 93.4252
STEP-2	Epoch: 80/200	classification_loss: 0.2320	gate_loss: 0.1467	step2_classification_accuracy: 91.2730	step_2_gate_accuracy: 94.2913
STEP-2	Epoch: 100/200	classification_loss: 0.2104	gate_loss: 0.1241	step2_classification_accuracy: 91.7060	step_2_gate_accuracy: 95.3937
STEP-2	Epoch: 120/200	classification_loss: 0.1948	gate_loss: 0.1113	step2_classification_accuracy: 92.3491	step_2_gate_accuracy: 95.7612
STEP-2	Epoch: 140/200	classification_loss: 0.1847	gate_loss: 0.1044	step2_classification_accuracy: 92.2703	step_2_gate_accuracy: 96.1286
STEP-2	Epoch: 160/200	classification_loss: 0.1766	gate_loss: 0.0956	step2_classification_accuracy: 92.7559	step_2_gate_accuracy: 96.2861
STEP-2	Epoch: 180/200	classification_loss: 0.1746	gate_loss: 0.0929	step2_classification_accuracy: 92.7297	step_2_gate_accuracy: 96.5617
STEP-2	Epoch: 200/200	classification_loss: 0.1648	gate_loss: 0.0845	step2_classification_accuracy: 92.8478	step_2_gate_accuracy: 96.6798
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.6899	gate_accuracy: 79.8450
	Task-1	val_accuracy: 79.2683	gate_accuracy: 85.3659
	Task-2	val_accuracy: 64.7059	gate_accuracy: 66.1765
	Task-3	val_accuracy: 77.1084	gate_accuracy: 74.6988
	Task-4	val_accuracy: 87.5000	gate_accuracy: 86.2500
	Task-5	val_accuracy: 81.3333	gate_accuracy: 86.6667
	Task-6	val_accuracy: 84.7222	gate_accuracy: 84.7222
	Task-7	val_accuracy: 81.6092	gate_accuracy: 77.0115
	Task-8	val_accuracy: 85.7143	gate_accuracy: 82.8571
	Task-9	val_accuracy: 87.6543	gate_accuracy: 92.5926
	Task-10	val_accuracy: 87.8049	gate_accuracy: 86.5854
	Task-11	val_accuracy: 80.2326	gate_accuracy: 77.9070
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.7085


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.3451	step1_train_accuracy: 46.4174
STEP-1	Epoch: 20/50	loss: 0.9390	step1_train_accuracy: 82.5545
STEP-1	Epoch: 30/50	loss: 0.5146	step1_train_accuracy: 93.1464
STEP-1	Epoch: 40/50	loss: 0.3350	step1_train_accuracy: 95.9502
STEP-1	Epoch: 50/50	loss: 0.2705	step1_train_accuracy: 97.1963
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.4796	gate_loss: 0.6044	step2_classification_accuracy: 84.0297	step_2_gate_accuracy: 81.9028
STEP-2	Epoch: 40/200	classification_loss: 0.3561	gate_loss: 0.2907	step2_classification_accuracy: 87.2640	step_2_gate_accuracy: 89.7055
STEP-2	Epoch: 60/200	classification_loss: 0.2943	gate_loss: 0.2119	step2_classification_accuracy: 89.3154	step_2_gate_accuracy: 92.1973
STEP-2	Epoch: 80/200	classification_loss: 0.2736	gate_loss: 0.1817	step2_classification_accuracy: 90.1334	step_2_gate_accuracy: 92.8895
STEP-2	Epoch: 100/200	classification_loss: 0.2459	gate_loss: 0.1569	step2_classification_accuracy: 90.7501	step_2_gate_accuracy: 93.8963
STEP-2	Epoch: 120/200	classification_loss: 0.2269	gate_loss: 0.1370	step2_classification_accuracy: 91.4800	step_2_gate_accuracy: 94.6011
STEP-2	Epoch: 140/200	classification_loss: 0.2139	gate_loss: 0.1252	step2_classification_accuracy: 91.5807	step_2_gate_accuracy: 94.9912
STEP-2	Epoch: 160/200	classification_loss: 0.1994	gate_loss: 0.1142	step2_classification_accuracy: 92.1722	step_2_gate_accuracy: 95.3939
STEP-2	Epoch: 180/200	classification_loss: 0.1917	gate_loss: 0.1065	step2_classification_accuracy: 92.4364	step_2_gate_accuracy: 95.5449
STEP-2	Epoch: 200/200	classification_loss: 0.1827	gate_loss: 0.0987	step2_classification_accuracy: 92.6378	step_2_gate_accuracy: 96.2623
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.1395	gate_accuracy: 82.1705
	Task-1	val_accuracy: 71.9512	gate_accuracy: 79.2683
	Task-2	val_accuracy: 55.8824	gate_accuracy: 57.3529
	Task-3	val_accuracy: 79.5181	gate_accuracy: 79.5181
	Task-4	val_accuracy: 80.0000	gate_accuracy: 78.7500
	Task-5	val_accuracy: 84.0000	gate_accuracy: 85.3333
	Task-6	val_accuracy: 83.3333	gate_accuracy: 84.7222
	Task-7	val_accuracy: 82.7586	gate_accuracy: 86.2069
	Task-8	val_accuracy: 88.5714	gate_accuracy: 88.5714
	Task-9	val_accuracy: 83.9506	gate_accuracy: 82.7160
	Task-10	val_accuracy: 81.7073	gate_accuracy: 80.4878
	Task-11	val_accuracy: 83.7209	gate_accuracy: 84.8837
	Task-12	val_accuracy: 73.7500	gate_accuracy: 71.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.3721


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 2.1281	step1_train_accuracy: 53.8462
STEP-1	Epoch: 20/50	loss: 0.8999	step1_train_accuracy: 79.8817
STEP-1	Epoch: 30/50	loss: 0.4933	step1_train_accuracy: 95.5621
STEP-1	Epoch: 40/50	loss: 0.3307	step1_train_accuracy: 97.3373
STEP-1	Epoch: 50/50	loss: 0.2484	step1_train_accuracy: 97.9290
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.4914	gate_loss: 0.5385	step2_classification_accuracy: 83.5353	step_2_gate_accuracy: 83.3652
STEP-2	Epoch: 40/200	classification_loss: 0.3702	gate_loss: 0.2921	step2_classification_accuracy: 87.2662	step_2_gate_accuracy: 89.7428
STEP-2	Epoch: 60/200	classification_loss: 0.3088	gate_loss: 0.2208	step2_classification_accuracy: 88.8287	step_2_gate_accuracy: 91.7092
STEP-2	Epoch: 80/200	classification_loss: 0.2761	gate_loss: 0.1853	step2_classification_accuracy: 89.9235	step_2_gate_accuracy: 92.7083
STEP-2	Epoch: 100/200	classification_loss: 0.2536	gate_loss: 0.1643	step2_classification_accuracy: 90.6463	step_2_gate_accuracy: 93.4843
STEP-2	Epoch: 120/200	classification_loss: 0.2408	gate_loss: 0.1499	step2_classification_accuracy: 90.9120	step_2_gate_accuracy: 94.0901
STEP-2	Epoch: 140/200	classification_loss: 0.2258	gate_loss: 0.1370	step2_classification_accuracy: 91.4541	step_2_gate_accuracy: 94.5685
STEP-2	Epoch: 160/200	classification_loss: 0.2115	gate_loss: 0.1268	step2_classification_accuracy: 91.9218	step_2_gate_accuracy: 94.8342
STEP-2	Epoch: 180/200	classification_loss: 0.2037	gate_loss: 0.1185	step2_classification_accuracy: 92.1769	step_2_gate_accuracy: 95.3656
STEP-2	Epoch: 200/200	classification_loss: 0.2043	gate_loss: 0.1179	step2_classification_accuracy: 92.1237	step_2_gate_accuracy: 95.6207
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.3643	gate_accuracy: 73.6434
	Task-1	val_accuracy: 76.8293	gate_accuracy: 84.1463
	Task-2	val_accuracy: 58.8235	gate_accuracy: 64.7059
	Task-3	val_accuracy: 72.2892	gate_accuracy: 72.2892
	Task-4	val_accuracy: 77.5000	gate_accuracy: 80.0000
	Task-5	val_accuracy: 82.6667	gate_accuracy: 84.0000
	Task-6	val_accuracy: 81.9444	gate_accuracy: 81.9444
	Task-7	val_accuracy: 80.4598	gate_accuracy: 79.3103
	Task-8	val_accuracy: 84.2857	gate_accuracy: 85.7143
	Task-9	val_accuracy: 82.7160	gate_accuracy: 82.7160
	Task-10	val_accuracy: 81.7073	gate_accuracy: 84.1463
	Task-11	val_accuracy: 79.0698	gate_accuracy: 73.2558
	Task-12	val_accuracy: 71.2500	gate_accuracy: 67.5000
	Task-13	val_accuracy: 83.3333	gate_accuracy: 73.8095
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 77.4806


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.8142	step1_train_accuracy: 52.7157
STEP-1	Epoch: 20/50	loss: 1.0343	step1_train_accuracy: 82.7476
STEP-1	Epoch: 30/50	loss: 0.4540	step1_train_accuracy: 95.5272
STEP-1	Epoch: 40/50	loss: 0.2658	step1_train_accuracy: 97.4441
STEP-1	Epoch: 50/50	loss: 0.1869	step1_train_accuracy: 98.4026
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.5391	gate_loss: 0.5973	step2_classification_accuracy: 82.5149	step_2_gate_accuracy: 81.3643
STEP-2	Epoch: 40/200	classification_loss: 0.3953	gate_loss: 0.3222	step2_classification_accuracy: 86.7680	step_2_gate_accuracy: 88.8124
STEP-2	Epoch: 60/200	classification_loss: 0.3330	gate_loss: 0.2445	step2_classification_accuracy: 88.5453	step_2_gate_accuracy: 91.0828
STEP-2	Epoch: 80/200	classification_loss: 0.2966	gate_loss: 0.2034	step2_classification_accuracy: 89.4185	step_2_gate_accuracy: 92.3053
STEP-2	Epoch: 100/200	classification_loss: 0.2735	gate_loss: 0.1827	step2_classification_accuracy: 90.0760	step_2_gate_accuracy: 92.8703
STEP-2	Epoch: 120/200	classification_loss: 0.2548	gate_loss: 0.1656	step2_classification_accuracy: 90.6924	step_2_gate_accuracy: 93.5689
STEP-2	Epoch: 140/200	classification_loss: 0.2408	gate_loss: 0.1538	step2_classification_accuracy: 90.6616	step_2_gate_accuracy: 93.8258
STEP-2	Epoch: 160/200	classification_loss: 0.2365	gate_loss: 0.1472	step2_classification_accuracy: 91.0212	step_2_gate_accuracy: 93.9182
STEP-2	Epoch: 180/200	classification_loss: 0.2184	gate_loss: 0.1356	step2_classification_accuracy: 91.6273	step_2_gate_accuracy: 94.5757
STEP-2	Epoch: 200/200	classification_loss: 0.2089	gate_loss: 0.1261	step2_classification_accuracy: 91.9663	step_2_gate_accuracy: 94.9147
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.8140	gate_accuracy: 74.4186
	Task-1	val_accuracy: 70.7317	gate_accuracy: 79.2683
	Task-2	val_accuracy: 60.2941	gate_accuracy: 64.7059
	Task-3	val_accuracy: 73.4940	gate_accuracy: 77.1084
	Task-4	val_accuracy: 76.2500	gate_accuracy: 77.5000
	Task-5	val_accuracy: 85.3333	gate_accuracy: 84.0000
	Task-6	val_accuracy: 70.8333	gate_accuracy: 75.0000
	Task-7	val_accuracy: 77.0115	gate_accuracy: 68.9655
	Task-8	val_accuracy: 84.2857	gate_accuracy: 82.8571
	Task-9	val_accuracy: 80.2469	gate_accuracy: 83.9506
	Task-10	val_accuracy: 82.9268	gate_accuracy: 85.3659
	Task-11	val_accuracy: 75.5814	gate_accuracy: 76.7442
	Task-12	val_accuracy: 73.7500	gate_accuracy: 67.5000
	Task-13	val_accuracy: 70.2381	gate_accuracy: 70.2381
	Task-14	val_accuracy: 84.6154	gate_accuracy: 80.7692
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.4753


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.5470	step1_train_accuracy: 45.8462
STEP-1	Epoch: 20/50	loss: 0.8474	step1_train_accuracy: 88.3077
STEP-1	Epoch: 30/50	loss: 0.4440	step1_train_accuracy: 92.0000
STEP-1	Epoch: 40/50	loss: 0.3228	step1_train_accuracy: 96.3077
STEP-1	Epoch: 50/50	loss: 0.2250	step1_train_accuracy: 96.9231
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.5498	gate_loss: 0.6148	step2_classification_accuracy: 82.3643	step_2_gate_accuracy: 81.0411
STEP-2	Epoch: 40/200	classification_loss: 0.3937	gate_loss: 0.3266	step2_classification_accuracy: 86.8070	step_2_gate_accuracy: 88.5551
STEP-2	Epoch: 60/200	classification_loss: 0.3241	gate_loss: 0.2443	step2_classification_accuracy: 88.4199	step_2_gate_accuracy: 90.9117
STEP-2	Epoch: 80/200	classification_loss: 0.2952	gate_loss: 0.2083	step2_classification_accuracy: 89.3568	step_2_gate_accuracy: 92.0224
STEP-2	Epoch: 100/200	classification_loss: 0.2696	gate_loss: 0.1833	step2_classification_accuracy: 90.3226	step_2_gate_accuracy: 92.8337
STEP-2	Epoch: 120/200	classification_loss: 0.2535	gate_loss: 0.1684	step2_classification_accuracy: 90.6896	step_2_gate_accuracy: 93.4421
STEP-2	Epoch: 140/200	classification_loss: 0.2373	gate_loss: 0.1558	step2_classification_accuracy: 91.1435	step_2_gate_accuracy: 93.9347
STEP-2	Epoch: 160/200	classification_loss: 0.2263	gate_loss: 0.1449	step2_classification_accuracy: 91.7520	step_2_gate_accuracy: 94.3693
STEP-2	Epoch: 180/200	classification_loss: 0.2144	gate_loss: 0.1338	step2_classification_accuracy: 91.9645	step_2_gate_accuracy: 94.6977
STEP-2	Epoch: 200/200	classification_loss: 0.2021	gate_loss: 0.1230	step2_classification_accuracy: 92.1093	step_2_gate_accuracy: 95.4414
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.0388	gate_accuracy: 75.1938
	Task-1	val_accuracy: 75.6098	gate_accuracy: 82.9268
	Task-2	val_accuracy: 60.2941	gate_accuracy: 64.7059
	Task-3	val_accuracy: 72.2892	gate_accuracy: 68.6747
	Task-4	val_accuracy: 80.0000	gate_accuracy: 81.2500
	Task-5	val_accuracy: 78.6667	gate_accuracy: 76.0000
	Task-6	val_accuracy: 70.8333	gate_accuracy: 70.8333
	Task-7	val_accuracy: 81.6092	gate_accuracy: 77.0115
	Task-8	val_accuracy: 78.5714	gate_accuracy: 80.0000
	Task-9	val_accuracy: 85.1852	gate_accuracy: 86.4198
	Task-10	val_accuracy: 81.7073	gate_accuracy: 78.0488
	Task-11	val_accuracy: 80.2326	gate_accuracy: 76.7442
	Task-12	val_accuracy: 63.7500	gate_accuracy: 63.7500
	Task-13	val_accuracy: 76.1905	gate_accuracy: 71.4286
	Task-14	val_accuracy: 80.7692	gate_accuracy: 80.7692
	Task-15	val_accuracy: 79.0123	gate_accuracy: 71.6049
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.4173


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.3860	step1_train_accuracy: 61.0778
STEP-1	Epoch: 20/50	loss: 0.9073	step1_train_accuracy: 86.5269
STEP-1	Epoch: 30/50	loss: 0.4932	step1_train_accuracy: 93.7126
STEP-1	Epoch: 40/50	loss: 0.3116	step1_train_accuracy: 95.8084
STEP-1	Epoch: 50/50	loss: 0.2259	step1_train_accuracy: 96.7066
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.5476	gate_loss: 0.5846	step2_classification_accuracy: 82.3711	step_2_gate_accuracy: 81.2412
STEP-2	Epoch: 40/200	classification_loss: 0.3938	gate_loss: 0.3190	step2_classification_accuracy: 86.9439	step_2_gate_accuracy: 88.4004
STEP-2	Epoch: 60/200	classification_loss: 0.3345	gate_loss: 0.2461	step2_classification_accuracy: 88.2857	step_2_gate_accuracy: 90.6691
STEP-2	Epoch: 80/200	classification_loss: 0.2841	gate_loss: 0.2013	step2_classification_accuracy: 89.7157	step_2_gate_accuracy: 91.9756
STEP-2	Epoch: 100/200	classification_loss: 0.2685	gate_loss: 0.1829	step2_classification_accuracy: 90.2631	step_2_gate_accuracy: 93.0350
STEP-2	Epoch: 120/200	classification_loss: 0.2500	gate_loss: 0.1655	step2_classification_accuracy: 90.6780	step_2_gate_accuracy: 93.5028
STEP-2	Epoch: 140/200	classification_loss: 0.2405	gate_loss: 0.1544	step2_classification_accuracy: 91.1194	step_2_gate_accuracy: 94.1119
STEP-2	Epoch: 160/200	classification_loss: 0.2220	gate_loss: 0.1409	step2_classification_accuracy: 91.6402	step_2_gate_accuracy: 94.4297
STEP-2	Epoch: 180/200	classification_loss: 0.2153	gate_loss: 0.1326	step2_classification_accuracy: 91.7726	step_2_gate_accuracy: 94.8093
STEP-2	Epoch: 200/200	classification_loss: 0.2075	gate_loss: 0.1274	step2_classification_accuracy: 92.1169	step_2_gate_accuracy: 94.9506
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.0388	gate_accuracy: 73.6434
	Task-1	val_accuracy: 75.6098	gate_accuracy: 80.4878
	Task-2	val_accuracy: 63.2353	gate_accuracy: 72.0588
	Task-3	val_accuracy: 67.4699	gate_accuracy: 69.8795
	Task-4	val_accuracy: 82.5000	gate_accuracy: 86.2500
	Task-5	val_accuracy: 84.0000	gate_accuracy: 86.6667
	Task-6	val_accuracy: 72.2222	gate_accuracy: 72.2222
	Task-7	val_accuracy: 80.4598	gate_accuracy: 80.4598
	Task-8	val_accuracy: 82.8571	gate_accuracy: 81.4286
	Task-9	val_accuracy: 77.7778	gate_accuracy: 79.0123
	Task-10	val_accuracy: 89.0244	gate_accuracy: 87.8049
	Task-11	val_accuracy: 74.4186	gate_accuracy: 75.5814
	Task-12	val_accuracy: 71.2500	gate_accuracy: 68.7500
	Task-13	val_accuracy: 72.6190	gate_accuracy: 71.4286
	Task-14	val_accuracy: 83.3333	gate_accuracy: 85.8974
	Task-15	val_accuracy: 76.5432	gate_accuracy: 75.3086
	Task-16	val_accuracy: 85.7143	gate_accuracy: 82.1429
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.0314


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 3.1104	step1_train_accuracy: 48.0144
STEP-1	Epoch: 20/50	loss: 1.1881	step1_train_accuracy: 71.1191
STEP-1	Epoch: 30/50	loss: 0.6359	step1_train_accuracy: 91.6968
STEP-1	Epoch: 40/50	loss: 0.4302	step1_train_accuracy: 94.9458
STEP-1	Epoch: 50/50	loss: 0.3153	step1_train_accuracy: 96.0289
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.5429	gate_loss: 0.6035	step2_classification_accuracy: 82.3443	step_2_gate_accuracy: 80.9643
STEP-2	Epoch: 40/200	classification_loss: 0.3995	gate_loss: 0.3216	step2_classification_accuracy: 86.5620	step_2_gate_accuracy: 88.5630
STEP-2	Epoch: 60/200	classification_loss: 0.3319	gate_loss: 0.2433	step2_classification_accuracy: 88.2180	step_2_gate_accuracy: 90.9091
STEP-2	Epoch: 80/200	classification_loss: 0.2989	gate_loss: 0.2061	step2_classification_accuracy: 89.2962	step_2_gate_accuracy: 92.0649
STEP-2	Epoch: 100/200	classification_loss: 0.2791	gate_loss: 0.1852	step2_classification_accuracy: 89.9603	step_2_gate_accuracy: 92.9188
STEP-2	Epoch: 120/200	classification_loss: 0.2601	gate_loss: 0.1695	step2_classification_accuracy: 90.7883	step_2_gate_accuracy: 93.6433
STEP-2	Epoch: 140/200	classification_loss: 0.2422	gate_loss: 0.1547	step2_classification_accuracy: 91.1247	step_2_gate_accuracy: 94.0745
STEP-2	Epoch: 160/200	classification_loss: 0.2288	gate_loss: 0.1433	step2_classification_accuracy: 91.3576	step_2_gate_accuracy: 94.3419
STEP-2	Epoch: 180/200	classification_loss: 0.2223	gate_loss: 0.1369	step2_classification_accuracy: 91.7630	step_2_gate_accuracy: 94.7904
STEP-2	Epoch: 200/200	classification_loss: 0.2209	gate_loss: 0.1349	step2_classification_accuracy: 91.6164	step_2_gate_accuracy: 94.8249
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 54.2636	gate_accuracy: 75.1938
	Task-1	val_accuracy: 74.3902	gate_accuracy: 79.2683
	Task-2	val_accuracy: 54.4118	gate_accuracy: 55.8824
	Task-3	val_accuracy: 78.3133	gate_accuracy: 78.3133
	Task-4	val_accuracy: 80.0000	gate_accuracy: 81.2500
	Task-5	val_accuracy: 84.0000	gate_accuracy: 85.3333
	Task-6	val_accuracy: 75.0000	gate_accuracy: 76.3889
	Task-7	val_accuracy: 66.6667	gate_accuracy: 67.8161
	Task-8	val_accuracy: 81.4286	gate_accuracy: 81.4286
	Task-9	val_accuracy: 75.3086	gate_accuracy: 77.7778
	Task-10	val_accuracy: 79.2683	gate_accuracy: 79.2683
	Task-11	val_accuracy: 76.7442	gate_accuracy: 76.7442
	Task-12	val_accuracy: 72.5000	gate_accuracy: 72.5000
	Task-13	val_accuracy: 78.5714	gate_accuracy: 73.8095
	Task-14	val_accuracy: 80.7692	gate_accuracy: 80.7692
	Task-15	val_accuracy: 77.7778	gate_accuracy: 75.3086
	Task-16	val_accuracy: 79.7619	gate_accuracy: 79.7619
	Task-17	val_accuracy: 75.3623	gate_accuracy: 81.1594
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.5466


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 2.3871	step1_train_accuracy: 51.2821
STEP-1	Epoch: 20/50	loss: 0.9847	step1_train_accuracy: 84.2949
STEP-1	Epoch: 30/50	loss: 0.5060	step1_train_accuracy: 95.1923
STEP-1	Epoch: 40/50	loss: 0.3105	step1_train_accuracy: 97.4359
STEP-1	Epoch: 50/50	loss: 0.2135	step1_train_accuracy: 98.3974
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.5610	gate_loss: 0.6197	step2_classification_accuracy: 81.9879	step_2_gate_accuracy: 80.7107
STEP-2	Epoch: 40/200	classification_loss: 0.4076	gate_loss: 0.3301	step2_classification_accuracy: 86.1880	step_2_gate_accuracy: 88.4641
STEP-2	Epoch: 60/200	classification_loss: 0.3387	gate_loss: 0.2452	step2_classification_accuracy: 88.0219	step_2_gate_accuracy: 91.1659
STEP-2	Epoch: 80/200	classification_loss: 0.3043	gate_loss: 0.2108	step2_classification_accuracy: 89.0454	step_2_gate_accuracy: 92.0501
STEP-2	Epoch: 100/200	classification_loss: 0.2751	gate_loss: 0.1825	step2_classification_accuracy: 89.8477	step_2_gate_accuracy: 92.9343
STEP-2	Epoch: 120/200	classification_loss: 0.2568	gate_loss: 0.1650	step2_classification_accuracy: 90.4045	step_2_gate_accuracy: 93.8513
STEP-2	Epoch: 140/200	classification_loss: 0.2470	gate_loss: 0.1564	step2_classification_accuracy: 90.7729	step_2_gate_accuracy: 93.8841
STEP-2	Epoch: 160/200	classification_loss: 0.2317	gate_loss: 0.1444	step2_classification_accuracy: 91.0758	step_2_gate_accuracy: 94.2116
STEP-2	Epoch: 180/200	classification_loss: 0.2258	gate_loss: 0.1376	step2_classification_accuracy: 91.3378	step_2_gate_accuracy: 94.7110
STEP-2	Epoch: 200/200	classification_loss: 0.2196	gate_loss: 0.1330	step2_classification_accuracy: 91.6489	step_2_gate_accuracy: 94.8256
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.5891	gate_accuracy: 79.0698
	Task-1	val_accuracy: 73.1707	gate_accuracy: 80.4878
	Task-2	val_accuracy: 47.0588	gate_accuracy: 58.8235
	Task-3	val_accuracy: 75.9036	gate_accuracy: 72.2892
	Task-4	val_accuracy: 81.2500	gate_accuracy: 76.2500
	Task-5	val_accuracy: 81.3333	gate_accuracy: 82.6667
	Task-6	val_accuracy: 70.8333	gate_accuracy: 70.8333
	Task-7	val_accuracy: 71.2644	gate_accuracy: 67.8161
	Task-8	val_accuracy: 85.7143	gate_accuracy: 82.8571
	Task-9	val_accuracy: 80.2469	gate_accuracy: 80.2469
	Task-10	val_accuracy: 76.8293	gate_accuracy: 80.4878
	Task-11	val_accuracy: 79.0698	gate_accuracy: 76.7442
	Task-12	val_accuracy: 68.7500	gate_accuracy: 68.7500
	Task-13	val_accuracy: 67.8571	gate_accuracy: 66.6667
	Task-14	val_accuracy: 78.2051	gate_accuracy: 75.6410
	Task-15	val_accuracy: 77.7778	gate_accuracy: 72.8395
	Task-16	val_accuracy: 82.1429	gate_accuracy: 80.9524
	Task-17	val_accuracy: 78.2609	gate_accuracy: 82.6087
	Task-18	val_accuracy: 70.5128	gate_accuracy: 71.7949
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.2744


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.2351	step1_train_accuracy: 65.5738
STEP-1	Epoch: 20/50	loss: 0.8269	step1_train_accuracy: 84.1530
STEP-1	Epoch: 30/50	loss: 0.4714	step1_train_accuracy: 92.0765
STEP-1	Epoch: 40/50	loss: 0.2903	step1_train_accuracy: 94.5355
STEP-1	Epoch: 50/50	loss: 0.2111	step1_train_accuracy: 96.9945
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.5802	gate_loss: 0.6177	step2_classification_accuracy: 81.5766	step_2_gate_accuracy: 80.9325
STEP-2	Epoch: 40/200	classification_loss: 0.4314	gate_loss: 0.3427	step2_classification_accuracy: 85.7634	step_2_gate_accuracy: 88.0105
STEP-2	Epoch: 60/200	classification_loss: 0.3598	gate_loss: 0.2626	step2_classification_accuracy: 87.5787	step_2_gate_accuracy: 90.2211
STEP-2	Epoch: 80/200	classification_loss: 0.3171	gate_loss: 0.2182	step2_classification_accuracy: 89.0792	step_2_gate_accuracy: 91.6191
STEP-2	Epoch: 100/200	classification_loss: 0.2883	gate_loss: 0.1932	step2_classification_accuracy: 89.6867	step_2_gate_accuracy: 92.5926
STEP-2	Epoch: 120/200	classification_loss: 0.2670	gate_loss: 0.1737	step2_classification_accuracy: 90.3382	step_2_gate_accuracy: 93.3538
STEP-2	Epoch: 140/200	classification_loss: 0.2486	gate_loss: 0.1552	step2_classification_accuracy: 91.0189	step_2_gate_accuracy: 94.0858
STEP-2	Epoch: 160/200	classification_loss: 0.2533	gate_loss: 0.1618	step2_classification_accuracy: 91.0994	step_2_gate_accuracy: 94.0711
STEP-2	Epoch: 180/200	classification_loss: 0.2440	gate_loss: 0.1572	step2_classification_accuracy: 91.1433	step_2_gate_accuracy: 94.2322
STEP-2	Epoch: 200/200	classification_loss: 0.2242	gate_loss: 0.1353	step2_classification_accuracy: 91.7069	step_2_gate_accuracy: 95.0227
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.0388	gate_accuracy: 74.4186
	Task-1	val_accuracy: 80.4878	gate_accuracy: 85.3659
	Task-2	val_accuracy: 50.0000	gate_accuracy: 52.9412
	Task-3	val_accuracy: 68.6747	gate_accuracy: 69.8795
	Task-4	val_accuracy: 77.5000	gate_accuracy: 75.0000
	Task-5	val_accuracy: 78.6667	gate_accuracy: 84.0000
	Task-6	val_accuracy: 73.6111	gate_accuracy: 73.6111
	Task-7	val_accuracy: 72.4138	gate_accuracy: 67.8161
	Task-8	val_accuracy: 85.7143	gate_accuracy: 85.7143
	Task-9	val_accuracy: 82.7160	gate_accuracy: 85.1852
	Task-10	val_accuracy: 76.8293	gate_accuracy: 75.6098
	Task-11	val_accuracy: 80.2326	gate_accuracy: 81.3953
	Task-12	val_accuracy: 68.7500	gate_accuracy: 66.2500
	Task-13	val_accuracy: 75.0000	gate_accuracy: 73.8095
	Task-14	val_accuracy: 78.2051	gate_accuracy: 76.9231
	Task-15	val_accuracy: 71.6049	gate_accuracy: 66.6667
	Task-16	val_accuracy: 86.9048	gate_accuracy: 82.1429
	Task-17	val_accuracy: 76.8116	gate_accuracy: 79.7101
	Task-18	val_accuracy: 76.9231	gate_accuracy: 79.4872
	Task-19	val_accuracy: 73.9130	gate_accuracy: 75.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.5637


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.2597	step1_train_accuracy: 53.3528
STEP-1	Epoch: 20/50	loss: 0.9143	step1_train_accuracy: 81.9242
STEP-1	Epoch: 30/50	loss: 0.5965	step1_train_accuracy: 88.0466
STEP-1	Epoch: 40/50	loss: 0.3809	step1_train_accuracy: 92.7114
STEP-1	Epoch: 50/50	loss: 0.2857	step1_train_accuracy: 95.3353
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.6305	gate_loss: 0.7045	step2_classification_accuracy: 80.2215	step_2_gate_accuracy: 77.9768
STEP-2	Epoch: 40/200	classification_loss: 0.4668	gate_loss: 0.3855	step2_classification_accuracy: 84.8521	step_2_gate_accuracy: 86.4427
STEP-2	Epoch: 60/200	classification_loss: 0.3874	gate_loss: 0.2907	step2_classification_accuracy: 87.0819	step_2_gate_accuracy: 89.4158
STEP-2	Epoch: 80/200	classification_loss: 0.3429	gate_loss: 0.2409	step2_classification_accuracy: 88.4570	step_2_gate_accuracy: 91.0882
STEP-2	Epoch: 100/200	classification_loss: 0.3129	gate_loss: 0.2152	step2_classification_accuracy: 89.3786	step_2_gate_accuracy: 91.9058
STEP-2	Epoch: 120/200	classification_loss: 0.2893	gate_loss: 0.1929	step2_classification_accuracy: 89.7651	step_2_gate_accuracy: 92.6639
STEP-2	Epoch: 140/200	classification_loss: 0.2753	gate_loss: 0.1781	step2_classification_accuracy: 90.2483	step_2_gate_accuracy: 93.4220
STEP-2	Epoch: 160/200	classification_loss: 0.2623	gate_loss: 0.1679	step2_classification_accuracy: 90.6942	step_2_gate_accuracy: 93.6376
STEP-2	Epoch: 180/200	classification_loss: 0.2489	gate_loss: 0.1575	step2_classification_accuracy: 91.0882	step_2_gate_accuracy: 94.0612
STEP-2	Epoch: 200/200	classification_loss: 0.2370	gate_loss: 0.1475	step2_classification_accuracy: 91.2219	step_2_gate_accuracy: 94.3660
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.5891	gate_accuracy: 73.6434
	Task-1	val_accuracy: 71.9512	gate_accuracy: 80.4878
	Task-2	val_accuracy: 51.4706	gate_accuracy: 57.3529
	Task-3	val_accuracy: 75.9036	gate_accuracy: 73.4940
	Task-4	val_accuracy: 76.2500	gate_accuracy: 76.2500
	Task-5	val_accuracy: 78.6667	gate_accuracy: 82.6667
	Task-6	val_accuracy: 68.0556	gate_accuracy: 66.6667
	Task-7	val_accuracy: 79.3103	gate_accuracy: 73.5632
	Task-8	val_accuracy: 75.7143	gate_accuracy: 74.2857
	Task-9	val_accuracy: 76.5432	gate_accuracy: 77.7778
	Task-10	val_accuracy: 79.2683	gate_accuracy: 73.1707
	Task-11	val_accuracy: 83.7209	gate_accuracy: 82.5581
	Task-12	val_accuracy: 66.2500	gate_accuracy: 62.5000
	Task-13	val_accuracy: 73.8095	gate_accuracy: 75.0000
	Task-14	val_accuracy: 79.4872	gate_accuracy: 75.6410
	Task-15	val_accuracy: 72.8395	gate_accuracy: 72.8395
	Task-16	val_accuracy: 82.1429	gate_accuracy: 82.1429
	Task-17	val_accuracy: 78.2609	gate_accuracy: 82.6087
	Task-18	val_accuracy: 78.2051	gate_accuracy: 78.2051
	Task-19	val_accuracy: 70.6522	gate_accuracy: 73.9130
	Task-20	val_accuracy: 59.3023	gate_accuracy: 62.7907
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.2328


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.0871	step1_train_accuracy: 62.8415
STEP-1	Epoch: 20/50	loss: 0.7798	step1_train_accuracy: 80.8743
STEP-1	Epoch: 30/50	loss: 0.4300	step1_train_accuracy: 91.5301
STEP-1	Epoch: 40/50	loss: 0.2971	step1_train_accuracy: 94.8087
STEP-1	Epoch: 50/50	loss: 0.2332	step1_train_accuracy: 95.6284
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6592	gate_loss: 0.7245	step2_classification_accuracy: 79.7797	step_2_gate_accuracy: 77.4449
STEP-2	Epoch: 40/200	classification_loss: 0.4948	gate_loss: 0.4087	step2_classification_accuracy: 84.3465	step_2_gate_accuracy: 85.8076
STEP-2	Epoch: 60/200	classification_loss: 0.4132	gate_loss: 0.3137	step2_classification_accuracy: 86.5198	step_2_gate_accuracy: 88.6711
STEP-2	Epoch: 80/200	classification_loss: 0.3654	gate_loss: 0.2625	step2_classification_accuracy: 87.7827	step_2_gate_accuracy: 90.1395
STEP-2	Epoch: 100/200	classification_loss: 0.3360	gate_loss: 0.2308	step2_classification_accuracy: 88.6417	step_2_gate_accuracy: 91.4244
STEP-2	Epoch: 120/200	classification_loss: 0.3143	gate_loss: 0.2121	step2_classification_accuracy: 89.1924	step_2_gate_accuracy: 92.0558
STEP-2	Epoch: 140/200	classification_loss: 0.2938	gate_loss: 0.1939	step2_classification_accuracy: 89.8164	step_2_gate_accuracy: 92.7974
STEP-2	Epoch: 160/200	classification_loss: 0.2827	gate_loss: 0.1834	step2_classification_accuracy: 90.1468	step_2_gate_accuracy: 93.0176
STEP-2	Epoch: 180/200	classification_loss: 0.2656	gate_loss: 0.1688	step2_classification_accuracy: 90.4772	step_2_gate_accuracy: 93.7078
STEP-2	Epoch: 200/200	classification_loss: 0.2569	gate_loss: 0.1599	step2_classification_accuracy: 90.8957	step_2_gate_accuracy: 94.0162
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 54.2636	gate_accuracy: 75.9690
	Task-1	val_accuracy: 68.2927	gate_accuracy: 71.9512
	Task-2	val_accuracy: 57.3529	gate_accuracy: 60.2941
	Task-3	val_accuracy: 72.2892	gate_accuracy: 71.0843
	Task-4	val_accuracy: 80.0000	gate_accuracy: 77.5000
	Task-5	val_accuracy: 80.0000	gate_accuracy: 78.6667
	Task-6	val_accuracy: 70.8333	gate_accuracy: 68.0556
	Task-7	val_accuracy: 78.1609	gate_accuracy: 73.5632
	Task-8	val_accuracy: 80.0000	gate_accuracy: 78.5714
	Task-9	val_accuracy: 81.4815	gate_accuracy: 85.1852
	Task-10	val_accuracy: 82.9268	gate_accuracy: 81.7073
	Task-11	val_accuracy: 80.2326	gate_accuracy: 75.5814
	Task-12	val_accuracy: 66.2500	gate_accuracy: 61.2500
	Task-13	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-14	val_accuracy: 82.0513	gate_accuracy: 76.9231
	Task-15	val_accuracy: 64.1975	gate_accuracy: 64.1975
	Task-16	val_accuracy: 83.3333	gate_accuracy: 77.3810
	Task-17	val_accuracy: 76.8116	gate_accuracy: 79.7101
	Task-18	val_accuracy: 79.4872	gate_accuracy: 83.3333
	Task-19	val_accuracy: 67.3913	gate_accuracy: 70.6522
	Task-20	val_accuracy: 65.1163	gate_accuracy: 68.6047
	Task-21	val_accuracy: 64.1304	gate_accuracy: 65.2174
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.2820


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.2766	step1_train_accuracy: 49.5775
STEP-1	Epoch: 20/50	loss: 1.0362	step1_train_accuracy: 76.0563
STEP-1	Epoch: 30/50	loss: 0.6202	step1_train_accuracy: 91.8310
STEP-1	Epoch: 40/50	loss: 0.4100	step1_train_accuracy: 95.2113
STEP-1	Epoch: 50/50	loss: 0.2898	step1_train_accuracy: 96.9014
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 34, 1: 34, 2: 34, 3: 34, 4: 34, 5: 34, 6: 34, 7: 34, 8: 34, 9: 34, 10: 34, 11: 34, 12: 34, 13: 34, 14: 34, 15: 34, 16: 34, 17: 34, 18: 34, 19: 34, 20: 34, 21: 34, 22: 34, 23: 34, 24: 34, 25: 34, 26: 34, 27: 34, 28: 34, 29: 34, 30: 34, 31: 34, 32: 34, 33: 34, 34: 34, 35: 34, 36: 34, 37: 34, 38: 34, 39: 34, 40: 34, 41: 34, 42: 34, 43: 34, 44: 34, 45: 34, 46: 34, 47: 34, 48: 34, 49: 34, 50: 34, 51: 34, 52: 34, 53: 34, 54: 34, 55: 34, 56: 34, 57: 34, 58: 34, 59: 34, 60: 34, 61: 34, 62: 34, 63: 34, 64: 34, 65: 34, 66: 34, 67: 34, 68: 34, 69: 34, 70: 34, 71: 34, 72: 34, 73: 34, 74: 34, 75: 34, 76: 34, 77: 34, 78: 34, 79: 34, 80: 34, 81: 34, 82: 34, 83: 34, 84: 34, 85: 34, 86: 34, 87: 34, 88: 34, 89: 34, 90: 34, 91: 34, 92: 34, 93: 34, 94: 34, 95: 34, 96: 34, 97: 34, 98: 34, 99: 34, 100: 34, 101: 34, 102: 34, 103: 34, 104: 34, 105: 34, 106: 34, 107: 34, 108: 34, 109: 34, 110: 34, 111: 34, 112: 34, 113: 34, 114: 34, 115: 34, 116: 34, 117: 34, 118: 34, 119: 34, 120: 34, 121: 34, 122: 34, 123: 34, 124: 34, 125: 34, 126: 34, 127: 34, 128: 34, 129: 34, 130: 34, 131: 34, 132: 34, 133: 34, 134: 34, 135: 34, 136: 34, 137: 34, 138: 34, 139: 34, 140: 34, 141: 34, 142: 34, 143: 34, 144: 34, 145: 34, 146: 34, 147: 34, 148: 34, 149: 34, 150: 34, 151: 34, 152: 34, 153: 34, 154: 34, 155: 34, 156: 34, 157: 34, 158: 34, 159: 34, 160: 34, 161: 34, 162: 34, 163: 34, 164: 34, 165: 34, 166: 34, 167: 34, 168: 34, 169: 34, 170: 34, 171: 34, 172: 34, 173: 34, 174: 34, 175: 34, 176: 34, 177: 34, 178: 34, 179: 34, 180: 34, 181: 34, 182: 34, 183: 34, 184: 34, 185: 34, 186: 34, 187: 34, 188: 34, 189: 34, 190: 34, 191: 34, 192: 34, 193: 34, 194: 34, 195: 34, 196: 34, 197: 34, 198: 34, 199: 34, 200: 34, 201: 34, 202: 34, 203: 34, 204: 34, 205: 34, 206: 34, 207: 34, 208: 34, 209: 34, 210: 34, 211: 34, 212: 34, 213: 34, 214: 34, 215: 34, 216: 34, 217: 34, 218: 34, 219: 34, 220: 34, 221: 34, 222: 34, 223: 34, 224: 34, 225: 34, 226: 34, 227: 34, 228: 34, 229: 34, 230: 34, 231: 34, 232: 34, 233: 34, 234: 34, 235: 34, 236: 34, 237: 34, 238: 34, 239: 34, 240: 34, 241: 34, 242: 34, 243: 34, 244: 34, 245: 34, 246: 34, 247: 34, 248: 34, 249: 34, 250: 34, 251: 34, 252: 34, 253: 34, 254: 34, 255: 34, 256: 34, 257: 34, 258: 34, 259: 34, 260: 34, 261: 34, 262: 34, 263: 34, 264: 34, 265: 34, 266: 34, 267: 34, 268: 34, 269: 34, 270: 34, 271: 34, 272: 34, 273: 34, 274: 34, 275: 34, 276: 34, 277: 34, 278: 34, 279: 34, 280: 34, 281: 34, 282: 34, 283: 34, 284: 34, 285: 34, 286: 34, 287: 34, 288: 34, 289: 34, 290: 34, 291: 34, 292: 34, 293: 34, 294: 34, 295: 34, 296: 34, 297: 34, 298: 34, 299: 34, 300: 34, 301: 34, 302: 34, 303: 34, 304: 34, 305: 34, 306: 34, 307: 34, 308: 34, 309: 34, 310: 34, 311: 34, 312: 34, 313: 34, 314: 34, 315: 34, 316: 34, 317: 34, 318: 34, 319: 34, 320: 34, 321: 34, 322: 34, 323: 34, 324: 34, 325: 34, 326: 34, 327: 34, 328: 34, 329: 34, 330: 34, 331: 34, 332: 34, 333: 34, 334: 34, 335: 34, 336: 34, 337: 34, 338: 34, 339: 34, 340: 34, 341: 34, 342: 34, 343: 34, 344: 34, 345: 34, 346: 34, 347: 34, 348: 34, 349: 34, 350: 34, 351: 34, 352: 34, 353: 34, 354: 34, 355: 34, 356: 34, 357: 34, 358: 34, 359: 34, 360: 34, 361: 34, 362: 34, 363: 34, 364: 34, 365: 34, 366: 34, 367: 34, 368: 34, 369: 34, 370: 34, 371: 34, 372: 34, 373: 34, 374: 34, 375: 34, 376: 34, 377: 34, 378: 34, 379: 34, 380: 34, 381: 34, 382: 34, 383: 34, 384: 34, 385: 34, 386: 34, 387: 34, 388: 34, 389: 34, 390: 34, 391: 34, 392: 34, 393: 34, 394: 34, 395: 34, 396: 34, 397: 34, 398: 34, 399: 34, 400: 34, 401: 34, 402: 34, 403: 34, 404: 34, 405: 34, 406: 34, 407: 34, 408: 34, 409: 34, 410: 34, 411: 34, 412: 34, 413: 34, 414: 34, 415: 34, 416: 34, 417: 34, 418: 34, 419: 34, 420: 34, 421: 34, 422: 34, 423: 34, 424: 34, 425: 34, 426: 34, 427: 34, 428: 34, 429: 34, 430: 34, 431: 34, 432: 34, 433: 34, 434: 34, 435: 34, 436: 34, 437: 34, 438: 34, 439: 34, 440: 34, 441: 34, 442: 34, 443: 34, 444: 34, 445: 34, 446: 34, 447: 34, 448: 34, 449: 34, 450: 34, 451: 34, 452: 34, 453: 34, 454: 34, 455: 34, 456: 34, 457: 34, 458: 34, 459: 34, 460: 34, 461: 34, 462: 34, 463: 34, 464: 34, 465: 34, 466: 34, 467: 34, 468: 34, 469: 34, 470: 34, 471: 34, 472: 34, 473: 34})
STEP-2	Epoch: 20/200	classification_loss: 0.6337	gate_loss: 0.6524	step2_classification_accuracy: 80.0012	step_2_gate_accuracy: 79.0022
STEP-2	Epoch: 40/200	classification_loss: 0.4682	gate_loss: 0.3830	step2_classification_accuracy: 84.5309	step_2_gate_accuracy: 86.1814
STEP-2	Epoch: 60/200	classification_loss: 0.3973	gate_loss: 0.3006	step2_classification_accuracy: 86.3862	step_2_gate_accuracy: 88.9178
STEP-2	Epoch: 80/200	classification_loss: 0.3546	gate_loss: 0.2554	step2_classification_accuracy: 87.8506	step_2_gate_accuracy: 90.4257
STEP-2	Epoch: 100/200	classification_loss: 0.3315	gate_loss: 0.2303	step2_classification_accuracy: 88.5455	step_2_gate_accuracy: 91.3564
STEP-2	Epoch: 120/200	classification_loss: 0.3039	gate_loss: 0.2097	step2_classification_accuracy: 89.1288	step_2_gate_accuracy: 92.0700
STEP-2	Epoch: 140/200	classification_loss: 0.2867	gate_loss: 0.1934	step2_classification_accuracy: 89.7803	step_2_gate_accuracy: 92.5850
STEP-2	Epoch: 160/200	classification_loss: 0.2778	gate_loss: 0.1847	step2_classification_accuracy: 89.9603	step_2_gate_accuracy: 92.9139
STEP-2	Epoch: 180/200	classification_loss: 0.2635	gate_loss: 0.1715	step2_classification_accuracy: 90.5063	step_2_gate_accuracy: 93.4227
STEP-2	Epoch: 200/200	classification_loss: 0.2538	gate_loss: 0.1655	step2_classification_accuracy: 90.7856	step_2_gate_accuracy: 93.4971
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.9380	gate_accuracy: 66.6667
	Task-1	val_accuracy: 76.8293	gate_accuracy: 81.7073
	Task-2	val_accuracy: 54.4118	gate_accuracy: 63.2353
	Task-3	val_accuracy: 71.0843	gate_accuracy: 73.4940
	Task-4	val_accuracy: 75.0000	gate_accuracy: 78.7500
	Task-5	val_accuracy: 78.6667	gate_accuracy: 82.6667
	Task-6	val_accuracy: 68.0556	gate_accuracy: 68.0556
	Task-7	val_accuracy: 79.3103	gate_accuracy: 77.0115
	Task-8	val_accuracy: 77.1429	gate_accuracy: 75.7143
	Task-9	val_accuracy: 77.7778	gate_accuracy: 75.3086
	Task-10	val_accuracy: 76.8293	gate_accuracy: 73.1707
	Task-11	val_accuracy: 79.0698	gate_accuracy: 70.9302
	Task-12	val_accuracy: 68.7500	gate_accuracy: 67.5000
	Task-13	val_accuracy: 73.8095	gate_accuracy: 69.0476
	Task-14	val_accuracy: 76.9231	gate_accuracy: 71.7949
	Task-15	val_accuracy: 69.1358	gate_accuracy: 69.1358
	Task-16	val_accuracy: 78.5714	gate_accuracy: 78.5714
	Task-17	val_accuracy: 75.3623	gate_accuracy: 73.9130
	Task-18	val_accuracy: 74.3590	gate_accuracy: 74.3590
	Task-19	val_accuracy: 71.7391	gate_accuracy: 75.0000
	Task-20	val_accuracy: 63.9535	gate_accuracy: 67.4419
	Task-21	val_accuracy: 70.6522	gate_accuracy: 72.8261
	Task-22	val_accuracy: 83.1461	gate_accuracy: 80.8989
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.2704


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 2.7786	step1_train_accuracy: 48.6577
STEP-1	Epoch: 20/50	loss: 0.8643	step1_train_accuracy: 92.2819
STEP-1	Epoch: 30/50	loss: 0.4770	step1_train_accuracy: 94.2953
STEP-1	Epoch: 40/50	loss: 0.3233	step1_train_accuracy: 95.9732
STEP-1	Epoch: 50/50	loss: 0.2426	step1_train_accuracy: 95.9732
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 27, 1: 27, 2: 27, 3: 27, 4: 27, 5: 27, 6: 27, 7: 27, 8: 27, 9: 27, 10: 27, 11: 27, 12: 27, 13: 27, 14: 27, 15: 27, 16: 27, 17: 27, 18: 27, 19: 27, 20: 27, 21: 27, 22: 27, 23: 27, 24: 27, 25: 27, 26: 27, 27: 27, 28: 27, 29: 27, 30: 27, 31: 27, 32: 27, 33: 27, 34: 27, 35: 27, 36: 27, 37: 27, 38: 27, 39: 27, 40: 27, 41: 27, 42: 27, 43: 27, 44: 27, 45: 27, 46: 27, 47: 27, 48: 27, 49: 27, 50: 27, 51: 27, 52: 27, 53: 27, 54: 27, 55: 27, 56: 27, 57: 27, 58: 27, 59: 27, 60: 27, 61: 27, 62: 27, 63: 27, 64: 27, 65: 27, 66: 27, 67: 27, 68: 27, 69: 27, 70: 27, 71: 27, 72: 27, 73: 27, 74: 27, 75: 27, 76: 27, 77: 27, 78: 27, 79: 27, 80: 27, 81: 27, 82: 27, 83: 27, 84: 27, 85: 27, 86: 27, 87: 27, 88: 27, 89: 27, 90: 27, 91: 27, 92: 27, 93: 27, 94: 27, 95: 27, 96: 27, 97: 27, 98: 27, 99: 27, 100: 27, 101: 27, 102: 27, 103: 27, 104: 27, 105: 27, 106: 27, 107: 27, 108: 27, 109: 27, 110: 27, 111: 27, 112: 27, 113: 27, 114: 27, 115: 27, 116: 27, 117: 27, 118: 27, 119: 27, 120: 27, 121: 27, 122: 27, 123: 27, 124: 27, 125: 27, 126: 27, 127: 27, 128: 27, 129: 27, 130: 27, 131: 27, 132: 27, 133: 27, 134: 27, 135: 27, 136: 27, 137: 27, 138: 27, 139: 27, 140: 27, 141: 27, 142: 27, 143: 27, 144: 27, 145: 27, 146: 27, 147: 27, 148: 27, 149: 27, 150: 27, 151: 27, 152: 27, 153: 27, 154: 27, 155: 27, 156: 27, 157: 27, 158: 27, 159: 27, 160: 27, 161: 27, 162: 27, 163: 27, 164: 27, 165: 27, 166: 27, 167: 27, 168: 27, 169: 27, 170: 27, 171: 27, 172: 27, 173: 27, 174: 27, 175: 27, 176: 27, 177: 27, 178: 27, 179: 27, 180: 27, 181: 27, 182: 27, 183: 27, 184: 27, 185: 27, 186: 27, 187: 27, 188: 27, 189: 27, 190: 27, 191: 27, 192: 27, 193: 27, 194: 27, 195: 27, 196: 27, 197: 27, 198: 27, 199: 27, 200: 27, 201: 27, 202: 27, 203: 27, 204: 27, 205: 27, 206: 27, 207: 27, 208: 27, 209: 27, 210: 27, 211: 27, 212: 27, 213: 27, 214: 27, 215: 27, 216: 27, 217: 27, 218: 27, 219: 27, 220: 27, 221: 27, 222: 27, 223: 27, 224: 27, 225: 27, 226: 27, 227: 27, 228: 27, 229: 27, 230: 27, 231: 27, 232: 27, 233: 27, 234: 27, 235: 27, 236: 27, 237: 27, 238: 27, 239: 27, 240: 27, 241: 27, 242: 27, 243: 27, 244: 27, 245: 27, 246: 27, 247: 27, 248: 27, 249: 27, 250: 27, 251: 27, 252: 27, 253: 27, 254: 27, 255: 27, 256: 27, 257: 27, 258: 27, 259: 27, 260: 27, 261: 27, 262: 27, 263: 27, 264: 27, 265: 27, 266: 27, 267: 27, 268: 27, 269: 27, 270: 27, 271: 27, 272: 27, 273: 27, 274: 27, 275: 27, 276: 27, 277: 27, 278: 27, 279: 27, 280: 27, 281: 27, 282: 27, 283: 27, 284: 27, 285: 27, 286: 27, 287: 27, 288: 27, 289: 27, 290: 27, 291: 27, 292: 27, 293: 27, 294: 27, 295: 27, 296: 27, 297: 27, 298: 27, 299: 27, 300: 27, 301: 27, 302: 27, 303: 27, 304: 27, 305: 27, 306: 27, 307: 27, 308: 27, 309: 27, 310: 27, 311: 27, 312: 27, 313: 27, 314: 27, 315: 27, 316: 27, 317: 27, 318: 27, 319: 27, 320: 27, 321: 27, 322: 27, 323: 27, 324: 27, 325: 27, 326: 27, 327: 27, 328: 27, 329: 27, 330: 27, 331: 27, 332: 27, 333: 27, 334: 27, 335: 27, 336: 27, 337: 27, 338: 27, 339: 27, 340: 27, 341: 27, 342: 27, 343: 27, 344: 27, 345: 27, 346: 27, 347: 27, 348: 27, 349: 27, 350: 27, 351: 27, 352: 27, 353: 27, 354: 27, 355: 27, 356: 27, 357: 27, 358: 27, 359: 27, 360: 27, 361: 27, 362: 27, 363: 27, 364: 27, 365: 27, 366: 27, 367: 27, 368: 27, 369: 27, 370: 27, 371: 27, 372: 27, 373: 27, 374: 27, 375: 27, 376: 27, 377: 27, 378: 27, 379: 27, 380: 27, 381: 27, 382: 27, 383: 27, 384: 27, 385: 27, 386: 27, 387: 27, 388: 27, 389: 27, 390: 27, 391: 27, 392: 27, 393: 27, 394: 27, 395: 27, 396: 27, 397: 27, 398: 27, 399: 27, 400: 27, 401: 27, 402: 27, 403: 27, 404: 27, 405: 27, 406: 27, 407: 27, 408: 27, 409: 27, 410: 27, 411: 27, 412: 27, 413: 27, 414: 27, 415: 27, 416: 27, 417: 27, 418: 27, 419: 27, 420: 27, 421: 27, 422: 27, 423: 27, 424: 27, 425: 27, 426: 27, 427: 27, 428: 27, 429: 27, 430: 27, 431: 27, 432: 27, 433: 27, 434: 27, 435: 27, 436: 27, 437: 27, 438: 27, 439: 27, 440: 27, 441: 27, 442: 27, 443: 27, 444: 27, 445: 27, 446: 27, 447: 27, 448: 27, 449: 27, 450: 27, 451: 27, 452: 27, 453: 27, 454: 27, 455: 27, 456: 27, 457: 27, 458: 27, 459: 27, 460: 27, 461: 27, 462: 27, 463: 27, 464: 27, 465: 27, 466: 27, 467: 27, 468: 27, 469: 27, 470: 27, 471: 27, 472: 27, 473: 27, 474: 27, 475: 27, 476: 27, 477: 27, 478: 27, 479: 27, 480: 27, 481: 27, 482: 27, 483: 27, 484: 27, 485: 27, 486: 27, 487: 27, 488: 27, 489: 27, 490: 27, 491: 27, 492: 27, 493: 27})
STEP-2	Epoch: 20/200	classification_loss: 0.7231	gate_loss: 0.8421	step2_classification_accuracy: 77.9727	step_2_gate_accuracy: 74.1041
STEP-2	Epoch: 40/200	classification_loss: 0.5409	gate_loss: 0.4670	step2_classification_accuracy: 83.2809	step_2_gate_accuracy: 84.3830
STEP-2	Epoch: 60/200	classification_loss: 0.4470	gate_loss: 0.3504	step2_classification_accuracy: 85.5526	step_2_gate_accuracy: 87.5843
STEP-2	Epoch: 80/200	classification_loss: 0.3930	gate_loss: 0.2928	step2_classification_accuracy: 87.0295	step_2_gate_accuracy: 89.3837
STEP-2	Epoch: 100/200	classification_loss: 0.3560	gate_loss: 0.2549	step2_classification_accuracy: 88.0792	step_2_gate_accuracy: 90.5833
STEP-2	Epoch: 120/200	classification_loss: 0.3313	gate_loss: 0.2304	step2_classification_accuracy: 88.8139	step_2_gate_accuracy: 91.3180
STEP-2	Epoch: 140/200	classification_loss: 0.3086	gate_loss: 0.2094	step2_classification_accuracy: 89.5187	step_2_gate_accuracy: 92.2252
STEP-2	Epoch: 160/200	classification_loss: 0.3118	gate_loss: 0.2079	step2_classification_accuracy: 89.6011	step_2_gate_accuracy: 92.4277
STEP-2	Epoch: 180/200	classification_loss: 0.2886	gate_loss: 0.1927	step2_classification_accuracy: 90.0060	step_2_gate_accuracy: 92.8700
STEP-2	Epoch: 200/200	classification_loss: 0.2699	gate_loss: 0.1758	step2_classification_accuracy: 90.3734	step_2_gate_accuracy: 93.2299
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.4884	gate_accuracy: 75.9690
	Task-1	val_accuracy: 69.5122	gate_accuracy: 74.3902
	Task-2	val_accuracy: 50.0000	gate_accuracy: 55.8824
	Task-3	val_accuracy: 68.6747	gate_accuracy: 74.6988
	Task-4	val_accuracy: 76.2500	gate_accuracy: 73.7500
	Task-5	val_accuracy: 73.3333	gate_accuracy: 73.3333
	Task-6	val_accuracy: 69.4444	gate_accuracy: 65.2778
	Task-7	val_accuracy: 73.5632	gate_accuracy: 67.8161
	Task-8	val_accuracy: 85.7143	gate_accuracy: 85.7143
	Task-9	val_accuracy: 76.5432	gate_accuracy: 76.5432
	Task-10	val_accuracy: 73.1707	gate_accuracy: 70.7317
	Task-11	val_accuracy: 70.9302	gate_accuracy: 69.7674
	Task-12	val_accuracy: 70.0000	gate_accuracy: 68.7500
	Task-13	val_accuracy: 69.0476	gate_accuracy: 58.3333
	Task-14	val_accuracy: 80.7692	gate_accuracy: 82.0513
	Task-15	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-16	val_accuracy: 77.3810	gate_accuracy: 73.8095
	Task-17	val_accuracy: 75.3623	gate_accuracy: 81.1594
	Task-18	val_accuracy: 73.0769	gate_accuracy: 71.7949
	Task-19	val_accuracy: 66.3043	gate_accuracy: 67.3913
	Task-20	val_accuracy: 62.7907	gate_accuracy: 61.6279
	Task-21	val_accuracy: 65.2174	gate_accuracy: 65.2174
	Task-22	val_accuracy: 79.7753	gate_accuracy: 77.5281
	Task-23	val_accuracy: 55.4054	gate_accuracy: 51.3514
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.6357


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.8366	step1_train_accuracy: 39.6552
STEP-1	Epoch: 20/50	loss: 0.9961	step1_train_accuracy: 83.4483
STEP-1	Epoch: 30/50	loss: 0.5223	step1_train_accuracy: 94.1379
STEP-1	Epoch: 40/50	loss: 0.3571	step1_train_accuracy: 95.8621
STEP-1	Epoch: 50/50	loss: 0.2472	step1_train_accuracy: 97.5862
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.7216	gate_loss: 0.7520	step2_classification_accuracy: 77.9905	step_2_gate_accuracy: 76.4466
STEP-2	Epoch: 40/200	classification_loss: 0.5306	gate_loss: 0.4410	step2_classification_accuracy: 83.1932	step_2_gate_accuracy: 84.5048
STEP-2	Epoch: 60/200	classification_loss: 0.4473	gate_loss: 0.3429	step2_classification_accuracy: 85.3395	step_2_gate_accuracy: 87.6177
STEP-2	Epoch: 80/200	classification_loss: 0.3936	gate_loss: 0.2904	step2_classification_accuracy: 86.6198	step_2_gate_accuracy: 89.1051
STEP-2	Epoch: 100/200	classification_loss: 0.3686	gate_loss: 0.2617	step2_classification_accuracy: 87.3917	step_2_gate_accuracy: 90.2284
STEP-2	Epoch: 120/200	classification_loss: 0.3361	gate_loss: 0.2314	step2_classification_accuracy: 88.4398	step_2_gate_accuracy: 91.1071
STEP-2	Epoch: 140/200	classification_loss: 0.3138	gate_loss: 0.2116	step2_classification_accuracy: 89.0988	step_2_gate_accuracy: 92.0861
STEP-2	Epoch: 160/200	classification_loss: 0.3009	gate_loss: 0.2031	step2_classification_accuracy: 89.4189	step_2_gate_accuracy: 92.4125
STEP-2	Epoch: 180/200	classification_loss: 0.2951	gate_loss: 0.1946	step2_classification_accuracy: 89.7138	step_2_gate_accuracy: 92.6760
STEP-2	Epoch: 200/200	classification_loss: 0.2853	gate_loss: 0.1878	step2_classification_accuracy: 89.9460	step_2_gate_accuracy: 92.8518
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 49.6124	gate_accuracy: 69.7674
	Task-1	val_accuracy: 74.3902	gate_accuracy: 79.2683
	Task-2	val_accuracy: 42.6471	gate_accuracy: 54.4118
	Task-3	val_accuracy: 72.2892	gate_accuracy: 74.6988
	Task-4	val_accuracy: 78.7500	gate_accuracy: 78.7500
	Task-5	val_accuracy: 78.6667	gate_accuracy: 77.3333
	Task-6	val_accuracy: 66.6667	gate_accuracy: 65.2778
	Task-7	val_accuracy: 78.1609	gate_accuracy: 68.9655
	Task-8	val_accuracy: 80.0000	gate_accuracy: 81.4286
	Task-9	val_accuracy: 77.7778	gate_accuracy: 76.5432
	Task-10	val_accuracy: 76.8293	gate_accuracy: 74.3902
	Task-11	val_accuracy: 73.2558	gate_accuracy: 69.7674
	Task-12	val_accuracy: 68.7500	gate_accuracy: 63.7500
	Task-13	val_accuracy: 67.8571	gate_accuracy: 59.5238
	Task-14	val_accuracy: 82.0513	gate_accuracy: 78.2051
	Task-15	val_accuracy: 75.3086	gate_accuracy: 71.6049
	Task-16	val_accuracy: 77.3810	gate_accuracy: 72.6190
	Task-17	val_accuracy: 76.8116	gate_accuracy: 82.6087
	Task-18	val_accuracy: 75.6410	gate_accuracy: 75.6410
	Task-19	val_accuracy: 67.3913	gate_accuracy: 69.5652
	Task-20	val_accuracy: 59.3023	gate_accuracy: 58.1395
	Task-21	val_accuracy: 65.2174	gate_accuracy: 70.6522
	Task-22	val_accuracy: 83.1461	gate_accuracy: 80.8989
	Task-23	val_accuracy: 51.3514	gate_accuracy: 45.9459
	Task-24	val_accuracy: 63.8889	gate_accuracy: 70.8333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.8374


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 3.4239	step1_train_accuracy: 38.7454
STEP-1	Epoch: 20/50	loss: 1.3118	step1_train_accuracy: 80.4428
STEP-1	Epoch: 30/50	loss: 0.5909	step1_train_accuracy: 93.7269
STEP-1	Epoch: 40/50	loss: 0.3552	step1_train_accuracy: 98.1550
STEP-1	Epoch: 50/50	loss: 0.2432	step1_train_accuracy: 98.5240
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29, 374: 29, 375: 29, 376: 29, 377: 29, 378: 29, 379: 29, 380: 29, 381: 29, 382: 29, 383: 29, 384: 29, 385: 29, 386: 29, 387: 29, 388: 29, 389: 29, 390: 29, 391: 29, 392: 29, 393: 29, 394: 29, 395: 29, 396: 29, 397: 29, 398: 29, 399: 29, 400: 29, 401: 29, 402: 29, 403: 29, 404: 29, 405: 29, 406: 29, 407: 29, 408: 29, 409: 29, 410: 29, 411: 29, 412: 29, 413: 29, 414: 29, 415: 29, 416: 29, 417: 29, 418: 29, 419: 29, 420: 29, 421: 29, 422: 29, 423: 29, 424: 29, 425: 29, 426: 29, 427: 29, 428: 29, 429: 29, 430: 29, 431: 29, 432: 29, 433: 29, 434: 29, 435: 29, 436: 29, 437: 29, 438: 29, 439: 29, 440: 29, 441: 29, 442: 29, 443: 29, 444: 29, 445: 29, 446: 29, 447: 29, 448: 29, 449: 29, 450: 29, 451: 29, 452: 29, 453: 29, 454: 29, 455: 29, 456: 29, 457: 29, 458: 29, 459: 29, 460: 29, 461: 29, 462: 29, 463: 29, 464: 29, 465: 29, 466: 29, 467: 29, 468: 29, 469: 29, 470: 29, 471: 29, 472: 29, 473: 29, 474: 29, 475: 29, 476: 29, 477: 29, 478: 29, 479: 29, 480: 29, 481: 29, 482: 29, 483: 29, 484: 29, 485: 29, 486: 29, 487: 29, 488: 29, 489: 29, 490: 29, 491: 29, 492: 29, 493: 29, 494: 29, 495: 29, 496: 29, 497: 29, 498: 29, 499: 29, 500: 29, 501: 29, 502: 29, 503: 29, 504: 29, 505: 29, 506: 29, 507: 29, 508: 29, 509: 29, 510: 29, 511: 29, 512: 29, 513: 29, 514: 29, 515: 29, 516: 29, 517: 29, 518: 29, 519: 29, 520: 29, 521: 29, 522: 29, 523: 29, 524: 29, 525: 29, 526: 29, 527: 29, 528: 29, 529: 29, 530: 29, 531: 29, 532: 29, 533: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.7412	gate_loss: 0.8255	step2_classification_accuracy: 77.3925	step_2_gate_accuracy: 74.3769
STEP-2	Epoch: 40/200	classification_loss: 0.5587	gate_loss: 0.4772	step2_classification_accuracy: 82.6811	step_2_gate_accuracy: 83.5012
STEP-2	Epoch: 60/200	classification_loss: 0.4702	gate_loss: 0.3744	step2_classification_accuracy: 84.7992	step_2_gate_accuracy: 86.5556
STEP-2	Epoch: 80/200	classification_loss: 0.4205	gate_loss: 0.3172	step2_classification_accuracy: 86.2198	step_2_gate_accuracy: 88.5832
STEP-2	Epoch: 100/200	classification_loss: 0.3888	gate_loss: 0.2845	step2_classification_accuracy: 87.1691	step_2_gate_accuracy: 89.4033
STEP-2	Epoch: 120/200	classification_loss: 0.3604	gate_loss: 0.2594	step2_classification_accuracy: 87.9439	step_2_gate_accuracy: 90.3203
STEP-2	Epoch: 140/200	classification_loss: 0.3423	gate_loss: 0.2403	step2_classification_accuracy: 88.3572	step_2_gate_accuracy: 90.9402
STEP-2	Epoch: 160/200	classification_loss: 0.3267	gate_loss: 0.2294	step2_classification_accuracy: 88.9901	step_2_gate_accuracy: 91.5149
STEP-2	Epoch: 180/200	classification_loss: 0.3074	gate_loss: 0.2108	step2_classification_accuracy: 89.3840	step_2_gate_accuracy: 91.9992
STEP-2	Epoch: 200/200	classification_loss: 0.3021	gate_loss: 0.2055	step2_classification_accuracy: 89.5067	step_2_gate_accuracy: 92.3544
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 49.6124	gate_accuracy: 78.2946
	Task-1	val_accuracy: 57.3171	gate_accuracy: 64.6341
	Task-2	val_accuracy: 50.0000	gate_accuracy: 48.5294
	Task-3	val_accuracy: 69.8795	gate_accuracy: 74.6988
	Task-4	val_accuracy: 77.5000	gate_accuracy: 76.2500
	Task-5	val_accuracy: 81.3333	gate_accuracy: 84.0000
	Task-6	val_accuracy: 68.0556	gate_accuracy: 68.0556
	Task-7	val_accuracy: 74.7126	gate_accuracy: 74.7126
	Task-8	val_accuracy: 84.2857	gate_accuracy: 80.0000
	Task-9	val_accuracy: 80.2469	gate_accuracy: 81.4815
	Task-10	val_accuracy: 81.7073	gate_accuracy: 78.0488
	Task-11	val_accuracy: 77.9070	gate_accuracy: 72.0930
	Task-12	val_accuracy: 58.7500	gate_accuracy: 56.2500
	Task-13	val_accuracy: 79.7619	gate_accuracy: 71.4286
	Task-14	val_accuracy: 83.3333	gate_accuracy: 79.4872
	Task-15	val_accuracy: 71.6049	gate_accuracy: 66.6667
	Task-16	val_accuracy: 79.7619	gate_accuracy: 77.3810
	Task-17	val_accuracy: 72.4638	gate_accuracy: 75.3623
	Task-18	val_accuracy: 67.9487	gate_accuracy: 67.9487
	Task-19	val_accuracy: 67.3913	gate_accuracy: 70.6522
	Task-20	val_accuracy: 51.1628	gate_accuracy: 53.4884
	Task-21	val_accuracy: 65.2174	gate_accuracy: 66.3043
	Task-22	val_accuracy: 83.1461	gate_accuracy: 78.6517
	Task-23	val_accuracy: 56.7568	gate_accuracy: 51.3514
	Task-24	val_accuracy: 56.9444	gate_accuracy: 59.7222
	Task-25	val_accuracy: 61.7647	gate_accuracy: 64.7059
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.3582


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 2.2279	step1_train_accuracy: 56.4033
STEP-1	Epoch: 20/50	loss: 0.8022	step1_train_accuracy: 87.1935
STEP-1	Epoch: 30/50	loss: 0.3995	step1_train_accuracy: 96.4578
STEP-1	Epoch: 40/50	loss: 0.2525	step1_train_accuracy: 97.5477
STEP-1	Epoch: 50/50	loss: 0.1872	step1_train_accuracy: 98.0926
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.7657	gate_loss: 0.7887	step2_classification_accuracy: 76.9968	step_2_gate_accuracy: 75.0226
STEP-2	Epoch: 40/200	classification_loss: 0.5837	gate_loss: 0.4862	step2_classification_accuracy: 81.6167	step_2_gate_accuracy: 83.1622
STEP-2	Epoch: 60/200	classification_loss: 0.4922	gate_loss: 0.3834	step2_classification_accuracy: 84.2227	step_2_gate_accuracy: 86.3211
STEP-2	Epoch: 80/200	classification_loss: 0.4440	gate_loss: 0.3327	step2_classification_accuracy: 85.2268	step_2_gate_accuracy: 87.6354
STEP-2	Epoch: 100/200	classification_loss: 0.4184	gate_loss: 0.3020	step2_classification_accuracy: 86.1519	step_2_gate_accuracy: 88.7128
STEP-2	Epoch: 120/200	classification_loss: 0.3905	gate_loss: 0.2792	step2_classification_accuracy: 86.9303	step_2_gate_accuracy: 89.6097
STEP-2	Epoch: 140/200	classification_loss: 0.3648	gate_loss: 0.2554	step2_classification_accuracy: 87.5338	step_2_gate_accuracy: 90.3768
STEP-2	Epoch: 160/200	classification_loss: 0.3520	gate_loss: 0.2420	step2_classification_accuracy: 88.0472	step_2_gate_accuracy: 90.9183
STEP-2	Epoch: 180/200	classification_loss: 0.3350	gate_loss: 0.2291	step2_classification_accuracy: 88.3856	step_2_gate_accuracy: 91.2963
STEP-2	Epoch: 200/200	classification_loss: 0.3244	gate_loss: 0.2200	step2_classification_accuracy: 88.8594	step_2_gate_accuracy: 91.7362
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.0388	gate_accuracy: 75.1938
	Task-1	val_accuracy: 68.2927	gate_accuracy: 71.9512
	Task-2	val_accuracy: 50.0000	gate_accuracy: 55.8824
	Task-3	val_accuracy: 71.0843	gate_accuracy: 72.2892
	Task-4	val_accuracy: 77.5000	gate_accuracy: 72.5000
	Task-5	val_accuracy: 78.6667	gate_accuracy: 78.6667
	Task-6	val_accuracy: 69.4444	gate_accuracy: 70.8333
	Task-7	val_accuracy: 74.7126	gate_accuracy: 67.8161
	Task-8	val_accuracy: 81.4286	gate_accuracy: 80.0000
	Task-9	val_accuracy: 76.5432	gate_accuracy: 76.5432
	Task-10	val_accuracy: 89.0244	gate_accuracy: 85.3659
	Task-11	val_accuracy: 76.7442	gate_accuracy: 70.9302
	Task-12	val_accuracy: 70.0000	gate_accuracy: 62.5000
	Task-13	val_accuracy: 67.8571	gate_accuracy: 63.0952
	Task-14	val_accuracy: 74.3590	gate_accuracy: 75.6410
	Task-15	val_accuracy: 60.4938	gate_accuracy: 59.2593
	Task-16	val_accuracy: 84.5238	gate_accuracy: 78.5714
	Task-17	val_accuracy: 78.2609	gate_accuracy: 82.6087
	Task-18	val_accuracy: 66.6667	gate_accuracy: 69.2308
	Task-19	val_accuracy: 70.6522	gate_accuracy: 67.3913
	Task-20	val_accuracy: 59.3023	gate_accuracy: 62.7907
	Task-21	val_accuracy: 67.3913	gate_accuracy: 68.4783
	Task-22	val_accuracy: 84.2697	gate_accuracy: 85.3933
	Task-23	val_accuracy: 67.5676	gate_accuracy: 62.1622
	Task-24	val_accuracy: 61.1111	gate_accuracy: 62.5000
	Task-25	val_accuracy: 64.7059	gate_accuracy: 64.7059
	Task-26	val_accuracy: 75.0000	gate_accuracy: 77.1739
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.2737


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.2750	step1_train_accuracy: 59.5174
STEP-1	Epoch: 20/50	loss: 0.9441	step1_train_accuracy: 84.1823
STEP-1	Epoch: 30/50	loss: 0.4773	step1_train_accuracy: 95.4424
STEP-1	Epoch: 40/50	loss: 0.3071	step1_train_accuracy: 96.2467
STEP-1	Epoch: 50/50	loss: 0.2211	step1_train_accuracy: 97.5871
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.7708	gate_loss: 0.8160	step2_classification_accuracy: 76.8686	step_2_gate_accuracy: 74.6375
STEP-2	Epoch: 40/200	classification_loss: 0.5795	gate_loss: 0.4888	step2_classification_accuracy: 82.3367	step_2_gate_accuracy: 83.5169
STEP-2	Epoch: 60/200	classification_loss: 0.4893	gate_loss: 0.3844	step2_classification_accuracy: 84.2644	step_2_gate_accuracy: 86.1189
STEP-2	Epoch: 80/200	classification_loss: 0.4333	gate_loss: 0.3246	step2_classification_accuracy: 85.7368	step_2_gate_accuracy: 88.3950
STEP-2	Epoch: 100/200	classification_loss: 0.4050	gate_loss: 0.2944	step2_classification_accuracy: 86.6135	step_2_gate_accuracy: 89.0750
STEP-2	Epoch: 120/200	classification_loss: 0.3795	gate_loss: 0.2693	step2_classification_accuracy: 87.5688	step_2_gate_accuracy: 90.1877
STEP-2	Epoch: 140/200	classification_loss: 0.3601	gate_loss: 0.2531	step2_classification_accuracy: 87.9285	step_2_gate_accuracy: 90.7216
STEP-2	Epoch: 160/200	classification_loss: 0.3412	gate_loss: 0.2375	step2_classification_accuracy: 88.5242	step_2_gate_accuracy: 91.3061
STEP-2	Epoch: 180/200	classification_loss: 0.3257	gate_loss: 0.2247	step2_classification_accuracy: 88.8558	step_2_gate_accuracy: 91.7556
STEP-2	Epoch: 200/200	classification_loss: 0.3117	gate_loss: 0.2116	step2_classification_accuracy: 89.3728	step_2_gate_accuracy: 92.2109
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.9380	gate_accuracy: 70.5426
	Task-1	val_accuracy: 70.7317	gate_accuracy: 71.9512
	Task-2	val_accuracy: 54.4118	gate_accuracy: 55.8824
	Task-3	val_accuracy: 65.0602	gate_accuracy: 63.8554
	Task-4	val_accuracy: 70.0000	gate_accuracy: 68.7500
	Task-5	val_accuracy: 78.6667	gate_accuracy: 78.6667
	Task-6	val_accuracy: 69.4444	gate_accuracy: 68.0556
	Task-7	val_accuracy: 80.4598	gate_accuracy: 72.4138
	Task-8	val_accuracy: 81.4286	gate_accuracy: 82.8571
	Task-9	val_accuracy: 74.0741	gate_accuracy: 77.7778
	Task-10	val_accuracy: 78.0488	gate_accuracy: 70.7317
	Task-11	val_accuracy: 77.9070	gate_accuracy: 73.2558
	Task-12	val_accuracy: 68.7500	gate_accuracy: 68.7500
	Task-13	val_accuracy: 69.0476	gate_accuracy: 64.2857
	Task-14	val_accuracy: 76.9231	gate_accuracy: 75.6410
	Task-15	val_accuracy: 69.1358	gate_accuracy: 67.9012
	Task-16	val_accuracy: 79.7619	gate_accuracy: 79.7619
	Task-17	val_accuracy: 73.9130	gate_accuracy: 78.2609
	Task-18	val_accuracy: 71.7949	gate_accuracy: 70.5128
	Task-19	val_accuracy: 64.1304	gate_accuracy: 65.2174
	Task-20	val_accuracy: 62.7907	gate_accuracy: 66.2791
	Task-21	val_accuracy: 65.2174	gate_accuracy: 67.3913
	Task-22	val_accuracy: 86.5169	gate_accuracy: 86.5169
	Task-23	val_accuracy: 56.7568	gate_accuracy: 56.7568
	Task-24	val_accuracy: 63.8889	gate_accuracy: 62.5000
	Task-25	val_accuracy: 61.7647	gate_accuracy: 63.2353
	Task-26	val_accuracy: 77.1739	gate_accuracy: 75.0000
	Task-27	val_accuracy: 75.2688	gate_accuracy: 75.2688
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.7846


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 1.9313	step1_train_accuracy: 56.6580
STEP-1	Epoch: 20/50	loss: 0.8375	step1_train_accuracy: 88.5117
STEP-1	Epoch: 30/50	loss: 0.4640	step1_train_accuracy: 96.0835
STEP-1	Epoch: 40/50	loss: 0.2809	step1_train_accuracy: 97.1279
STEP-1	Epoch: 50/50	loss: 0.1925	step1_train_accuracy: 97.9112
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.7737	gate_loss: 0.8146	step2_classification_accuracy: 76.5099	step_2_gate_accuracy: 74.2898
STEP-2	Epoch: 40/200	classification_loss: 0.5771	gate_loss: 0.4931	step2_classification_accuracy: 81.8603	step_2_gate_accuracy: 83.1965
STEP-2	Epoch: 60/200	classification_loss: 0.4966	gate_loss: 0.3978	step2_classification_accuracy: 84.1540	step_2_gate_accuracy: 85.7323
STEP-2	Epoch: 80/200	classification_loss: 0.4403	gate_loss: 0.3411	step2_classification_accuracy: 85.5482	step_2_gate_accuracy: 87.5894
STEP-2	Epoch: 100/200	classification_loss: 0.4021	gate_loss: 0.3025	step2_classification_accuracy: 86.5267	step_2_gate_accuracy: 88.7679
STEP-2	Epoch: 120/200	classification_loss: 0.3748	gate_loss: 0.2761	step2_classification_accuracy: 87.3632	step_2_gate_accuracy: 89.6307
STEP-2	Epoch: 140/200	classification_loss: 0.3528	gate_loss: 0.2542	step2_classification_accuracy: 87.8104	step_2_gate_accuracy: 90.6303
STEP-2	Epoch: 160/200	classification_loss: 0.3393	gate_loss: 0.2419	step2_classification_accuracy: 88.5838	step_2_gate_accuracy: 91.1458
STEP-2	Epoch: 180/200	classification_loss: 0.3260	gate_loss: 0.2296	step2_classification_accuracy: 88.7521	step_2_gate_accuracy: 91.2879
STEP-2	Epoch: 200/200	classification_loss: 0.3209	gate_loss: 0.2262	step2_classification_accuracy: 88.8889	step_2_gate_accuracy: 91.5930
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.1628	gate_accuracy: 71.3178
	Task-1	val_accuracy: 70.7317	gate_accuracy: 75.6098
	Task-2	val_accuracy: 52.9412	gate_accuracy: 57.3529
	Task-3	val_accuracy: 69.8795	gate_accuracy: 72.2892
	Task-4	val_accuracy: 77.5000	gate_accuracy: 77.5000
	Task-5	val_accuracy: 74.6667	gate_accuracy: 68.0000
	Task-6	val_accuracy: 68.0556	gate_accuracy: 68.0556
	Task-7	val_accuracy: 70.1149	gate_accuracy: 67.8161
	Task-8	val_accuracy: 81.4286	gate_accuracy: 80.0000
	Task-9	val_accuracy: 74.0741	gate_accuracy: 71.6049
	Task-10	val_accuracy: 82.9268	gate_accuracy: 84.1463
	Task-11	val_accuracy: 76.7442	gate_accuracy: 70.9302
	Task-12	val_accuracy: 66.2500	gate_accuracy: 58.7500
	Task-13	val_accuracy: 72.6190	gate_accuracy: 64.2857
	Task-14	val_accuracy: 78.2051	gate_accuracy: 79.4872
	Task-15	val_accuracy: 66.6667	gate_accuracy: 60.4938
	Task-16	val_accuracy: 83.3333	gate_accuracy: 78.5714
	Task-17	val_accuracy: 82.6087	gate_accuracy: 84.0580
	Task-18	val_accuracy: 69.2308	gate_accuracy: 65.3846
	Task-19	val_accuracy: 68.4783	gate_accuracy: 70.6522
	Task-20	val_accuracy: 55.8140	gate_accuracy: 59.3023
	Task-21	val_accuracy: 65.2174	gate_accuracy: 63.0435
	Task-22	val_accuracy: 84.2697	gate_accuracy: 78.6517
	Task-23	val_accuracy: 58.1081	gate_accuracy: 48.6486
	Task-24	val_accuracy: 59.7222	gate_accuracy: 68.0556
	Task-25	val_accuracy: 64.7059	gate_accuracy: 66.1765
	Task-26	val_accuracy: 76.0870	gate_accuracy: 73.9130
	Task-27	val_accuracy: 70.9677	gate_accuracy: 72.0430
	Task-28	val_accuracy: 83.3333	gate_accuracy: 81.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.4120


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.7766	step1_train_accuracy: 45.7576
STEP-1	Epoch: 20/50	loss: 0.9104	step1_train_accuracy: 86.3636
STEP-1	Epoch: 30/50	loss: 0.5035	step1_train_accuracy: 93.0303
STEP-1	Epoch: 40/50	loss: 0.3134	step1_train_accuracy: 96.3636
STEP-1	Epoch: 50/50	loss: 0.2197	step1_train_accuracy: 97.2727
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.7905	gate_loss: 0.8095	step2_classification_accuracy: 76.5574	step_2_gate_accuracy: 74.8219
STEP-2	Epoch: 40/200	classification_loss: 0.5980	gate_loss: 0.5075	step2_classification_accuracy: 81.9422	step_2_gate_accuracy: 82.5886
STEP-2	Epoch: 60/200	classification_loss: 0.5066	gate_loss: 0.4025	step2_classification_accuracy: 84.1561	step_2_gate_accuracy: 85.7746
STEP-2	Epoch: 80/200	classification_loss: 0.4515	gate_loss: 0.3438	step2_classification_accuracy: 85.5711	step_2_gate_accuracy: 87.5509
STEP-2	Epoch: 100/200	classification_loss: 0.4155	gate_loss: 0.3090	step2_classification_accuracy: 86.7518	step_2_gate_accuracy: 88.8284
STEP-2	Epoch: 120/200	classification_loss: 0.3978	gate_loss: 0.2920	step2_classification_accuracy: 87.1641	step_2_gate_accuracy: 89.2610
STEP-2	Epoch: 140/200	classification_loss: 0.3720	gate_loss: 0.2675	step2_classification_accuracy: 87.8156	step_2_gate_accuracy: 89.9684
STEP-2	Epoch: 160/200	classification_loss: 0.3542	gate_loss: 0.2501	step2_classification_accuracy: 88.2533	step_2_gate_accuracy: 90.7421
STEP-2	Epoch: 180/200	classification_loss: 0.3427	gate_loss: 0.2428	step2_classification_accuracy: 88.5077	step_2_gate_accuracy: 91.0729
STEP-2	Epoch: 200/200	classification_loss: 0.3367	gate_loss: 0.2360	step2_classification_accuracy: 88.6095	step_2_gate_accuracy: 91.2357
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 49.6124	gate_accuracy: 70.5426
	Task-1	val_accuracy: 71.9512	gate_accuracy: 73.1707
	Task-2	val_accuracy: 52.9412	gate_accuracy: 61.7647
	Task-3	val_accuracy: 69.8795	gate_accuracy: 61.4458
	Task-4	val_accuracy: 77.5000	gate_accuracy: 76.2500
	Task-5	val_accuracy: 81.3333	gate_accuracy: 82.6667
	Task-6	val_accuracy: 69.4444	gate_accuracy: 70.8333
	Task-7	val_accuracy: 72.4138	gate_accuracy: 66.6667
	Task-8	val_accuracy: 81.4286	gate_accuracy: 80.0000
	Task-9	val_accuracy: 75.3086	gate_accuracy: 71.6049
	Task-10	val_accuracy: 70.7317	gate_accuracy: 69.5122
	Task-11	val_accuracy: 75.5814	gate_accuracy: 68.6047
	Task-12	val_accuracy: 68.7500	gate_accuracy: 68.7500
	Task-13	val_accuracy: 73.8095	gate_accuracy: 67.8571
	Task-14	val_accuracy: 75.6410	gate_accuracy: 71.7949
	Task-15	val_accuracy: 72.8395	gate_accuracy: 64.1975
	Task-16	val_accuracy: 82.1429	gate_accuracy: 76.1905
	Task-17	val_accuracy: 76.8116	gate_accuracy: 79.7101
	Task-18	val_accuracy: 70.5128	gate_accuracy: 71.7949
	Task-19	val_accuracy: 65.2174	gate_accuracy: 67.3913
	Task-20	val_accuracy: 51.1628	gate_accuracy: 52.3256
	Task-21	val_accuracy: 65.2174	gate_accuracy: 65.2174
	Task-22	val_accuracy: 84.2697	gate_accuracy: 83.1461
	Task-23	val_accuracy: 51.3514	gate_accuracy: 47.2973
	Task-24	val_accuracy: 62.5000	gate_accuracy: 62.5000
	Task-25	val_accuracy: 66.1765	gate_accuracy: 69.1176
	Task-26	val_accuracy: 72.8261	gate_accuracy: 71.7391
	Task-27	val_accuracy: 67.7419	gate_accuracy: 70.9677
	Task-28	val_accuracy: 80.2083	gate_accuracy: 81.2500
	Task-29	val_accuracy: 70.7317	gate_accuracy: 78.0488
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.1408


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 2.2414	step1_train_accuracy: 51.7520
STEP-1	Epoch: 20/50	loss: 0.9095	step1_train_accuracy: 77.6280
STEP-1	Epoch: 30/50	loss: 0.4826	step1_train_accuracy: 94.8787
STEP-1	Epoch: 40/50	loss: 0.3180	step1_train_accuracy: 96.4960
STEP-1	Epoch: 50/50	loss: 0.2197	step1_train_accuracy: 97.3046
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.8299	gate_loss: 0.8512	step2_classification_accuracy: 75.6408	step_2_gate_accuracy: 72.9692
STEP-2	Epoch: 40/200	classification_loss: 0.6225	gate_loss: 0.5319	step2_classification_accuracy: 81.2303	step_2_gate_accuracy: 81.9647
STEP-2	Epoch: 60/200	classification_loss: 0.5357	gate_loss: 0.4291	step2_classification_accuracy: 83.2512	step_2_gate_accuracy: 84.8778
STEP-2	Epoch: 80/200	classification_loss: 0.4787	gate_loss: 0.3659	step2_classification_accuracy: 84.5475	step_2_gate_accuracy: 86.7311
STEP-2	Epoch: 100/200	classification_loss: 0.4365	gate_loss: 0.3258	step2_classification_accuracy: 86.0114	step_2_gate_accuracy: 88.0619
STEP-2	Epoch: 120/200	classification_loss: 0.4043	gate_loss: 0.2964	step2_classification_accuracy: 86.8789	step_2_gate_accuracy: 89.1364
STEP-2	Epoch: 140/200	classification_loss: 0.3851	gate_loss: 0.2776	step2_classification_accuracy: 87.2240	step_2_gate_accuracy: 89.7476
STEP-2	Epoch: 160/200	classification_loss: 0.3633	gate_loss: 0.2604	step2_classification_accuracy: 87.9584	step_2_gate_accuracy: 90.4476
STEP-2	Epoch: 180/200	classification_loss: 0.3548	gate_loss: 0.2514	step2_classification_accuracy: 88.1654	step_2_gate_accuracy: 90.4771
STEP-2	Epoch: 200/200	classification_loss: 0.3432	gate_loss: 0.2398	step2_classification_accuracy: 88.3922	step_2_gate_accuracy: 91.0883
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.1628	gate_accuracy: 68.9922
	Task-1	val_accuracy: 67.0732	gate_accuracy: 70.7317
	Task-2	val_accuracy: 44.1176	gate_accuracy: 51.4706
	Task-3	val_accuracy: 72.2892	gate_accuracy: 74.6988
	Task-4	val_accuracy: 81.2500	gate_accuracy: 76.2500
	Task-5	val_accuracy: 76.0000	gate_accuracy: 70.6667
	Task-6	val_accuracy: 68.0556	gate_accuracy: 65.2778
	Task-7	val_accuracy: 74.7126	gate_accuracy: 75.8621
	Task-8	val_accuracy: 81.4286	gate_accuracy: 78.5714
	Task-9	val_accuracy: 74.0741	gate_accuracy: 75.3086
	Task-10	val_accuracy: 75.6098	gate_accuracy: 74.3902
	Task-11	val_accuracy: 75.5814	gate_accuracy: 68.6047
	Task-12	val_accuracy: 71.2500	gate_accuracy: 61.2500
	Task-13	val_accuracy: 75.0000	gate_accuracy: 71.4286
	Task-14	val_accuracy: 78.2051	gate_accuracy: 82.0513
	Task-15	val_accuracy: 67.9012	gate_accuracy: 65.4321
	Task-16	val_accuracy: 83.3333	gate_accuracy: 82.1429
	Task-17	val_accuracy: 76.8116	gate_accuracy: 75.3623
	Task-18	val_accuracy: 74.3590	gate_accuracy: 66.6667
	Task-19	val_accuracy: 64.1304	gate_accuracy: 65.2174
	Task-20	val_accuracy: 66.2791	gate_accuracy: 69.7674
	Task-21	val_accuracy: 70.6522	gate_accuracy: 75.0000
	Task-22	val_accuracy: 77.5281	gate_accuracy: 75.2809
	Task-23	val_accuracy: 56.7568	gate_accuracy: 54.0541
	Task-24	val_accuracy: 59.7222	gate_accuracy: 63.8889
	Task-25	val_accuracy: 52.9412	gate_accuracy: 51.4706
	Task-26	val_accuracy: 72.8261	gate_accuracy: 71.7391
	Task-27	val_accuracy: 63.4409	gate_accuracy: 60.2151
	Task-28	val_accuracy: 81.2500	gate_accuracy: 75.0000
	Task-29	val_accuracy: 71.9512	gate_accuracy: 75.6098
	Task-30	val_accuracy: 61.2903	gate_accuracy: 61.2903
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.6664


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 2.3228	step1_train_accuracy: 51.9637
STEP-1	Epoch: 20/50	loss: 0.8011	step1_train_accuracy: 83.6858
STEP-1	Epoch: 30/50	loss: 0.4153	step1_train_accuracy: 96.0725
STEP-1	Epoch: 40/50	loss: 0.2643	step1_train_accuracy: 96.6767
STEP-1	Epoch: 50/50	loss: 0.1857	step1_train_accuracy: 97.8852
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.8154	gate_loss: 0.8427	step2_classification_accuracy: 76.0847	step_2_gate_accuracy: 73.7624
STEP-2	Epoch: 40/200	classification_loss: 0.6175	gate_loss: 0.5294	step2_classification_accuracy: 81.0780	step_2_gate_accuracy: 81.9954
STEP-2	Epoch: 60/200	classification_loss: 0.5230	gate_loss: 0.4201	step2_classification_accuracy: 83.5054	step_2_gate_accuracy: 85.3211
STEP-2	Epoch: 80/200	classification_loss: 0.4650	gate_loss: 0.3602	step2_classification_accuracy: 85.3450	step_2_gate_accuracy: 87.2706
STEP-2	Epoch: 100/200	classification_loss: 0.4299	gate_loss: 0.3249	step2_classification_accuracy: 86.0665	step_2_gate_accuracy: 88.3410
STEP-2	Epoch: 120/200	classification_loss: 0.4047	gate_loss: 0.2997	step2_classification_accuracy: 86.7737	step_2_gate_accuracy: 89.1390
STEP-2	Epoch: 140/200	classification_loss: 0.3903	gate_loss: 0.2853	step2_classification_accuracy: 87.2945	step_2_gate_accuracy: 89.6263
STEP-2	Epoch: 160/200	classification_loss: 0.3636	gate_loss: 0.2626	step2_classification_accuracy: 88.0161	step_2_gate_accuracy: 90.3288
STEP-2	Epoch: 180/200	classification_loss: 0.3507	gate_loss: 0.2523	step2_classification_accuracy: 88.3410	step_2_gate_accuracy: 90.8544
STEP-2	Epoch: 200/200	classification_loss: 0.3439	gate_loss: 0.2440	step2_classification_accuracy: 88.5464	step_2_gate_accuracy: 91.0837
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 48.0620	gate_accuracy: 69.7674
	Task-1	val_accuracy: 69.5122	gate_accuracy: 71.9512
	Task-2	val_accuracy: 50.0000	gate_accuracy: 55.8824
	Task-3	val_accuracy: 66.2651	gate_accuracy: 66.2651
	Task-4	val_accuracy: 82.5000	gate_accuracy: 82.5000
	Task-5	val_accuracy: 77.3333	gate_accuracy: 66.6667
	Task-6	val_accuracy: 62.5000	gate_accuracy: 63.8889
	Task-7	val_accuracy: 74.7126	gate_accuracy: 70.1149
	Task-8	val_accuracy: 82.8571	gate_accuracy: 78.5714
	Task-9	val_accuracy: 75.3086	gate_accuracy: 72.8395
	Task-10	val_accuracy: 74.3902	gate_accuracy: 69.5122
	Task-11	val_accuracy: 67.4419	gate_accuracy: 58.1395
	Task-12	val_accuracy: 63.7500	gate_accuracy: 55.0000
	Task-13	val_accuracy: 73.8095	gate_accuracy: 70.2381
	Task-14	val_accuracy: 75.6410	gate_accuracy: 67.9487
	Task-15	val_accuracy: 67.9012	gate_accuracy: 65.4321
	Task-16	val_accuracy: 76.1905	gate_accuracy: 70.2381
	Task-17	val_accuracy: 76.8116	gate_accuracy: 75.3623
	Task-18	val_accuracy: 69.2308	gate_accuracy: 70.5128
	Task-19	val_accuracy: 61.9565	gate_accuracy: 64.1304
	Task-20	val_accuracy: 69.7674	gate_accuracy: 70.9302
	Task-21	val_accuracy: 64.1304	gate_accuracy: 67.3913
	Task-22	val_accuracy: 77.5281	gate_accuracy: 74.1573
	Task-23	val_accuracy: 59.4595	gate_accuracy: 56.7568
	Task-24	val_accuracy: 58.3333	gate_accuracy: 62.5000
	Task-25	val_accuracy: 55.8824	gate_accuracy: 55.8824
	Task-26	val_accuracy: 72.8261	gate_accuracy: 70.6522
	Task-27	val_accuracy: 75.2688	gate_accuracy: 78.4946
	Task-28	val_accuracy: 81.2500	gate_accuracy: 78.1250
	Task-29	val_accuracy: 63.4146	gate_accuracy: 68.2927
	Task-30	val_accuracy: 74.1935	gate_accuracy: 72.0430
	Task-31	val_accuracy: 63.8554	gate_accuracy: 67.4699
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.6208


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 3.3319	step1_train_accuracy: 38.6441
STEP-1	Epoch: 20/50	loss: 1.3151	step1_train_accuracy: 78.3051
STEP-1	Epoch: 30/50	loss: 0.6259	step1_train_accuracy: 93.2203
STEP-1	Epoch: 40/50	loss: 0.3962	step1_train_accuracy: 93.8983
STEP-1	Epoch: 50/50	loss: 0.2952	step1_train_accuracy: 95.2542
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30, 514: 30, 515: 30, 516: 30, 517: 30, 518: 30, 519: 30, 520: 30, 521: 30, 522: 30, 523: 30, 524: 30, 525: 30, 526: 30, 527: 30, 528: 30, 529: 30, 530: 30, 531: 30, 532: 30, 533: 30, 534: 30, 535: 30, 536: 30, 537: 30, 538: 30, 539: 30, 540: 30, 541: 30, 542: 30, 543: 30, 544: 30, 545: 30, 546: 30, 547: 30, 548: 30, 549: 30, 550: 30, 551: 30, 552: 30, 553: 30, 554: 30, 555: 30, 556: 30, 557: 30, 558: 30, 559: 30, 560: 30, 561: 30, 562: 30, 563: 30, 564: 30, 565: 30, 566: 30, 567: 30, 568: 30, 569: 30, 570: 30, 571: 30, 572: 30, 573: 30, 574: 30, 575: 30, 576: 30, 577: 30, 578: 30, 579: 30, 580: 30, 581: 30, 582: 30, 583: 30, 584: 30, 585: 30, 586: 30, 587: 30, 588: 30, 589: 30, 590: 30, 591: 30, 592: 30, 593: 30, 594: 30, 595: 30, 596: 30, 597: 30, 598: 30, 599: 30, 600: 30, 601: 30, 602: 30, 603: 30, 604: 30, 605: 30, 606: 30, 607: 30, 608: 30, 609: 30, 610: 30, 611: 30, 612: 30, 613: 30, 614: 30, 615: 30, 616: 30, 617: 30, 618: 30, 619: 30, 620: 30, 621: 30, 622: 30, 623: 30, 624: 30, 625: 30, 626: 30, 627: 30, 628: 30, 629: 30, 630: 30, 631: 30, 632: 30, 633: 30, 634: 30, 635: 30, 636: 30, 637: 30, 638: 30, 639: 30, 640: 30, 641: 30, 642: 30, 643: 30, 644: 30, 645: 30, 646: 30, 647: 30, 648: 30, 649: 30, 650: 30, 651: 30, 652: 30, 653: 30, 654: 30, 655: 30, 656: 30, 657: 30, 658: 30, 659: 30, 660: 30, 661: 30, 662: 30, 663: 30, 664: 30, 665: 30, 666: 30, 667: 30, 668: 30, 669: 30, 670: 30, 671: 30, 672: 30, 673: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.8571	gate_loss: 0.9060	step2_classification_accuracy: 74.8714	step_2_gate_accuracy: 72.0277
STEP-2	Epoch: 40/200	classification_loss: 0.6488	gate_loss: 0.5618	step2_classification_accuracy: 80.2374	step_2_gate_accuracy: 81.1029
STEP-2	Epoch: 60/200	classification_loss: 0.5478	gate_loss: 0.4436	step2_classification_accuracy: 82.9179	step_2_gate_accuracy: 84.3719
STEP-2	Epoch: 80/200	classification_loss: 0.4953	gate_loss: 0.3824	step2_classification_accuracy: 84.1939	step_2_gate_accuracy: 86.4738
STEP-2	Epoch: 100/200	classification_loss: 0.4576	gate_loss: 0.3436	step2_classification_accuracy: 85.0791	step_2_gate_accuracy: 87.6607
STEP-2	Epoch: 120/200	classification_loss: 0.4274	gate_loss: 0.3158	step2_classification_accuracy: 86.1919	step_2_gate_accuracy: 88.6647
STEP-2	Epoch: 140/200	classification_loss: 0.3986	gate_loss: 0.2919	step2_classification_accuracy: 86.7656	step_2_gate_accuracy: 89.4510
STEP-2	Epoch: 160/200	classification_loss: 0.3815	gate_loss: 0.2754	step2_classification_accuracy: 87.2552	step_2_gate_accuracy: 89.9308
STEP-2	Epoch: 180/200	classification_loss: 0.3676	gate_loss: 0.2613	step2_classification_accuracy: 87.7844	step_2_gate_accuracy: 90.5490
STEP-2	Epoch: 200/200	classification_loss: 0.3578	gate_loss: 0.2519	step2_classification_accuracy: 88.2245	step_2_gate_accuracy: 90.7270
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.7132	gate_accuracy: 67.4419
	Task-1	val_accuracy: 76.8293	gate_accuracy: 78.0488
	Task-2	val_accuracy: 48.5294	gate_accuracy: 54.4118
	Task-3	val_accuracy: 68.6747	gate_accuracy: 71.0843
	Task-4	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-5	val_accuracy: 74.6667	gate_accuracy: 70.6667
	Task-6	val_accuracy: 69.4444	gate_accuracy: 69.4444
	Task-7	val_accuracy: 74.7126	gate_accuracy: 73.5632
	Task-8	val_accuracy: 84.2857	gate_accuracy: 75.7143
	Task-9	val_accuracy: 75.3086	gate_accuracy: 70.3704
	Task-10	val_accuracy: 73.1707	gate_accuracy: 70.7317
	Task-11	val_accuracy: 74.4186	gate_accuracy: 68.6047
	Task-12	val_accuracy: 67.5000	gate_accuracy: 62.5000
	Task-13	val_accuracy: 66.6667	gate_accuracy: 61.9048
	Task-14	val_accuracy: 73.0769	gate_accuracy: 69.2308
	Task-15	val_accuracy: 70.3704	gate_accuracy: 70.3704
	Task-16	val_accuracy: 75.0000	gate_accuracy: 72.6190
	Task-17	val_accuracy: 71.0145	gate_accuracy: 78.2609
	Task-18	val_accuracy: 74.3590	gate_accuracy: 73.0769
	Task-19	val_accuracy: 68.4783	gate_accuracy: 73.9130
	Task-20	val_accuracy: 55.8140	gate_accuracy: 61.6279
	Task-21	val_accuracy: 63.0435	gate_accuracy: 64.1304
	Task-22	val_accuracy: 75.2809	gate_accuracy: 70.7865
	Task-23	val_accuracy: 56.7568	gate_accuracy: 54.0541
	Task-24	val_accuracy: 56.9444	gate_accuracy: 65.2778
	Task-25	val_accuracy: 63.2353	gate_accuracy: 55.8824
	Task-26	val_accuracy: 73.9130	gate_accuracy: 71.7391
	Task-27	val_accuracy: 67.7419	gate_accuracy: 73.1183
	Task-28	val_accuracy: 81.2500	gate_accuracy: 80.2083
	Task-29	val_accuracy: 65.8537	gate_accuracy: 73.1707
	Task-30	val_accuracy: 75.2688	gate_accuracy: 75.2688
	Task-31	val_accuracy: 63.8554	gate_accuracy: 63.8554
	Task-32	val_accuracy: 62.1622	gate_accuracy: 63.5135
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.4333


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.3678	step1_train_accuracy: 46.5347
STEP-1	Epoch: 20/50	loss: 0.7924	step1_train_accuracy: 91.4191
STEP-1	Epoch: 30/50	loss: 0.3954	step1_train_accuracy: 97.6898
STEP-1	Epoch: 40/50	loss: 0.2500	step1_train_accuracy: 98.0198
STEP-1	Epoch: 50/50	loss: 0.1722	step1_train_accuracy: 99.0099
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29, 374: 29, 375: 29, 376: 29, 377: 29, 378: 29, 379: 29, 380: 29, 381: 29, 382: 29, 383: 29, 384: 29, 385: 29, 386: 29, 387: 29, 388: 29, 389: 29, 390: 29, 391: 29, 392: 29, 393: 29, 394: 29, 395: 29, 396: 29, 397: 29, 398: 29, 399: 29, 400: 29, 401: 29, 402: 29, 403: 29, 404: 29, 405: 29, 406: 29, 407: 29, 408: 29, 409: 29, 410: 29, 411: 29, 412: 29, 413: 29, 414: 29, 415: 29, 416: 29, 417: 29, 418: 29, 419: 29, 420: 29, 421: 29, 422: 29, 423: 29, 424: 29, 425: 29, 426: 29, 427: 29, 428: 29, 429: 29, 430: 29, 431: 29, 432: 29, 433: 29, 434: 29, 435: 29, 436: 29, 437: 29, 438: 29, 439: 29, 440: 29, 441: 29, 442: 29, 443: 29, 444: 29, 445: 29, 446: 29, 447: 29, 448: 29, 449: 29, 450: 29, 451: 29, 452: 29, 453: 29, 454: 29, 455: 29, 456: 29, 457: 29, 458: 29, 459: 29, 460: 29, 461: 29, 462: 29, 463: 29, 464: 29, 465: 29, 466: 29, 467: 29, 468: 29, 469: 29, 470: 29, 471: 29, 472: 29, 473: 29, 474: 29, 475: 29, 476: 29, 477: 29, 478: 29, 479: 29, 480: 29, 481: 29, 482: 29, 483: 29, 484: 29, 485: 29, 486: 29, 487: 29, 488: 29, 489: 29, 490: 29, 491: 29, 492: 29, 493: 29, 494: 29, 495: 29, 496: 29, 497: 29, 498: 29, 499: 29, 500: 29, 501: 29, 502: 29, 503: 29, 504: 29, 505: 29, 506: 29, 507: 29, 508: 29, 509: 29, 510: 29, 511: 29, 512: 29, 513: 29, 514: 29, 515: 29, 516: 29, 517: 29, 518: 29, 519: 29, 520: 29, 521: 29, 522: 29, 523: 29, 524: 29, 525: 29, 526: 29, 527: 29, 528: 29, 529: 29, 530: 29, 531: 29, 532: 29, 533: 29, 534: 29, 535: 29, 536: 29, 537: 29, 538: 29, 539: 29, 540: 29, 541: 29, 542: 29, 543: 29, 544: 29, 545: 29, 546: 29, 547: 29, 548: 29, 549: 29, 550: 29, 551: 29, 552: 29, 553: 29, 554: 29, 555: 29, 556: 29, 557: 29, 558: 29, 559: 29, 560: 29, 561: 29, 562: 29, 563: 29, 564: 29, 565: 29, 566: 29, 567: 29, 568: 29, 569: 29, 570: 29, 571: 29, 572: 29, 573: 29, 574: 29, 575: 29, 576: 29, 577: 29, 578: 29, 579: 29, 580: 29, 581: 29, 582: 29, 583: 29, 584: 29, 585: 29, 586: 29, 587: 29, 588: 29, 589: 29, 590: 29, 591: 29, 592: 29, 593: 29, 594: 29, 595: 29, 596: 29, 597: 29, 598: 29, 599: 29, 600: 29, 601: 29, 602: 29, 603: 29, 604: 29, 605: 29, 606: 29, 607: 29, 608: 29, 609: 29, 610: 29, 611: 29, 612: 29, 613: 29, 614: 29, 615: 29, 616: 29, 617: 29, 618: 29, 619: 29, 620: 29, 621: 29, 622: 29, 623: 29, 624: 29, 625: 29, 626: 29, 627: 29, 628: 29, 629: 29, 630: 29, 631: 29, 632: 29, 633: 29, 634: 29, 635: 29, 636: 29, 637: 29, 638: 29, 639: 29, 640: 29, 641: 29, 642: 29, 643: 29, 644: 29, 645: 29, 646: 29, 647: 29, 648: 29, 649: 29, 650: 29, 651: 29, 652: 29, 653: 29, 654: 29, 655: 29, 656: 29, 657: 29, 658: 29, 659: 29, 660: 29, 661: 29, 662: 29, 663: 29, 664: 29, 665: 29, 666: 29, 667: 29, 668: 29, 669: 29, 670: 29, 671: 29, 672: 29, 673: 29, 674: 29, 675: 29, 676: 29, 677: 29, 678: 29, 679: 29, 680: 29, 681: 29, 682: 29, 683: 29, 684: 29, 685: 29, 686: 29, 687: 29, 688: 29, 689: 29, 690: 29, 691: 29, 692: 29, 693: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.8912	gate_loss: 0.9661	step2_classification_accuracy: 74.0833	step_2_gate_accuracy: 70.2226
STEP-2	Epoch: 40/200	classification_loss: 0.6650	gate_loss: 0.5833	step2_classification_accuracy: 80.0855	step_2_gate_accuracy: 80.4432
STEP-2	Epoch: 60/200	classification_loss: 0.5676	gate_loss: 0.4608	step2_classification_accuracy: 82.5251	step_2_gate_accuracy: 83.9660
STEP-2	Epoch: 80/200	classification_loss: 0.5120	gate_loss: 0.3988	step2_classification_accuracy: 84.0505	step_2_gate_accuracy: 85.9038
STEP-2	Epoch: 100/200	classification_loss: 0.4680	gate_loss: 0.3571	step2_classification_accuracy: 85.2281	step_2_gate_accuracy: 87.0565
STEP-2	Epoch: 120/200	classification_loss: 0.4402	gate_loss: 0.3285	step2_classification_accuracy: 85.8790	step_2_gate_accuracy: 88.1248
STEP-2	Epoch: 140/200	classification_loss: 0.4126	gate_loss: 0.3022	step2_classification_accuracy: 86.8330	step_2_gate_accuracy: 89.0838
STEP-2	Epoch: 160/200	classification_loss: 0.3952	gate_loss: 0.2868	step2_classification_accuracy: 87.4093	step_2_gate_accuracy: 89.7198
STEP-2	Epoch: 180/200	classification_loss: 0.3826	gate_loss: 0.2749	step2_classification_accuracy: 87.4689	step_2_gate_accuracy: 90.0179
STEP-2	Epoch: 200/200	classification_loss: 0.3851	gate_loss: 0.2752	step2_classification_accuracy: 87.5385	step_2_gate_accuracy: 89.7695
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 49.6124	gate_accuracy: 64.3411
	Task-1	val_accuracy: 68.2927	gate_accuracy: 71.9512
	Task-2	val_accuracy: 55.8824	gate_accuracy: 54.4118
	Task-3	val_accuracy: 61.4458	gate_accuracy: 65.0602
	Task-4	val_accuracy: 73.7500	gate_accuracy: 73.7500
	Task-5	val_accuracy: 77.3333	gate_accuracy: 77.3333
	Task-6	val_accuracy: 63.8889	gate_accuracy: 68.0556
	Task-7	val_accuracy: 72.4138	gate_accuracy: 67.8161
	Task-8	val_accuracy: 80.0000	gate_accuracy: 74.2857
	Task-9	val_accuracy: 71.6049	gate_accuracy: 67.9012
	Task-10	val_accuracy: 75.6098	gate_accuracy: 68.2927
	Task-11	val_accuracy: 74.4186	gate_accuracy: 66.2791
	Task-12	val_accuracy: 66.2500	gate_accuracy: 60.0000
	Task-13	val_accuracy: 65.4762	gate_accuracy: 59.5238
	Task-14	val_accuracy: 82.0513	gate_accuracy: 76.9231
	Task-15	val_accuracy: 61.7284	gate_accuracy: 59.2593
	Task-16	val_accuracy: 86.9048	gate_accuracy: 73.8095
	Task-17	val_accuracy: 78.2609	gate_accuracy: 81.1594
	Task-18	val_accuracy: 67.9487	gate_accuracy: 65.3846
	Task-19	val_accuracy: 64.1304	gate_accuracy: 63.0435
	Task-20	val_accuracy: 67.4419	gate_accuracy: 67.4419
	Task-21	val_accuracy: 63.0435	gate_accuracy: 70.6522
	Task-22	val_accuracy: 77.5281	gate_accuracy: 76.4045
	Task-23	val_accuracy: 52.7027	gate_accuracy: 43.2432
	Task-24	val_accuracy: 55.5556	gate_accuracy: 63.8889
	Task-25	val_accuracy: 57.3529	gate_accuracy: 50.0000
	Task-26	val_accuracy: 68.4783	gate_accuracy: 65.2174
	Task-27	val_accuracy: 70.9677	gate_accuracy: 73.1183
	Task-28	val_accuracy: 78.1250	gate_accuracy: 73.9583
	Task-29	val_accuracy: 69.5122	gate_accuracy: 76.8293
	Task-30	val_accuracy: 63.4409	gate_accuracy: 67.7419
	Task-31	val_accuracy: 73.4940	gate_accuracy: 73.4940
	Task-32	val_accuracy: 60.8108	gate_accuracy: 66.2162
	Task-33	val_accuracy: 69.7368	gate_accuracy: 69.7368
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.6628


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 2.8583	step1_train_accuracy: 55.6291
STEP-1	Epoch: 20/50	loss: 0.8467	step1_train_accuracy: 82.4503
STEP-1	Epoch: 30/50	loss: 0.5171	step1_train_accuracy: 89.4040
STEP-1	Epoch: 40/50	loss: 0.3510	step1_train_accuracy: 91.7219
STEP-1	Epoch: 50/50	loss: 0.2618	step1_train_accuracy: 95.0331
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.8449	gate_loss: 0.8713	step2_classification_accuracy: 75.1875	step_2_gate_accuracy: 73.0053
STEP-2	Epoch: 40/200	classification_loss: 0.6432	gate_loss: 0.5519	step2_classification_accuracy: 80.2476	step_2_gate_accuracy: 81.0427
STEP-2	Epoch: 60/200	classification_loss: 0.5454	gate_loss: 0.4412	step2_classification_accuracy: 82.9990	step_2_gate_accuracy: 84.4176
STEP-2	Epoch: 80/200	classification_loss: 0.4864	gate_loss: 0.3783	step2_classification_accuracy: 84.3860	step_2_gate_accuracy: 86.4778
STEP-2	Epoch: 100/200	classification_loss: 0.4508	gate_loss: 0.3412	step2_classification_accuracy: 85.5155	step_2_gate_accuracy: 87.8784
STEP-2	Epoch: 120/200	classification_loss: 0.4195	gate_loss: 0.3131	step2_classification_accuracy: 86.3152	step_2_gate_accuracy: 88.6871
STEP-2	Epoch: 140/200	classification_loss: 0.3982	gate_loss: 0.2942	step2_classification_accuracy: 86.8573	step_2_gate_accuracy: 89.3693
STEP-2	Epoch: 160/200	classification_loss: 0.3824	gate_loss: 0.2779	step2_classification_accuracy: 87.5802	step_2_gate_accuracy: 90.0696
STEP-2	Epoch: 180/200	classification_loss: 0.3703	gate_loss: 0.2652	step2_classification_accuracy: 87.7699	step_2_gate_accuracy: 90.1373
STEP-2	Epoch: 200/200	classification_loss: 0.3597	gate_loss: 0.2578	step2_classification_accuracy: 87.8829	step_2_gate_accuracy: 90.6524
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.1628	gate_accuracy: 65.8915
	Task-1	val_accuracy: 75.6098	gate_accuracy: 76.8293
	Task-2	val_accuracy: 50.0000	gate_accuracy: 57.3529
	Task-3	val_accuracy: 68.6747	gate_accuracy: 63.8554
	Task-4	val_accuracy: 80.0000	gate_accuracy: 78.7500
	Task-5	val_accuracy: 74.6667	gate_accuracy: 73.3333
	Task-6	val_accuracy: 65.2778	gate_accuracy: 68.0556
	Task-7	val_accuracy: 71.2644	gate_accuracy: 65.5172
	Task-8	val_accuracy: 80.0000	gate_accuracy: 81.4286
	Task-9	val_accuracy: 76.5432	gate_accuracy: 72.8395
	Task-10	val_accuracy: 74.3902	gate_accuracy: 69.5122
	Task-11	val_accuracy: 70.9302	gate_accuracy: 59.3023
	Task-12	val_accuracy: 62.5000	gate_accuracy: 56.2500
	Task-13	val_accuracy: 71.4286	gate_accuracy: 67.8571
	Task-14	val_accuracy: 76.9231	gate_accuracy: 74.3590
	Task-15	val_accuracy: 70.3704	gate_accuracy: 62.9630
	Task-16	val_accuracy: 76.1905	gate_accuracy: 67.8571
	Task-17	val_accuracy: 72.4638	gate_accuracy: 72.4638
	Task-18	val_accuracy: 70.5128	gate_accuracy: 67.9487
	Task-19	val_accuracy: 66.3043	gate_accuracy: 67.3913
	Task-20	val_accuracy: 56.9767	gate_accuracy: 68.6047
	Task-21	val_accuracy: 65.2174	gate_accuracy: 67.3913
	Task-22	val_accuracy: 76.4045	gate_accuracy: 73.0337
	Task-23	val_accuracy: 54.0541	gate_accuracy: 54.0541
	Task-24	val_accuracy: 59.7222	gate_accuracy: 59.7222
	Task-25	val_accuracy: 60.2941	gate_accuracy: 57.3529
	Task-26	val_accuracy: 73.9130	gate_accuracy: 68.4783
	Task-27	val_accuracy: 70.9677	gate_accuracy: 70.9677
	Task-28	val_accuracy: 82.2917	gate_accuracy: 79.1667
	Task-29	val_accuracy: 68.2927	gate_accuracy: 76.8293
	Task-30	val_accuracy: 62.3656	gate_accuracy: 61.2903
	Task-31	val_accuracy: 68.6747	gate_accuracy: 69.8795
	Task-32	val_accuracy: 67.5676	gate_accuracy: 72.9730
	Task-33	val_accuracy: 78.9474	gate_accuracy: 78.9474
	Task-34	val_accuracy: 82.8947	gate_accuracy: 88.1579
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.0336


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.9282	step1_train_accuracy: 54.1379
STEP-1	Epoch: 20/50	loss: 0.9906	step1_train_accuracy: 86.5517
STEP-1	Epoch: 30/50	loss: 0.5618	step1_train_accuracy: 93.1034
STEP-1	Epoch: 40/50	loss: 0.3605	step1_train_accuracy: 96.5517
STEP-1	Epoch: 50/50	loss: 0.2547	step1_train_accuracy: 98.6207
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29, 374: 29, 375: 29, 376: 29, 377: 29, 378: 29, 379: 29, 380: 29, 381: 29, 382: 29, 383: 29, 384: 29, 385: 29, 386: 29, 387: 29, 388: 29, 389: 29, 390: 29, 391: 29, 392: 29, 393: 29, 394: 29, 395: 29, 396: 29, 397: 29, 398: 29, 399: 29, 400: 29, 401: 29, 402: 29, 403: 29, 404: 29, 405: 29, 406: 29, 407: 29, 408: 29, 409: 29, 410: 29, 411: 29, 412: 29, 413: 29, 414: 29, 415: 29, 416: 29, 417: 29, 418: 29, 419: 29, 420: 29, 421: 29, 422: 29, 423: 29, 424: 29, 425: 29, 426: 29, 427: 29, 428: 29, 429: 29, 430: 29, 431: 29, 432: 29, 433: 29, 434: 29, 435: 29, 436: 29, 437: 29, 438: 29, 439: 29, 440: 29, 441: 29, 442: 29, 443: 29, 444: 29, 445: 29, 446: 29, 447: 29, 448: 29, 449: 29, 450: 29, 451: 29, 452: 29, 453: 29, 454: 29, 455: 29, 456: 29, 457: 29, 458: 29, 459: 29, 460: 29, 461: 29, 462: 29, 463: 29, 464: 29, 465: 29, 466: 29, 467: 29, 468: 29, 469: 29, 470: 29, 471: 29, 472: 29, 473: 29, 474: 29, 475: 29, 476: 29, 477: 29, 478: 29, 479: 29, 480: 29, 481: 29, 482: 29, 483: 29, 484: 29, 485: 29, 486: 29, 487: 29, 488: 29, 489: 29, 490: 29, 491: 29, 492: 29, 493: 29, 494: 29, 495: 29, 496: 29, 497: 29, 498: 29, 499: 29, 500: 29, 501: 29, 502: 29, 503: 29, 504: 29, 505: 29, 506: 29, 507: 29, 508: 29, 509: 29, 510: 29, 511: 29, 512: 29, 513: 29, 514: 29, 515: 29, 516: 29, 517: 29, 518: 29, 519: 29, 520: 29, 521: 29, 522: 29, 523: 29, 524: 29, 525: 29, 526: 29, 527: 29, 528: 29, 529: 29, 530: 29, 531: 29, 532: 29, 533: 29, 534: 29, 535: 29, 536: 29, 537: 29, 538: 29, 539: 29, 540: 29, 541: 29, 542: 29, 543: 29, 544: 29, 545: 29, 546: 29, 547: 29, 548: 29, 549: 29, 550: 29, 551: 29, 552: 29, 553: 29, 554: 29, 555: 29, 556: 29, 557: 29, 558: 29, 559: 29, 560: 29, 561: 29, 562: 29, 563: 29, 564: 29, 565: 29, 566: 29, 567: 29, 568: 29, 569: 29, 570: 29, 571: 29, 572: 29, 573: 29, 574: 29, 575: 29, 576: 29, 577: 29, 578: 29, 579: 29, 580: 29, 581: 29, 582: 29, 583: 29, 584: 29, 585: 29, 586: 29, 587: 29, 588: 29, 589: 29, 590: 29, 591: 29, 592: 29, 593: 29, 594: 29, 595: 29, 596: 29, 597: 29, 598: 29, 599: 29, 600: 29, 601: 29, 602: 29, 603: 29, 604: 29, 605: 29, 606: 29, 607: 29, 608: 29, 609: 29, 610: 29, 611: 29, 612: 29, 613: 29, 614: 29, 615: 29, 616: 29, 617: 29, 618: 29, 619: 29, 620: 29, 621: 29, 622: 29, 623: 29, 624: 29, 625: 29, 626: 29, 627: 29, 628: 29, 629: 29, 630: 29, 631: 29, 632: 29, 633: 29, 634: 29, 635: 29, 636: 29, 637: 29, 638: 29, 639: 29, 640: 29, 641: 29, 642: 29, 643: 29, 644: 29, 645: 29, 646: 29, 647: 29, 648: 29, 649: 29, 650: 29, 651: 29, 652: 29, 653: 29, 654: 29, 655: 29, 656: 29, 657: 29, 658: 29, 659: 29, 660: 29, 661: 29, 662: 29, 663: 29, 664: 29, 665: 29, 666: 29, 667: 29, 668: 29, 669: 29, 670: 29, 671: 29, 672: 29, 673: 29, 674: 29, 675: 29, 676: 29, 677: 29, 678: 29, 679: 29, 680: 29, 681: 29, 682: 29, 683: 29, 684: 29, 685: 29, 686: 29, 687: 29, 688: 29, 689: 29, 690: 29, 691: 29, 692: 29, 693: 29, 694: 29, 695: 29, 696: 29, 697: 29, 698: 29, 699: 29, 700: 29, 701: 29, 702: 29, 703: 29, 704: 29, 705: 29, 706: 29, 707: 29, 708: 29, 709: 29, 710: 29, 711: 29, 712: 29, 713: 29, 714: 29, 715: 29, 716: 29, 717: 29, 718: 29, 719: 29, 720: 29, 721: 29, 722: 29, 723: 29, 724: 29, 725: 29, 726: 29, 727: 29, 728: 29, 729: 29, 730: 29, 731: 29, 732: 29, 733: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.8856	gate_loss: 0.9505	step2_classification_accuracy: 74.1520	step_2_gate_accuracy: 70.8259
STEP-2	Epoch: 40/200	classification_loss: 0.6840	gate_loss: 0.5971	step2_classification_accuracy: 79.3573	step_2_gate_accuracy: 80.0996
STEP-2	Epoch: 60/200	classification_loss: 0.5798	gate_loss: 0.4767	step2_classification_accuracy: 81.8049	step_2_gate_accuracy: 83.3881
STEP-2	Epoch: 80/200	classification_loss: 0.5209	gate_loss: 0.4127	step2_classification_accuracy: 83.5338	step_2_gate_accuracy: 85.4646
STEP-2	Epoch: 100/200	classification_loss: 0.4698	gate_loss: 0.3645	step2_classification_accuracy: 84.9572	step_2_gate_accuracy: 86.9351
STEP-2	Epoch: 120/200	classification_loss: 0.4407	gate_loss: 0.3353	step2_classification_accuracy: 85.6431	step_2_gate_accuracy: 88.1049
STEP-2	Epoch: 140/200	classification_loss: 0.4218	gate_loss: 0.3184	step2_classification_accuracy: 86.0237	step_2_gate_accuracy: 88.5136
STEP-2	Epoch: 160/200	classification_loss: 0.4021	gate_loss: 0.2970	step2_classification_accuracy: 86.8176	step_2_gate_accuracy: 89.3310
STEP-2	Epoch: 180/200	classification_loss: 0.3871	gate_loss: 0.2836	step2_classification_accuracy: 87.1982	step_2_gate_accuracy: 89.8431
STEP-2	Epoch: 200/200	classification_loss: 0.3812	gate_loss: 0.2791	step2_classification_accuracy: 87.4049	step_2_gate_accuracy: 89.8666
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.4884	gate_accuracy: 72.8682
	Task-1	val_accuracy: 69.5122	gate_accuracy: 71.9512
	Task-2	val_accuracy: 39.7059	gate_accuracy: 44.1176
	Task-3	val_accuracy: 68.6747	gate_accuracy: 65.0602
	Task-4	val_accuracy: 75.0000	gate_accuracy: 68.7500
	Task-5	val_accuracy: 77.3333	gate_accuracy: 66.6667
	Task-6	val_accuracy: 63.8889	gate_accuracy: 65.2778
	Task-7	val_accuracy: 71.2644	gate_accuracy: 63.2184
	Task-8	val_accuracy: 72.8571	gate_accuracy: 72.8571
	Task-9	val_accuracy: 75.3086	gate_accuracy: 72.8395
	Task-10	val_accuracy: 71.9512	gate_accuracy: 65.8537
	Task-11	val_accuracy: 74.4186	gate_accuracy: 62.7907
	Task-12	val_accuracy: 57.5000	gate_accuracy: 50.0000
	Task-13	val_accuracy: 67.8571	gate_accuracy: 64.2857
	Task-14	val_accuracy: 75.6410	gate_accuracy: 74.3590
	Task-15	val_accuracy: 69.1358	gate_accuracy: 60.4938
	Task-16	val_accuracy: 79.7619	gate_accuracy: 70.2381
	Task-17	val_accuracy: 79.7101	gate_accuracy: 81.1594
	Task-18	val_accuracy: 70.5128	gate_accuracy: 67.9487
	Task-19	val_accuracy: 59.7826	gate_accuracy: 61.9565
	Task-20	val_accuracy: 69.7674	gate_accuracy: 68.6047
	Task-21	val_accuracy: 63.0435	gate_accuracy: 64.1304
	Task-22	val_accuracy: 71.9101	gate_accuracy: 69.6629
	Task-23	val_accuracy: 60.8108	gate_accuracy: 59.4595
	Task-24	val_accuracy: 61.1111	gate_accuracy: 61.1111
	Task-25	val_accuracy: 51.4706	gate_accuracy: 50.0000
	Task-26	val_accuracy: 66.3043	gate_accuracy: 66.3043
	Task-27	val_accuracy: 69.8925	gate_accuracy: 74.1935
	Task-28	val_accuracy: 81.2500	gate_accuracy: 78.1250
	Task-29	val_accuracy: 70.7317	gate_accuracy: 70.7317
	Task-30	val_accuracy: 67.7419	gate_accuracy: 75.2688
	Task-31	val_accuracy: 63.8554	gate_accuracy: 63.8554
	Task-32	val_accuracy: 66.2162	gate_accuracy: 67.5676
	Task-33	val_accuracy: 73.6842	gate_accuracy: 71.0526
	Task-34	val_accuracy: 80.2632	gate_accuracy: 85.5263
	Task-35	val_accuracy: 72.2222	gate_accuracy: 75.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.5566


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 2.0619	step1_train_accuracy: 50.9589
STEP-1	Epoch: 20/50	loss: 0.8550	step1_train_accuracy: 78.3562
STEP-1	Epoch: 30/50	loss: 0.5349	step1_train_accuracy: 89.0411
STEP-1	Epoch: 40/50	loss: 0.3757	step1_train_accuracy: 94.7945
STEP-1	Epoch: 50/50	loss: 0.2648	step1_train_accuracy: 96.4384
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33, 714: 33, 715: 33, 716: 33, 717: 33, 718: 33, 719: 33, 720: 33, 721: 33, 722: 33, 723: 33, 724: 33, 725: 33, 726: 33, 727: 33, 728: 33, 729: 33, 730: 33, 731: 33, 732: 33, 733: 33, 734: 33, 735: 33, 736: 33, 737: 33, 738: 33, 739: 33, 740: 33, 741: 33, 742: 33, 743: 33, 744: 33, 745: 33, 746: 33, 747: 33, 748: 33, 749: 33, 750: 33, 751: 33, 752: 33, 753: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.8651	gate_loss: 0.8687	step2_classification_accuracy: 74.9216	step_2_gate_accuracy: 72.7996
STEP-2	Epoch: 40/200	classification_loss: 0.6650	gate_loss: 0.5711	step2_classification_accuracy: 80.2186	step_2_gate_accuracy: 80.8657
STEP-2	Epoch: 60/200	classification_loss: 0.5729	gate_loss: 0.4658	step2_classification_accuracy: 82.0915	step_2_gate_accuracy: 83.5383
STEP-2	Epoch: 80/200	classification_loss: 0.5105	gate_loss: 0.4024	step2_classification_accuracy: 83.9281	step_2_gate_accuracy: 85.7447
STEP-2	Epoch: 100/200	classification_loss: 0.4738	gate_loss: 0.3624	step2_classification_accuracy: 84.9128	step_2_gate_accuracy: 86.8178
STEP-2	Epoch: 120/200	classification_loss: 0.4528	gate_loss: 0.3433	step2_classification_accuracy: 85.5197	step_2_gate_accuracy: 87.7462
STEP-2	Epoch: 140/200	classification_loss: 0.4250	gate_loss: 0.3161	step2_classification_accuracy: 86.1145	step_2_gate_accuracy: 88.6102
STEP-2	Epoch: 160/200	classification_loss: 0.4076	gate_loss: 0.3017	step2_classification_accuracy: 86.5565	step_2_gate_accuracy: 88.8755
STEP-2	Epoch: 180/200	classification_loss: 0.3896	gate_loss: 0.2870	step2_classification_accuracy: 87.0911	step_2_gate_accuracy: 89.3337
STEP-2	Epoch: 200/200	classification_loss: 0.3867	gate_loss: 0.2813	step2_classification_accuracy: 87.3282	step_2_gate_accuracy: 89.6391
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 45.7364	gate_accuracy: 71.3178
	Task-1	val_accuracy: 73.1707	gate_accuracy: 80.4878
	Task-2	val_accuracy: 44.1176	gate_accuracy: 51.4706
	Task-3	val_accuracy: 73.4940	gate_accuracy: 73.4940
	Task-4	val_accuracy: 87.5000	gate_accuracy: 83.7500
	Task-5	val_accuracy: 66.6667	gate_accuracy: 64.0000
	Task-6	val_accuracy: 61.1111	gate_accuracy: 58.3333
	Task-7	val_accuracy: 65.5172	gate_accuracy: 59.7701
	Task-8	val_accuracy: 80.0000	gate_accuracy: 78.5714
	Task-9	val_accuracy: 71.6049	gate_accuracy: 69.1358
	Task-10	val_accuracy: 68.2927	gate_accuracy: 60.9756
	Task-11	val_accuracy: 52.3256	gate_accuracy: 50.0000
	Task-12	val_accuracy: 65.0000	gate_accuracy: 58.7500
	Task-13	val_accuracy: 69.0476	gate_accuracy: 66.6667
	Task-14	val_accuracy: 62.8205	gate_accuracy: 61.5385
	Task-15	val_accuracy: 65.4321	gate_accuracy: 56.7901
	Task-16	val_accuracy: 82.1429	gate_accuracy: 72.6190
	Task-17	val_accuracy: 62.3188	gate_accuracy: 63.7681
	Task-18	val_accuracy: 71.7949	gate_accuracy: 65.3846
	Task-19	val_accuracy: 58.6957	gate_accuracy: 58.6957
	Task-20	val_accuracy: 56.9767	gate_accuracy: 60.4651
	Task-21	val_accuracy: 68.4783	gate_accuracy: 65.2174
	Task-22	val_accuracy: 73.0337	gate_accuracy: 70.7865
	Task-23	val_accuracy: 55.4054	gate_accuracy: 54.0541
	Task-24	val_accuracy: 59.7222	gate_accuracy: 59.7222
	Task-25	val_accuracy: 57.3529	gate_accuracy: 55.8824
	Task-26	val_accuracy: 65.2174	gate_accuracy: 64.1304
	Task-27	val_accuracy: 69.8925	gate_accuracy: 70.9677
	Task-28	val_accuracy: 72.9167	gate_accuracy: 70.8333
	Task-29	val_accuracy: 65.8537	gate_accuracy: 70.7317
	Task-30	val_accuracy: 59.1398	gate_accuracy: 60.2151
	Task-31	val_accuracy: 59.0361	gate_accuracy: 61.4458
	Task-32	val_accuracy: 55.4054	gate_accuracy: 54.0541
	Task-33	val_accuracy: 77.6316	gate_accuracy: 81.5789
	Task-34	val_accuracy: 73.6842	gate_accuracy: 80.2632
	Task-35	val_accuracy: 54.1667	gate_accuracy: 55.5556
	Task-36	val_accuracy: 46.1538	gate_accuracy: 50.5495
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.8197


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.9187	step1_train_accuracy: 49.3671
STEP-1	Epoch: 20/50	loss: 1.0229	step1_train_accuracy: 79.1139
STEP-1	Epoch: 30/50	loss: 0.4399	step1_train_accuracy: 96.8354
STEP-1	Epoch: 40/50	loss: 0.2680	step1_train_accuracy: 98.1013
STEP-1	Epoch: 50/50	loss: 0.1854	step1_train_accuracy: 98.4177
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32, 734: 32, 735: 32, 736: 32, 737: 32, 738: 32, 739: 32, 740: 32, 741: 32, 742: 32, 743: 32, 744: 32, 745: 32, 746: 32, 747: 32, 748: 32, 749: 32, 750: 32, 751: 32, 752: 32, 753: 32, 754: 32, 755: 32, 756: 32, 757: 32, 758: 32, 759: 32, 760: 32, 761: 32, 762: 32, 763: 32, 764: 32, 765: 32, 766: 32, 767: 32, 768: 32, 769: 32, 770: 32, 771: 32, 772: 32, 773: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.9145	gate_loss: 0.9498	step2_classification_accuracy: 73.5950	step_2_gate_accuracy: 70.7364
STEP-2	Epoch: 40/200	classification_loss: 0.6926	gate_loss: 0.6057	step2_classification_accuracy: 79.6068	step_2_gate_accuracy: 79.8934
STEP-2	Epoch: 60/200	classification_loss: 0.6038	gate_loss: 0.4955	step2_classification_accuracy: 81.6941	step_2_gate_accuracy: 82.7681
STEP-2	Epoch: 80/200	classification_loss: 0.5469	gate_loss: 0.4360	step2_classification_accuracy: 83.0749	step_2_gate_accuracy: 84.7505
STEP-2	Epoch: 100/200	classification_loss: 0.4997	gate_loss: 0.3904	step2_classification_accuracy: 84.4840	step_2_gate_accuracy: 86.2201
STEP-2	Epoch: 120/200	classification_loss: 0.4747	gate_loss: 0.3636	step2_classification_accuracy: 85.1785	step_2_gate_accuracy: 87.0438
STEP-2	Epoch: 140/200	classification_loss: 0.4433	gate_loss: 0.3345	step2_classification_accuracy: 85.8285	step_2_gate_accuracy: 87.8916
STEP-2	Epoch: 160/200	classification_loss: 0.4694	gate_loss: 0.3631	step2_classification_accuracy: 85.0250	step_2_gate_accuracy: 86.7410
STEP-2	Epoch: 180/200	classification_loss: 0.4094	gate_loss: 0.3073	step2_classification_accuracy: 86.7934	step_2_gate_accuracy: 88.9737
STEP-2	Epoch: 200/200	classification_loss: 0.3963	gate_loss: 0.2949	step2_classification_accuracy: 87.2456	step_2_gate_accuracy: 89.4299
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 49.6124	gate_accuracy: 69.7674
	Task-1	val_accuracy: 64.6341	gate_accuracy: 65.8537
	Task-2	val_accuracy: 48.5294	gate_accuracy: 55.8824
	Task-3	val_accuracy: 65.0602	gate_accuracy: 59.0361
	Task-4	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-5	val_accuracy: 76.0000	gate_accuracy: 69.3333
	Task-6	val_accuracy: 63.8889	gate_accuracy: 62.5000
	Task-7	val_accuracy: 71.2644	gate_accuracy: 67.8161
	Task-8	val_accuracy: 71.4286	gate_accuracy: 68.5714
	Task-9	val_accuracy: 76.5432	gate_accuracy: 72.8395
	Task-10	val_accuracy: 75.6098	gate_accuracy: 68.2927
	Task-11	val_accuracy: 72.0930	gate_accuracy: 66.2791
	Task-12	val_accuracy: 63.7500	gate_accuracy: 56.2500
	Task-13	val_accuracy: 64.2857	gate_accuracy: 59.5238
	Task-14	val_accuracy: 70.5128	gate_accuracy: 69.2308
	Task-15	val_accuracy: 66.6667	gate_accuracy: 58.0247
	Task-16	val_accuracy: 75.0000	gate_accuracy: 69.0476
	Task-17	val_accuracy: 72.4638	gate_accuracy: 75.3623
	Task-18	val_accuracy: 69.2308	gate_accuracy: 70.5128
	Task-19	val_accuracy: 65.2174	gate_accuracy: 65.2174
	Task-20	val_accuracy: 61.6279	gate_accuracy: 63.9535
	Task-21	val_accuracy: 69.5652	gate_accuracy: 68.4783
	Task-22	val_accuracy: 82.0225	gate_accuracy: 83.1461
	Task-23	val_accuracy: 59.4595	gate_accuracy: 52.7027
	Task-24	val_accuracy: 58.3333	gate_accuracy: 58.3333
	Task-25	val_accuracy: 54.4118	gate_accuracy: 48.5294
	Task-26	val_accuracy: 75.0000	gate_accuracy: 71.7391
	Task-27	val_accuracy: 70.9677	gate_accuracy: 70.9677
	Task-28	val_accuracy: 77.0833	gate_accuracy: 75.0000
	Task-29	val_accuracy: 68.2927	gate_accuracy: 73.1707
	Task-30	val_accuracy: 58.0645	gate_accuracy: 62.3656
	Task-31	val_accuracy: 55.4217	gate_accuracy: 55.4217
	Task-32	val_accuracy: 63.5135	gate_accuracy: 64.8649
	Task-33	val_accuracy: 73.6842	gate_accuracy: 71.0526
	Task-34	val_accuracy: 72.3684	gate_accuracy: 82.8947
	Task-35	val_accuracy: 62.5000	gate_accuracy: 68.0556
	Task-36	val_accuracy: 64.8352	gate_accuracy: 76.9231
	Task-37	val_accuracy: 54.4304	gate_accuracy: 64.5570
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.9863


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 2.2932	step1_train_accuracy: 56.3452
STEP-1	Epoch: 20/50	loss: 0.7836	step1_train_accuracy: 82.7411
STEP-1	Epoch: 30/50	loss: 0.4111	step1_train_accuracy: 94.6701
STEP-1	Epoch: 40/50	loss: 0.2693	step1_train_accuracy: 96.9543
STEP-1	Epoch: 50/50	loss: 0.1935	step1_train_accuracy: 97.9695
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9301	gate_loss: 0.9581	step2_classification_accuracy: 73.4298	step_2_gate_accuracy: 70.4558
STEP-2	Epoch: 40/200	classification_loss: 0.7080	gate_loss: 0.6193	step2_classification_accuracy: 79.0363	step_2_gate_accuracy: 79.3613
STEP-2	Epoch: 60/200	classification_loss: 0.6058	gate_loss: 0.4988	step2_classification_accuracy: 81.7462	step_2_gate_accuracy: 83.0137
STEP-2	Epoch: 80/200	classification_loss: 0.5441	gate_loss: 0.4346	step2_classification_accuracy: 83.2656	step_2_gate_accuracy: 84.9923
STEP-2	Epoch: 100/200	classification_loss: 0.5012	gate_loss: 0.3930	step2_classification_accuracy: 84.4072	step_2_gate_accuracy: 86.1623
STEP-2	Epoch: 120/200	classification_loss: 0.4707	gate_loss: 0.3634	step2_classification_accuracy: 85.0898	step_2_gate_accuracy: 86.8327
STEP-2	Epoch: 140/200	classification_loss: 0.4573	gate_loss: 0.3530	step2_classification_accuracy: 85.5204	step_2_gate_accuracy: 87.5477
STEP-2	Epoch: 160/200	classification_loss: 0.4268	gate_loss: 0.3229	step2_classification_accuracy: 86.3980	step_2_gate_accuracy: 88.6040
STEP-2	Epoch: 180/200	classification_loss: 0.4151	gate_loss: 0.3147	step2_classification_accuracy: 86.6336	step_2_gate_accuracy: 88.4578
STEP-2	Epoch: 200/200	classification_loss: 0.3903	gate_loss: 0.2908	step2_classification_accuracy: 87.1293	step_2_gate_accuracy: 89.4369
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.3876	gate_accuracy: 71.3178
	Task-1	val_accuracy: 70.7317	gate_accuracy: 69.5122
	Task-2	val_accuracy: 50.0000	gate_accuracy: 51.4706
	Task-3	val_accuracy: 68.6747	gate_accuracy: 62.6506
	Task-4	val_accuracy: 76.2500	gate_accuracy: 71.2500
	Task-5	val_accuracy: 77.3333	gate_accuracy: 74.6667
	Task-6	val_accuracy: 59.7222	gate_accuracy: 59.7222
	Task-7	val_accuracy: 77.0115	gate_accuracy: 73.5632
	Task-8	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-9	val_accuracy: 70.3704	gate_accuracy: 71.6049
	Task-10	val_accuracy: 68.2927	gate_accuracy: 67.0732
	Task-11	val_accuracy: 62.7907	gate_accuracy: 55.8140
	Task-12	val_accuracy: 67.5000	gate_accuracy: 60.0000
	Task-13	val_accuracy: 66.6667	gate_accuracy: 61.9048
	Task-14	val_accuracy: 74.3590	gate_accuracy: 70.5128
	Task-15	val_accuracy: 72.8395	gate_accuracy: 67.9012
	Task-16	val_accuracy: 78.5714	gate_accuracy: 64.2857
	Task-17	val_accuracy: 73.9130	gate_accuracy: 76.8116
	Task-18	val_accuracy: 73.0769	gate_accuracy: 71.7949
	Task-19	val_accuracy: 65.2174	gate_accuracy: 61.9565
	Task-20	val_accuracy: 55.8140	gate_accuracy: 55.8140
	Task-21	val_accuracy: 68.4783	gate_accuracy: 70.6522
	Task-22	val_accuracy: 75.2809	gate_accuracy: 75.2809
	Task-23	val_accuracy: 55.4054	gate_accuracy: 50.0000
	Task-24	val_accuracy: 61.1111	gate_accuracy: 63.8889
	Task-25	val_accuracy: 51.4706	gate_accuracy: 45.5882
	Task-26	val_accuracy: 72.8261	gate_accuracy: 69.5652
	Task-27	val_accuracy: 70.9677	gate_accuracy: 70.9677
	Task-28	val_accuracy: 75.0000	gate_accuracy: 72.9167
	Task-29	val_accuracy: 64.6341	gate_accuracy: 74.3902
	Task-30	val_accuracy: 61.2903	gate_accuracy: 63.4409
	Task-31	val_accuracy: 65.0602	gate_accuracy: 62.6506
	Task-32	val_accuracy: 63.5135	gate_accuracy: 63.5135
	Task-33	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-34	val_accuracy: 73.6842	gate_accuracy: 78.9474
	Task-35	val_accuracy: 56.9444	gate_accuracy: 62.5000
	Task-36	val_accuracy: 61.5385	gate_accuracy: 72.5275
	Task-37	val_accuracy: 56.9620	gate_accuracy: 58.2278
	Task-38	val_accuracy: 65.3061	gate_accuracy: 69.3878
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.8733


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 2.4529	step1_train_accuracy: 45.0746
STEP-1	Epoch: 20/50	loss: 0.9818	step1_train_accuracy: 80.5970
STEP-1	Epoch: 30/50	loss: 0.5187	step1_train_accuracy: 93.1343
STEP-1	Epoch: 40/50	loss: 0.3348	step1_train_accuracy: 95.2239
STEP-1	Epoch: 50/50	loss: 0.2453	step1_train_accuracy: 96.7164
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32, 734: 32, 735: 32, 736: 32, 737: 32, 738: 32, 739: 32, 740: 32, 741: 32, 742: 32, 743: 32, 744: 32, 745: 32, 746: 32, 747: 32, 748: 32, 749: 32, 750: 32, 751: 32, 752: 32, 753: 32, 754: 32, 755: 32, 756: 32, 757: 32, 758: 32, 759: 32, 760: 32, 761: 32, 762: 32, 763: 32, 764: 32, 765: 32, 766: 32, 767: 32, 768: 32, 769: 32, 770: 32, 771: 32, 772: 32, 773: 32, 774: 32, 775: 32, 776: 32, 777: 32, 778: 32, 779: 32, 780: 32, 781: 32, 782: 32, 783: 32, 784: 32, 785: 32, 786: 32, 787: 32, 788: 32, 789: 32, 790: 32, 791: 32, 792: 32, 793: 32, 794: 32, 795: 32, 796: 32, 797: 32, 798: 32, 799: 32, 800: 32, 801: 32, 802: 32, 803: 32, 804: 32, 805: 32, 806: 32, 807: 32, 808: 32, 809: 32, 810: 32, 811: 32, 812: 32, 813: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.9557	gate_loss: 0.9795	step2_classification_accuracy: 72.9077	step_2_gate_accuracy: 70.0591
STEP-2	Epoch: 40/200	classification_loss: 0.7368	gate_loss: 0.6352	step2_classification_accuracy: 78.4244	step_2_gate_accuracy: 79.2767
STEP-2	Epoch: 60/200	classification_loss: 0.6205	gate_loss: 0.5078	step2_classification_accuracy: 81.0081	step_2_gate_accuracy: 82.3672
STEP-2	Epoch: 80/200	classification_loss: 0.5539	gate_loss: 0.4407	step2_classification_accuracy: 82.6321	step_2_gate_accuracy: 84.5170
STEP-2	Epoch: 100/200	classification_loss: 0.5106	gate_loss: 0.3982	step2_classification_accuracy: 83.8375	step_2_gate_accuracy: 85.9145
STEP-2	Epoch: 120/200	classification_loss: 0.4907	gate_loss: 0.3783	step2_classification_accuracy: 84.4518	step_2_gate_accuracy: 86.4097
STEP-2	Epoch: 140/200	classification_loss: 0.4602	gate_loss: 0.3507	step2_classification_accuracy: 85.3885	step_2_gate_accuracy: 87.4347
STEP-2	Epoch: 160/200	classification_loss: 0.4379	gate_loss: 0.3287	step2_classification_accuracy: 85.7955	step_2_gate_accuracy: 88.2102
STEP-2	Epoch: 180/200	classification_loss: 0.4297	gate_loss: 0.3187	step2_classification_accuracy: 86.0719	step_2_gate_accuracy: 88.4828
STEP-2	Epoch: 200/200	classification_loss: 0.4229	gate_loss: 0.3170	step2_classification_accuracy: 86.1947	step_2_gate_accuracy: 88.3983
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.1628	gate_accuracy: 66.6667
	Task-1	val_accuracy: 65.8537	gate_accuracy: 70.7317
	Task-2	val_accuracy: 45.5882	gate_accuracy: 54.4118
	Task-3	val_accuracy: 68.6747	gate_accuracy: 66.2651
	Task-4	val_accuracy: 81.2500	gate_accuracy: 76.2500
	Task-5	val_accuracy: 73.3333	gate_accuracy: 70.6667
	Task-6	val_accuracy: 62.5000	gate_accuracy: 65.2778
	Task-7	val_accuracy: 78.1609	gate_accuracy: 70.1149
	Task-8	val_accuracy: 78.5714	gate_accuracy: 72.8571
	Task-9	val_accuracy: 71.6049	gate_accuracy: 71.6049
	Task-10	val_accuracy: 74.3902	gate_accuracy: 71.9512
	Task-11	val_accuracy: 73.2558	gate_accuracy: 67.4419
	Task-12	val_accuracy: 61.2500	gate_accuracy: 55.0000
	Task-13	val_accuracy: 76.1905	gate_accuracy: 65.4762
	Task-14	val_accuracy: 78.2051	gate_accuracy: 78.2051
	Task-15	val_accuracy: 65.4321	gate_accuracy: 58.0247
	Task-16	val_accuracy: 78.5714	gate_accuracy: 69.0476
	Task-17	val_accuracy: 69.5652	gate_accuracy: 75.3623
	Task-18	val_accuracy: 69.2308	gate_accuracy: 66.6667
	Task-19	val_accuracy: 66.3043	gate_accuracy: 69.5652
	Task-20	val_accuracy: 55.8140	gate_accuracy: 51.1628
	Task-21	val_accuracy: 71.7391	gate_accuracy: 71.7391
	Task-22	val_accuracy: 76.4045	gate_accuracy: 76.4045
	Task-23	val_accuracy: 64.8649	gate_accuracy: 59.4595
	Task-24	val_accuracy: 54.1667	gate_accuracy: 56.9444
	Task-25	val_accuracy: 63.2353	gate_accuracy: 52.9412
	Task-26	val_accuracy: 67.3913	gate_accuracy: 67.3913
	Task-27	val_accuracy: 68.8172	gate_accuracy: 72.0430
	Task-28	val_accuracy: 81.2500	gate_accuracy: 77.0833
	Task-29	val_accuracy: 71.9512	gate_accuracy: 75.6098
	Task-30	val_accuracy: 61.2903	gate_accuracy: 66.6667
	Task-31	val_accuracy: 61.4458	gate_accuracy: 63.8554
	Task-32	val_accuracy: 67.5676	gate_accuracy: 68.9189
	Task-33	val_accuracy: 72.3684	gate_accuracy: 73.6842
	Task-34	val_accuracy: 64.4737	gate_accuracy: 72.3684
	Task-35	val_accuracy: 63.8889	gate_accuracy: 68.0556
	Task-36	val_accuracy: 61.5385	gate_accuracy: 68.1319
	Task-37	val_accuracy: 58.2278	gate_accuracy: 62.0253
	Task-38	val_accuracy: 76.5306	gate_accuracy: 76.5306
	Task-39	val_accuracy: 72.6190	gate_accuracy: 76.1905
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.1667


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 3.3861	step1_train_accuracy: 34.4828
STEP-1	Epoch: 20/50	loss: 1.0423	step1_train_accuracy: 82.4138
STEP-1	Epoch: 30/50	loss: 0.4623	step1_train_accuracy: 95.8621
STEP-1	Epoch: 40/50	loss: 0.2833	step1_train_accuracy: 98.2759
STEP-1	Epoch: 50/50	loss: 0.1957	step1_train_accuracy: 98.9655
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31, 794: 31, 795: 31, 796: 31, 797: 31, 798: 31, 799: 31, 800: 31, 801: 31, 802: 31, 803: 31, 804: 31, 805: 31, 806: 31, 807: 31, 808: 31, 809: 31, 810: 31, 811: 31, 812: 31, 813: 31, 814: 31, 815: 31, 816: 31, 817: 31, 818: 31, 819: 31, 820: 31, 821: 31, 822: 31, 823: 31, 824: 31, 825: 31, 826: 31, 827: 31, 828: 31, 829: 31, 830: 31, 831: 31, 832: 31, 833: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9638	gate_loss: 1.0028	step2_classification_accuracy: 72.7547	step_2_gate_accuracy: 69.4515
STEP-2	Epoch: 40/200	classification_loss: 0.7494	gate_loss: 0.6610	step2_classification_accuracy: 78.3167	step_2_gate_accuracy: 78.1813
STEP-2	Epoch: 60/200	classification_loss: 0.6462	gate_loss: 0.5351	step2_classification_accuracy: 80.8502	step_2_gate_accuracy: 81.7978
STEP-2	Epoch: 80/200	classification_loss: 0.5740	gate_loss: 0.4612	step2_classification_accuracy: 82.6797	step_2_gate_accuracy: 83.9522
STEP-2	Epoch: 100/200	classification_loss: 0.5317	gate_loss: 0.4157	step2_classification_accuracy: 83.5925	step_2_gate_accuracy: 85.3214
STEP-2	Epoch: 120/200	classification_loss: 0.4992	gate_loss: 0.3855	step2_classification_accuracy: 84.6097	step_2_gate_accuracy: 86.2420
STEP-2	Epoch: 140/200	classification_loss: 0.4788	gate_loss: 0.3657	step2_classification_accuracy: 85.1126	step_2_gate_accuracy: 87.0504
STEP-2	Epoch: 160/200	classification_loss: 0.4561	gate_loss: 0.3434	step2_classification_accuracy: 85.7585	step_2_gate_accuracy: 87.9284
STEP-2	Epoch: 180/200	classification_loss: 0.4404	gate_loss: 0.3319	step2_classification_accuracy: 86.0292	step_2_gate_accuracy: 88.1218
STEP-2	Epoch: 200/200	classification_loss: 0.4340	gate_loss: 0.3256	step2_classification_accuracy: 86.2265	step_2_gate_accuracy: 88.4583
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.3876	gate_accuracy: 67.4419
	Task-1	val_accuracy: 63.4146	gate_accuracy: 64.6341
	Task-2	val_accuracy: 47.0588	gate_accuracy: 52.9412
	Task-3	val_accuracy: 68.6747	gate_accuracy: 71.0843
	Task-4	val_accuracy: 71.2500	gate_accuracy: 68.7500
	Task-5	val_accuracy: 74.6667	gate_accuracy: 76.0000
	Task-6	val_accuracy: 66.6667	gate_accuracy: 63.8889
	Task-7	val_accuracy: 72.4138	gate_accuracy: 68.9655
	Task-8	val_accuracy: 75.7143	gate_accuracy: 68.5714
	Task-9	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-10	val_accuracy: 75.6098	gate_accuracy: 63.4146
	Task-11	val_accuracy: 69.7674	gate_accuracy: 61.6279
	Task-12	val_accuracy: 60.0000	gate_accuracy: 51.2500
	Task-13	val_accuracy: 72.6190	gate_accuracy: 61.9048
	Task-14	val_accuracy: 71.7949	gate_accuracy: 66.6667
	Task-15	val_accuracy: 71.6049	gate_accuracy: 65.4321
	Task-16	val_accuracy: 77.3810	gate_accuracy: 63.0952
	Task-17	val_accuracy: 72.4638	gate_accuracy: 71.0145
	Task-18	val_accuracy: 67.9487	gate_accuracy: 62.8205
	Task-19	val_accuracy: 60.8696	gate_accuracy: 65.2174
	Task-20	val_accuracy: 61.6279	gate_accuracy: 58.1395
	Task-21	val_accuracy: 63.0435	gate_accuracy: 65.2174
	Task-22	val_accuracy: 64.0449	gate_accuracy: 60.6742
	Task-23	val_accuracy: 52.7027	gate_accuracy: 44.5946
	Task-24	val_accuracy: 56.9444	gate_accuracy: 56.9444
	Task-25	val_accuracy: 58.8235	gate_accuracy: 57.3529
	Task-26	val_accuracy: 72.8261	gate_accuracy: 69.5652
	Task-27	val_accuracy: 73.1183	gate_accuracy: 72.0430
	Task-28	val_accuracy: 78.1250	gate_accuracy: 75.0000
	Task-29	val_accuracy: 67.0732	gate_accuracy: 70.7317
	Task-30	val_accuracy: 53.7634	gate_accuracy: 53.7634
	Task-31	val_accuracy: 59.0361	gate_accuracy: 60.2410
	Task-32	val_accuracy: 60.8108	gate_accuracy: 63.5135
	Task-33	val_accuracy: 85.5263	gate_accuracy: 85.5263
	Task-34	val_accuracy: 77.6316	gate_accuracy: 85.5263
	Task-35	val_accuracy: 61.1111	gate_accuracy: 63.8889
	Task-36	val_accuracy: 65.9341	gate_accuracy: 75.8242
	Task-37	val_accuracy: 60.7595	gate_accuracy: 69.6203
	Task-38	val_accuracy: 70.4082	gate_accuracy: 68.3673
	Task-39	val_accuracy: 73.8095	gate_accuracy: 83.3333
	Task-40	val_accuracy: 56.9444	gate_accuracy: 61.1111
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.1543


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 2.5336	step1_train_accuracy: 42.5150
STEP-1	Epoch: 20/50	loss: 0.9876	step1_train_accuracy: 86.8263
STEP-1	Epoch: 30/50	loss: 0.5102	step1_train_accuracy: 91.0180
STEP-1	Epoch: 40/50	loss: 0.3332	step1_train_accuracy: 93.7126
STEP-1	Epoch: 50/50	loss: 0.2493	step1_train_accuracy: 93.4132
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33, 714: 33, 715: 33, 716: 33, 717: 33, 718: 33, 719: 33, 720: 33, 721: 33, 722: 33, 723: 33, 724: 33, 725: 33, 726: 33, 727: 33, 728: 33, 729: 33, 730: 33, 731: 33, 732: 33, 733: 33, 734: 33, 735: 33, 736: 33, 737: 33, 738: 33, 739: 33, 740: 33, 741: 33, 742: 33, 743: 33, 744: 33, 745: 33, 746: 33, 747: 33, 748: 33, 749: 33, 750: 33, 751: 33, 752: 33, 753: 33, 754: 33, 755: 33, 756: 33, 757: 33, 758: 33, 759: 33, 760: 33, 761: 33, 762: 33, 763: 33, 764: 33, 765: 33, 766: 33, 767: 33, 768: 33, 769: 33, 770: 33, 771: 33, 772: 33, 773: 33, 774: 33, 775: 33, 776: 33, 777: 33, 778: 33, 779: 33, 780: 33, 781: 33, 782: 33, 783: 33, 784: 33, 785: 33, 786: 33, 787: 33, 788: 33, 789: 33, 790: 33, 791: 33, 792: 33, 793: 33, 794: 33, 795: 33, 796: 33, 797: 33, 798: 33, 799: 33, 800: 33, 801: 33, 802: 33, 803: 33, 804: 33, 805: 33, 806: 33, 807: 33, 808: 33, 809: 33, 810: 33, 811: 33, 812: 33, 813: 33, 814: 33, 815: 33, 816: 33, 817: 33, 818: 33, 819: 33, 820: 33, 821: 33, 822: 33, 823: 33, 824: 33, 825: 33, 826: 33, 827: 33, 828: 33, 829: 33, 830: 33, 831: 33, 832: 33, 833: 33, 834: 33, 835: 33, 836: 33, 837: 33, 838: 33, 839: 33, 840: 33, 841: 33, 842: 33, 843: 33, 844: 33, 845: 33, 846: 33, 847: 33, 848: 33, 849: 33, 850: 33, 851: 33, 852: 33, 853: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.9805	gate_loss: 1.0080	step2_classification_accuracy: 72.3511	step_2_gate_accuracy: 68.7070
STEP-2	Epoch: 40/200	classification_loss: 0.7603	gate_loss: 0.6663	step2_classification_accuracy: 77.8156	step_2_gate_accuracy: 78.1421
STEP-2	Epoch: 60/200	classification_loss: 0.6507	gate_loss: 0.5413	step2_classification_accuracy: 80.5017	step_2_gate_accuracy: 81.6230
STEP-2	Epoch: 80/200	classification_loss: 0.5872	gate_loss: 0.4770	step2_classification_accuracy: 82.0453	step_2_gate_accuracy: 83.4256
STEP-2	Epoch: 100/200	classification_loss: 0.5459	gate_loss: 0.4353	step2_classification_accuracy: 83.3262	step_2_gate_accuracy: 85.0224
STEP-2	Epoch: 120/200	classification_loss: 0.5243	gate_loss: 0.4173	step2_classification_accuracy: 83.7024	step_2_gate_accuracy: 85.3027
STEP-2	Epoch: 140/200	classification_loss: 0.4839	gate_loss: 0.3774	step2_classification_accuracy: 84.6427	step_2_gate_accuracy: 86.6653
STEP-2	Epoch: 160/200	classification_loss: 0.4648	gate_loss: 0.3617	step2_classification_accuracy: 85.1820	step_2_gate_accuracy: 87.3004
STEP-2	Epoch: 180/200	classification_loss: 0.4463	gate_loss: 0.3406	step2_classification_accuracy: 85.7711	step_2_gate_accuracy: 87.9001
STEP-2	Epoch: 200/200	classification_loss: 0.4298	gate_loss: 0.3258	step2_classification_accuracy: 86.2359	step_2_gate_accuracy: 88.4394
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.4884	gate_accuracy: 71.3178
	Task-1	val_accuracy: 68.2927	gate_accuracy: 74.3902
	Task-2	val_accuracy: 44.1176	gate_accuracy: 51.4706
	Task-3	val_accuracy: 67.4699	gate_accuracy: 66.2651
	Task-4	val_accuracy: 66.2500	gate_accuracy: 66.2500
	Task-5	val_accuracy: 76.0000	gate_accuracy: 70.6667
	Task-6	val_accuracy: 56.9444	gate_accuracy: 55.5556
	Task-7	val_accuracy: 74.7126	gate_accuracy: 62.0690
	Task-8	val_accuracy: 72.8571	gate_accuracy: 70.0000
	Task-9	val_accuracy: 71.6049	gate_accuracy: 69.1358
	Task-10	val_accuracy: 73.1707	gate_accuracy: 68.2927
	Task-11	val_accuracy: 73.2558	gate_accuracy: 61.6279
	Task-12	val_accuracy: 65.0000	gate_accuracy: 62.5000
	Task-13	val_accuracy: 70.2381	gate_accuracy: 59.5238
	Task-14	val_accuracy: 78.2051	gate_accuracy: 74.3590
	Task-15	val_accuracy: 67.9012	gate_accuracy: 59.2593
	Task-16	val_accuracy: 64.2857	gate_accuracy: 63.0952
	Task-17	val_accuracy: 71.0145	gate_accuracy: 68.1159
	Task-18	val_accuracy: 75.6410	gate_accuracy: 76.9231
	Task-19	val_accuracy: 57.6087	gate_accuracy: 51.0870
	Task-20	val_accuracy: 65.1163	gate_accuracy: 63.9535
	Task-21	val_accuracy: 65.2174	gate_accuracy: 56.5217
	Task-22	val_accuracy: 73.0337	gate_accuracy: 68.5393
	Task-23	val_accuracy: 55.4054	gate_accuracy: 50.0000
	Task-24	val_accuracy: 62.5000	gate_accuracy: 59.7222
	Task-25	val_accuracy: 48.5294	gate_accuracy: 48.5294
	Task-26	val_accuracy: 73.9130	gate_accuracy: 66.3043
	Task-27	val_accuracy: 66.6667	gate_accuracy: 72.0430
	Task-28	val_accuracy: 79.1667	gate_accuracy: 75.0000
	Task-29	val_accuracy: 63.4146	gate_accuracy: 71.9512
	Task-30	val_accuracy: 55.9140	gate_accuracy: 56.9892
	Task-31	val_accuracy: 59.0361	gate_accuracy: 57.8313
	Task-32	val_accuracy: 67.5676	gate_accuracy: 64.8649
	Task-33	val_accuracy: 78.9474	gate_accuracy: 72.3684
	Task-34	val_accuracy: 68.4211	gate_accuracy: 76.3158
	Task-35	val_accuracy: 61.1111	gate_accuracy: 68.0556
	Task-36	val_accuracy: 50.5495	gate_accuracy: 56.0440
	Task-37	val_accuracy: 59.4937	gate_accuracy: 65.8228
	Task-38	val_accuracy: 72.4490	gate_accuracy: 74.4898
	Task-39	val_accuracy: 60.7143	gate_accuracy: 71.4286
	Task-40	val_accuracy: 55.5556	gate_accuracy: 56.9444
	Task-41	val_accuracy: 63.8554	gate_accuracy: 71.0843
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.1183


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 2.2217	step1_train_accuracy: 55.1320
STEP-1	Epoch: 20/50	loss: 0.7198	step1_train_accuracy: 84.4575
STEP-1	Epoch: 30/50	loss: 0.3811	step1_train_accuracy: 95.8944
STEP-1	Epoch: 40/50	loss: 0.2774	step1_train_accuracy: 96.7742
STEP-1	Epoch: 50/50	loss: 0.1858	step1_train_accuracy: 98.5337
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32, 734: 32, 735: 32, 736: 32, 737: 32, 738: 32, 739: 32, 740: 32, 741: 32, 742: 32, 743: 32, 744: 32, 745: 32, 746: 32, 747: 32, 748: 32, 749: 32, 750: 32, 751: 32, 752: 32, 753: 32, 754: 32, 755: 32, 756: 32, 757: 32, 758: 32, 759: 32, 760: 32, 761: 32, 762: 32, 763: 32, 764: 32, 765: 32, 766: 32, 767: 32, 768: 32, 769: 32, 770: 32, 771: 32, 772: 32, 773: 32, 774: 32, 775: 32, 776: 32, 777: 32, 778: 32, 779: 32, 780: 32, 781: 32, 782: 32, 783: 32, 784: 32, 785: 32, 786: 32, 787: 32, 788: 32, 789: 32, 790: 32, 791: 32, 792: 32, 793: 32, 794: 32, 795: 32, 796: 32, 797: 32, 798: 32, 799: 32, 800: 32, 801: 32, 802: 32, 803: 32, 804: 32, 805: 32, 806: 32, 807: 32, 808: 32, 809: 32, 810: 32, 811: 32, 812: 32, 813: 32, 814: 32, 815: 32, 816: 32, 817: 32, 818: 32, 819: 32, 820: 32, 821: 32, 822: 32, 823: 32, 824: 32, 825: 32, 826: 32, 827: 32, 828: 32, 829: 32, 830: 32, 831: 32, 832: 32, 833: 32, 834: 32, 835: 32, 836: 32, 837: 32, 838: 32, 839: 32, 840: 32, 841: 32, 842: 32, 843: 32, 844: 32, 845: 32, 846: 32, 847: 32, 848: 32, 849: 32, 850: 32, 851: 32, 852: 32, 853: 32, 854: 32, 855: 32, 856: 32, 857: 32, 858: 32, 859: 32, 860: 32, 861: 32, 862: 32, 863: 32, 864: 32, 865: 32, 866: 32, 867: 32, 868: 32, 869: 32, 870: 32, 871: 32, 872: 32, 873: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.9895	gate_loss: 1.0200	step2_classification_accuracy: 72.3899	step_2_gate_accuracy: 69.0253
STEP-2	Epoch: 40/200	classification_loss: 0.7769	gate_loss: 0.6814	step2_classification_accuracy: 77.6495	step_2_gate_accuracy: 77.9534
STEP-2	Epoch: 60/200	classification_loss: 0.6681	gate_loss: 0.5566	step2_classification_accuracy: 80.4098	step_2_gate_accuracy: 81.3680
STEP-2	Epoch: 80/200	classification_loss: 0.6021	gate_loss: 0.4857	step2_classification_accuracy: 81.9079	step_2_gate_accuracy: 83.4346
STEP-2	Epoch: 100/200	classification_loss: 0.5530	gate_loss: 0.4384	step2_classification_accuracy: 83.0485	step_2_gate_accuracy: 84.9435
STEP-2	Epoch: 120/200	classification_loss: 0.5224	gate_loss: 0.4053	step2_classification_accuracy: 83.9066	step_2_gate_accuracy: 86.1127
STEP-2	Epoch: 140/200	classification_loss: 0.4991	gate_loss: 0.3847	step2_classification_accuracy: 84.6217	step_2_gate_accuracy: 86.6455
STEP-2	Epoch: 160/200	classification_loss: 0.4783	gate_loss: 0.3659	step2_classification_accuracy: 85.0365	step_2_gate_accuracy: 87.1103
STEP-2	Epoch: 180/200	classification_loss: 0.4550	gate_loss: 0.3460	step2_classification_accuracy: 85.6622	step_2_gate_accuracy: 87.6824
STEP-2	Epoch: 200/200	classification_loss: 0.4502	gate_loss: 0.3407	step2_classification_accuracy: 85.7730	step_2_gate_accuracy: 87.9720
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.9380	gate_accuracy: 72.0930
	Task-1	val_accuracy: 68.2927	gate_accuracy: 68.2927
	Task-2	val_accuracy: 48.5294	gate_accuracy: 51.4706
	Task-3	val_accuracy: 71.0843	gate_accuracy: 63.8554
	Task-4	val_accuracy: 76.2500	gate_accuracy: 71.2500
	Task-5	val_accuracy: 78.6667	gate_accuracy: 77.3333
	Task-6	val_accuracy: 50.0000	gate_accuracy: 50.0000
	Task-7	val_accuracy: 62.0690	gate_accuracy: 50.5747
	Task-8	val_accuracy: 71.4286	gate_accuracy: 64.2857
	Task-9	val_accuracy: 76.5432	gate_accuracy: 74.0741
	Task-10	val_accuracy: 80.4878	gate_accuracy: 68.2927
	Task-11	val_accuracy: 70.9302	gate_accuracy: 62.7907
	Task-12	val_accuracy: 65.0000	gate_accuracy: 48.7500
	Task-13	val_accuracy: 66.6667	gate_accuracy: 55.9524
	Task-14	val_accuracy: 74.3590	gate_accuracy: 69.2308
	Task-15	val_accuracy: 67.9012	gate_accuracy: 62.9630
	Task-16	val_accuracy: 76.1905	gate_accuracy: 64.2857
	Task-17	val_accuracy: 66.6667	gate_accuracy: 63.7681
	Task-18	val_accuracy: 73.0769	gate_accuracy: 67.9487
	Task-19	val_accuracy: 67.3913	gate_accuracy: 66.3043
	Task-20	val_accuracy: 54.6512	gate_accuracy: 52.3256
	Task-21	val_accuracy: 67.3913	gate_accuracy: 68.4783
	Task-22	val_accuracy: 77.5281	gate_accuracy: 77.5281
	Task-23	val_accuracy: 51.3514	gate_accuracy: 45.9459
	Task-24	val_accuracy: 59.7222	gate_accuracy: 55.5556
	Task-25	val_accuracy: 41.1765	gate_accuracy: 35.2941
	Task-26	val_accuracy: 66.3043	gate_accuracy: 59.7826
	Task-27	val_accuracy: 73.1183	gate_accuracy: 73.1183
	Task-28	val_accuracy: 80.2083	gate_accuracy: 75.0000
	Task-29	val_accuracy: 60.9756	gate_accuracy: 60.9756
	Task-30	val_accuracy: 63.4409	gate_accuracy: 64.5161
	Task-31	val_accuracy: 63.8554	gate_accuracy: 65.0602
	Task-32	val_accuracy: 58.1081	gate_accuracy: 60.8108
	Task-33	val_accuracy: 81.5789	gate_accuracy: 86.8421
	Task-34	val_accuracy: 60.5263	gate_accuracy: 72.3684
	Task-35	val_accuracy: 62.5000	gate_accuracy: 72.2222
	Task-36	val_accuracy: 58.2418	gate_accuracy: 63.7363
	Task-37	val_accuracy: 62.0253	gate_accuracy: 68.3544
	Task-38	val_accuracy: 75.5102	gate_accuracy: 80.6122
	Task-39	val_accuracy: 75.0000	gate_accuracy: 80.9524
	Task-40	val_accuracy: 52.7778	gate_accuracy: 55.5556
	Task-41	val_accuracy: 60.2410	gate_accuracy: 68.6747
	Task-42	val_accuracy: 76.4706	gate_accuracy: 78.8235
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.4745


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 2.7486	step1_train_accuracy: 48.1481
STEP-1	Epoch: 20/50	loss: 1.0970	step1_train_accuracy: 80.2469
STEP-1	Epoch: 30/50	loss: 0.5899	step1_train_accuracy: 91.0494
STEP-1	Epoch: 40/50	loss: 0.3846	step1_train_accuracy: 95.0617
STEP-1	Epoch: 50/50	loss: 0.2706	step1_train_accuracy: 96.2963
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31, 794: 31, 795: 31, 796: 31, 797: 31, 798: 31, 799: 31, 800: 31, 801: 31, 802: 31, 803: 31, 804: 31, 805: 31, 806: 31, 807: 31, 808: 31, 809: 31, 810: 31, 811: 31, 812: 31, 813: 31, 814: 31, 815: 31, 816: 31, 817: 31, 818: 31, 819: 31, 820: 31, 821: 31, 822: 31, 823: 31, 824: 31, 825: 31, 826: 31, 827: 31, 828: 31, 829: 31, 830: 31, 831: 31, 832: 31, 833: 31, 834: 31, 835: 31, 836: 31, 837: 31, 838: 31, 839: 31, 840: 31, 841: 31, 842: 31, 843: 31, 844: 31, 845: 31, 846: 31, 847: 31, 848: 31, 849: 31, 850: 31, 851: 31, 852: 31, 853: 31, 854: 31, 855: 31, 856: 31, 857: 31, 858: 31, 859: 31, 860: 31, 861: 31, 862: 31, 863: 31, 864: 31, 865: 31, 866: 31, 867: 31, 868: 31, 869: 31, 870: 31, 871: 31, 872: 31, 873: 31, 874: 31, 875: 31, 876: 31, 877: 31, 878: 31, 879: 31, 880: 31, 881: 31, 882: 31, 883: 31, 884: 31, 885: 31, 886: 31, 887: 31, 888: 31, 889: 31, 890: 31, 891: 31, 892: 31, 893: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9983	gate_loss: 1.0425	step2_classification_accuracy: 71.7543	step_2_gate_accuracy: 68.2218
STEP-2	Epoch: 40/200	classification_loss: 0.7783	gate_loss: 0.6852	step2_classification_accuracy: 77.4518	step_2_gate_accuracy: 77.5637
STEP-2	Epoch: 60/200	classification_loss: 0.6657	gate_loss: 0.5542	step2_classification_accuracy: 80.4792	step_2_gate_accuracy: 81.1431
STEP-2	Epoch: 80/200	classification_loss: 0.5962	gate_loss: 0.4788	step2_classification_accuracy: 82.0596	step_2_gate_accuracy: 83.4091
STEP-2	Epoch: 100/200	classification_loss: 0.5528	gate_loss: 0.4396	step2_classification_accuracy: 83.1168	step_2_gate_accuracy: 84.6576
STEP-2	Epoch: 120/200	classification_loss: 0.5213	gate_loss: 0.4054	step2_classification_accuracy: 83.9864	step_2_gate_accuracy: 85.7978
STEP-2	Epoch: 140/200	classification_loss: 0.4901	gate_loss: 0.3748	step2_classification_accuracy: 84.7514	step_2_gate_accuracy: 86.5844
STEP-2	Epoch: 160/200	classification_loss: 0.4740	gate_loss: 0.3632	step2_classification_accuracy: 85.0509	step_2_gate_accuracy: 87.0932
STEP-2	Epoch: 180/200	classification_loss: 0.4594	gate_loss: 0.3463	step2_classification_accuracy: 85.6029	step_2_gate_accuracy: 87.8762
STEP-2	Epoch: 200/200	classification_loss: 0.4475	gate_loss: 0.3373	step2_classification_accuracy: 85.8303	step_2_gate_accuracy: 87.9592
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 44.9612	gate_accuracy: 61.2403
	Task-1	val_accuracy: 69.5122	gate_accuracy: 70.7317
	Task-2	val_accuracy: 51.4706	gate_accuracy: 60.2941
	Task-3	val_accuracy: 66.2651	gate_accuracy: 62.6506
	Task-4	val_accuracy: 55.0000	gate_accuracy: 42.5000
	Task-5	val_accuracy: 80.0000	gate_accuracy: 74.6667
	Task-6	val_accuracy: 68.0556	gate_accuracy: 62.5000
	Task-7	val_accuracy: 71.2644	gate_accuracy: 70.1149
	Task-8	val_accuracy: 84.2857	gate_accuracy: 72.8571
	Task-9	val_accuracy: 72.8395	gate_accuracy: 69.1358
	Task-10	val_accuracy: 73.1707	gate_accuracy: 67.0732
	Task-11	val_accuracy: 69.7674	gate_accuracy: 59.3023
	Task-12	val_accuracy: 66.2500	gate_accuracy: 58.7500
	Task-13	val_accuracy: 69.0476	gate_accuracy: 61.9048
	Task-14	val_accuracy: 58.9744	gate_accuracy: 58.9744
	Task-15	val_accuracy: 61.7284	gate_accuracy: 53.0864
	Task-16	val_accuracy: 86.9048	gate_accuracy: 76.1905
	Task-17	val_accuracy: 71.0145	gate_accuracy: 69.5652
	Task-18	val_accuracy: 70.5128	gate_accuracy: 64.1026
	Task-19	val_accuracy: 57.6087	gate_accuracy: 56.5217
	Task-20	val_accuracy: 50.0000	gate_accuracy: 41.8605
	Task-21	val_accuracy: 66.3043	gate_accuracy: 70.6522
	Task-22	val_accuracy: 74.1573	gate_accuracy: 69.6629
	Task-23	val_accuracy: 67.5676	gate_accuracy: 58.1081
	Task-24	val_accuracy: 58.3333	gate_accuracy: 59.7222
	Task-25	val_accuracy: 54.4118	gate_accuracy: 45.5882
	Task-26	val_accuracy: 69.5652	gate_accuracy: 65.2174
	Task-27	val_accuracy: 74.1935	gate_accuracy: 77.4194
	Task-28	val_accuracy: 68.7500	gate_accuracy: 67.7083
	Task-29	val_accuracy: 64.6341	gate_accuracy: 67.0732
	Task-30	val_accuracy: 69.8925	gate_accuracy: 69.8925
	Task-31	val_accuracy: 57.8313	gate_accuracy: 61.4458
	Task-32	val_accuracy: 56.7568	gate_accuracy: 56.7568
	Task-33	val_accuracy: 81.5789	gate_accuracy: 78.9474
	Task-34	val_accuracy: 69.7368	gate_accuracy: 76.3158
	Task-35	val_accuracy: 62.5000	gate_accuracy: 66.6667
	Task-36	val_accuracy: 63.7363	gate_accuracy: 78.0220
	Task-37	val_accuracy: 65.8228	gate_accuracy: 65.8228
	Task-38	val_accuracy: 70.4082	gate_accuracy: 73.4694
	Task-39	val_accuracy: 70.2381	gate_accuracy: 76.1905
	Task-40	val_accuracy: 52.7778	gate_accuracy: 59.7222
	Task-41	val_accuracy: 56.6265	gate_accuracy: 66.2651
	Task-42	val_accuracy: 65.8824	gate_accuracy: 76.4706
	Task-43	val_accuracy: 62.9630	gate_accuracy: 62.9630
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.2533


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 20:26:31	Wall time: 19:38:12
CPU time: 73591.634007631	Wall time: 70692.0551404953
FAA: 73.90737652442338
FF: 31.752685106649547

TRAINER.METRIC.ACCURACY
0: [77.51937984496125]
1: [76.74418604651163, 89.02439024390245]
2: [70.54263565891473, 85.36585365853658, 79.41176470588235]
3: [69.76744186046511, 85.36585365853658, 69.11764705882352, 84.33734939759037]
4: [66.66666666666666, 81.70731707317073, 66.17647058823529, 79.51807228915662, 95.0]
5: [62.01550387596899, 81.70731707317073, 79.41176470588235, 72.28915662650603, 93.75, 86.66666666666667]
6: [62.7906976744186, 82.92682926829268, 67.64705882352942, 74.69879518072288, 90.0, 86.66666666666667, 84.72222222222221]
7: [59.68992248062015, 78.04878048780488, 67.64705882352942, 78.3132530120482, 87.5, 82.66666666666667, 88.88888888888889, 89.65517241379311]
8: [60.46511627906976, 82.92682926829268, 67.64705882352942, 77.10843373493977, 87.5, 82.66666666666667, 87.5, 86.20689655172413, 85.71428571428571]
9: [59.68992248062015, 76.82926829268293, 69.11764705882352, 78.3132530120482, 88.75, 85.33333333333334, 86.11111111111111, 85.0574712643678, 81.42857142857143, 85.18518518518519]
10: [58.91472868217055, 79.26829268292683, 63.23529411764706, 77.10843373493977, 87.5, 85.33333333333334, 81.94444444444444, 85.0574712643678, 82.85714285714286, 86.41975308641975, 85.36585365853658]
11: [59.68992248062015, 79.26829268292683, 64.70588235294117, 77.10843373493977, 87.5, 81.33333333333333, 84.72222222222221, 81.60919540229885, 85.71428571428571, 87.65432098765432, 87.8048780487805, 80.23255813953489]
12: [58.139534883720934, 71.95121951219512, 55.88235294117647, 79.51807228915662, 80.0, 84.0, 83.33333333333334, 82.75862068965517, 88.57142857142857, 83.9506172839506, 81.70731707317073, 83.72093023255815, 73.75]
13: [57.36434108527132, 76.82926829268293, 58.82352941176471, 72.28915662650603, 77.5, 82.66666666666667, 81.94444444444444, 80.45977011494253, 84.28571428571429, 82.71604938271605, 81.70731707317073, 79.06976744186046, 71.25, 83.33333333333334]
14: [55.81395348837209, 70.73170731707317, 60.29411764705882, 73.49397590361446, 76.25, 85.33333333333334, 70.83333333333334, 77.01149425287356, 84.28571428571429, 80.24691358024691, 82.92682926829268, 75.5813953488372, 73.75, 70.23809523809523, 84.61538461538461]
15: [55.03875968992248, 75.60975609756098, 60.29411764705882, 72.28915662650603, 80.0, 78.66666666666666, 70.83333333333334, 81.60919540229885, 78.57142857142857, 85.18518518518519, 81.70731707317073, 80.23255813953489, 63.74999999999999, 76.19047619047619, 80.76923076923077, 79.01234567901234]
16: [55.03875968992248, 75.60975609756098, 63.23529411764706, 67.46987951807229, 82.5, 84.0, 72.22222222222221, 80.45977011494253, 82.85714285714286, 77.77777777777779, 89.02439024390245, 74.4186046511628, 71.25, 72.61904761904762, 83.33333333333334, 76.5432098765432, 85.71428571428571]
17: [54.263565891472865, 74.39024390243902, 54.41176470588235, 78.3132530120482, 80.0, 84.0, 75.0, 66.66666666666666, 81.42857142857143, 75.30864197530865, 79.26829268292683, 76.74418604651163, 72.5, 78.57142857142857, 80.76923076923077, 77.77777777777779, 79.76190476190477, 75.36231884057972]
18: [56.58914728682171, 73.17073170731707, 47.05882352941176, 75.90361445783132, 81.25, 81.33333333333333, 70.83333333333334, 71.26436781609196, 85.71428571428571, 80.24691358024691, 76.82926829268293, 79.06976744186046, 68.75, 67.85714285714286, 78.2051282051282, 77.77777777777779, 82.14285714285714, 78.26086956521739, 70.51282051282051]
19: [55.03875968992248, 80.48780487804879, 50.0, 68.67469879518072, 77.5, 78.66666666666666, 73.61111111111111, 72.41379310344827, 85.71428571428571, 82.71604938271605, 76.82926829268293, 80.23255813953489, 68.75, 75.0, 78.2051282051282, 71.60493827160494, 86.90476190476191, 76.81159420289855, 76.92307692307693, 73.91304347826086]
20: [56.58914728682171, 71.95121951219512, 51.470588235294116, 75.90361445783132, 76.25, 78.66666666666666, 68.05555555555556, 79.3103448275862, 75.71428571428571, 76.5432098765432, 79.26829268292683, 83.72093023255815, 66.25, 73.80952380952381, 79.48717948717949, 72.8395061728395, 82.14285714285714, 78.26086956521739, 78.2051282051282, 70.65217391304348, 59.30232558139535]
21: [54.263565891472865, 68.29268292682927, 57.35294117647059, 72.28915662650603, 80.0, 80.0, 70.83333333333334, 78.16091954022988, 80.0, 81.48148148148148, 82.92682926829268, 80.23255813953489, 66.25, 66.66666666666666, 82.05128205128204, 64.19753086419753, 83.33333333333334, 76.81159420289855, 79.48717948717949, 67.3913043478261, 65.11627906976744, 64.13043478260869]
22: [51.93798449612403, 76.82926829268293, 54.41176470588235, 71.08433734939759, 75.0, 78.66666666666666, 68.05555555555556, 79.3103448275862, 77.14285714285715, 77.77777777777779, 76.82926829268293, 79.06976744186046, 68.75, 73.80952380952381, 76.92307692307693, 69.1358024691358, 78.57142857142857, 75.36231884057972, 74.35897435897436, 71.73913043478261, 63.95348837209303, 70.65217391304348, 83.14606741573034]
23: [53.48837209302325, 69.51219512195121, 50.0, 68.67469879518072, 76.25, 73.33333333333333, 69.44444444444444, 73.5632183908046, 85.71428571428571, 76.5432098765432, 73.17073170731707, 70.93023255813954, 70.0, 69.04761904761905, 80.76923076923077, 72.8395061728395, 77.38095238095238, 75.36231884057972, 73.07692307692307, 66.30434782608695, 62.7906976744186, 65.21739130434783, 79.7752808988764, 55.4054054054054]
24: [49.6124031007752, 74.39024390243902, 42.64705882352941, 72.28915662650603, 78.75, 78.66666666666666, 66.66666666666666, 78.16091954022988, 80.0, 77.77777777777779, 76.82926829268293, 73.25581395348837, 68.75, 67.85714285714286, 82.05128205128204, 75.30864197530865, 77.38095238095238, 76.81159420289855, 75.64102564102564, 67.3913043478261, 59.30232558139535, 65.21739130434783, 83.14606741573034, 51.35135135135135, 63.888888888888886]
25: [49.6124031007752, 57.3170731707317, 50.0, 69.87951807228916, 77.5, 81.33333333333333, 68.05555555555556, 74.71264367816092, 84.28571428571429, 80.24691358024691, 81.70731707317073, 77.90697674418605, 58.75, 79.76190476190477, 83.33333333333334, 71.60493827160494, 79.76190476190477, 72.46376811594203, 67.94871794871796, 67.3913043478261, 51.162790697674424, 65.21739130434783, 83.14606741573034, 56.75675675675676, 56.94444444444444, 61.76470588235294]
26: [55.03875968992248, 68.29268292682927, 50.0, 71.08433734939759, 77.5, 78.66666666666666, 69.44444444444444, 74.71264367816092, 81.42857142857143, 76.5432098765432, 89.02439024390245, 76.74418604651163, 70.0, 67.85714285714286, 74.35897435897436, 60.49382716049383, 84.52380952380952, 78.26086956521739, 66.66666666666666, 70.65217391304348, 59.30232558139535, 67.3913043478261, 84.26966292134831, 67.56756756756756, 61.111111111111114, 64.70588235294117, 75.0]
27: [51.93798449612403, 70.73170731707317, 54.41176470588235, 65.06024096385542, 70.0, 78.66666666666666, 69.44444444444444, 80.45977011494253, 81.42857142857143, 74.07407407407408, 78.04878048780488, 77.90697674418605, 68.75, 69.04761904761905, 76.92307692307693, 69.1358024691358, 79.76190476190477, 73.91304347826086, 71.7948717948718, 64.13043478260869, 62.7906976744186, 65.21739130434783, 86.51685393258427, 56.75675675675676, 63.888888888888886, 61.76470588235294, 77.17391304347827, 75.26881720430107]
28: [51.162790697674424, 70.73170731707317, 52.94117647058824, 69.87951807228916, 77.5, 74.66666666666667, 68.05555555555556, 70.11494252873564, 81.42857142857143, 74.07407407407408, 82.92682926829268, 76.74418604651163, 66.25, 72.61904761904762, 78.2051282051282, 66.66666666666666, 83.33333333333334, 82.6086956521739, 69.23076923076923, 68.47826086956522, 55.81395348837209, 65.21739130434783, 84.26966292134831, 58.108108108108105, 59.72222222222222, 64.70588235294117, 76.08695652173914, 70.96774193548387, 83.33333333333334]
29: [49.6124031007752, 71.95121951219512, 52.94117647058824, 69.87951807228916, 77.5, 81.33333333333333, 69.44444444444444, 72.41379310344827, 81.42857142857143, 75.30864197530865, 70.73170731707317, 75.5813953488372, 68.75, 73.80952380952381, 75.64102564102564, 72.8395061728395, 82.14285714285714, 76.81159420289855, 70.51282051282051, 65.21739130434783, 51.162790697674424, 65.21739130434783, 84.26966292134831, 51.35135135135135, 62.5, 66.17647058823529, 72.82608695652173, 67.74193548387096, 80.20833333333334, 70.73170731707317]
30: [51.162790697674424, 67.07317073170732, 44.11764705882353, 72.28915662650603, 81.25, 76.0, 68.05555555555556, 74.71264367816092, 81.42857142857143, 74.07407407407408, 75.60975609756098, 75.5813953488372, 71.25, 75.0, 78.2051282051282, 67.90123456790124, 83.33333333333334, 76.81159420289855, 74.35897435897436, 64.13043478260869, 66.27906976744185, 70.65217391304348, 77.52808988764045, 56.75675675675676, 59.72222222222222, 52.94117647058824, 72.82608695652173, 63.44086021505376, 81.25, 71.95121951219512, 61.29032258064516]
31: [48.06201550387597, 69.51219512195121, 50.0, 66.26506024096386, 82.5, 77.33333333333333, 62.5, 74.71264367816092, 82.85714285714286, 75.30864197530865, 74.39024390243902, 67.44186046511628, 63.74999999999999, 73.80952380952381, 75.64102564102564, 67.90123456790124, 76.19047619047619, 76.81159420289855, 69.23076923076923, 61.95652173913043, 69.76744186046511, 64.13043478260869, 77.52808988764045, 59.45945945945946, 58.333333333333336, 55.88235294117647, 72.82608695652173, 75.26881720430107, 81.25, 63.41463414634146, 74.19354838709677, 63.85542168674698]
32: [52.71317829457365, 76.82926829268293, 48.529411764705884, 68.67469879518072, 80.0, 74.66666666666667, 69.44444444444444, 74.71264367816092, 84.28571428571429, 75.30864197530865, 73.17073170731707, 74.4186046511628, 67.5, 66.66666666666666, 73.07692307692307, 70.37037037037037, 75.0, 71.01449275362319, 74.35897435897436, 68.47826086956522, 55.81395348837209, 63.04347826086957, 75.28089887640449, 56.75675675675676, 56.94444444444444, 63.23529411764706, 73.91304347826086, 67.74193548387096, 81.25, 65.85365853658537, 75.26881720430107, 63.85542168674698, 62.16216216216216]
33: [49.6124031007752, 68.29268292682927, 55.88235294117647, 61.44578313253012, 73.75, 77.33333333333333, 63.888888888888886, 72.41379310344827, 80.0, 71.60493827160494, 75.60975609756098, 74.4186046511628, 66.25, 65.47619047619048, 82.05128205128204, 61.72839506172839, 86.90476190476191, 78.26086956521739, 67.94871794871796, 64.13043478260869, 67.44186046511628, 63.04347826086957, 77.52808988764045, 52.702702702702695, 55.55555555555556, 57.35294117647059, 68.47826086956522, 70.96774193548387, 78.125, 69.51219512195121, 63.44086021505376, 73.49397590361446, 60.810810810810814, 69.73684210526315]
34: [51.162790697674424, 75.60975609756098, 50.0, 68.67469879518072, 80.0, 74.66666666666667, 65.27777777777779, 71.26436781609196, 80.0, 76.5432098765432, 74.39024390243902, 70.93023255813954, 62.5, 71.42857142857143, 76.92307692307693, 70.37037037037037, 76.19047619047619, 72.46376811594203, 70.51282051282051, 66.30434782608695, 56.97674418604651, 65.21739130434783, 76.40449438202246, 54.054054054054056, 59.72222222222222, 60.29411764705882, 73.91304347826086, 70.96774193548387, 82.29166666666666, 68.29268292682927, 62.365591397849464, 68.67469879518072, 67.56756756756756, 78.94736842105263, 82.89473684210526]
35: [53.48837209302325, 69.51219512195121, 39.705882352941174, 68.67469879518072, 75.0, 77.33333333333333, 63.888888888888886, 71.26436781609196, 72.85714285714285, 75.30864197530865, 71.95121951219512, 74.4186046511628, 57.49999999999999, 67.85714285714286, 75.64102564102564, 69.1358024691358, 79.76190476190477, 79.71014492753623, 70.51282051282051, 59.78260869565217, 69.76744186046511, 63.04347826086957, 71.91011235955057, 60.810810810810814, 61.111111111111114, 51.470588235294116, 66.30434782608695, 69.89247311827957, 81.25, 70.73170731707317, 67.74193548387096, 63.85542168674698, 66.21621621621621, 73.68421052631578, 80.26315789473685, 72.22222222222221]
36: [45.73643410852713, 73.17073170731707, 44.11764705882353, 73.49397590361446, 87.5, 66.66666666666666, 61.111111111111114, 65.51724137931035, 80.0, 71.60493827160494, 68.29268292682927, 52.32558139534884, 65.0, 69.04761904761905, 62.82051282051282, 65.4320987654321, 82.14285714285714, 62.31884057971014, 71.7948717948718, 58.69565217391305, 56.97674418604651, 68.47826086956522, 73.03370786516854, 55.4054054054054, 59.72222222222222, 57.35294117647059, 65.21739130434783, 69.89247311827957, 72.91666666666666, 65.85365853658537, 59.13978494623656, 59.036144578313255, 55.4054054054054, 77.63157894736842, 73.68421052631578, 54.166666666666664, 46.15384615384615]
37: [49.6124031007752, 64.63414634146342, 48.529411764705884, 65.06024096385542, 75.0, 76.0, 63.888888888888886, 71.26436781609196, 71.42857142857143, 76.5432098765432, 75.60975609756098, 72.09302325581395, 63.74999999999999, 64.28571428571429, 70.51282051282051, 66.66666666666666, 75.0, 72.46376811594203, 69.23076923076923, 65.21739130434783, 61.627906976744185, 69.56521739130434, 82.02247191011236, 59.45945945945946, 58.333333333333336, 54.41176470588235, 75.0, 70.96774193548387, 77.08333333333334, 68.29268292682927, 58.06451612903226, 55.42168674698795, 63.51351351351351, 73.68421052631578, 72.36842105263158, 62.5, 64.83516483516483, 54.43037974683544]
38: [50.3875968992248, 70.73170731707317, 50.0, 68.67469879518072, 76.25, 77.33333333333333, 59.72222222222222, 77.01149425287356, 80.0, 70.37037037037037, 68.29268292682927, 62.7906976744186, 67.5, 66.66666666666666, 74.35897435897436, 72.8395061728395, 78.57142857142857, 73.91304347826086, 73.07692307692307, 65.21739130434783, 55.81395348837209, 68.47826086956522, 75.28089887640449, 55.4054054054054, 61.111111111111114, 51.470588235294116, 72.82608695652173, 70.96774193548387, 75.0, 64.63414634146342, 61.29032258064516, 65.06024096385542, 63.51351351351351, 75.0, 73.68421052631578, 56.94444444444444, 61.53846153846154, 56.9620253164557, 65.3061224489796]
39: [51.162790697674424, 65.85365853658537, 45.588235294117645, 68.67469879518072, 81.25, 73.33333333333333, 62.5, 78.16091954022988, 78.57142857142857, 71.60493827160494, 74.39024390243902, 73.25581395348837, 61.25000000000001, 76.19047619047619, 78.2051282051282, 65.4320987654321, 78.57142857142857, 69.56521739130434, 69.23076923076923, 66.30434782608695, 55.81395348837209, 71.73913043478261, 76.40449438202246, 64.86486486486487, 54.166666666666664, 63.23529411764706, 67.3913043478261, 68.81720430107528, 81.25, 71.95121951219512, 61.29032258064516, 61.44578313253012, 67.56756756756756, 72.36842105263158, 64.47368421052632, 63.888888888888886, 61.53846153846154, 58.22784810126582, 76.53061224489795, 72.61904761904762]
40: [50.3875968992248, 63.41463414634146, 47.05882352941176, 68.67469879518072, 71.25, 74.66666666666667, 66.66666666666666, 72.41379310344827, 75.71428571428571, 72.8395061728395, 75.60975609756098, 69.76744186046511, 60.0, 72.61904761904762, 71.7948717948718, 71.60493827160494, 77.38095238095238, 72.46376811594203, 67.94871794871796, 60.86956521739131, 61.627906976744185, 63.04347826086957, 64.04494382022472, 52.702702702702695, 56.94444444444444, 58.82352941176471, 72.82608695652173, 73.11827956989248, 78.125, 67.07317073170732, 53.76344086021505, 59.036144578313255, 60.810810810810814, 85.52631578947368, 77.63157894736842, 61.111111111111114, 65.93406593406593, 60.75949367088608, 70.40816326530613, 73.80952380952381, 56.94444444444444]
41: [53.48837209302325, 68.29268292682927, 44.11764705882353, 67.46987951807229, 66.25, 76.0, 56.94444444444444, 74.71264367816092, 72.85714285714285, 71.60493827160494, 73.17073170731707, 73.25581395348837, 65.0, 70.23809523809523, 78.2051282051282, 67.90123456790124, 64.28571428571429, 71.01449275362319, 75.64102564102564, 57.608695652173914, 65.11627906976744, 65.21739130434783, 73.03370786516854, 55.4054054054054, 62.5, 48.529411764705884, 73.91304347826086, 66.66666666666666, 79.16666666666666, 63.41463414634146, 55.91397849462365, 59.036144578313255, 67.56756756756756, 78.94736842105263, 68.42105263157895, 61.111111111111114, 50.54945054945055, 59.49367088607595, 72.44897959183673, 60.71428571428571, 55.55555555555556, 63.85542168674698]
42: [51.93798449612403, 68.29268292682927, 48.529411764705884, 71.08433734939759, 76.25, 78.66666666666666, 50.0, 62.06896551724138, 71.42857142857143, 76.5432098765432, 80.48780487804879, 70.93023255813954, 65.0, 66.66666666666666, 74.35897435897436, 67.90123456790124, 76.19047619047619, 66.66666666666666, 73.07692307692307, 67.3913043478261, 54.65116279069767, 67.3913043478261, 77.52808988764045, 51.35135135135135, 59.72222222222222, 41.17647058823529, 66.30434782608695, 73.11827956989248, 80.20833333333334, 60.97560975609756, 63.44086021505376, 63.85542168674698, 58.108108108108105, 81.57894736842105, 60.526315789473685, 62.5, 58.24175824175825, 62.0253164556962, 75.51020408163265, 75.0, 52.77777777777778, 60.24096385542169, 76.47058823529412]
43: [44.96124031007752, 69.51219512195121, 51.470588235294116, 66.26506024096386, 55.00000000000001, 80.0, 68.05555555555556, 71.26436781609196, 84.28571428571429, 72.8395061728395, 73.17073170731707, 69.76744186046511, 66.25, 69.04761904761905, 58.97435897435898, 61.72839506172839, 86.90476190476191, 71.01449275362319, 70.51282051282051, 57.608695652173914, 50.0, 66.30434782608695, 74.15730337078652, 67.56756756756756, 58.333333333333336, 54.41176470588235, 69.56521739130434, 74.19354838709677, 68.75, 64.63414634146342, 69.89247311827957, 57.831325301204814, 56.75675675675676, 81.57894736842105, 69.73684210526315, 62.5, 63.73626373626373, 65.82278481012658, 70.40816326530613, 70.23809523809523, 52.77777777777778, 56.62650602409639, 65.88235294117646, 62.96296296296296]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 44.6328
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 70.5357
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 49.4737
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 63.3929
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 53.7037
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 72.8155
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 68.6869
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 71.5517
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 84.5361
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 80.0000
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 80.0000
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 66.9565
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 62.0370
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 65.4867
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 70.7547
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 62.7273
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 67.8571
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 64.5833
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 71.9626
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 67.2131
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 50.0000
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 63.9344
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 74.7899
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 65.6863
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 65.0000
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 55.3191
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 67.2131
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 79.8387
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 65.8730
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 63.3929
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 62.6016
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 59.8214
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 66.3366
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 79.8077
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 75.9615
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 61.0000
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 66.1157
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 57.0093
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 66.1538
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 64.6018
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 61.0000
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 60.7143
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 66.9565
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 66.3636

====================

f1_score(micro): 65.77181208053692
f1_score(macro): 62.64584552765583
Classification report:
              precision    recall  f1-score   support

           0       0.67      1.00      0.80         4
           1       1.00      0.67      0.80         9
           2       1.00      0.78      0.88         9
           3       1.00      0.00      0.00         5
           4       0.00      0.00      0.00         9
           5       1.00      0.80      0.89         5
           6       1.00      0.00      0.00         4
           7       1.00      0.25      0.40         4
           8       0.60      0.75      0.67         4
           9       1.00      1.00      1.00         4
          10       1.00      0.00      0.00         9
          11       1.00      0.00      0.00         4
          12       1.00      0.00      0.00         4
          13       1.00      0.25      0.40         4
          14       1.00      1.00      1.00         4
          15       1.00      1.00      1.00         9
          16       1.00      0.89      0.94         9
          17       1.00      0.00      0.00         4
          18       1.00      0.00      0.00         4
          19       0.75      0.75      0.75         4
          20       1.00      0.75      0.86         4
          21       0.75      0.75      0.75         4
          22       1.00      0.60      0.75         5
          23       1.00      0.75      0.86         4
          24       0.67      0.40      0.50         5
          25       1.00      0.00      0.00         4
          26       1.00      0.20      0.33         5
          27       0.80      0.80      0.80         5
          28       0.75      0.75      0.75         4
          29       0.00      0.00      0.00         4
          30       1.00      0.50      0.67         4
          31       1.00      0.00      0.00         4
          32       0.00      0.00      0.00         9
          33       0.20      0.25      0.22         4
          34       0.62      0.89      0.73         9
          35       0.80      1.00      0.89         4
          36       1.00      1.00      1.00         4
          37       1.00      1.00      1.00         9
          38       0.71      1.00      0.83         5
          39       1.00      0.75      0.86         4
          40       0.10      0.25      0.14         4
          41       1.00      1.00      1.00         5
          42       0.00      0.00      0.00         4
          43       1.00      0.00      0.00         4
          44       1.00      0.25      0.40         4
          45       0.71      0.56      0.63         9
          46       1.00      0.80      0.89         5
          47       0.75      0.75      0.75         4
          48       0.67      0.50      0.57         8
          49       0.50      0.50      0.50         4
          50       1.00      0.89      0.94         9
          51       0.78      0.78      0.78         9
          52       1.00      0.75      0.86         4
          53       0.75      0.75      0.75         4
          54       1.00      1.00      1.00         5
          55       0.07      0.25      0.11         4
          56       0.60      0.75      0.67         4
          57       1.00      0.25      0.40         4
          58       0.75      0.67      0.71         9
          59       0.83      1.00      0.91         5
          60       0.75      0.75      0.75         4
          61       0.50      0.80      0.62         5
          62       0.08      0.11      0.10         9
          63       0.67      0.50      0.57         4
          64       1.00      0.50      0.67         4
          65       1.00      1.00      1.00         4
          66       1.00      0.00      0.00         4
          67       1.00      0.00      0.00         4
          68       0.33      0.60      0.43         5
          69       0.67      0.40      0.50         5
          70       1.00      0.00      0.00         4
          71       0.60      0.75      0.67         4
          72       0.00      0.00      0.00         4
          73       1.00      0.50      0.67         4
          74       0.00      0.00      0.00         4
          75       1.00      0.89      0.94         9
          76       0.33      0.25      0.29         4
          77       1.00      0.75      0.86         4
          78       1.00      1.00      1.00         4
          79       0.00      0.00      0.00         4
          80       1.00      1.00      1.00         4
          81       0.60      0.75      0.67         4
          82       0.00      0.00      0.00         4
          83       1.00      0.80      0.89         5
          84       0.00      0.00      0.00         4
          85       1.00      1.00      1.00         9
          86       0.62      0.56      0.59         9
          87       0.83      1.00      0.91         5
          88       0.60      0.75      0.67         4
          89       0.90      1.00      0.95         9
          90       0.00      0.00      0.00         4
          91       1.00      1.00      1.00         9
          92       0.00      0.00      0.00         9
          93       1.00      1.00      1.00         4
          94       0.60      0.67      0.63         9
          95       1.00      0.75      0.86         4
          96       1.00      0.50      0.67         4
          97       1.00      0.80      0.89         5
          98       0.67      0.50      0.57         4
          99       0.00      0.00      0.00         4
         100       1.00      0.40      0.57         5
         101       0.89      0.89      0.89         9
         102       0.67      0.50      0.57         4
         103       1.00      1.00      1.00         4
         104       1.00      0.00      0.00         4
         105       0.75      1.00      0.86         9
         106       1.00      0.00      0.00         5
         107       1.00      0.25      0.40         4
         108       0.00      0.00      0.00         4
         109       1.00      0.75      0.86         4
         110       0.80      0.44      0.57         9
         111       0.86      0.67      0.75         9
         112       1.00      0.50      0.67         4
         113       1.00      0.00      0.00         4
         114       1.00      0.75      0.86         4
         115       1.00      0.50      0.67         4
         116       0.83      1.00      0.91         5
         117       1.00      0.75      0.86         4
         118       1.00      0.00      0.00         4
         119       1.00      0.75      0.86         4
         120       1.00      0.75      0.86         4
         121       0.00      0.00      0.00         4
         122       1.00      1.00      1.00         4
         123       0.82      1.00      0.90         9
         124       0.60      0.67      0.63         9
         125       1.00      0.75      0.86         4
         126       1.00      1.00      1.00         4
         127       0.75      0.75      0.75         4
         128       0.67      0.67      0.67         9
         129       0.50      0.60      0.55         5
         130       0.83      1.00      0.91         5
         131       0.57      1.00      0.73         4
         132       0.75      0.75      0.75         4
         133       0.60      0.67      0.63         9
         134       1.00      1.00      1.00         4
         135       1.00      1.00      1.00         4
         136       0.00      0.00      0.00         9
         137       0.67      0.50      0.57         4
         138       0.75      1.00      0.86         9
         139       0.75      0.75      0.75         4
         140       0.75      1.00      0.86         9
         141       0.50      0.50      0.50         4
         142       1.00      1.00      1.00         5
         143       0.00      0.00      0.00         4
         144       0.00      0.00      0.00         4
         145       0.50      0.75      0.60         4
         146       0.83      1.00      0.91         5
         147       0.33      0.25      0.29         4
         148       1.00      0.80      0.89         5
         149       0.43      0.75      0.55         4
         150       0.25      0.60      0.35         5
         151       0.75      0.75      0.75         4
         152       1.00      1.00      1.00         4
         153       0.57      1.00      0.73         4
         154       1.00      0.50      0.67         4
         155       0.50      0.44      0.47         9
         156       1.00      0.78      0.88         9
         157       0.73      0.89      0.80         9
         158       0.69      1.00      0.82         9
         159       0.00      0.00      0.00         4
         160       0.00      0.00      0.00         4
         161       0.60      0.67      0.63         9
         162       0.80      0.80      0.80         5
         163       1.00      1.00      1.00         4
         164       0.80      1.00      0.89         4
         165       0.80      1.00      0.89         4
         166       0.40      1.00      0.57         4
         167       0.80      1.00      0.89         4
         168       0.64      1.00      0.78         9
         169       0.25      0.25      0.25         4
         170       0.80      1.00      0.89         4
         171       0.50      0.56      0.53         9
         172       1.00      1.00      1.00         4
         173       0.00      0.00      0.00         4
         174       1.00      0.75      0.86         4
         175       1.00      1.00      1.00         4
         176       1.00      1.00      1.00         4
         177       0.75      0.75      0.75         4
         178       0.71      1.00      0.83         5
         179       0.80      1.00      0.89         4
         180       0.00      0.00      0.00         4
         181       0.67      0.50      0.57         4
         182       0.22      1.00      0.36         4
         183       0.69      1.00      0.82         9
         184       0.67      0.50      0.57         4
         185       1.00      1.00      1.00         4
         186       1.00      0.75      0.86         4
         187       0.89      0.89      0.89         9
         188       0.57      1.00      0.73         4
         189       0.43      0.75      0.55         4
         190       0.80      1.00      0.89         4
         191       0.30      0.75      0.43         4
         192       0.83      1.00      0.91         5
         193       0.57      0.89      0.70         9
         194       0.00      0.00      0.00         4
         195       0.80      0.80      0.80         5
         196       0.67      1.00      0.80         4
         197       1.00      1.00      1.00         4
         198       1.00      1.00      1.00         4
         199       1.00      1.00      1.00         4
         200       1.00      0.00      0.00         4
         201       1.00      0.80      0.89         5
         202       0.83      1.00      0.91         5
         203       0.86      0.67      0.75         9
         204       1.00      1.00      1.00         5
         205       0.89      0.89      0.89         9
         206       1.00      0.75      0.86         4
         207       0.90      1.00      0.95         9
         208       0.14      0.25      0.18         4
         209       1.00      0.89      0.94         9
         210       1.00      1.00      1.00         4
         211       1.00      1.00      1.00         5
         212       1.00      0.75      0.86         4
         213       0.64      0.78      0.70         9
         214       1.00      1.00      1.00         9
         215       0.00      0.00      0.00         4
         216       0.80      1.00      0.89         4
         217       1.00      0.75      0.86         4
         218       1.00      0.44      0.62         9
         219       1.00      0.75      0.86         4
         220       1.00      1.00      1.00         9
         221       1.00      0.50      0.67         4
         222       1.00      1.00      1.00         4
         223       0.64      0.78      0.70         9
         224       1.00      1.00      1.00         4
         225       0.90      1.00      0.95         9
         226       0.80      1.00      0.89         4
         227       0.00      0.00      0.00         4
         228       0.60      1.00      0.75         9
         229       0.67      1.00      0.80         4
         230       0.50      0.50      0.50         4
         231       0.80      1.00      0.89         4
         232       0.80      1.00      0.89         4
         233       0.75      0.75      0.75         4
         234       0.50      0.20      0.29         5
         235       1.00      0.60      0.75         5
         236       1.00      0.00      0.00         4
         237       1.00      0.80      0.89         5
         238       0.31      0.56      0.40         9
         239       0.00      0.00      0.00         4
         240       0.82      1.00      0.90         9
         241       1.00      0.75      0.86         4
         242       1.00      1.00      1.00         4
         243       0.25      0.25      0.25         4
         244       0.00      0.00      0.00         4
         245       0.50      0.89      0.64         9
         246       0.80      0.89      0.84         9
         247       0.60      0.60      0.60         5
         248       1.00      0.50      0.67         4
         249       0.70      0.78      0.74         9
         250       0.80      1.00      0.89         4
         251       1.00      0.25      0.40         4
         252       0.69      1.00      0.82         9
         253       1.00      1.00      1.00         5
         254       0.46      0.67      0.55         9
         255       0.60      0.75      0.67         4
         256       0.75      0.75      0.75         4
         257       0.75      0.67      0.71         9
         258       1.00      0.00      0.00         4
         259       0.70      0.78      0.74         9
         260       1.00      1.00      1.00         4
         261       0.33      0.40      0.36         5
         262       0.38      0.75      0.50         4
         263       1.00      1.00      1.00         9
         264       0.58      0.78      0.67         9
         265       0.00      0.00      0.00         4
         266       1.00      0.75      0.86         4
         267       0.50      0.75      0.60         4
         268       0.00      0.00      0.00         5
         269       1.00      0.50      0.67         4
         270       0.83      1.00      0.91         5
         271       1.00      0.00      0.00         4
         272       1.00      0.75      0.86         4
         273       0.14      0.25      0.18         4
         274       1.00      0.75      0.86         4
         275       0.50      0.40      0.44         5
         276       0.89      0.89      0.89         9
         277       0.00      0.00      0.00         4
         278       0.64      0.78      0.70         9
         279       1.00      1.00      1.00         5
         280       0.69      1.00      0.82         9
         281       0.50      0.60      0.55         5
         282       0.80      1.00      0.89         4
         283       0.75      0.75      0.75         4
         284       0.50      0.50      0.50         4
         285       0.80      0.89      0.84         9
         286       0.80      1.00      0.89         4
         287       0.00      0.00      0.00         4
         288       1.00      1.00      1.00         4
         289       0.00      0.00      0.00         4
         290       0.00      0.00      0.00         9
         291       0.44      0.44      0.44         9
         292       0.44      1.00      0.62         4
         293       0.80      1.00      0.89         4
         294       0.67      0.40      0.50         5
         295       0.89      0.89      0.89         9
         296       0.62      1.00      0.77         5
         297       1.00      1.00      1.00         4
         298       1.00      1.00      1.00         4
         299       0.00      0.00      0.00         4
         300       0.89      0.89      0.89         9
         301       1.00      0.80      0.89         5
         302       1.00      0.00      0.00         4
         303       0.78      0.78      0.78         9
         304       1.00      0.75      0.86         4
         305       0.29      0.40      0.33         5
         306       1.00      1.00      1.00         4
         307       0.67      0.80      0.73         5
         308       0.75      0.75      0.75         4
         309       0.36      0.80      0.50         5
         310       1.00      1.00      1.00         4
         311       1.00      0.75      0.86         4
         312       0.75      0.67      0.71         9
         313       0.00      0.00      0.00         4
         314       1.00      1.00      1.00         5
         315       0.75      0.67      0.71         9
         316       1.00      1.00      1.00         4
         317       0.00      0.00      0.00         9
         318       0.82      1.00      0.90         9
         319       1.00      1.00      1.00         4
         320       0.90      1.00      0.95         9
         321       1.00      0.75      0.86         4
         322       0.38      0.60      0.46         5
         323       0.67      0.50      0.57         4
         324       0.11      0.25      0.15         4
         325       1.00      0.75      0.86         4
         326       0.86      0.67      0.75         9
         327       1.00      0.75      0.86         4
         328       0.57      0.57      0.57         7
         329       0.57      1.00      0.73         4
         330       0.67      0.50      0.57         4
         331       0.00      0.00      0.00         4
         332       0.00      0.00      0.00         4
         333       0.50      0.25      0.33         4
         334       0.83      1.00      0.91         5
         335       1.00      0.75      0.86         4
         336       0.70      0.78      0.74         9
         337       0.90      1.00      0.95         9
         338       0.00      0.00      0.00         4
         339       1.00      0.50      0.67         4
         340       0.43      0.75      0.55         4
         341       0.80      1.00      0.89         4
         342       1.00      1.00      1.00         4
         343       0.14      0.44      0.22         9
         344       0.00      0.00      0.00         4
         345       1.00      0.50      0.67         4
         346       0.89      0.89      0.89         9
         347       0.22      0.50      0.31         4
         348       0.60      0.60      0.60         5
         349       1.00      0.25      0.40         4
         350       0.64      0.78      0.70         9
         351       0.89      0.89      0.89         9
         352       0.75      0.75      0.75         4
         353       0.20      0.25      0.22         4
         354       0.75      0.75      0.75         4
         355       0.33      0.25      0.29         4
         356       0.80      1.00      0.89         4
         357       0.50      0.50      0.50         4
         358       0.67      1.00      0.80         6
         359       0.75      0.75      0.75         4
         360       0.33      0.50      0.40         4
         361       0.00      0.00      0.00         4
         362       0.86      0.75      0.80         8
         363       0.36      1.00      0.53         4
         364       0.00      0.00      0.00         4
         365       0.50      0.25      0.33         4
         366       1.00      1.00      1.00         9
         367       0.67      1.00      0.80         4
         368       0.75      0.75      0.75         4
         369       0.50      0.50      0.50         4
         370       0.67      0.44      0.53         9
         371       1.00      1.00      1.00         4
         372       0.67      1.00      0.80         4
         373       0.00      0.00      0.00         4
         374       1.00      1.00      1.00         4
         375       0.75      0.75      0.75         4
         376       1.00      1.00      1.00         5
         377       1.00      1.00      1.00         4
         378       0.50      0.33      0.40         9
         379       0.60      0.67      0.63         9
         380       0.43      0.75      0.55         4
         381       1.00      1.00      1.00         4
         382       0.80      0.44      0.57         9
         383       0.69      1.00      0.82         9
         384       1.00      1.00      1.00         4
         385       0.17      0.25      0.20         4
         386       1.00      1.00      1.00         4
         387       1.00      1.00      1.00         4
         388       0.75      0.75      0.75         4
         389       0.75      0.60      0.67         5
         390       1.00      1.00      1.00         4
         391       0.75      0.75      0.75         4
         392       0.50      0.75      0.60         4
         393       0.33      0.33      0.33         9
         394       0.50      1.00      0.67         4
         395       0.80      1.00      0.89         4
         396       0.89      0.89      0.89         9
         397       0.80      0.89      0.84         9
         398       1.00      0.00      0.00         4
         399       0.62      0.56      0.59         9
         400       0.82      1.00      0.90         9
         401       0.89      0.89      0.89         9
         402       0.43      0.75      0.55         4
         403       0.67      0.67      0.67         9
         404       0.00      0.00      0.00         4
         405       0.57      1.00      0.73         4
         406       0.00      0.00      0.00         9
         407       1.00      0.50      0.67         4
         408       0.56      1.00      0.71         5
         409       0.00      0.00      0.00         4
         410       0.75      0.75      0.75         4
         411       0.00      0.00      0.00         4
         412       0.57      0.80      0.67         5
         413       0.60      1.00      0.75         9
         414       0.25      0.25      0.25         4
         415       0.67      0.80      0.73         5
         416       0.25      0.25      0.25         4
         417       0.64      0.78      0.70         9
         418       0.67      1.00      0.80         4
         419       1.00      0.00      0.00         4
         420       1.00      0.80      0.89         5
         421       0.43      0.33      0.38         9
         422       1.00      1.00      1.00         5
         423       0.00      0.00      0.00         9
         424       0.75      0.60      0.67         5
         425       1.00      0.44      0.62         9
         426       0.50      0.11      0.18         9
         427       0.38      0.75      0.50         4
         428       0.00      0.00      0.00         4
         429       1.00      0.80      0.89         5
         430       0.00      0.00      0.00         4
         431       1.00      0.89      0.94         9
         432       1.00      1.00      1.00         4
         433       0.67      0.40      0.50         5
         434       0.73      0.89      0.80         9
         435       1.00      0.75      0.86         4
         436       1.00      0.89      0.94         9
         437       1.00      0.75      0.86         4
         438       0.10      0.11      0.11         9
         439       0.00      0.00      0.00         4
         440       0.75      1.00      0.86         9
         441       0.00      0.00      0.00         4
         442       0.45      0.56      0.50         9
         443       1.00      0.25      0.40         4
         444       0.80      1.00      0.89         4
         445       0.82      1.00      0.90         9
         446       0.67      0.50      0.57         4
         447       0.00      0.00      0.00         4
         448       0.78      0.78      0.78         9
         449       0.75      0.60      0.67         5
         450       1.00      1.00      1.00         4
         451       1.00      1.00      1.00         5
         452       0.86      0.67      0.75         9
         453       1.00      0.00      0.00         4
         454       0.33      0.25      0.29         4
         455       0.67      1.00      0.80         4
         456       1.00      1.00      1.00         4
         457       0.56      0.56      0.56         9
         458       1.00      1.00      1.00         4
         459       1.00      0.50      0.67         4
         460       0.82      1.00      0.90         9
         461       0.67      0.67      0.67         9
         462       0.80      1.00      0.89         4
         463       1.00      0.40      0.57         5
         464       0.86      0.67      0.75         9
         465       1.00      1.00      1.00         5
         466       0.00      0.00      0.00         4
         467       0.90      1.00      0.95         9
         468       0.70      0.78      0.74         9
         469       0.86      0.67      0.75         9
         470       0.12      0.25      0.17         4
         471       1.00      1.00      1.00         5
         472       0.80      1.00      0.89         4
         473       0.83      1.00      0.91         5
         474       0.75      0.75      0.75         4
         475       0.46      0.86      0.60         7
         476       0.33      0.25      0.29         4
         477       0.11      0.33      0.16         9
         478       0.25      0.50      0.33         4
         479       0.07      0.25      0.11         4
         480       1.00      0.00      0.00         4
         481       0.20      0.50      0.29         4
         482       1.00      0.80      0.89         5
         483       0.62      1.00      0.77         5
         484       0.20      0.25      0.22         4
         485       1.00      1.00      1.00         5
         486       0.23      0.75      0.35         4
         487       1.00      1.00      1.00         4
         488       0.60      1.00      0.75         9
         489       1.00      0.50      0.67         4
         490       0.89      0.89      0.89         9
         491       0.60      0.75      0.67         4
         492       0.57      0.80      0.67         5
         493       0.50      0.25      0.33         4
         494       0.67      1.00      0.80         4
         495       0.80      0.80      0.80         5
         496       0.10      0.11      0.11         9
         497       1.00      0.25      0.40         4
         498       1.00      1.00      1.00         4
         499       1.00      1.00      1.00         9
         500       0.83      0.56      0.67         9
         501       1.00      0.75      0.86         4
         502       1.00      1.00      1.00         4
         503       1.00      1.00      1.00         5
         504       1.00      1.00      1.00         4
         505       0.80      0.80      0.80         5
         506       0.75      0.60      0.67         5
         507       0.57      1.00      0.73         4
         508       0.50      0.60      0.55         5
         509       1.00      0.00      0.00         4
         510       1.00      0.75      0.86         4
         511       0.00      0.00      0.00         4
         512       0.00      0.00      0.00         4
         513       1.00      1.00      1.00         4
         514       0.80      0.80      0.80         5
         515       0.10      0.25      0.14         4
         516       0.46      0.67      0.55         9
         517       1.00      0.44      0.62         9
         518       0.60      0.75      0.67         4
         519       0.00      0.00      0.00         4
         520       0.44      1.00      0.62         4
         521       1.00      0.25      0.40         4
         522       1.00      0.50      0.67         4
         523       0.50      1.00      0.67         4
         524       1.00      0.80      0.89         5
         525       0.50      0.20      0.29         5
         526       1.00      1.00      1.00         4
         527       0.06      0.25      0.10         4
         528       0.25      0.25      0.25         4
         529       1.00      0.50      0.67         4
         530       0.33      0.25      0.29         4
         531       1.00      1.00      1.00         4
         532       0.83      1.00      0.91         5
         533       0.00      0.00      0.00         4
         534       0.67      0.89      0.76         9
         535       0.80      1.00      0.89         4
         536       0.89      0.89      0.89         9
         537       0.00      0.00      0.00         4
         538       0.89      0.89      0.89         9
         539       1.00      0.75      0.86         4
         540       0.17      0.25      0.20         4
         541       0.43      0.75      0.55         4
         542       1.00      0.40      0.57         5
         543       0.75      0.67      0.71         9
         544       1.00      1.00      1.00         4
         545       0.75      0.75      0.75         4
         546       0.00      0.00      0.00         4
         547       0.11      0.11      0.11         9
         548       0.89      0.89      0.89         9
         549       1.00      1.00      1.00         5
         550       1.00      0.75      0.86         4
         551       1.00      0.56      0.71         9
         552       0.75      0.75      0.75         4
         553       0.88      0.78      0.82         9
         554       1.00      0.80      0.89         5
         555       0.50      0.80      0.62         5
         556       0.75      0.75      0.75         4
         557       0.89      0.89      0.89         9
         558       0.50      0.50      0.50         4
         559       0.50      0.75      0.60         4
         560       0.73      0.89      0.80         9
         561       1.00      0.89      0.94         9
         562       0.00      0.00      0.00         4
         563       0.67      0.50      0.57         4
         564       0.23      0.60      0.33         5
         565       0.75      1.00      0.86         9
         566       0.82      1.00      0.90         9
         567       0.60      0.75      0.67         4
         568       0.70      0.78      0.74         9
         569       0.50      0.60      0.55         5
         570       1.00      0.89      0.94         9
         571       0.80      1.00      0.89         4
         572       0.80      0.89      0.84         9
         573       0.60      0.75      0.67         4
         574       0.75      0.67      0.71         9
         575       1.00      0.75      0.86         4
         576       1.00      0.75      0.86         4
         577       1.00      0.00      0.00         4
         578       1.00      0.00      0.00         4
         579       1.00      0.25      0.40         4
         580       0.88      0.78      0.82         9
         581       1.00      0.50      0.67         4
         582       1.00      0.67      0.80         9
         583       0.88      0.78      0.82         9
         584       1.00      0.00      0.00         4
         585       0.80      0.89      0.84         9
         586       0.50      0.50      0.50         4
         587       0.75      0.75      0.75         4
         588       0.57      0.80      0.67         5
         589       1.00      0.56      0.71         9
         590       0.64      1.00      0.78         9
         591       0.00      0.00      0.00         4
         592       0.90      1.00      0.95         9
         593       0.80      0.89      0.84         9
         594       1.00      1.00      1.00         4
         595       1.00      0.50      0.67         4
         596       0.67      1.00      0.80         4
         597       0.80      1.00      0.89         4
         598       0.67      0.50      0.57         4
         599       1.00      1.00      1.00         5
         600       0.67      0.80      0.73         5
         601       1.00      0.00      0.00         4
         602       1.00      0.80      0.89         5
         603       0.82      1.00      0.90         9
         604       0.83      1.00      0.91         5
         605       0.75      0.60      0.67         5
         606       0.00      0.00      0.00         9
         607       1.00      1.00      1.00         9
         608       1.00      0.60      0.75         5
         609       1.00      0.22      0.36         9
         610       1.00      0.00      0.00         4
         611       1.00      0.67      0.80         9
         612       0.75      0.75      0.75         4
         613       1.00      0.40      0.57         5
         614       0.80      1.00      0.89         4
         615       1.00      0.25      0.40         4
         616       1.00      0.00      0.00         4
         617       1.00      1.00      1.00         4
         618       0.83      0.56      0.67         9
         619       0.58      0.78      0.67         9
         620       1.00      0.89      0.94         9
         621       0.00      0.00      0.00         4
         622       1.00      0.50      0.67         4
         623       0.00      0.00      0.00         4
         624       1.00      0.89      0.94         9
         625       0.00      0.00      0.00         9
         626       0.50      0.40      0.44         5
         627       1.00      0.75      0.86         4
         628       1.00      0.80      0.89         5
         629       1.00      0.67      0.80         9
         630       0.80      0.89      0.84         9
         631       1.00      0.75      0.86         4
         632       0.83      1.00      0.91         5
         633       0.64      0.78      0.70         9
         634       0.67      0.50      0.57         4
         635       0.00      0.00      0.00         4
         636       1.00      1.00      1.00         5
         637       1.00      0.50      0.67         4
         638       1.00      1.00      1.00         4
         639       0.50      0.56      0.53         9
         640       0.00      0.00      0.00         4
         641       0.80      0.44      0.57         9
         642       1.00      0.25      0.40         4
         643       1.00      1.00      1.00         4
         644       1.00      1.00      1.00         5
         645       1.00      0.25      0.40         4
         646       0.88      0.78      0.82         9
         647       1.00      0.75      0.86         4
         648       0.40      0.44      0.42         9
         649       0.89      0.89      0.89         9
         650       1.00      0.00      0.00         4
         651       1.00      1.00      1.00         4
         652       1.00      0.00      0.00         4
         653       0.73      0.89      0.80         9
         654       0.62      1.00      0.77         5
         655       1.00      0.00      0.00         4
         656       1.00      1.00      1.00         4
         657       1.00      0.00      0.00         4
         658       0.17      0.25      0.20         4
         659       0.67      0.50      0.57         4
         660       1.00      0.00      0.00         4
         661       0.40      0.50      0.44         4
         662       0.82      1.00      0.90         9
         663       0.86      0.67      0.75         9
         664       1.00      0.00      0.00         4
         665       0.75      0.75      0.75         4
         666       0.67      1.00      0.80         4
         667       1.00      0.89      0.94         9
         668       0.57      1.00      0.73         4
         669       1.00      0.89      0.94         9
         670       1.00      1.00      1.00         4
         671       0.40      0.50      0.44         4
         672       0.80      1.00      0.89         4
         673       1.00      0.25      0.40         4
         674       0.57      1.00      0.73         4
         675       0.69      1.00      0.82         9
         676       0.75      0.75      0.75         4
         677       1.00      1.00      1.00         5
         678       0.62      1.00      0.77         5
         679       0.07      0.25      0.11         4
         680       0.00      0.00      0.00         4
         681       0.67      0.50      0.57         4
         682       1.00      1.00      1.00         4
         683       0.30      0.60      0.40         5
         684       1.00      0.75      0.86         4
         685       0.83      1.00      0.91         5
         686       0.82      1.00      0.90         9
         687       0.33      0.75      0.46         4
         688       0.70      0.78      0.74         9
         689       0.40      0.50      0.44         4
         690       0.50      0.75      0.60         4
         691       0.89      0.89      0.89         9
         692       0.67      1.00      0.80         4
         693       0.75      0.75      0.75         4
         694       0.67      1.00      0.80         4
         695       0.33      0.40      0.36         5
         696       0.83      1.00      0.91         5
         697       1.00      0.80      0.89         5
         698       1.00      1.00      1.00         4
         699       1.00      0.25      0.40         4
         700       0.67      0.50      0.57         4
         701       1.00      0.50      0.67         4
         702       1.00      1.00      1.00         4
         703       1.00      1.00      1.00         4
         704       1.00      1.00      1.00         4
         705       1.00      1.00      1.00         9
         706       1.00      0.89      0.94         9
         707       1.00      0.75      0.86         4
         708       0.83      1.00      0.91         5
         709       1.00      1.00      1.00         4
         710       0.50      0.50      0.50         4
         711       0.67      0.50      0.57         4
         712       0.40      0.22      0.29         9
         713       0.67      0.89      0.76         9
         714       0.60      0.75      0.67         4
         715       0.80      0.80      0.80         5
         716       0.60      0.75      0.67         4
         717       1.00      0.75      0.86         4
         718       0.50      0.20      0.29         5
         719       1.00      0.78      0.88         9
         720       0.00      0.00      0.00         4
         721       0.75      0.75      0.75         4
         722       0.00      0.00      0.00         4
         723       0.43      0.75      0.55         4
         724       0.36      0.80      0.50         5
         725       0.43      0.75      0.55         4
         726       0.50      0.60      0.55         5
         727       1.00      0.75      0.86         4
         728       0.67      0.40      0.50         5
         729       0.50      0.44      0.47         9
         730       1.00      0.75      0.86         4
         731       0.67      0.80      0.73         5
         732       1.00      0.88      0.93         8
         733       0.33      0.25      0.29         4
         734       1.00      1.00      1.00         4
         735       1.00      0.50      0.67         4
         736       1.00      1.00      1.00         4
         737       1.00      0.56      0.71         9
         738       0.60      0.75      0.67         4
         739       0.12      0.11      0.12         9
         740       1.00      0.80      0.89         5
         741       0.33      0.25      0.29         4
         742       0.75      0.75      0.75         4
         743       0.80      0.89      0.84         9
         744       1.00      1.00      1.00         9
         745       0.67      0.89      0.76         9
         746       0.00      0.00      0.00         4
         747       0.40      0.50      0.44         4
         748       0.50      0.75      0.60         4
         749       0.80      0.89      0.84         9
         750       0.05      0.11      0.06         9
         751       1.00      1.00      1.00         4
         752       0.67      0.50      0.57         4
         753       0.89      0.89      0.89         9
         754       0.75      0.75      0.75         4
         755       1.00      0.75      0.86         4
         756       1.00      0.00      0.00         4
         757       0.50      0.25      0.33         4
         758       1.00      0.89      0.94         9
         759       1.00      1.00      1.00         5
         760       1.00      0.75      0.86         4
         761       0.50      0.25      0.33         4
         762       1.00      0.25      0.40         4
         763       1.00      0.89      0.94         9
         764       0.80      0.80      0.80         5
         765       1.00      0.50      0.67         4
         766       0.46      0.67      0.55         9
         767       0.14      0.25      0.18         4
         768       0.00      0.00      0.00         4
         769       0.67      0.67      0.67         9
         770       1.00      1.00      1.00         4
         771       0.00      0.00      0.00         9
         772       0.14      0.25      0.18         4
         773       0.50      1.00      0.67         4
         774       0.50      0.75      0.60         4
         775       1.00      0.89      0.94         9
         776       0.50      0.44      0.47         9
         777       1.00      1.00      1.00         4
         778       0.83      0.56      0.67         9
         779       0.75      0.75      0.75         4
         780       1.00      0.80      0.89         5
         781       0.80      1.00      0.89         4
         782       0.50      0.75      0.60         4
         783       0.67      0.22      0.33         9
         784       0.83      0.56      0.67         9
         785       0.75      0.67      0.71         9
         786       1.00      0.71      0.83         7
         787       0.71      0.56      0.63         9
         788       1.00      1.00      1.00         5
         789       1.00      0.50      0.67         4
         790       1.00      0.89      0.94         9
         791       1.00      0.44      0.62         9
         792       0.50      0.75      0.60         4
         793       0.75      0.75      0.75         4
         794       1.00      0.75      0.86         4
         795       0.75      0.67      0.71         9
         796       0.90      1.00      0.95         9
         797       1.00      1.00      1.00         4
         798       0.33      0.25      0.29         4
         799       0.83      1.00      0.91         5
         800       1.00      0.78      0.88         9
         801       0.75      0.75      0.75         4
         802       1.00      0.75      0.86         4
         803       1.00      0.00      0.00         4
         804       1.00      0.50      0.67         4
         805       0.64      0.78      0.70         9
         806       1.00      0.20      0.33         5
         807       0.57      1.00      0.73         4
         808       0.00      0.00      0.00         4
         809       0.60      0.33      0.43         9
         810       1.00      0.80      0.89         5
         811       1.00      1.00      1.00         4
         812       0.67      0.44      0.53         9
         813       1.00      0.75      0.86         4
         814       0.50      0.50      0.50         4
         815       1.00      0.20      0.33         5
         816       1.00      0.75      0.86         4
         817       0.00      0.00      0.00         4
         818       1.00      1.00      1.00         4
         819       0.60      0.75      0.67         4
         820       1.00      0.89      0.94         9
         821       1.00      0.75      0.86         4
         822       1.00      1.00      1.00         5
         823       0.38      0.60      0.46         5
         824       0.71      1.00      0.83         5
         825       0.60      0.75      0.67         4
         826       1.00      0.50      0.67         4
         827       0.75      0.75      0.75         4
         828       0.00      0.00      0.00         4
         829       0.67      0.44      0.53         9
         830       1.00      1.00      1.00         4
         831       0.50      0.33      0.40         9
         832       0.83      1.00      0.91         5
         833       1.00      0.00      0.00         4
         834       0.89      0.89      0.89         9
         835       0.75      1.00      0.86         9
         836       1.00      0.80      0.89         5
         837       1.00      1.00      1.00         5
         838       0.00      0.00      0.00         4
         839       1.00      0.50      0.67         4
         840       1.00      0.89      0.94         9
         841       0.75      0.67      0.71         9
         842       0.75      0.75      0.75         4
         843       1.00      0.50      0.67         4
         844       1.00      0.50      0.67         4
         845       1.00      0.80      0.89         5
         846       0.00      0.00      0.00         9
         847       0.60      0.75      0.67         4
         848       0.67      0.80      0.73         5
         849       0.75      0.60      0.67         5
         850       0.67      0.40      0.50         5
         851       1.00      0.75      0.86         4
         852       1.00      0.00      0.00         4
         853       0.00      0.00      0.00         5
         854       1.00      0.00      0.00         4
         855       0.75      0.75      0.75         4
         856       1.00      0.89      0.94         9
         857       0.44      1.00      0.62         4
         858       0.71      1.00      0.83         5
         859       1.00      0.60      0.75         5
         860       1.00      0.78      0.88         9
         861       1.00      0.75      0.86         4
         862       0.82      1.00      0.90         9
         863       0.80      0.89      0.84         9
         864       1.00      0.44      0.62         9
         865       1.00      1.00      1.00         5
         866       0.00      0.00      0.00         4
         867       0.75      0.33      0.46         9
         868       1.00      0.00      0.00         4
         869       1.00      0.80      0.89         5
         870       1.00      1.00      1.00         5
         871       1.00      0.25      0.40         4
         872       0.25      0.25      0.25         4
         873       1.00      1.00      1.00         4
         874       0.83      1.00      0.91         5
         875       0.75      0.75      0.75         4
         876       0.67      1.00      0.80         4
         877       1.00      0.25      0.40         4
         878       0.00      0.00      0.00         4
         879       1.00      1.00      1.00         4
         880       0.25      0.25      0.25         4
         881       0.00      0.00      0.00         4
         882       1.00      0.80      0.89         5
         883       1.00      0.40      0.57         5
         884       0.69      1.00      0.82         9
         885       0.75      0.60      0.67         5
         886       1.00      1.00      1.00         4
         887       0.75      0.67      0.71         9
         888       0.75      0.67      0.71         9
         889       1.00      0.75      0.86         4
         890       1.00      1.00      1.00         9
         891       0.75      0.60      0.67         5
         892       1.00      1.00      1.00         4
         893       0.15      0.22      0.18         9

    accuracy                           0.66      4917
   macro avg       0.71      0.64      0.63      4917
weighted avg       0.72      0.66      0.65      4917

