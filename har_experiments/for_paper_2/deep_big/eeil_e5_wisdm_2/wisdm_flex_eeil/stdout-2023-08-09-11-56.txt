	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [540, 278, 406, 297, 551, 501, 439, 280, 159, 143, 644, 852, 66, 500, 146, 73, 715, 112, 319, 264, 1, 607, 331, 739, 799, 829, 80, 268, 442, 57, 654, 105, 372, 395, 156, 124, 482, 663, 63, 37, 54, 290, 76, 490, 212, 779, 662, 369, 255, 659, 312, 36, 419, 544, 45, 592, 651, 877, 788, 313, 171, 869, 216, 602, 246, 821, 96, 2, 769, 350, 12, 648, 646, 463, 292, 668, 858, 14, 229, 253, 426, 446, 431, 600, 631, 273, 848, 587, 243, 819, 460, 270, 262, 797, 522, 332, 416, 807, 466, 269, 191, 753, 415, 515, 340, 523, 19, 320, 778, 141, 180, 837, 586, 720, 303, 487, 342, 575, 339, 75, 335, 793, 21, 747, 528, 830, 458, 197, 302, 816, 163, 224, 558, 385, 167, 250, 145, 228, 864, 402, 790, 214, 430, 202, 344, 730, 669, 705, 172, 606, 871, 409, 314, 136, 184, 438, 265, 565, 595, 90, 410, 554, 610, 546, 712, 18, 497, 584, 440, 689, 605, 624, 576, 808, 803, 360, 387, 109, 337, 178, 259, 674, 43, 681, 236, 619, 673, 539, 305, 885, 507, 682, 242, 859, 102, 267, 588, 502, 716, 437, 684, 467, 199, 815, 878, 215, 556, 408, 636, 707, 718, 465, 187, 99, 702, 566, 564, 738, 148, 449, 827, 845, 277, 469, 405, 133, 371, 628, 583, 294, 173, 451, 794, 251, 757, 613, 390, 113, 181, 283, 751, 367, 220, 87, 517, 650, 530, 802, 325, 846, 434, 64, 653, 414, 748, 835, 504, 16, 842, 823, 727, 93, 759, 750, 629, 119, 137, 616, 107, 680, 357, 413, 772, 383, 645, 474, 758, 84, 208, 714, 537, 722, 281, 304, 822, 327, 468, 50, 392, 774, 795, 796, 60, 742, 164, 472, 732, 287, 114, 223, 812, 614, 17, 683, 810, 638, 786, 760, 233, 464, 24, 881, 700, 293, 749, 382, 323, 690, 489, 3, 665, 271, 160, 887, 840, 388, 130, 725, 692, 519, 177, 617, 274, 783, 824, 338, 481, 321, 116, 633, 578, 534, 150, 782, 632, 65, 235, 11, 289, 574, 368, 687, 447, 356, 346, 328, 389, 86, 767, 543, 545, 635, 195, 498, 398, 153, 817, 6, 527, 672, 135, 761, 240, 874, 122, 219, 456, 701, 422, 514, 284, 144, 505, 324, 604, 298, 396, 126, 627, 893, 326, 100, 493, 311, 763, 462, 161, 488, 756, 642, 29, 225, 879, 272, 204, 82, 470, 226, 234, 850, 85, 384, 593, 56, 31, 768, 851, 892, 35, 506, 678, 23, 20, 658, 97, 865, 210, 729, 623, 570, 89, 59, 138, 249, 115, 95, 838, 834, 806, 731, 780, 888, 708, 52, 376, 590, 194, 491, 876, 275, 40, 866, 553, 455, 300, 555, 435, 222, 266, 883, 189, 221, 361, 860, 811, 695, 839, 120, 299, 8, 355, 104, 542, 479, 128, 577, 276, 444, 496, 106, 26, 800, 365, 499, 573, 649, 55, 572, 296, 552, 341, 403, 118, 316, 535, 359, 599, 724, 818, 509, 666, 7, 41, 13, 550, 47, 211, 801, 315, 38, 450, 307, 282, 660, 719, 513, 626, 363, 110, 880, 611, 140, 71, 201, 149, 190, 571, 541, 647, 867, 386, 721, 231, 596, 179, 92, 247, 404, 728, 734, 185, 427, 103, 670, 775, 861, 352, 310, 475, 744, 32, 370, 868, 568, 762, 111, 375, 379, 694, 529, 698, 709, 789, 699, 42, 640, 814, 4, 393, 890, 127, 22, 98, 743, 520, 48, 608, 74, 704, 322, 582, 884, 158, 667, 407, 677, 891, 717, 854, 198, 72, 766, 345, 366, 510, 263, 844, 5, 347, 478, 69, 461, 618, 182, 151, 412, 531, 639, 549, 34, 792, 343, 227, 421, 459, 691, 688, 581, 764, 291, 567, 863, 205, 166, 401, 334, 295, 83, 241, 46, 661, 349, 831, 309, 131, 237, 473, 330, 78, 805, 777, 0, 755, 598, 652, 391, 849, 615, 671, 286, 525, 641, 726, 872, 828, 285, 428, 15, 561, 889, 192, 142, 399, 397, 547, 62, 476, 245, 857, 594, 139, 162, 776, 516, 585, 634, 685, 132, 603, 317, 193, 791, 417, 875, 873, 832, 512, 785, 563, 533, 27, 686, 306, 601, 836, 25, 833, 710, 196, 351, 589, 798, 882, 480, 886, 30, 333, 418, 637, 261, 125, 155, 745, 621, 254, 609, 232, 679, 203, 560, 622, 207, 657, 847, 825, 508, 483, 557, 411, 569, 773, 33, 484, 134, 445, 420, 257, 260, 206, 675, 129, 492, 10, 244, 68, 580, 279, 723, 123, 61, 735, 457, 354, 477, 518, 318, 711, 485, 362, 188, 870, 752, 51, 218, 855, 856, 713, 256, 781, 826, 579, 248, 301, 117, 532, 77, 175, 209, 703, 353, 176, 536, 200, 471, 394, 612, 841, 538, 526, 147, 154, 308, 853, 486, 238, 288, 697, 771, 453, 676, 58, 740, 770, 239, 70, 746, 424, 741, 591, 170, 91, 597, 454, 787, 423, 79, 336, 121, 252, 655, 737, 862, 108, 643, 157, 81, 53, 494, 364, 495, 706, 377, 524, 348, 813, 733, 693, 765, 754, 511, 88, 503, 625, 441, 152, 258, 174, 436, 784, 804, 381, 809, 168, 664, 9, 230, 217, 452, 548, 696, 28, 49, 843, 432, 94, 425, 656, 183, 562, 433, 358, 400, 165, 67, 443, 39, 448, 373, 101, 820, 329, 736, 169, 380, 521, 429, 620, 44, 559, 374, 630, 213, 186, 378]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.845, TAw acc= 25.2% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.378, TAw acc= 45.0% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.067, TAw acc= 47.3% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.788, TAw acc= 62.6% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.570, TAw acc= 69.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 290 train exemplars, time=  0.0s
290
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.406 | TAw acc= 77.7%, forg=  0.0%| TAg acc= 77.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=3.109, TAw acc= 52.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.407, TAw acc= 63.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.915, TAw acc= 77.5% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.592, TAw acc= 81.2% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.335, TAw acc= 81.2% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.334, TAw acc= 81.2% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 81.2% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 81.2% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.330, TAw acc= 81.2% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.328, TAw acc= 81.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 450 train exemplars, time=  0.0s
450
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.434 | TAw acc= 91.6%, forg=-14.0%| TAg acc= 79.9%, forg= -2.2% <<<
>>> Test on task  1 : loss=1.326 | TAw acc= 86.9%, forg=  0.0%| TAg acc= 83.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.281, TAw acc= 53.1% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.562, TAw acc= 53.1% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.109, TAw acc= 65.4% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.795, TAw acc= 81.5% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.566, TAw acc= 91.4% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.564, TAw acc= 91.4% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.562, TAw acc= 91.4% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.560, TAw acc= 91.4% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.558, TAw acc= 92.6% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.556, TAw acc= 92.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 610 train exemplars, time=  0.0s
610
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.257 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 85.5%, forg= -5.6% <<<
>>> Test on task  1 : loss=1.453 | TAw acc= 96.3%, forg= -9.3%| TAg acc= 72.0%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.648 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 69.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.657, TAw acc= 39.2% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.794, TAw acc= 58.2% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.302, TAw acc= 65.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.998, TAw acc= 74.7% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.737, TAw acc= 83.5% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.735, TAw acc= 84.8% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.733, TAw acc= 84.8% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.731, TAw acc= 84.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.729, TAw acc= 84.8% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.727, TAw acc= 84.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 770 train exemplars, time=  0.0s
770
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.249 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 81.0%, forg=  4.5% <<<
>>> Test on task  1 : loss=1.248 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 88.8%, forg= -5.6% <<<
>>> Test on task  2 : loss=1.760 | TAw acc= 93.8%, forg= -1.8%| TAg acc= 62.5%, forg=  7.1% <<<
>>> Test on task  3 : loss=1.570 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 70.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.883, TAw acc= 56.2% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.844, TAw acc= 63.5% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.211, TAw acc= 66.7% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.815, TAw acc= 72.9% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.515, TAw acc= 86.5% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.513, TAw acc= 86.5% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.510, TAw acc= 86.5% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.508, TAw acc= 86.5% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.505, TAw acc= 86.5% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.503, TAw acc= 86.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 930 train exemplars, time=  0.0s
930
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.097 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 88.3%, forg= -2.8% <<<
>>> Test on task  1 : loss=1.104 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 90.7%, forg= -1.9% <<<
>>> Test on task  2 : loss=1.590 | TAw acc= 95.5%, forg= -1.8%| TAg acc= 82.1%, forg=-12.5% <<<
>>> Test on task  3 : loss=1.624 | TAw acc= 92.6%, forg= -2.8%| TAg acc= 68.5%, forg=  1.9% <<<
>>> Test on task  4 : loss=1.448 | TAw acc= 87.4%, forg=  0.0%| TAg acc= 71.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.951, TAw acc= 45.1% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.880, TAw acc= 50.5% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.286, TAw acc= 71.4% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.956, TAw acc= 74.7% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.684, TAw acc= 78.0% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.683, TAw acc= 78.0% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.681, TAw acc= 78.0% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.679, TAw acc= 78.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.678, TAw acc= 79.1% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.676, TAw acc= 79.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1090 train exemplars, time=  0.0s
1090
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.059 | TAw acc= 92.2%, forg= -0.6%| TAg acc= 83.2%, forg=  5.0% <<<
>>> Test on task  1 : loss=1.106 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.5%, forg= 12.1% <<<
>>> Test on task  2 : loss=1.481 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 67.9%, forg= 14.3% <<<
>>> Test on task  3 : loss=1.529 | TAw acc= 93.5%, forg= -0.9%| TAg acc= 63.0%, forg=  7.4% <<<
>>> Test on task  4 : loss=1.438 | TAw acc= 97.6%, forg=-10.2%| TAg acc= 73.2%, forg= -1.6% <<<
>>> Test on task  5 : loss=1.541 | TAw acc= 83.6%, forg=  0.0%| TAg acc= 71.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.104, TAw acc= 47.2% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.608, TAw acc= 68.5% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.009, TAw acc= 74.2% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.687, TAw acc= 79.8% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.483, TAw acc= 87.6% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.481, TAw acc= 87.6% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.480, TAw acc= 87.6% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.477, TAw acc= 87.6% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.475, TAw acc= 87.6% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.474, TAw acc= 87.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1242 train exemplars, time=  0.0s
1242
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.058 | TAw acc= 91.6%, forg=  0.6%| TAg acc= 82.7%, forg=  5.6% <<<
>>> Test on task  1 : loss=1.074 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 81.3%, forg=  9.3% <<<
>>> Test on task  2 : loss=1.373 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 67.9%, forg= 14.3% <<<
>>> Test on task  3 : loss=1.321 | TAw acc= 94.4%, forg= -0.9%| TAg acc= 80.6%, forg=-10.2% <<<
>>> Test on task  4 : loss=1.225 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 80.3%, forg= -7.1% <<<
>>> Test on task  5 : loss=1.705 | TAw acc= 97.5%, forg=-13.9%| TAg acc= 62.3%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.447 | TAw acc= 84.7%, forg=  0.0%| TAg acc= 78.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.235, TAw acc= 46.6% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.068, TAw acc= 54.5% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.422, TAw acc= 59.1% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.044, TAw acc= 70.5% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.767, TAw acc= 73.9% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.765, TAw acc= 73.9% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.763, TAw acc= 73.9% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.761, TAw acc= 73.9% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.759, TAw acc= 73.9% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.758, TAw acc= 73.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1382 train exemplars, time=  0.0s
1382
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.061 | TAw acc= 91.6%, forg=  0.6%| TAg acc= 79.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=0.881 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 92.5%, forg= -1.9% <<<
>>> Test on task  2 : loss=1.207 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 80.4%, forg=  1.8% <<<
>>> Test on task  3 : loss=1.283 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  4 : loss=1.035 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 84.3%, forg= -3.9% <<<
>>> Test on task  5 : loss=1.485 | TAw acc= 98.4%, forg= -0.8%| TAg acc= 75.4%, forg= -4.1% <<<
>>> Test on task  6 : loss=1.782 | TAw acc= 89.8%, forg= -5.1%| TAg acc= 55.1%, forg= 22.9% <<<
>>> Test on task  7 : loss=1.628 | TAw acc= 74.6%, forg=  0.0%| TAg acc= 55.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.315, TAw acc= 41.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.968, TAw acc= 50.6% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.205, TAw acc= 80.0% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.784, TAw acc= 85.9% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.497, TAw acc= 91.8% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.495, TAw acc= 91.8% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.492, TAw acc= 91.8% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.489, TAw acc= 91.8% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.487, TAw acc= 91.8% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.484, TAw acc= 91.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1522 train exemplars, time=  0.0s
1522
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.002 | TAw acc= 92.7%, forg= -0.6%| TAg acc= 83.2%, forg=  5.0% <<<
>>> Test on task  1 : loss=0.825 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 92.5%, forg=  0.0% <<<
>>> Test on task  2 : loss=1.198 | TAw acc= 96.4%, forg= -0.9%| TAg acc= 73.2%, forg=  8.9% <<<
>>> Test on task  3 : loss=1.165 | TAw acc= 95.4%, forg= -0.9%| TAg acc= 83.3%, forg= -2.8% <<<
>>> Test on task  4 : loss=0.925 | TAw acc= 98.4%, forg= -0.8%| TAg acc= 89.0%, forg= -4.7% <<<
>>> Test on task  5 : loss=1.342 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 73.0%, forg=  2.5% <<<
>>> Test on task  6 : loss=1.668 | TAw acc= 91.5%, forg= -1.7%| TAg acc= 55.1%, forg= 22.9% <<<
>>> Test on task  7 : loss=1.874 | TAw acc= 88.1%, forg=-13.6%| TAg acc= 46.6%, forg=  9.3% <<<
>>> Test on task  8 : loss=1.429 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 70.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=4.336, TAw acc= 37.5% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.976, TAw acc= 58.8% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.233, TAw acc= 67.5% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.778, TAw acc= 88.8% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.540, TAw acc= 91.2% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.533, TAw acc= 91.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.527, TAw acc= 91.2% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.522, TAw acc= 91.2% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.517, TAw acc= 92.5% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.513, TAw acc= 92.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1662 train exemplars, time=  0.0s
1662
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.918 | TAw acc= 92.2%, forg=  0.6%| TAg acc= 83.8%, forg=  4.5% <<<
>>> Test on task  1 : loss=0.847 | TAw acc= 98.1%, forg= -1.9%| TAg acc= 85.0%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.193 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 79.5%, forg=  2.7% <<<
>>> Test on task  3 : loss=1.223 | TAw acc= 97.2%, forg= -1.9%| TAg acc= 74.1%, forg=  9.3% <<<
>>> Test on task  4 : loss=0.947 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 87.4%, forg=  1.6% <<<
>>> Test on task  5 : loss=1.181 | TAw acc= 99.2%, forg= -0.8%| TAg acc= 82.0%, forg= -6.6% <<<
>>> Test on task  6 : loss=1.695 | TAw acc= 92.4%, forg= -0.8%| TAg acc= 57.6%, forg= 20.3% <<<
>>> Test on task  7 : loss=1.619 | TAw acc= 89.0%, forg= -0.8%| TAg acc= 54.2%, forg=  1.7% <<<
>>> Test on task  8 : loss=1.863 | TAw acc= 95.6%, forg= -1.8%| TAg acc= 42.5%, forg= 28.3% <<<
>>> Test on task  9 : loss=1.567 | TAw acc= 89.0%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=5.008, TAw acc= 46.1% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=3.351, TAw acc= 63.2% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.535, TAw acc= 69.7% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.016, TAw acc= 72.4% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.754, TAw acc= 84.2% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.750, TAw acc= 84.2% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.746, TAw acc= 84.2% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.742, TAw acc= 84.2% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.739, TAw acc= 84.2% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.735, TAw acc= 84.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1802 train exemplars, time=  0.0s
1802
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.950 | TAw acc= 91.6%, forg=  1.1%| TAg acc= 80.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=0.779 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 90.7%, forg=  1.9% <<<
>>> Test on task  2 : loss=1.117 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 76.8%, forg=  5.4% <<<
>>> Test on task  3 : loss=1.168 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 80.6%, forg=  2.8% <<<
>>> Test on task  4 : loss=0.898 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 88.2%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.142 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 80.3%, forg=  1.6% <<<
>>> Test on task  6 : loss=1.517 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 69.5%, forg=  8.5% <<<
>>> Test on task  7 : loss=1.474 | TAw acc= 90.7%, forg= -1.7%| TAg acc= 62.7%, forg= -6.8% <<<
>>> Test on task  8 : loss=1.603 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 51.3%, forg= 19.5% <<<
>>> Test on task  9 : loss=1.688 | TAw acc= 95.4%, forg= -6.4%| TAg acc= 44.0%, forg= 15.6% <<<
>>> Test on task 10 : loss=1.703 | TAw acc= 83.8%, forg=  0.0%| TAg acc= 59.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.475, TAw acc= 17.8% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=3.680, TAw acc= 53.4% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.681, TAw acc= 79.5% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.117, TAw acc= 82.2% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.778, TAw acc= 90.4% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.776, TAw acc= 90.4% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.774, TAw acc= 90.4% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.771, TAw acc= 90.4% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.769, TAw acc= 90.4% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.767, TAw acc= 90.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1942 train exemplars, time=  0.0s
1942
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.947 | TAw acc= 93.3%, forg= -0.6%| TAg acc= 82.1%, forg=  6.1% <<<
>>> Test on task  1 : loss=0.871 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 82.2%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.107 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 74.1%, forg=  8.0% <<<
>>> Test on task  3 : loss=1.153 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.1%, forg= 10.2% <<<
>>> Test on task  4 : loss=0.880 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 87.4%, forg=  1.6% <<<
>>> Test on task  5 : loss=1.075 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 86.1%, forg= -4.1% <<<
>>> Test on task  6 : loss=1.458 | TAw acc= 94.9%, forg= -2.5%| TAg acc= 69.5%, forg=  8.5% <<<
>>> Test on task  7 : loss=1.384 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 66.1%, forg= -3.4% <<<
>>> Test on task  8 : loss=1.430 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 63.7%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.604 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 47.7%, forg= 11.9% <<<
>>> Test on task 10 : loss=1.772 | TAw acc= 87.6%, forg= -3.8%| TAg acc= 52.4%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.572 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 62.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.262, TAw acc= 31.4% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=3.834, TAw acc= 40.0% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=3.034, TAw acc= 54.3% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.450, TAw acc= 71.4% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.216, TAw acc= 82.9% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.212, TAw acc= 82.9% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.207, TAw acc= 82.9% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.204, TAw acc= 82.9% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.200, TAw acc= 82.9% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.197, TAw acc= 82.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2082 train exemplars, time=  0.0s
2082
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.914 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 79.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=0.747 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 89.7%, forg=  2.8% <<<
>>> Test on task  2 : loss=1.114 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 76.8%, forg=  5.4% <<<
>>> Test on task  3 : loss=1.150 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.9%, forg=  6.5% <<<
>>> Test on task  4 : loss=0.826 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 87.4%, forg=  1.6% <<<
>>> Test on task  5 : loss=0.996 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 90.2%, forg= -4.1% <<<
>>> Test on task  6 : loss=1.402 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 70.3%, forg=  7.6% <<<
>>> Test on task  7 : loss=1.330 | TAw acc= 89.8%, forg=  0.8%| TAg acc= 65.3%, forg=  0.8% <<<
>>> Test on task  8 : loss=1.325 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task  9 : loss=1.447 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 56.0%, forg=  3.7% <<<
>>> Test on task 10 : loss=1.663 | TAw acc= 89.5%, forg= -1.9%| TAg acc= 54.3%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.648 | TAw acc= 98.0%, forg= -3.0%| TAg acc= 47.0%, forg= 15.0% <<<
>>> Test on task 12 : loss=1.880 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 48.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=5.458, TAw acc= 34.3% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=3.712, TAw acc= 55.2% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.782, TAw acc= 59.7% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.222, TAw acc= 76.1% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.939, TAw acc= 85.1% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.936, TAw acc= 88.1% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.933, TAw acc= 88.1% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.930, TAw acc= 88.1% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.927, TAw acc= 89.6% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.925, TAw acc= 89.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2222 train exemplars, time=  0.0s
2222
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.873 | TAw acc= 93.9%, forg= -0.6%| TAg acc= 83.8%, forg=  4.5% <<<
>>> Test on task  1 : loss=0.819 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.0%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.078 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 71.4%, forg= 10.7% <<<
>>> Test on task  3 : loss=1.135 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.1%, forg= 10.2% <<<
>>> Test on task  4 : loss=0.774 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 87.4%, forg=  1.6% <<<
>>> Test on task  5 : loss=1.096 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.1%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.400 | TAw acc= 93.2%, forg=  1.7%| TAg acc= 68.6%, forg=  9.3% <<<
>>> Test on task  7 : loss=1.251 | TAw acc= 89.8%, forg=  0.8%| TAg acc= 67.8%, forg= -1.7% <<<
>>> Test on task  8 : loss=1.279 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 68.1%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.343 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 58.7%, forg=  0.9% <<<
>>> Test on task 10 : loss=1.524 | TAw acc= 88.6%, forg=  1.0%| TAg acc= 64.8%, forg= -5.7% <<<
>>> Test on task 11 : loss=1.522 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 58.0%, forg=  4.0% <<<
>>> Test on task 12 : loss=1.819 | TAw acc= 93.8%, forg= -3.1%| TAg acc= 51.5%, forg= -3.1% <<<
>>> Test on task 13 : loss=1.593 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 62.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=5.572, TAw acc= 38.6% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=3.735, TAw acc= 52.9% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.644, TAw acc= 64.3% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.056, TAw acc= 71.4% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.772, TAw acc= 77.1% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.768, TAw acc= 77.1% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.764, TAw acc= 77.1% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.759, TAw acc= 77.1% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.756, TAw acc= 78.6% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.752, TAw acc= 78.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2362 train exemplars, time=  0.0s
2362
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.934 | TAw acc= 93.3%, forg=  0.6%| TAg acc= 79.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=0.800 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.0%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.096 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 75.9%, forg=  6.2% <<<
>>> Test on task  3 : loss=1.188 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.9%, forg= 19.4% <<<
>>> Test on task  4 : loss=0.826 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 85.8%, forg=  3.1% <<<
>>> Test on task  5 : loss=0.989 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 88.5%, forg=  1.6% <<<
>>> Test on task  6 : loss=1.333 | TAw acc= 95.8%, forg= -0.8%| TAg acc= 71.2%, forg=  6.8% <<<
>>> Test on task  7 : loss=1.277 | TAw acc= 89.8%, forg=  0.8%| TAg acc= 69.5%, forg= -1.7% <<<
>>> Test on task  8 : loss=1.329 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 61.1%, forg=  9.7% <<<
>>> Test on task  9 : loss=1.189 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 67.9%, forg= -8.3% <<<
>>> Test on task 10 : loss=1.447 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 68.6%, forg= -3.8% <<<
>>> Test on task 11 : loss=1.369 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 65.0%, forg= -3.0% <<<
>>> Test on task 12 : loss=1.710 | TAw acc= 95.9%, forg= -2.1%| TAg acc= 52.6%, forg= -1.0% <<<
>>> Test on task 13 : loss=1.606 | TAw acc= 97.9%, forg= -4.3%| TAg acc= 46.8%, forg= 16.0% <<<
>>> Test on task 14 : loss=1.515 | TAw acc= 85.7%, forg=  0.0%| TAg acc= 54.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=5.102, TAw acc= 51.9% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=3.184, TAw acc= 58.0% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.298, TAw acc= 64.2% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.816, TAw acc= 74.1% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.612, TAw acc= 82.7% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.609, TAw acc= 82.7% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.606, TAw acc= 82.7% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.603, TAw acc= 82.7% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.601, TAw acc= 82.7% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.599, TAw acc= 82.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2502 train exemplars, time=  0.0s
2502
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.922 | TAw acc= 94.4%, forg= -0.6%| TAg acc= 81.0%, forg=  7.3% <<<
>>> Test on task  1 : loss=0.735 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 87.9%, forg=  4.7% <<<
>>> Test on task  2 : loss=1.109 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 74.1%, forg=  8.0% <<<
>>> Test on task  3 : loss=1.119 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.6%, forg= 15.7% <<<
>>> Test on task  4 : loss=0.853 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 85.8%, forg=  3.1% <<<
>>> Test on task  5 : loss=1.035 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.0%, forg=  8.2% <<<
>>> Test on task  6 : loss=1.302 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 71.2%, forg=  6.8% <<<
>>> Test on task  7 : loss=1.298 | TAw acc= 89.0%, forg=  1.7%| TAg acc= 62.7%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.282 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 62.8%, forg=  8.0% <<<
>>> Test on task  9 : loss=1.182 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 65.1%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.445 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 67.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.313 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.0%, forg= -3.0% <<<
>>> Test on task 12 : loss=1.587 | TAw acc= 94.8%, forg=  1.0%| TAg acc= 57.7%, forg= -5.2% <<<
>>> Test on task 13 : loss=1.411 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 57.4%, forg=  5.3% <<<
>>> Test on task 14 : loss=1.599 | TAw acc= 96.9%, forg=-11.2%| TAg acc= 63.3%, forg= -9.2% <<<
>>> Test on task 15 : loss=1.520 | TAw acc= 88.2%, forg=  0.0%| TAg acc= 60.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=6.018, TAw acc= 52.9% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=3.581, TAw acc= 60.0% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.502, TAw acc= 75.7% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.011, TAw acc= 84.3% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.739, TAw acc= 90.0% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.737, TAw acc= 90.0% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.735, TAw acc= 90.0% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.733, TAw acc= 90.0% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.731, TAw acc= 90.0% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.728, TAw acc= 90.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2642 train exemplars, time=  0.0s
2642
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.903 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 80.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=0.735 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 86.0%, forg=  6.5% <<<
>>> Test on task  2 : loss=1.057 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 76.8%, forg=  5.4% <<<
>>> Test on task  3 : loss=1.147 | TAw acc= 98.1%, forg= -0.9%| TAg acc= 66.7%, forg= 16.7% <<<
>>> Test on task  4 : loss=0.837 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 83.5%, forg=  5.5% <<<
>>> Test on task  5 : loss=0.945 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 87.7%, forg=  2.5% <<<
>>> Test on task  6 : loss=1.287 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 70.3%, forg=  7.6% <<<
>>> Test on task  7 : loss=1.362 | TAw acc= 89.0%, forg=  1.7%| TAg acc= 57.6%, forg= 11.9% <<<
>>> Test on task  8 : loss=1.240 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 62.8%, forg=  8.0% <<<
>>> Test on task  9 : loss=1.143 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 74.3%, forg= -6.4% <<<
>>> Test on task 10 : loss=1.396 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 72.4%, forg= -3.8% <<<
>>> Test on task 11 : loss=1.274 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 65.0%, forg=  3.0% <<<
>>> Test on task 12 : loss=1.509 | TAw acc= 96.9%, forg= -1.0%| TAg acc= 72.2%, forg=-14.4% <<<
>>> Test on task 13 : loss=1.293 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 64.9%, forg= -2.1% <<<
>>> Test on task 14 : loss=1.513 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 56.1%, forg=  7.1% <<<
>>> Test on task 15 : loss=1.700 | TAw acc= 98.2%, forg=-10.0%| TAg acc= 45.5%, forg= 14.5% <<<
>>> Test on task 16 : loss=1.718 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 58.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=5.001, TAw acc= 39.5% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.930, TAw acc= 71.6% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.030, TAw acc= 81.5% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.649, TAw acc= 86.4% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.345, TAw acc= 86.4% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.344, TAw acc= 86.4% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.343, TAw acc= 86.4% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.342, TAw acc= 86.4% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.341, TAw acc= 86.4% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.340, TAw acc= 86.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2782 train exemplars, time=  0.0s
2782
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.935 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 79.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=0.740 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.0%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.080 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 73.2%, forg=  8.9% <<<
>>> Test on task  3 : loss=1.125 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.7%, forg=  4.6% <<<
>>> Test on task  4 : loss=0.821 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 85.0%, forg=  3.9% <<<
>>> Test on task  5 : loss=0.974 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 86.1%, forg=  4.1% <<<
>>> Test on task  6 : loss=1.314 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 69.5%, forg=  8.5% <<<
>>> Test on task  7 : loss=1.222 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 69.5%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.180 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 65.5%, forg=  5.3% <<<
>>> Test on task  9 : loss=1.266 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 60.6%, forg= 13.8% <<<
>>> Test on task 10 : loss=1.387 | TAw acc= 90.5%, forg= -1.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.171 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.0%, forg= -3.0% <<<
>>> Test on task 12 : loss=1.439 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 75.3%, forg= -3.1% <<<
>>> Test on task 13 : loss=1.273 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 62.8%, forg=  2.1% <<<
>>> Test on task 14 : loss=1.341 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.4%, forg= -9.2% <<<
>>> Test on task 15 : loss=1.518 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 56.4%, forg=  3.6% <<<
>>> Test on task 16 : loss=1.894 | TAw acc= 96.9%, forg= -3.1%| TAg acc= 39.6%, forg= 18.8% <<<
>>> Test on task 17 : loss=1.234 | TAw acc= 92.7%, forg=  0.0%| TAg acc= 69.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=5.452, TAw acc= 43.5% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=3.100, TAw acc= 60.0% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=2.111, TAw acc= 81.2% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.645, TAw acc= 85.9% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 85.9% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.452, TAw acc= 85.9% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.450, TAw acc= 85.9% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.448, TAw acc= 85.9% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.446, TAw acc= 85.9% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.444, TAw acc= 85.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2922 train exemplars, time=  0.0s
2922
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.900 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 81.6%, forg=  6.7% <<<
>>> Test on task  1 : loss=0.710 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 84.1%, forg=  8.4% <<<
>>> Test on task  2 : loss=1.187 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 67.9%, forg= 14.3% <<<
>>> Test on task  3 : loss=1.135 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 76.9%, forg=  6.5% <<<
>>> Test on task  4 : loss=0.808 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 85.0%, forg=  3.9% <<<
>>> Test on task  5 : loss=1.045 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.8%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.285 | TAw acc= 94.9%, forg=  0.8%| TAg acc= 72.0%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.248 | TAw acc= 89.8%, forg=  0.8%| TAg acc= 66.9%, forg=  2.5% <<<
>>> Test on task  8 : loss=1.247 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 62.8%, forg=  8.0% <<<
>>> Test on task  9 : loss=1.103 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 71.6%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.353 | TAw acc= 90.5%, forg=  0.0%| TAg acc= 76.2%, forg= -3.8% <<<
>>> Test on task 11 : loss=1.125 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg= -3.0% <<<
>>> Test on task 12 : loss=1.451 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 70.1%, forg=  5.2% <<<
>>> Test on task 13 : loss=1.124 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 67.0%, forg= -2.1% <<<
>>> Test on task 14 : loss=1.270 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 75.5%, forg= -3.1% <<<
>>> Test on task 15 : loss=1.539 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 54.5%, forg=  5.5% <<<
>>> Test on task 16 : loss=1.748 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 45.8%, forg= 12.5% <<<
>>> Test on task 17 : loss=1.394 | TAw acc= 95.4%, forg= -2.8%| TAg acc= 66.1%, forg=  3.7% <<<
>>> Test on task 18 : loss=1.573 | TAw acc= 85.3%, forg=  0.0%| TAg acc= 64.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=5.930, TAw acc= 38.0% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=3.309, TAw acc= 69.0% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.274, TAw acc= 81.7% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.815, TAw acc= 90.1% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.559, TAw acc= 94.4% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.3s loss=1.557, TAw acc= 94.4% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.555, TAw acc= 94.4% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.553, TAw acc= 94.4% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.551, TAw acc= 94.4% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.549, TAw acc= 94.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3062 train exemplars, time=  0.1s
3062
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.922 | TAw acc= 95.5%, forg= -1.1%| TAg acc= 82.7%, forg=  5.6% <<<
>>> Test on task  1 : loss=0.759 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 84.1%, forg=  8.4% <<<
>>> Test on task  2 : loss=1.145 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 69.6%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.124 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 70.4%, forg= 13.0% <<<
>>> Test on task  4 : loss=0.794 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 84.3%, forg=  4.7% <<<
>>> Test on task  5 : loss=0.935 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 86.9%, forg=  3.3% <<<
>>> Test on task  6 : loss=1.310 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.7%, forg=  4.2% <<<
>>> Test on task  7 : loss=1.188 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 70.3%, forg= -0.8% <<<
>>> Test on task  8 : loss=1.247 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 61.9%, forg=  8.8% <<<
>>> Test on task  9 : loss=1.202 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 64.2%, forg= 10.1% <<<
>>> Test on task 10 : loss=1.345 | TAw acc= 90.5%, forg=  0.0%| TAg acc= 74.3%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.153 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.389 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 71.1%, forg=  4.1% <<<
>>> Test on task 13 : loss=1.027 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 75.5%, forg= -8.5% <<<
>>> Test on task 14 : loss=1.260 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 73.5%, forg=  2.0% <<<
>>> Test on task 15 : loss=1.398 | TAw acc=100.0%, forg= -1.8%| TAg acc= 65.5%, forg= -5.5% <<<
>>> Test on task 16 : loss=1.601 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 58.3%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.263 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 68.8%, forg=  0.9% <<<
>>> Test on task 18 : loss=1.825 | TAw acc= 91.4%, forg= -6.0%| TAg acc= 50.0%, forg= 14.7% <<<
>>> Test on task 19 : loss=1.390 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 71.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=5.168, TAw acc= 51.3% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.757, TAw acc= 69.7% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.876, TAw acc= 89.5% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.454, TAw acc= 97.4% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc=100.0% | *
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.327, TAw acc=100.0% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.323, TAw acc=100.0% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.320, TAw acc=100.0% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.316, TAw acc=100.0% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.313, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3202 train exemplars, time=  0.0s
3202
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.918 | TAw acc= 95.0%, forg=  0.6%| TAg acc= 80.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=0.805 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 77.6%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.129 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 73.2%, forg=  8.9% <<<
>>> Test on task  3 : loss=1.024 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 78.7%, forg=  4.6% <<<
>>> Test on task  4 : loss=0.762 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 82.7%, forg=  6.3% <<<
>>> Test on task  5 : loss=0.945 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 84.4%, forg=  5.7% <<<
>>> Test on task  6 : loss=1.297 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 66.1%, forg= 11.9% <<<
>>> Test on task  7 : loss=1.203 | TAw acc= 89.8%, forg=  0.8%| TAg acc= 67.8%, forg=  2.5% <<<
>>> Test on task  8 : loss=1.229 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 63.7%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.188 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  7.3% <<<
>>> Test on task 10 : loss=1.310 | TAw acc= 89.5%, forg=  1.0%| TAg acc= 69.5%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.166 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.370 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 78.4%, forg= -3.1% <<<
>>> Test on task 13 : loss=1.097 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 72.3%, forg=  3.2% <<<
>>> Test on task 14 : loss=1.130 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 74.5%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.358 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 65.5%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.531 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 61.5%, forg= -3.1% <<<
>>> Test on task 17 : loss=1.171 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 70.6%, forg= -0.9% <<<
>>> Test on task 18 : loss=1.692 | TAw acc= 92.2%, forg= -0.9%| TAg acc= 59.5%, forg=  5.2% <<<
>>> Test on task 19 : loss=1.587 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 54.6%, forg= 16.5% <<<
>>> Test on task 20 : loss=1.384 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=4.869, TAw acc= 61.4% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=2.595, TAw acc= 76.1% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.846, TAw acc= 83.0% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.511, TAw acc= 87.5% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.341, TAw acc= 88.6% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.339, TAw acc= 89.8% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.337, TAw acc= 89.8% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.335, TAw acc= 89.8% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 89.8% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 89.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3342 train exemplars, time=  0.0s
3342
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.963 | TAw acc= 95.0%, forg=  0.6%| TAg acc= 77.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=0.790 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 82.2%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.110 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 73.2%, forg=  8.9% <<<
>>> Test on task  3 : loss=1.104 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.4%, forg= 13.0% <<<
>>> Test on task  4 : loss=0.817 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 78.7%, forg= 10.2% <<<
>>> Test on task  5 : loss=0.936 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 89.3%, forg=  0.8% <<<
>>> Test on task  6 : loss=1.342 | TAw acc= 94.9%, forg=  0.8%| TAg acc= 66.1%, forg= 11.9% <<<
>>> Test on task  7 : loss=1.211 | TAw acc= 91.5%, forg= -0.8%| TAg acc= 69.5%, forg=  0.8% <<<
>>> Test on task  8 : loss=1.243 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 61.1%, forg=  9.7% <<<
>>> Test on task  9 : loss=1.200 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 68.8%, forg=  5.5% <<<
>>> Test on task 10 : loss=1.322 | TAw acc= 90.5%, forg=  0.0%| TAg acc= 72.4%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.138 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.0%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.287 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.2%, forg=  6.2% <<<
>>> Test on task 13 : loss=0.992 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 76.6%, forg= -1.1% <<<
>>> Test on task 14 : loss=1.109 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 82.7%, forg= -7.1% <<<
>>> Test on task 15 : loss=1.288 | TAw acc= 94.5%, forg=  5.5%| TAg acc= 69.1%, forg= -3.6% <<<
>>> Test on task 16 : loss=1.457 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 62.5%, forg= -1.0% <<<
>>> Test on task 17 : loss=1.123 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.5%, forg= -1.8% <<<
>>> Test on task 18 : loss=1.654 | TAw acc= 89.7%, forg=  2.6%| TAg acc= 60.3%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.407 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 59.8%, forg= 11.3% <<<
>>> Test on task 20 : loss=1.566 | TAw acc= 98.1%, forg= -1.0%| TAg acc= 61.9%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.209 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 70.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=5.384, TAw acc= 32.1% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.969, TAw acc= 65.4% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.951, TAw acc= 88.9% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.624, TAw acc= 92.6% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.420, TAw acc= 96.3% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.417, TAw acc= 96.3% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.413, TAw acc= 96.3% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.411, TAw acc= 96.3% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.408, TAw acc= 96.3% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.406, TAw acc= 96.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3482 train exemplars, time=  0.0s
3482
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.970 | TAw acc= 94.4%, forg=  1.1%| TAg acc= 76.0%, forg= 12.3% <<<
>>> Test on task  1 : loss=0.790 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 83.2%, forg=  9.3% <<<
>>> Test on task  2 : loss=1.144 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 72.3%, forg=  9.8% <<<
>>> Test on task  3 : loss=1.121 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 73.1%, forg= 10.2% <<<
>>> Test on task  4 : loss=0.826 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 77.2%, forg= 11.8% <<<
>>> Test on task  5 : loss=0.930 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.6%, forg=  6.6% <<<
>>> Test on task  6 : loss=1.275 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 72.0%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.269 | TAw acc= 92.4%, forg= -0.8%| TAg acc= 65.3%, forg=  5.1% <<<
>>> Test on task  8 : loss=1.235 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task  9 : loss=1.151 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.9%, forg=  6.4% <<<
>>> Test on task 10 : loss=1.273 | TAw acc= 90.5%, forg=  0.0%| TAg acc= 74.3%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.076 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg= -1.0% <<<
>>> Test on task 12 : loss=1.282 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.2%, forg=  6.2% <<<
>>> Test on task 13 : loss=1.034 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 72.3%, forg=  4.3% <<<
>>> Test on task 14 : loss=1.088 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.4%, forg= 10.2% <<<
>>> Test on task 15 : loss=1.221 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 75.5%, forg= -6.4% <<<
>>> Test on task 16 : loss=1.533 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 59.4%, forg=  3.1% <<<
>>> Test on task 17 : loss=1.027 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.0%, forg= -5.5% <<<
>>> Test on task 18 : loss=1.538 | TAw acc= 90.5%, forg=  1.7%| TAg acc= 61.2%, forg=  3.4% <<<
>>> Test on task 19 : loss=1.258 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.1%, forg=  0.0% <<<
>>> Test on task 20 : loss=1.361 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.6%, forg= -1.0% <<<
>>> Test on task 21 : loss=1.467 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 52.5%, forg= 17.8% <<<
>>> Test on task 22 : loss=1.219 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=5.832, TAw acc= 43.8% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=3.281, TAw acc= 57.5% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.219, TAw acc= 78.8% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.726, TAw acc= 82.5% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.423, TAw acc= 91.2% | *
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.419, TAw acc= 91.2% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 91.2% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.412, TAw acc= 91.2% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.409, TAw acc= 91.2% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.405, TAw acc= 91.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3622 train exemplars, time=  0.0s
3622
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.920 | TAw acc= 94.4%, forg=  1.1%| TAg acc= 79.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=0.779 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 82.2%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.133 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 71.4%, forg= 10.7% <<<
>>> Test on task  3 : loss=1.065 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 73.1%, forg= 10.2% <<<
>>> Test on task  4 : loss=0.876 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 76.4%, forg= 12.6% <<<
>>> Test on task  5 : loss=0.924 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 87.7%, forg=  2.5% <<<
>>> Test on task  6 : loss=1.287 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 72.0%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.204 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 66.1%, forg=  4.2% <<<
>>> Test on task  8 : loss=1.246 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 63.7%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.163 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 68.8%, forg=  5.5% <<<
>>> Test on task 10 : loss=1.298 | TAw acc= 90.5%, forg=  0.0%| TAg acc= 73.3%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.068 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.263 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.2%, forg=  6.2% <<<
>>> Test on task 13 : loss=0.939 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 79.8%, forg= -3.2% <<<
>>> Test on task 14 : loss=1.058 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 68.4%, forg= 14.3% <<<
>>> Test on task 15 : loss=1.213 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 74.5%, forg=  0.9% <<<
>>> Test on task 16 : loss=1.461 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 66.7%, forg= -4.2% <<<
>>> Test on task 17 : loss=1.053 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.480 | TAw acc= 90.5%, forg=  1.7%| TAg acc= 65.5%, forg= -0.9% <<<
>>> Test on task 19 : loss=1.259 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.1%, forg=  2.1% <<<
>>> Test on task 20 : loss=1.332 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 61.9%, forg=  5.7% <<<
>>> Test on task 21 : loss=1.390 | TAw acc= 97.5%, forg= -1.7%| TAg acc= 55.1%, forg= 15.3% <<<
>>> Test on task 22 : loss=1.371 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 59.6%, forg=  9.2% <<<
>>> Test on task 23 : loss=1.118 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=6.124, TAw acc= 55.1% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=3.365, TAw acc= 75.6% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.260, TAw acc= 92.3% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.814, TAw acc= 96.2% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.683, TAw acc= 96.2% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.680, TAw acc= 96.2% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.677, TAw acc= 96.2% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.674, TAw acc= 96.2% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.671, TAw acc= 96.2% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.669, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3762 train exemplars, time=  0.0s
3762
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.910 | TAw acc= 96.1%, forg= -0.6%| TAg acc= 80.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=0.864 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 76.6%, forg= 15.9% <<<
>>> Test on task  2 : loss=1.217 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 69.6%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.039 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.5%, forg=  1.9% <<<
>>> Test on task  4 : loss=0.823 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 81.1%, forg=  7.9% <<<
>>> Test on task  5 : loss=0.897 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 86.1%, forg=  4.1% <<<
>>> Test on task  6 : loss=1.341 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 68.6%, forg=  9.3% <<<
>>> Test on task  7 : loss=1.237 | TAw acc= 91.5%, forg=  0.8%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.236 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 60.2%, forg= 10.6% <<<
>>> Test on task  9 : loss=1.178 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.6%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.333 | TAw acc= 91.4%, forg= -1.0%| TAg acc= 70.5%, forg=  5.7% <<<
>>> Test on task 11 : loss=1.042 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
>>> Test on task 12 : loss=1.322 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 70.1%, forg=  8.2% <<<
>>> Test on task 13 : loss=0.921 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 77.7%, forg=  2.1% <<<
>>> Test on task 14 : loss=1.049 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.4%, forg= 10.2% <<<
>>> Test on task 15 : loss=1.174 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 78.2%, forg= -2.7% <<<
>>> Test on task 16 : loss=1.417 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 66.7%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.005 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.9%, forg= -0.9% <<<
>>> Test on task 18 : loss=1.464 | TAw acc= 90.5%, forg=  1.7%| TAg acc= 68.1%, forg= -2.6% <<<
>>> Test on task 19 : loss=1.197 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.1%, forg=  0.0% <<<
>>> Test on task 20 : loss=1.312 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 71.4%, forg= -3.8% <<<
>>> Test on task 21 : loss=1.333 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 55.1%, forg= 15.3% <<<
>>> Test on task 22 : loss=1.330 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 65.1%, forg=  3.7% <<<
>>> Test on task 23 : loss=1.272 | TAw acc= 98.2%, forg= -1.8%| TAg acc= 55.0%, forg= 19.3% <<<
>>> Test on task 24 : loss=1.510 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 58.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=5.492, TAw acc= 30.8% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=3.277, TAw acc= 61.5% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.217, TAw acc= 92.3% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.712, TAw acc= 92.3% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.568, TAw acc= 97.4% | *
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.565, TAw acc= 97.4% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.563, TAw acc= 97.4% | *
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.3s loss=1.560, TAw acc= 97.4% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.558, TAw acc= 97.4% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.555, TAw acc= 97.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3902 train exemplars, time=  0.0s
3902
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.980 | TAw acc= 95.5%, forg=  0.6%| TAg acc= 77.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=0.805 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 82.2%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.174 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg= 10.7% <<<
>>> Test on task  3 : loss=1.068 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.5% <<<
>>> Test on task  4 : loss=0.862 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 80.3%, forg=  8.7% <<<
>>> Test on task  5 : loss=0.975 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.8%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.422 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 69.5%, forg=  8.5% <<<
>>> Test on task  7 : loss=1.316 | TAw acc= 91.5%, forg=  0.8%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.216 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 64.6%, forg=  6.2% <<<
>>> Test on task  9 : loss=1.195 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 66.1%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.325 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 70.5%, forg=  5.7% <<<
>>> Test on task 11 : loss=1.041 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
>>> Test on task 12 : loss=1.310 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 71.1%, forg=  7.2% <<<
>>> Test on task 13 : loss=0.946 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 73.4%, forg=  6.4% <<<
>>> Test on task 14 : loss=1.064 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 71.4%, forg= 11.2% <<<
>>> Test on task 15 : loss=1.165 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 75.5%, forg=  2.7% <<<
>>> Test on task 16 : loss=1.438 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.8%, forg= -2.1% <<<
>>> Test on task 17 : loss=0.987 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 80.7%, forg= -1.8% <<<
>>> Test on task 18 : loss=1.409 | TAw acc= 90.5%, forg=  1.7%| TAg acc= 69.0%, forg= -0.9% <<<
>>> Test on task 19 : loss=1.144 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.2%, forg= -2.1% <<<
>>> Test on task 20 : loss=1.277 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.5%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.237 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.4%, forg=  5.9% <<<
>>> Test on task 22 : loss=1.170 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 70.6%, forg= -1.8% <<<
>>> Test on task 23 : loss=1.065 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 73.4%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.743 | TAw acc= 95.4%, forg= -1.9%| TAg acc= 39.8%, forg= 18.5% <<<
>>> Test on task 25 : loss=1.516 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 60.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=5.061, TAw acc= 64.8% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.326, TAw acc= 85.2% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.621, TAw acc= 89.8% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 93.2% | *
| Epoch   5, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.218, TAw acc= 92.0% | *
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.214, TAw acc= 92.0% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.211, TAw acc= 92.0% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.207, TAw acc= 92.0% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 92.0% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.201, TAw acc= 92.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4042 train exemplars, time=  0.0s
4042
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.921 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 81.6%, forg=  6.7% <<<
>>> Test on task  1 : loss=0.802 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 81.3%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.210 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 69.6%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.056 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.5% <<<
>>> Test on task  4 : loss=0.876 | TAw acc= 96.9%, forg=  1.6%| TAg acc= 78.7%, forg= 10.2% <<<
>>> Test on task  5 : loss=0.972 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.8%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.388 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 66.9%, forg= 11.0% <<<
>>> Test on task  7 : loss=1.189 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 70.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.206 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task  9 : loss=1.195 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 66.1%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.328 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 74.3%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.021 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.0%, forg= -2.0% <<<
>>> Test on task 12 : loss=1.284 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 71.1%, forg=  7.2% <<<
>>> Test on task 13 : loss=0.916 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 77.7%, forg=  2.1% <<<
>>> Test on task 14 : loss=1.008 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 71.4%, forg= 11.2% <<<
>>> Test on task 15 : loss=1.140 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 78.2%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.438 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 66.7%, forg=  2.1% <<<
>>> Test on task 17 : loss=1.034 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 80.7%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.453 | TAw acc= 90.5%, forg=  1.7%| TAg acc= 70.7%, forg= -1.7% <<<
>>> Test on task 19 : loss=1.184 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.2%, forg=  1.0% <<<
>>> Test on task 20 : loss=1.326 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 63.8%, forg=  7.6% <<<
>>> Test on task 21 : loss=1.206 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task 22 : loss=1.182 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.9%, forg=  2.8% <<<
>>> Test on task 23 : loss=1.035 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 76.1%, forg= -1.8% <<<
>>> Test on task 24 : loss=1.601 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 52.8%, forg=  5.6% <<<
>>> Test on task 25 : loss=1.898 | TAw acc= 96.2%, forg= -1.9%| TAg acc= 32.4%, forg= 27.6% <<<
>>> Test on task 26 : loss=1.251 | TAw acc= 92.3%, forg=  0.0%| TAg acc= 70.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=5.191, TAw acc= 55.1% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=2.753, TAw acc= 68.5% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.949, TAw acc= 80.9% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.456, TAw acc= 84.3% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.250, TAw acc= 91.0% | *
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.250, TAw acc= 91.0% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.3s loss=1.250, TAw acc= 91.0% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 91.0% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 91.0% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 91.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4182 train exemplars, time=  0.0s
4182
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.991 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 76.5%, forg= 11.7% <<<
>>> Test on task  1 : loss=0.885 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 76.6%, forg= 15.9% <<<
>>> Test on task  2 : loss=1.157 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 74.1%, forg=  8.0% <<<
>>> Test on task  3 : loss=1.072 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.0%, forg=  8.3% <<<
>>> Test on task  4 : loss=0.902 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 78.7%, forg= 10.2% <<<
>>> Test on task  5 : loss=0.983 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 78.7%, forg= 11.5% <<<
>>> Test on task  6 : loss=1.440 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 66.1%, forg= 11.9% <<<
>>> Test on task  7 : loss=1.229 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 62.7%, forg=  7.6% <<<
>>> Test on task  8 : loss=1.183 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 63.7%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.217 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 68.8%, forg=  5.5% <<<
>>> Test on task 10 : loss=1.310 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 73.3%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.035 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.246 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 73.2%, forg=  5.2% <<<
>>> Test on task 13 : loss=1.043 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 69.1%, forg= 10.6% <<<
>>> Test on task 14 : loss=1.036 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 69.4%, forg= 13.3% <<<
>>> Test on task 15 : loss=1.141 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 78.2%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.458 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.939 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 78.9%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.441 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 67.2%, forg=  3.4% <<<
>>> Test on task 19 : loss=1.164 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.0%, forg=  5.2% <<<
>>> Test on task 20 : loss=1.205 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.5%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.250 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 59.3%, forg= 11.0% <<<
>>> Test on task 22 : loss=1.116 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  0.9% <<<
>>> Test on task 23 : loss=0.934 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 78.9%, forg= -2.8% <<<
>>> Test on task 24 : loss=1.599 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 49.1%, forg=  9.3% <<<
>>> Test on task 25 : loss=1.719 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 46.7%, forg= 13.3% <<<
>>> Test on task 26 : loss=1.464 | TAw acc= 92.3%, forg=  0.0%| TAg acc= 62.4%, forg=  8.5% <<<
>>> Test on task 27 : loss=1.169 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=5.064, TAw acc= 62.0% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=2.578, TAw acc= 79.3% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.880, TAw acc= 88.0% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.580, TAw acc= 97.8% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.367, TAw acc= 97.8% | *
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.365, TAw acc= 97.8% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.363, TAw acc= 97.8% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.361, TAw acc= 97.8% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.358, TAw acc= 97.8% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.356, TAw acc= 97.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4322 train exemplars, time=  0.1s
4322
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.980 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 80.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=0.868 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 78.5%, forg= 14.0% <<<
>>> Test on task  2 : loss=1.158 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 70.5%, forg= 11.6% <<<
>>> Test on task  3 : loss=1.131 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 68.5%, forg= 14.8% <<<
>>> Test on task  4 : loss=0.879 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 79.5%, forg=  9.4% <<<
>>> Test on task  5 : loss=1.021 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.5%, forg= 10.7% <<<
>>> Test on task  6 : loss=1.557 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 64.4%, forg= 13.6% <<<
>>> Test on task  7 : loss=1.317 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 60.2%, forg= 10.2% <<<
>>> Test on task  8 : loss=1.191 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.8%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.227 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.0%, forg=  7.3% <<<
>>> Test on task 10 : loss=1.349 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 69.5%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.029 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.283 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.0%, forg= 10.3% <<<
>>> Test on task 13 : loss=0.969 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 81.9%, forg= -2.1% <<<
>>> Test on task 14 : loss=1.022 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.4%, forg= 10.2% <<<
>>> Test on task 15 : loss=1.113 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 77.3%, forg=  0.9% <<<
>>> Test on task 16 : loss=1.422 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.921 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.7%, forg= -0.9% <<<
>>> Test on task 18 : loss=1.431 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 66.4%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.061 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.3%, forg= -3.1% <<<
>>> Test on task 20 : loss=1.165 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 74.3%, forg= -2.9% <<<
>>> Test on task 21 : loss=1.309 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.069 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  0.9% <<<
>>> Test on task 23 : loss=0.918 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 80.7%, forg= -1.8% <<<
>>> Test on task 24 : loss=1.517 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 50.9%, forg=  7.4% <<<
>>> Test on task 25 : loss=1.741 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 50.5%, forg=  9.5% <<<
>>> Test on task 26 : loss=1.450 | TAw acc= 92.3%, forg=  0.0%| TAg acc= 69.2%, forg=  1.7% <<<
>>> Test on task 27 : loss=1.427 | TAw acc= 96.7%, forg= -4.2%| TAg acc= 64.2%, forg= 10.8% <<<
>>> Test on task 28 : loss=1.213 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 69.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=6.031, TAw acc= 37.7% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=3.284, TAw acc= 77.9% | *
| Epoch   3, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.952, TAw acc= 93.5% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.464, TAw acc= 96.1% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.376, TAw acc= 97.4% | *
| Epoch   1, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.375, TAw acc= 97.4% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.373, TAw acc= 97.4% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.372, TAw acc= 97.4% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.371, TAw acc= 97.4% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.369, TAw acc= 97.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4462 train exemplars, time=  0.0s
4462
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.955 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 81.6%, forg=  6.7% <<<
>>> Test on task  1 : loss=0.810 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 83.2%, forg=  9.3% <<<
>>> Test on task  2 : loss=1.207 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 71.4%, forg= 10.7% <<<
>>> Test on task  3 : loss=1.108 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.9%, forg=  7.4% <<<
>>> Test on task  4 : loss=0.882 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 80.3%, forg=  8.7% <<<
>>> Test on task  5 : loss=0.989 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.1%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.519 | TAw acc= 94.9%, forg=  0.8%| TAg acc= 63.6%, forg= 14.4% <<<
>>> Test on task  7 : loss=1.218 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 67.8%, forg=  2.5% <<<
>>> Test on task  8 : loss=1.190 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task  9 : loss=1.188 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 71.6%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.379 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 69.5%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.014 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  3.0% <<<
>>> Test on task 12 : loss=1.309 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 69.1%, forg=  9.3% <<<
>>> Test on task 13 : loss=0.971 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 73.4%, forg=  8.5% <<<
>>> Test on task 14 : loss=1.023 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 69.4%, forg= 13.3% <<<
>>> Test on task 15 : loss=1.092 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 75.5%, forg=  2.7% <<<
>>> Test on task 16 : loss=1.384 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.925 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.441 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 69.8%, forg=  0.9% <<<
>>> Test on task 19 : loss=1.123 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.1%, forg=  7.2% <<<
>>> Test on task 20 : loss=1.219 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.5%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.215 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.057 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 70.6%, forg=  0.0% <<<
>>> Test on task 23 : loss=0.857 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 83.5%, forg= -2.8% <<<
>>> Test on task 24 : loss=1.486 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 52.8%, forg=  5.6% <<<
>>> Test on task 25 : loss=1.582 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 61.0%, forg= -1.0% <<<
>>> Test on task 26 : loss=1.375 | TAw acc= 93.2%, forg= -0.9%| TAg acc= 71.8%, forg= -0.9% <<<
>>> Test on task 27 : loss=1.358 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 63.3%, forg= 11.7% <<<
>>> Test on task 28 : loss=1.550 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 48.4%, forg= 21.3% <<<
>>> Test on task 29 : loss=1.340 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 70.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=4.919, TAw acc= 51.1% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=2.130, TAw acc= 78.7% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.384, TAw acc= 93.6% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.163, TAw acc= 94.7% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 96.8% |
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.167, TAw acc= 94.7% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.172, TAw acc= 94.7% |
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 94.7% |
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.179, TAw acc= 95.7% |
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.182, TAw acc= 95.7% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4602 train exemplars, time=  0.0s
4602
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.998 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 78.2%, forg= 10.1% <<<
>>> Test on task  1 : loss=0.929 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 79.4%, forg= 13.1% <<<
>>> Test on task  2 : loss=1.158 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 72.3%, forg=  9.8% <<<
>>> Test on task  3 : loss=1.091 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.9%, forg=  7.4% <<<
>>> Test on task  4 : loss=0.901 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 79.5%, forg=  9.4% <<<
>>> Test on task  5 : loss=1.025 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 77.9%, forg= 12.3% <<<
>>> Test on task  6 : loss=1.552 | TAw acc= 94.9%, forg=  0.8%| TAg acc= 62.7%, forg= 15.3% <<<
>>> Test on task  7 : loss=1.220 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 65.3%, forg=  5.1% <<<
>>> Test on task  8 : loss=1.297 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 62.8%, forg=  8.0% <<<
>>> Test on task  9 : loss=1.221 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 63.3%, forg= 11.0% <<<
>>> Test on task 10 : loss=1.492 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 71.4%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.075 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.0%, forg=  6.0% <<<
>>> Test on task 12 : loss=1.306 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 71.1%, forg=  7.2% <<<
>>> Test on task 13 : loss=0.972 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 73.4%, forg=  8.5% <<<
>>> Test on task 14 : loss=1.034 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 70.4%, forg= 12.2% <<<
>>> Test on task 15 : loss=1.179 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 77.3%, forg=  0.9% <<<
>>> Test on task 16 : loss=1.439 | TAw acc= 95.8%, forg=  1.0%| TAg acc= 67.7%, forg=  1.0% <<<
>>> Test on task 17 : loss=0.953 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task 18 : loss=1.480 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 67.2%, forg=  3.4% <<<
>>> Test on task 19 : loss=1.136 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.1%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.277 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 64.8%, forg=  9.5% <<<
>>> Test on task 21 : loss=1.285 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 58.5%, forg= 11.9% <<<
>>> Test on task 22 : loss=1.013 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 74.3%, forg= -3.7% <<<
>>> Test on task 23 : loss=0.868 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.7%, forg=  1.8% <<<
>>> Test on task 24 : loss=1.483 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 53.7%, forg=  4.6% <<<
>>> Test on task 25 : loss=1.637 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 59.0%, forg=  1.9% <<<
>>> Test on task 26 : loss=1.421 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 71.8%, forg=  0.0% <<<
>>> Test on task 27 : loss=1.337 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 62.5%, forg= 12.5% <<<
>>> Test on task 28 : loss=1.532 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 51.6%, forg= 18.0% <<<
>>> Test on task 29 : loss=1.672 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 49.5%, forg= 21.0% <<<
>>> Test on task 30 : loss=1.265 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 69.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=7.346, TAw acc= 43.8% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=4.069, TAw acc= 56.2% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=2.619, TAw acc= 89.1% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=2.018, TAw acc= 89.1% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.778, TAw acc= 95.3% | *
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.776, TAw acc= 95.3% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.774, TAw acc= 95.3% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.772, TAw acc= 95.3% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.771, TAw acc= 95.3% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.769, TAw acc= 95.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4742 train exemplars, time=  0.0s
4742
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.967 | TAw acc= 96.6%, forg= -0.6%| TAg acc= 77.1%, forg= 11.2% <<<
>>> Test on task  1 : loss=0.847 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 80.4%, forg= 12.1% <<<
>>> Test on task  2 : loss=1.227 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.8%, forg= 13.4% <<<
>>> Test on task  3 : loss=1.181 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.4%, forg= 13.9% <<<
>>> Test on task  4 : loss=0.865 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 82.7%, forg=  6.3% <<<
>>> Test on task  5 : loss=0.947 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.0%, forg=  8.2% <<<
>>> Test on task  6 : loss=1.494 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 65.3%, forg= 12.7% <<<
>>> Test on task  7 : loss=1.300 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.215 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task  9 : loss=1.170 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.393 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 76.2%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.000 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.310 | TAw acc= 97.9%, forg= -1.0%| TAg acc= 70.1%, forg=  8.2% <<<
>>> Test on task 13 : loss=0.892 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 79.8%, forg=  2.1% <<<
>>> Test on task 14 : loss=1.007 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 73.5%, forg=  9.2% <<<
>>> Test on task 15 : loss=1.137 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 75.5%, forg=  2.7% <<<
>>> Test on task 16 : loss=1.420 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.889 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 83.5%, forg= -1.8% <<<
>>> Test on task 18 : loss=1.447 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 67.2%, forg=  3.4% <<<
>>> Test on task 19 : loss=1.099 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.0%, forg=  8.2% <<<
>>> Test on task 20 : loss=1.163 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 71.4%, forg=  2.9% <<<
>>> Test on task 21 : loss=1.211 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.1%, forg=  4.2% <<<
>>> Test on task 22 : loss=1.012 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 70.6%, forg=  3.7% <<<
>>> Test on task 23 : loss=0.829 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.6%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.425 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 49.1%, forg=  9.3% <<<
>>> Test on task 25 : loss=1.662 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 57.1%, forg=  3.8% <<<
>>> Test on task 26 : loss=1.355 | TAw acc= 92.3%, forg=  0.9%| TAg acc= 73.5%, forg= -1.7% <<<
>>> Test on task 27 : loss=1.222 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 70.0%, forg=  5.0% <<<
>>> Test on task 28 : loss=1.444 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 56.6%, forg= 13.1% <<<
>>> Test on task 29 : loss=1.501 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 61.0%, forg=  9.5% <<<
>>> Test on task 30 : loss=1.713 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 48.8%, forg= 20.8% <<<
>>> Test on task 31 : loss=1.574 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 52.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=5.666, TAw acc= 51.8% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=3.043, TAw acc= 78.8% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.939, TAw acc= 91.8% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.529, TAw acc= 95.3% | *
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.326, TAw acc= 96.5% | *
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.322, TAw acc= 96.5% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.319, TAw acc= 96.5% | *
| Epoch   3, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.315, TAw acc= 96.5% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.312, TAw acc= 96.5% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.310, TAw acc= 96.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4882 train exemplars, time=  0.0s
4882
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.987 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 78.2%, forg= 10.1% <<<
>>> Test on task  1 : loss=0.842 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.3%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.191 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg= 10.7% <<<
>>> Test on task  3 : loss=1.173 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 68.5%, forg= 14.8% <<<
>>> Test on task  4 : loss=0.890 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 78.7%, forg= 10.2% <<<
>>> Test on task  5 : loss=1.036 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 77.9%, forg= 12.3% <<<
>>> Test on task  6 : loss=1.532 | TAw acc= 93.2%, forg=  2.5%| TAg acc= 60.2%, forg= 17.8% <<<
>>> Test on task  7 : loss=1.346 | TAw acc= 89.0%, forg=  3.4%| TAg acc= 58.5%, forg= 11.9% <<<
>>> Test on task  8 : loss=1.202 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 63.7%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.197 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.9%, forg=  6.4% <<<
>>> Test on task 10 : loss=1.451 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 74.3%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.036 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.0%, forg=  6.0% <<<
>>> Test on task 12 : loss=1.303 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 70.1%, forg=  8.2% <<<
>>> Test on task 13 : loss=0.875 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 79.8%, forg=  2.1% <<<
>>> Test on task 14 : loss=0.982 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 74.5%, forg=  8.2% <<<
>>> Test on task 15 : loss=1.154 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 75.5%, forg=  2.7% <<<
>>> Test on task 16 : loss=1.423 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.927 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 81.7%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.482 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 67.2%, forg=  3.4% <<<
>>> Test on task 19 : loss=1.109 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.1%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.183 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.241 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 65.3%, forg=  5.1% <<<
>>> Test on task 22 : loss=1.059 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 23 : loss=0.856 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.6%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.431 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 53.7%, forg=  4.6% <<<
>>> Test on task 25 : loss=1.649 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 57.1%, forg=  3.8% <<<
>>> Test on task 26 : loss=1.444 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 65.8%, forg=  7.7% <<<
>>> Test on task 27 : loss=1.149 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
>>> Test on task 28 : loss=1.392 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.436 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 64.8%, forg=  5.7% <<<
>>> Test on task 30 : loss=1.615 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 50.4%, forg= 19.2% <<<
>>> Test on task 31 : loss=1.677 | TAw acc= 97.8%, forg= -1.1%| TAg acc= 44.4%, forg=  7.8% <<<
>>> Test on task 32 : loss=1.200 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 70.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=5.268, TAw acc= 62.4% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=2.898, TAw acc= 77.4% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.843, TAw acc= 87.1% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.399, TAw acc= 94.6% | *
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.210, TAw acc= 95.7% | *
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.208, TAw acc= 95.7% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.206, TAw acc= 95.7% | *
| Epoch   3, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 95.7% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.202, TAw acc= 95.7% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.200, TAw acc= 95.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5022 train exemplars, time=  0.0s
5022
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.017 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 75.4%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.865 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.6%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.284 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 67.0%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.129 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.5% <<<
>>> Test on task  4 : loss=0.880 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 82.7%, forg=  6.3% <<<
>>> Test on task  5 : loss=0.977 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.6%, forg=  6.6% <<<
>>> Test on task  6 : loss=1.547 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 63.6%, forg= 14.4% <<<
>>> Test on task  7 : loss=1.329 | TAw acc= 89.0%, forg=  3.4%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.283 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 61.9%, forg=  8.8% <<<
>>> Test on task  9 : loss=1.241 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.423 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 74.3%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.046 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.323 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 69.1%, forg=  9.3% <<<
>>> Test on task 13 : loss=0.900 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.7%, forg=  3.2% <<<
>>> Test on task 14 : loss=1.043 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 69.4%, forg= 13.3% <<<
>>> Test on task 15 : loss=1.073 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 80.0%, forg= -1.8% <<<
>>> Test on task 16 : loss=1.454 | TAw acc= 95.8%, forg=  1.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.936 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 83.5%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.422 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 72.4%, forg= -1.7% <<<
>>> Test on task 19 : loss=1.043 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.3%, forg=  1.0% <<<
>>> Test on task 20 : loss=1.247 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.6%, forg=  5.7% <<<
>>> Test on task 21 : loss=1.213 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.1%, forg=  4.2% <<<
>>> Test on task 22 : loss=1.014 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 73.4%, forg=  0.9% <<<
>>> Test on task 23 : loss=0.797 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 83.5%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.388 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 55.6%, forg=  2.8% <<<
>>> Test on task 25 : loss=1.627 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 60.0%, forg=  1.0% <<<
>>> Test on task 26 : loss=1.416 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 70.1%, forg=  3.4% <<<
>>> Test on task 27 : loss=1.131 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 70.8%, forg=  4.2% <<<
>>> Test on task 28 : loss=1.406 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.426 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 65.7%, forg=  4.8% <<<
>>> Test on task 30 : loss=1.596 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 55.2%, forg= 14.4% <<<
>>> Test on task 31 : loss=1.462 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 55.6%, forg= -3.3% <<<
>>> Test on task 32 : loss=1.576 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 54.4%, forg= 15.8% <<<
>>> Test on task 33 : loss=1.085 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 74.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=5.479, TAw acc= 51.2% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.648, TAw acc= 73.8% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.802, TAw acc= 91.7% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.400, TAw acc= 95.2% | *
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 96.4% | *
| Epoch   1, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.284, TAw acc= 96.4% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 96.4% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.272, TAw acc= 96.4% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.266, TAw acc= 96.4% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.261, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5162 train exemplars, time=  0.0s
5162
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.021 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 75.4%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.873 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 76.6%, forg= 15.9% <<<
>>> Test on task  2 : loss=1.259 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.5%, forg= 11.6% <<<
>>> Test on task  3 : loss=1.222 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 67.6%, forg= 15.7% <<<
>>> Test on task  4 : loss=0.889 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 82.7%, forg=  6.3% <<<
>>> Test on task  5 : loss=0.992 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.1%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.606 | TAw acc= 96.6%, forg= -0.8%| TAg acc= 64.4%, forg= 13.6% <<<
>>> Test on task  7 : loss=1.308 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.250 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.5%, forg=  5.3% <<<
>>> Test on task  9 : loss=1.208 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 66.1%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.393 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 76.2%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.050 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.323 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 69.1%, forg=  9.3% <<<
>>> Test on task 13 : loss=0.873 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 76.6%, forg=  5.3% <<<
>>> Test on task 14 : loss=1.050 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 68.4%, forg= 14.3% <<<
>>> Test on task 15 : loss=1.122 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 77.3%, forg=  2.7% <<<
>>> Test on task 16 : loss=1.510 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 63.5%, forg=  5.2% <<<
>>> Test on task 17 : loss=0.952 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 81.7%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.454 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 68.1%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.013 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 80.4%, forg= -4.1% <<<
>>> Test on task 20 : loss=1.195 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.304 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 61.9%, forg=  8.5% <<<
>>> Test on task 22 : loss=1.013 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 73.4%, forg=  0.9% <<<
>>> Test on task 23 : loss=0.834 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 79.8%, forg=  3.7% <<<
>>> Test on task 24 : loss=1.340 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 58.3%, forg=  0.0% <<<
>>> Test on task 25 : loss=1.582 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 59.0%, forg=  1.9% <<<
>>> Test on task 26 : loss=1.445 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 70.9%, forg=  2.6% <<<
>>> Test on task 27 : loss=1.124 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 72.5%, forg=  2.5% <<<
>>> Test on task 28 : loss=1.369 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 57.4%, forg= 12.3% <<<
>>> Test on task 29 : loss=1.417 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 64.8%, forg=  5.7% <<<
>>> Test on task 30 : loss=1.586 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 57.6%, forg= 12.0% <<<
>>> Test on task 31 : loss=1.408 | TAw acc= 98.9%, forg= -1.1%| TAg acc= 62.2%, forg= -6.7% <<<
>>> Test on task 32 : loss=1.485 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 53.5%, forg= 16.7% <<<
>>> Test on task 33 : loss=1.280 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 71.5%, forg=  2.4% <<<
>>> Test on task 34 : loss=1.397 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 69.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.3s loss=6.593, TAw acc= 42.9% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=3.661, TAw acc= 68.6% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.327, TAw acc= 90.0% | *
| Epoch   4, time=  4.4s | Train: skip eval | Valid: time=  0.3s loss=1.940, TAw acc= 90.0% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.668, TAw acc= 95.7% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.663, TAw acc= 95.7% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.658, TAw acc= 95.7% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.654, TAw acc= 95.7% | *
| Epoch   4, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.650, TAw acc= 95.7% | *
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.645, TAw acc= 97.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5302 train exemplars, time=  0.1s
5302
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.009 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 78.2%, forg= 10.1% <<<
>>> Test on task  1 : loss=0.921 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.6%, forg= 15.9% <<<
>>> Test on task  2 : loss=1.260 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.6%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.155 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.9%, forg=  7.4% <<<
>>> Test on task  4 : loss=0.918 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 80.3%, forg=  8.7% <<<
>>> Test on task  5 : loss=1.047 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.5%, forg= 10.7% <<<
>>> Test on task  6 : loss=1.576 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.6%, forg= 14.4% <<<
>>> Test on task  7 : loss=1.396 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 61.0%, forg=  9.3% <<<
>>> Test on task  8 : loss=1.241 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 68.1%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.324 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 62.4%, forg= 11.9% <<<
>>> Test on task 10 : loss=1.440 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 73.3%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.116 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.0%, forg=  8.0% <<<
>>> Test on task 12 : loss=1.299 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 70.1%, forg=  8.2% <<<
>>> Test on task 13 : loss=0.807 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 81.9%, forg=  0.0% <<<
>>> Test on task 14 : loss=1.035 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 69.4%, forg= 13.3% <<<
>>> Test on task 15 : loss=1.126 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 78.2%, forg=  1.8% <<<
>>> Test on task 16 : loss=1.482 | TAw acc= 95.8%, forg=  1.0%| TAg acc= 66.7%, forg=  2.1% <<<
>>> Test on task 17 : loss=0.888 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 83.5%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.477 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 68.1%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.071 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 67.0%, forg= 13.4% <<<
>>> Test on task 20 : loss=1.296 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.6%, forg=  6.7% <<<
>>> Test on task 21 : loss=1.247 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.4%, forg=  5.9% <<<
>>> Test on task 22 : loss=1.005 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 72.5%, forg=  1.8% <<<
>>> Test on task 23 : loss=0.764 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.6%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.478 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 56.5%, forg=  1.9% <<<
>>> Test on task 25 : loss=1.600 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 65.7%, forg= -4.8% <<<
>>> Test on task 26 : loss=1.414 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task 27 : loss=1.086 | TAw acc= 97.5%, forg= -0.8%| TAg acc= 75.0%, forg=  0.0% <<<
>>> Test on task 28 : loss=1.408 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 55.7%, forg= 13.9% <<<
>>> Test on task 29 : loss=1.280 | TAw acc= 98.1%, forg= -1.0%| TAg acc= 70.5%, forg=  0.0% <<<
>>> Test on task 30 : loss=1.542 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 63.2%, forg=  6.4% <<<
>>> Test on task 31 : loss=1.358 | TAw acc= 96.7%, forg=  2.2%| TAg acc= 65.6%, forg= -3.3% <<<
>>> Test on task 32 : loss=1.439 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 53.5%, forg= 16.7% <<<
>>> Test on task 33 : loss=1.211 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 71.5%, forg=  2.4% <<<
>>> Test on task 34 : loss=1.694 | TAw acc= 96.5%, forg= -1.8%| TAg acc= 62.8%, forg=  7.1% <<<
>>> Test on task 35 : loss=1.851 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 48.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=5.916, TAw acc= 52.0% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=2.998, TAw acc= 73.3% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.786, TAw acc= 89.3% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.501, TAw acc= 97.3% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.321, TAw acc= 97.3% | *
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.317, TAw acc= 97.3% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.313, TAw acc= 97.3% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.309, TAw acc= 97.3% | *
| Epoch   4, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.306, TAw acc= 97.3% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.303, TAw acc= 97.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5442 train exemplars, time=  0.0s
5442
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.027 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 75.4%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.942 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 77.6%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.268 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.8%, forg= 13.4% <<<
>>> Test on task  3 : loss=1.194 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.2%, forg= 11.1% <<<
>>> Test on task  4 : loss=0.970 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 78.0%, forg= 11.0% <<<
>>> Test on task  5 : loss=1.051 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.1%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.613 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 64.4%, forg= 13.6% <<<
>>> Test on task  7 : loss=1.282 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 64.4%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.300 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 67.3%, forg=  3.5% <<<
>>> Test on task  9 : loss=1.269 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.9%, forg=  6.4% <<<
>>> Test on task 10 : loss=1.423 | TAw acc= 90.5%, forg=  1.0%| TAg acc= 75.2%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.092 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.316 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 70.1%, forg=  8.2% <<<
>>> Test on task 13 : loss=0.911 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 74.5%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.035 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 71.4%, forg= 11.2% <<<
>>> Test on task 15 : loss=1.152 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 78.2%, forg=  1.8% <<<
>>> Test on task 16 : loss=1.477 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 67.7%, forg=  1.0% <<<
>>> Test on task 17 : loss=0.935 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 80.7%, forg=  2.8% <<<
>>> Test on task 18 : loss=1.493 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 66.4%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.083 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.1%, forg= 10.3% <<<
>>> Test on task 20 : loss=1.223 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.303 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.7%, forg=  7.6% <<<
>>> Test on task 22 : loss=1.019 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 75.2%, forg= -0.9% <<<
>>> Test on task 23 : loss=0.741 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 84.4%, forg= -0.9% <<<
>>> Test on task 24 : loss=1.370 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 58.3%, forg=  0.0% <<<
>>> Test on task 25 : loss=1.561 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 65.7%, forg=  0.0% <<<
>>> Test on task 26 : loss=1.468 | TAw acc= 94.0%, forg= -0.9%| TAg acc= 70.9%, forg=  2.6% <<<
>>> Test on task 27 : loss=1.030 | TAw acc= 96.7%, forg=  0.8%| TAg acc= 73.3%, forg=  1.7% <<<
>>> Test on task 28 : loss=1.380 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.250 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 70.5%, forg=  0.0% <<<
>>> Test on task 30 : loss=1.475 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 62.4%, forg=  7.2% <<<
>>> Test on task 31 : loss=1.302 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 71.1%, forg= -5.6% <<<
>>> Test on task 32 : loss=1.402 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 60.5%, forg=  9.6% <<<
>>> Test on task 33 : loss=1.136 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 73.2%, forg=  0.8% <<<
>>> Test on task 34 : loss=1.575 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 64.6%, forg=  5.3% <<<
>>> Test on task 35 : loss=2.137 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 30.9%, forg= 17.5% <<<
>>> Test on task 36 : loss=1.620 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 55.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=6.595, TAw acc= 44.2% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=3.470, TAw acc= 72.7% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.3s loss=2.204, TAw acc= 90.9% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.3s loss=1.806, TAw acc= 92.2% | *
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.711, TAw acc= 90.9% | *
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.708, TAw acc= 90.9% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.706, TAw acc= 90.9% | *
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.704, TAw acc= 90.9% | *
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.701, TAw acc= 92.2% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.699, TAw acc= 92.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5582 train exemplars, time=  0.0s
5582
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.049 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 73.7%, forg= 14.5% <<<
>>> Test on task  1 : loss=0.874 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 81.3%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.268 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 67.9%, forg= 14.3% <<<
>>> Test on task  3 : loss=1.273 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.7%, forg= 17.6% <<<
>>> Test on task  4 : loss=0.949 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 80.3%, forg=  8.7% <<<
>>> Test on task  5 : loss=1.002 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.8%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.691 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 60.2%, forg= 17.8% <<<
>>> Test on task  7 : loss=1.342 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task  8 : loss=1.497 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 59.3%, forg= 11.5% <<<
>>> Test on task  9 : loss=1.295 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 62.4%, forg= 11.9% <<<
>>> Test on task 10 : loss=1.511 | TAw acc= 92.4%, forg= -1.0%| TAg acc= 70.5%, forg=  5.7% <<<
>>> Test on task 11 : loss=1.038 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.0%, forg=  4.0% <<<
>>> Test on task 12 : loss=1.304 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 71.1%, forg=  7.2% <<<
>>> Test on task 13 : loss=0.904 | TAw acc= 98.9%, forg= -1.1%| TAg acc= 73.4%, forg=  8.5% <<<
>>> Test on task 14 : loss=1.023 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 70.4%, forg= 12.2% <<<
>>> Test on task 15 : loss=1.178 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 78.2%, forg=  1.8% <<<
>>> Test on task 16 : loss=1.472 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 67.7%, forg=  1.0% <<<
>>> Test on task 17 : loss=0.964 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 80.7%, forg=  2.8% <<<
>>> Test on task 18 : loss=1.446 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 68.1%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.011 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.2%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.230 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.6%, forg=  5.7% <<<
>>> Test on task 21 : loss=1.261 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task 22 : loss=1.018 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 75.2%, forg=  0.0% <<<
>>> Test on task 23 : loss=0.769 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 84.4%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.430 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 59.3%, forg= -0.9% <<<
>>> Test on task 25 : loss=1.637 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 60.0%, forg=  5.7% <<<
>>> Test on task 26 : loss=1.432 | TAw acc= 92.3%, forg=  1.7%| TAg acc= 71.8%, forg=  1.7% <<<
>>> Test on task 27 : loss=0.988 | TAw acc= 96.7%, forg=  0.8%| TAg acc= 75.8%, forg= -0.8% <<<
>>> Test on task 28 : loss=1.413 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 60.7%, forg=  9.0% <<<
>>> Test on task 29 : loss=1.255 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.4%, forg= -1.9% <<<
>>> Test on task 30 : loss=1.477 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 62.4%, forg=  7.2% <<<
>>> Test on task 31 : loss=1.302 | TAw acc= 96.7%, forg=  2.2%| TAg acc= 67.8%, forg=  3.3% <<<
>>> Test on task 32 : loss=1.433 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 57.0%, forg= 13.2% <<<
>>> Test on task 33 : loss=1.186 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 69.9%, forg=  4.1% <<<
>>> Test on task 34 : loss=1.510 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 67.3%, forg=  2.7% <<<
>>> Test on task 35 : loss=2.093 | TAw acc= 92.8%, forg= -2.1%| TAg acc= 33.0%, forg= 15.5% <<<
>>> Test on task 36 : loss=1.895 | TAw acc= 95.1%, forg= -1.0%| TAg acc= 48.0%, forg=  7.8% <<<
>>> Test on task 37 : loss=1.507 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 60.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=4.984, TAw acc= 51.2% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=2.679, TAw acc= 74.4% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.906, TAw acc= 86.0% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.671, TAw acc= 88.4% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.417, TAw acc= 88.4% | *
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.415, TAw acc= 88.4% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.413, TAw acc= 88.4% | *
| Epoch   3, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.411, TAw acc= 87.2% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.409, TAw acc= 87.2% | *
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.407, TAw acc= 87.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5722 train exemplars, time=  0.0s
5722
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.037 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 74.9%, forg= 13.4% <<<
>>> Test on task  1 : loss=0.939 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 76.6%, forg= 15.9% <<<
>>> Test on task  2 : loss=1.229 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.5%, forg= 11.6% <<<
>>> Test on task  3 : loss=1.230 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.4%, forg= 13.9% <<<
>>> Test on task  4 : loss=1.023 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 78.0%, forg= 11.0% <<<
>>> Test on task  5 : loss=1.010 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.1%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.652 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 63.6%, forg= 14.4% <<<
>>> Test on task  7 : loss=1.317 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 65.3%, forg=  5.1% <<<
>>> Test on task  8 : loss=1.249 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 69.9%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.386 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 61.5%, forg= 12.8% <<<
>>> Test on task 10 : loss=1.441 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 74.3%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.059 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.0%, forg=  6.0% <<<
>>> Test on task 12 : loss=1.359 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 68.0%, forg= 10.3% <<<
>>> Test on task 13 : loss=0.834 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 74.5%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.074 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 69.4%, forg= 13.3% <<<
>>> Test on task 15 : loss=1.358 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 60.9%, forg= 19.1% <<<
>>> Test on task 16 : loss=1.469 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 71.9%, forg= -3.1% <<<
>>> Test on task 17 : loss=0.949 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 79.8%, forg=  3.7% <<<
>>> Test on task 18 : loss=1.503 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 68.1%, forg=  4.3% <<<
>>> Test on task 19 : loss=0.940 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.3%, forg=  3.1% <<<
>>> Test on task 20 : loss=1.264 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.5%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.204 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 68.6%, forg=  1.7% <<<
>>> Test on task 22 : loss=1.167 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 65.1%, forg= 10.1% <<<
>>> Test on task 23 : loss=0.786 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.6%, forg=  1.8% <<<
>>> Test on task 24 : loss=1.383 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 57.4%, forg=  1.9% <<<
>>> Test on task 25 : loss=1.596 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 64.8%, forg=  1.0% <<<
>>> Test on task 26 : loss=1.455 | TAw acc= 93.2%, forg=  0.9%| TAg acc= 70.9%, forg=  2.6% <<<
>>> Test on task 27 : loss=1.032 | TAw acc= 96.7%, forg=  0.8%| TAg acc= 73.3%, forg=  2.5% <<<
>>> Test on task 28 : loss=1.409 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.254 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.4%, forg=  0.0% <<<
>>> Test on task 30 : loss=1.466 | TAw acc= 92.8%, forg=  1.6%| TAg acc= 63.2%, forg=  6.4% <<<
>>> Test on task 31 : loss=1.202 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 68.9%, forg=  2.2% <<<
>>> Test on task 32 : loss=1.415 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 56.1%, forg= 14.0% <<<
>>> Test on task 33 : loss=1.121 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 77.2%, forg= -3.3% <<<
>>> Test on task 34 : loss=1.476 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 63.7%, forg=  6.2% <<<
>>> Test on task 35 : loss=1.990 | TAw acc= 89.7%, forg=  3.1%| TAg acc= 38.1%, forg= 10.3% <<<
>>> Test on task 36 : loss=1.927 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 47.1%, forg=  8.8% <<<
>>> Test on task 37 : loss=1.949 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 48.1%, forg= 12.3% <<<
>>> Test on task 38 : loss=1.380 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 70.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=4.553, TAw acc= 55.6% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=2.285, TAw acc= 80.8% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 88.9% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.227, TAw acc= 91.9% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.112, TAw acc= 94.9% | *
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.106, TAw acc= 94.9% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.101, TAw acc= 94.9% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.096, TAw acc= 94.9% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.092, TAw acc= 94.9% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.088, TAw acc= 94.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5862 train exemplars, time=  0.0s
5862
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.037 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 75.4%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.924 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 75.7%, forg= 16.8% <<<
>>> Test on task  2 : loss=1.278 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 67.9%, forg= 14.3% <<<
>>> Test on task  3 : loss=1.170 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.1%, forg= 10.2% <<<
>>> Test on task  4 : loss=1.072 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 78.0%, forg= 11.0% <<<
>>> Test on task  5 : loss=1.038 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.0%, forg=  8.2% <<<
>>> Test on task  6 : loss=1.700 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 61.9%, forg= 16.1% <<<
>>> Test on task  7 : loss=1.313 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 65.3%, forg=  5.1% <<<
>>> Test on task  8 : loss=1.440 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 63.7%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.340 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 63.3%, forg= 11.0% <<<
>>> Test on task 10 : loss=1.505 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 71.4%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.084 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.390 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 66.0%, forg= 12.4% <<<
>>> Test on task 13 : loss=0.810 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 83.0%, forg= -1.1% <<<
>>> Test on task 14 : loss=1.062 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 73.5%, forg=  9.2% <<<
>>> Test on task 15 : loss=1.303 | TAw acc= 97.3%, forg=  2.7%| TAg acc= 62.7%, forg= 17.3% <<<
>>> Test on task 16 : loss=1.491 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 69.8%, forg=  2.1% <<<
>>> Test on task 17 : loss=0.976 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 79.8%, forg=  3.7% <<<
>>> Test on task 18 : loss=1.501 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 68.1%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.032 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.2%, forg=  8.2% <<<
>>> Test on task 20 : loss=1.258 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.6%, forg=  5.7% <<<
>>> Test on task 21 : loss=1.280 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.1%, forg=  4.2% <<<
>>> Test on task 22 : loss=1.015 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 73.4%, forg=  1.8% <<<
>>> Test on task 23 : loss=0.783 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.6%, forg=  1.8% <<<
>>> Test on task 24 : loss=1.362 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 57.4%, forg=  1.9% <<<
>>> Test on task 25 : loss=1.600 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 61.9%, forg=  3.8% <<<
>>> Test on task 26 : loss=1.523 | TAw acc= 92.3%, forg=  1.7%| TAg acc= 69.2%, forg=  4.3% <<<
>>> Test on task 27 : loss=0.978 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 72.5%, forg=  3.3% <<<
>>> Test on task 28 : loss=1.468 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.8%, forg=  9.8% <<<
>>> Test on task 29 : loss=1.243 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.4%, forg=  0.0% <<<
>>> Test on task 30 : loss=1.544 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 58.4%, forg= 11.2% <<<
>>> Test on task 31 : loss=1.197 | TAw acc= 96.7%, forg=  2.2%| TAg acc= 71.1%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.443 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 57.0%, forg= 13.2% <<<
>>> Test on task 33 : loss=1.137 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 77.2%, forg=  0.0% <<<
>>> Test on task 34 : loss=1.504 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 63.7%, forg=  6.2% <<<
>>> Test on task 35 : loss=1.977 | TAw acc= 90.7%, forg=  2.1%| TAg acc= 42.3%, forg=  6.2% <<<
>>> Test on task 36 : loss=1.840 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 52.9%, forg=  2.9% <<<
>>> Test on task 37 : loss=1.817 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 54.7%, forg=  5.7% <<<
>>> Test on task 38 : loss=1.562 | TAw acc= 93.9%, forg= -1.7%| TAg acc= 57.4%, forg= 13.0% <<<
>>> Test on task 39 : loss=1.012 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=5.264, TAw acc= 48.2% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=2.880, TAw acc= 78.8% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.936, TAw acc= 84.7% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.688, TAw acc= 89.4% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.3s loss=1.442, TAw acc= 94.1% | *
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.433, TAw acc= 94.1% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.425, TAw acc= 94.1% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.418, TAw acc= 94.1% | *
| Epoch   4, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.412, TAw acc= 94.1% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.406, TAw acc= 94.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6002 train exemplars, time=  0.0s
6002
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.044 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 74.9%, forg= 13.4% <<<
>>> Test on task  1 : loss=1.009 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 75.7%, forg= 16.8% <<<
>>> Test on task  2 : loss=1.203 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.6%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.220 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.4%, forg= 13.9% <<<
>>> Test on task  4 : loss=1.035 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 78.0%, forg= 11.0% <<<
>>> Test on task  5 : loss=0.999 | TAw acc=100.0%, forg= -0.8%| TAg acc= 82.8%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.790 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 62.7%, forg= 15.3% <<<
>>> Test on task  7 : loss=1.298 | TAw acc= 91.5%, forg=  0.8%| TAg acc= 64.4%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.457 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.6%, forg=  6.2% <<<
>>> Test on task  9 : loss=1.338 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 60.6%, forg= 13.8% <<<
>>> Test on task 10 : loss=1.541 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 73.3%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.009 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.0%, forg=  4.0% <<<
>>> Test on task 12 : loss=1.364 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 68.0%, forg= 10.3% <<<
>>> Test on task 13 : loss=0.828 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 76.6%, forg=  6.4% <<<
>>> Test on task 14 : loss=1.048 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 70.4%, forg= 12.2% <<<
>>> Test on task 15 : loss=1.254 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 74.5%, forg=  5.5% <<<
>>> Test on task 16 : loss=1.531 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 67.7%, forg=  4.2% <<<
>>> Test on task 17 : loss=1.008 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 76.1%, forg=  7.3% <<<
>>> Test on task 18 : loss=1.535 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 63.8%, forg=  8.6% <<<
>>> Test on task 19 : loss=1.023 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.2%, forg=  7.2% <<<
>>> Test on task 20 : loss=1.283 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.6%, forg=  5.7% <<<
>>> Test on task 21 : loss=1.277 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.7%, forg=  7.6% <<<
>>> Test on task 22 : loss=1.099 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 70.6%, forg=  4.6% <<<
>>> Test on task 23 : loss=0.769 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.7%, forg=  2.8% <<<
>>> Test on task 24 : loss=1.469 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 55.6%, forg=  3.7% <<<
>>> Test on task 25 : loss=1.673 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 62.9%, forg=  2.9% <<<
>>> Test on task 26 : loss=1.515 | TAw acc= 93.2%, forg=  0.9%| TAg acc= 70.9%, forg=  2.6% <<<
>>> Test on task 27 : loss=0.982 | TAw acc= 96.7%, forg=  0.8%| TAg acc= 74.2%, forg=  1.7% <<<
>>> Test on task 28 : loss=1.500 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.219 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 74.3%, forg= -1.9% <<<
>>> Test on task 30 : loss=1.542 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 63.2%, forg=  6.4% <<<
>>> Test on task 31 : loss=1.224 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 72.2%, forg= -1.1% <<<
>>> Test on task 32 : loss=1.389 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 58.8%, forg= 11.4% <<<
>>> Test on task 33 : loss=1.079 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 79.7%, forg= -2.4% <<<
>>> Test on task 34 : loss=1.490 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 68.1%, forg=  1.8% <<<
>>> Test on task 35 : loss=1.969 | TAw acc= 92.8%, forg=  0.0%| TAg acc= 43.3%, forg=  5.2% <<<
>>> Test on task 36 : loss=1.751 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 55.9%, forg=  0.0% <<<
>>> Test on task 37 : loss=1.786 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 56.6%, forg=  3.8% <<<
>>> Test on task 38 : loss=1.447 | TAw acc= 97.4%, forg= -3.5%| TAg acc= 65.2%, forg=  5.2% <<<
>>> Test on task 39 : loss=1.339 | TAw acc= 97.7%, forg= -3.1%| TAg acc= 63.1%, forg= 13.8% <<<
>>> Test on task 40 : loss=1.292 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 63.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=5.918, TAw acc= 63.1% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=2.560, TAw acc= 79.8% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.465, TAw acc= 94.0% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.352, TAw acc= 94.0% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.125, TAw acc= 95.2% | *
| Epoch   1, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.119, TAw acc= 95.2% | *
| Epoch   2, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.113, TAw acc= 95.2% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.108, TAw acc= 95.2% | *
| Epoch   4, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.104, TAw acc= 95.2% | *
| Epoch   5, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.099, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6142 train exemplars, time=  0.1s
6142
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.085 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 73.2%, forg= 15.1% <<<
>>> Test on task  1 : loss=1.000 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 75.7%, forg= 16.8% <<<
>>> Test on task  2 : loss=1.234 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.6%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.199 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.4%, forg= 13.9% <<<
>>> Test on task  4 : loss=1.090 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 75.6%, forg= 13.4% <<<
>>> Test on task  5 : loss=1.039 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 81.1%, forg=  9.0% <<<
>>> Test on task  6 : loss=1.802 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.6%, forg= 14.4% <<<
>>> Test on task  7 : loss=1.322 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 64.4%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.461 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 62.8%, forg=  8.0% <<<
>>> Test on task  9 : loss=1.350 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.2%, forg= 10.1% <<<
>>> Test on task 10 : loss=1.521 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 69.5%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.037 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.349 | TAw acc= 97.9%, forg=  1.0%| TAg acc= 70.1%, forg=  8.2% <<<
>>> Test on task 13 : loss=0.931 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 73.4%, forg=  9.6% <<<
>>> Test on task 14 : loss=1.139 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 70.4%, forg= 12.2% <<<
>>> Test on task 15 : loss=1.199 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 75.5%, forg=  4.5% <<<
>>> Test on task 16 : loss=1.556 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 69.8%, forg=  2.1% <<<
>>> Test on task 17 : loss=0.959 | TAw acc= 95.4%, forg=  1.8%| TAg acc= 79.8%, forg=  3.7% <<<
>>> Test on task 18 : loss=1.578 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 64.7%, forg=  7.8% <<<
>>> Test on task 19 : loss=1.044 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.2%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.232 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.4%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.313 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.6%, forg=  6.8% <<<
>>> Test on task 22 : loss=1.113 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 68.8%, forg=  6.4% <<<
>>> Test on task 23 : loss=0.775 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 83.5%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.356 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 57.4%, forg=  1.9% <<<
>>> Test on task 25 : loss=1.605 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 63.8%, forg=  1.9% <<<
>>> Test on task 26 : loss=1.508 | TAw acc= 93.2%, forg=  0.9%| TAg acc= 70.9%, forg=  2.6% <<<
>>> Test on task 27 : loss=0.977 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 73.3%, forg=  2.5% <<<
>>> Test on task 28 : loss=1.463 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 60.7%, forg=  9.0% <<<
>>> Test on task 29 : loss=1.196 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.2%, forg= -1.9% <<<
>>> Test on task 30 : loss=1.555 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 60.8%, forg=  8.8% <<<
>>> Test on task 31 : loss=1.212 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 67.8%, forg=  4.4% <<<
>>> Test on task 32 : loss=1.431 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 59.6%, forg= 10.5% <<<
>>> Test on task 33 : loss=1.083 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 75.6%, forg=  4.1% <<<
>>> Test on task 34 : loss=1.495 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 67.3%, forg=  2.7% <<<
>>> Test on task 35 : loss=1.910 | TAw acc= 90.7%, forg=  2.1%| TAg acc= 46.4%, forg=  2.1% <<<
>>> Test on task 36 : loss=1.810 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 52.9%, forg=  2.9% <<<
>>> Test on task 37 : loss=1.775 | TAw acc= 88.7%, forg=  3.8%| TAg acc= 61.3%, forg= -0.9% <<<
>>> Test on task 38 : loss=1.456 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 58.3%, forg= 12.2% <<<
>>> Test on task 39 : loss=1.215 | TAw acc= 96.9%, forg=  0.8%| TAg acc= 73.8%, forg=  3.1% <<<
>>> Test on task 40 : loss=1.561 | TAw acc= 94.7%, forg= -1.8%| TAg acc= 50.0%, forg= 13.2% <<<
>>> Test on task 41 : loss=1.478 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 58.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=5.708, TAw acc= 45.0% | *
| Epoch   2, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=2.645, TAw acc= 75.0% | *
| Epoch   3, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.747, TAw acc= 93.8% | *
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.554, TAw acc= 93.8% | *
| Epoch   5, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.506, TAw acc= 92.5% | *
| Epoch   1, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.498, TAw acc= 92.5% | *
| Epoch   2, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.490, TAw acc= 92.5% | *
| Epoch   3, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.483, TAw acc= 92.5% | *
| Epoch   4, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.477, TAw acc= 92.5% | *
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.471, TAw acc= 92.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6282 train exemplars, time=  0.0s
6282
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.084 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 73.2%, forg= 15.1% <<<
>>> Test on task  1 : loss=0.983 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 75.7%, forg= 16.8% <<<
>>> Test on task  2 : loss=1.227 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.8%, forg= 13.4% <<<
>>> Test on task  3 : loss=1.191 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 68.5%, forg= 14.8% <<<
>>> Test on task  4 : loss=1.158 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 71.7%, forg= 17.3% <<<
>>> Test on task  5 : loss=1.031 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 80.3%, forg=  9.8% <<<
>>> Test on task  6 : loss=1.786 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 61.9%, forg= 16.1% <<<
>>> Test on task  7 : loss=1.345 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 62.7%, forg=  7.6% <<<
>>> Test on task  8 : loss=1.360 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 65.5%, forg=  5.3% <<<
>>> Test on task  9 : loss=1.338 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 62.4%, forg= 11.9% <<<
>>> Test on task 10 : loss=1.519 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 72.4%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.084 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.0%, forg=  8.0% <<<
>>> Test on task 12 : loss=1.385 | TAw acc= 96.9%, forg=  2.1%| TAg acc= 64.9%, forg= 13.4% <<<
>>> Test on task 13 : loss=0.882 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 77.7%, forg=  5.3% <<<
>>> Test on task 14 : loss=1.106 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 72.4%, forg= 10.2% <<<
>>> Test on task 15 : loss=1.329 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 66.4%, forg= 13.6% <<<
>>> Test on task 16 : loss=1.734 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 63.5%, forg=  8.3% <<<
>>> Test on task 17 : loss=1.038 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 76.1%, forg=  7.3% <<<
>>> Test on task 18 : loss=1.503 | TAw acc= 90.5%, forg=  1.7%| TAg acc= 68.1%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.020 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.2%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.328 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 63.8%, forg= 10.5% <<<
>>> Test on task 21 : loss=1.267 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 68.6%, forg=  1.7% <<<
>>> Test on task 22 : loss=1.076 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 74.3%, forg=  0.9% <<<
>>> Test on task 23 : loss=0.772 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.6%, forg=  1.8% <<<
>>> Test on task 24 : loss=1.462 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 55.6%, forg=  3.7% <<<
>>> Test on task 25 : loss=1.632 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 63.8%, forg=  1.9% <<<
>>> Test on task 26 : loss=1.604 | TAw acc= 93.2%, forg=  0.9%| TAg acc= 68.4%, forg=  5.1% <<<
>>> Test on task 27 : loss=1.022 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 72.5%, forg=  3.3% <<<
>>> Test on task 28 : loss=1.501 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.184 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.4%, forg=  3.8% <<<
>>> Test on task 30 : loss=1.590 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 57.6%, forg= 12.0% <<<
>>> Test on task 31 : loss=1.171 | TAw acc= 96.7%, forg=  2.2%| TAg acc= 72.2%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.483 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 57.9%, forg= 12.3% <<<
>>> Test on task 33 : loss=1.004 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 77.2%, forg=  2.4% <<<
>>> Test on task 34 : loss=1.474 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 66.4%, forg=  3.5% <<<
>>> Test on task 35 : loss=1.941 | TAw acc= 92.8%, forg=  0.0%| TAg acc= 44.3%, forg=  4.1% <<<
>>> Test on task 36 : loss=1.735 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 56.9%, forg= -1.0% <<<
>>> Test on task 37 : loss=1.744 | TAw acc= 89.6%, forg=  2.8%| TAg acc= 58.5%, forg=  2.8% <<<
>>> Test on task 38 : loss=1.507 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 54.8%, forg= 15.7% <<<
>>> Test on task 39 : loss=1.137 | TAw acc= 96.9%, forg=  0.8%| TAg acc= 74.6%, forg=  2.3% <<<
>>> Test on task 40 : loss=1.576 | TAw acc= 95.6%, forg= -0.9%| TAg acc= 53.5%, forg=  9.6% <<<
>>> Test on task 41 : loss=1.891 | TAw acc= 93.8%, forg= -0.9%| TAg acc= 46.4%, forg= 12.5% <<<
>>> Test on task 42 : loss=1.332 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.7s | Train: skip eval | Valid: time=  0.2s loss=4.503, TAw acc= 58.1% | *
| Epoch   2, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=2.287, TAw acc= 89.5% | *
| Epoch   3, time=  6.6s | Train: skip eval | Valid: time=  0.2s loss=1.484, TAw acc= 93.0% | *
| Epoch   4, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 95.3% | *
| Epoch   5, time=  6.6s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 95.3% |
| Epoch   1, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.248, TAw acc= 95.3% | *
| Epoch   2, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=1.257, TAw acc= 95.3% |
| Epoch   3, time=  6.9s | Train: skip eval | Valid: time=  0.2s loss=1.265, TAw acc= 95.3% |
| Epoch   4, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=1.272, TAw acc= 95.3% |
| Epoch   5, time=  6.7s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 95.3% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6422 train exemplars, time=  0.0s
6422
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.089 | TAw acc= 96.1%, forg=  0.6%| TAg acc= 74.9%, forg= 13.4% <<<
>>> Test on task  1 : loss=0.987 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 74.8%, forg= 17.8% <<<
>>> Test on task  2 : loss=1.339 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 67.0%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.218 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.4%, forg= 13.0% <<<
>>> Test on task  4 : loss=1.078 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 77.2%, forg= 11.8% <<<
>>> Test on task  5 : loss=1.085 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 79.5%, forg= 10.7% <<<
>>> Test on task  6 : loss=1.824 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 61.0%, forg= 16.9% <<<
>>> Test on task  7 : loss=1.406 | TAw acc= 91.5%, forg=  0.8%| TAg acc= 59.3%, forg= 11.0% <<<
>>> Test on task  8 : loss=1.363 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task  9 : loss=1.418 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 63.3%, forg= 11.0% <<<
>>> Test on task 10 : loss=1.524 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 70.5%, forg=  5.7% <<<
>>> Test on task 11 : loss=1.113 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.0%, forg=  6.0% <<<
>>> Test on task 12 : loss=1.417 | TAw acc= 96.9%, forg=  2.1%| TAg acc= 71.1%, forg=  7.2% <<<
>>> Test on task 13 : loss=0.872 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 75.5%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.211 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 66.3%, forg= 16.3% <<<
>>> Test on task 15 : loss=1.313 | TAw acc= 96.4%, forg=  3.6%| TAg acc= 68.2%, forg= 11.8% <<<
>>> Test on task 16 : loss=1.546 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 72.9%, forg= -1.0% <<<
>>> Test on task 17 : loss=1.038 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 74.3%, forg=  9.2% <<<
>>> Test on task 18 : loss=1.495 | TAw acc= 91.4%, forg=  0.9%| TAg acc= 66.4%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.024 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.3%, forg=  4.1% <<<
>>> Test on task 20 : loss=1.307 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 63.8%, forg= 10.5% <<<
>>> Test on task 21 : loss=1.340 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 62.7%, forg=  7.6% <<<
>>> Test on task 22 : loss=1.119 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 73.4%, forg=  1.8% <<<
>>> Test on task 23 : loss=0.751 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 83.5%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.428 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 59.3%, forg=  0.0% <<<
>>> Test on task 25 : loss=1.698 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 61.0%, forg=  4.8% <<<
>>> Test on task 26 : loss=1.584 | TAw acc= 93.2%, forg=  0.9%| TAg acc= 69.2%, forg=  4.3% <<<
>>> Test on task 27 : loss=1.015 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 75.0%, forg=  0.8% <<<
>>> Test on task 28 : loss=1.455 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 29 : loss=1.238 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.5%, forg=  5.7% <<<
>>> Test on task 30 : loss=1.664 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 58.4%, forg= 11.2% <<<
>>> Test on task 31 : loss=1.168 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 74.4%, forg= -2.2% <<<
>>> Test on task 32 : loss=1.590 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 56.1%, forg= 14.0% <<<
>>> Test on task 33 : loss=1.131 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 69.1%, forg= 10.6% <<<
>>> Test on task 34 : loss=1.570 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 64.6%, forg=  5.3% <<<
>>> Test on task 35 : loss=1.933 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 45.4%, forg=  3.1% <<<
>>> Test on task 36 : loss=1.815 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 55.9%, forg=  1.0% <<<
>>> Test on task 37 : loss=1.843 | TAw acc= 89.6%, forg=  2.8%| TAg acc= 54.7%, forg=  6.6% <<<
>>> Test on task 38 : loss=1.521 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 57.4%, forg= 13.0% <<<
>>> Test on task 39 : loss=1.138 | TAw acc= 96.2%, forg=  1.5%| TAg acc= 72.3%, forg=  4.6% <<<
>>> Test on task 40 : loss=1.494 | TAw acc= 94.7%, forg=  0.9%| TAg acc= 57.0%, forg=  6.1% <<<
>>> Test on task 41 : loss=2.002 | TAw acc= 91.1%, forg=  2.7%| TAg acc= 33.9%, forg= 25.0% <<<
>>> Test on task 42 : loss=1.632 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 49.1%, forg= 18.5% <<<
>>> Test on task 43 : loss=1.385 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 66.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_2/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 77.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.7% 
	 91.6%  86.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 89.3% 
	 91.6%  96.3%  92.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.3% 
	 91.6%  96.3%  93.8%  89.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.9% 
	 91.6%  96.3%  95.5%  92.6%  87.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.7% 
	 92.2%  96.3%  95.5%  93.5%  97.6%  83.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.1% 
	 91.6%  95.3%  95.5%  94.4%  97.6%  97.5%  84.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.8% 
	 91.6%  96.3%  95.5%  94.4%  97.6%  98.4%  89.8%  74.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.3% 
	 92.7%  96.3%  96.4%  95.4%  98.4%  98.4%  91.5%  88.1%  93.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 92.2%  98.1%  96.4%  97.2%  98.4%  99.2%  92.4%  89.0%  95.6%  89.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 91.6%  98.1%  96.4%  97.2%  98.4%  99.2%  92.4%  90.7%  95.6%  95.4%  83.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.4% 
	 93.3%  96.3%  96.4%  97.2%  97.6%  99.2%  94.9%  90.7%  95.6%  96.3%  87.6%  95.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 93.3%  96.3%  96.4%  97.2%  97.6%  99.2%  94.1%  89.8%  96.5%  96.3%  89.5%  98.0%  90.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 93.9%  98.1%  96.4%  97.2%  97.6%  99.2%  93.2%  89.8%  96.5%  96.3%  88.6%  98.0%  93.8%  93.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 93.3%  98.1%  97.3%  97.2%  97.6%  99.2%  95.8%  89.8%  96.5%  97.2%  89.5%  99.0%  95.9%  97.9%  85.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 94.4%  98.1%  98.2%  97.2%  96.9%  99.2%  95.8%  89.0%  95.6%  97.2%  89.5%  99.0%  94.8%  97.9%  96.9%  88.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 94.4%  98.1%  97.3%  98.1%  96.9%  99.2%  94.1%  89.0%  96.5%  97.2%  89.5%  99.0%  96.9%  97.9%  96.9%  98.2%  93.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 94.4%  98.1%  96.4%  98.1%  96.9%  99.2%  95.8%  90.7%  96.5%  97.2%  90.5%  99.0%  96.9%  97.9%  95.9%  96.4%  96.9%  92.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 94.4%  97.2%  96.4%  97.2%  96.9%  99.2%  94.9%  89.8%  96.5%  97.2%  90.5%  99.0%  96.9%  97.9%  96.9%  96.4%  96.9%  95.4%  85.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 95.5%  97.2%  96.4%  98.1%  96.9%  99.2%  95.8%  90.7%  96.5%  97.2%  90.5%  99.0%  96.9%  97.9%  95.9% 100.0%  96.9%  96.3%  91.4%  97.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.3% 
	 95.0%  97.2%  96.4%  99.1%  96.9%  99.2%  94.1%  89.8%  96.5%  97.2%  89.5%  99.0%  95.9%  97.9%  95.9%  97.3%  96.9%  96.3%  92.2%  99.0%  97.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 95.0%  97.2%  96.4%  99.1%  96.9%  99.2%  94.9%  91.5%  97.3%  98.2%  90.5%  99.0%  95.9%  97.9%  95.9%  94.5%  96.9%  96.3%  89.7%  99.0%  98.1%  95.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 94.4%  97.2%  96.4%  98.1%  96.9%  99.2%  95.8%  92.4%  96.5%  98.2%  90.5%  99.0%  95.9%  97.9%  95.9%  97.3%  96.9%  96.3%  90.5%  99.0%  98.1%  95.8%  97.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.3% 
	 94.4%  97.2%  96.4%  98.1%  96.9%  99.2%  94.1%  90.7%  97.3%  99.1%  90.5%  99.0%  95.9%  97.9%  95.9%  96.4%  96.9%  96.3%  90.5%  99.0%  98.1%  97.5%  97.2%  96.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.3% 
	 96.1%  97.2%  96.4%  99.1%  96.9%  99.2%  95.8%  91.5%  97.3%  99.1%  91.4%  99.0%  95.9%  97.9%  95.9%  96.4%  96.9%  96.3%  90.5%  99.0%  98.1%  97.5%  97.2%  98.2%  93.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 95.5%  97.2%  98.2%  99.1%  97.6%  99.2%  95.8%  91.5%  97.3%  99.1%  90.5%  99.0%  96.9%  97.9%  95.9%  97.3%  96.9%  96.3%  90.5%  99.0%  98.1%  97.5%  98.2%  98.2%  95.4%  94.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.6% 
	 96.1%  97.2%  96.4%  99.1%  96.9%  99.2%  95.8%  90.7%  98.2%  99.1%  90.5%  99.0%  96.9%  97.9%  95.9%  97.3%  96.9%  96.3%  90.5%  99.0%  98.1%  97.5%  98.2%  99.1%  96.3%  96.2%  92.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.1%  97.2%  96.4%  99.1%  97.6%  99.2%  95.8%  90.7%  99.1%  98.2%  90.5%  99.0%  96.9%  97.9%  95.9%  96.4%  96.9%  97.2%  91.4%  99.0%  98.1%  97.5%  98.2%  99.1%  95.4%  96.2%  92.3%  92.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.4% 
	 96.1%  97.2%  96.4%  99.1%  97.6%  99.2%  94.1%  89.8%  99.1%  98.2%  90.5%  99.0%  96.9%  97.9%  95.9%  96.4%  96.9%  97.2%  91.4%  99.0%  98.1%  97.5%  98.2%  99.1%  96.3%  96.2%  92.3%  96.7%  95.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  94.9%  90.7%  98.2%  98.2%  90.5%  99.0%  96.9%  97.9%  95.9%  97.3%  96.9%  97.2%  91.4%  99.0%  98.1%  97.5%  98.2%  99.1%  95.4%  95.2%  93.2%  95.8%  95.9%  97.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.1%  98.1%  97.3%  99.1%  97.6%  99.2%  94.9%  89.8%  99.1%  98.2%  90.5%  99.0%  96.9%  97.9%  95.9%  97.3%  95.8%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  95.4%  96.2%  93.2%  96.7%  95.9%  97.1%  94.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.6%  98.1%  97.3%  99.1%  97.6%  99.2%  95.8%  89.8%  99.1%  98.2%  91.4%  99.0%  97.9%  97.9%  95.9%  97.3%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  95.2%  92.3%  95.8%  95.9%  97.1%  94.4%  96.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.1%  98.1%  98.2%  99.1%  97.6%  99.2%  93.2%  89.0%  99.1%  98.2%  91.4%  99.0%  96.9%  97.9%  95.9%  96.4%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  98.2%  99.1%  97.2%  94.3%  93.2%  96.7%  95.9%  97.1%  94.4%  97.8%  98.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.6% 
	 96.6%  98.1%  97.3%  99.1%  97.6%  99.2%  95.8%  89.0%  99.1%  98.2%  90.5%  99.0%  96.9%  97.9%  95.9%  97.3%  95.8%  97.2%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  94.3%  93.2%  95.8%  95.9%  97.1%  94.4%  97.8%  98.2%  95.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  96.6%  89.8%  99.1%  98.2%  91.4%  99.0%  97.9%  97.9%  95.9%  96.4%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  94.3%  93.2%  95.8%  95.9%  97.1%  94.4%  98.9%  96.5%  95.1%  94.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 96.6%  98.1%  97.3%  99.1%  97.6%  99.2%  96.6%  89.8%  99.1%  98.2%  90.5%  99.0%  97.9%  97.9%  95.9%  97.3%  95.8%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  95.2%  93.2%  97.5%  95.9%  98.1%  93.6%  96.7%  98.2%  95.1%  96.5%  90.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.4% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  95.8%  90.7%  99.1%  98.2%  90.5%  99.0%  96.9%  97.9%  95.9%  97.3%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  97.2%  95.2%  94.0%  96.7%  95.9%  98.1%  93.6%  98.9%  98.2%  95.1%  96.5%  89.7%  94.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.4% 
	 96.6%  97.2%  97.3%  99.1%  97.6%  99.2%  95.8%  89.8%  99.1%  98.2%  92.4%  99.0%  96.9%  98.9%  95.9%  97.3%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  97.2%  95.2%  92.3%  96.7%  95.9%  98.1%  93.6%  96.7%  98.2%  95.1%  96.5%  92.8%  95.1%  92.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.4% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  95.8%  89.8%  98.2%  98.2%  92.4%  99.0%  96.9%  98.9%  95.9%  96.4%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  97.2%  94.3%  93.2%  96.7%  95.9%  98.1%  92.8%  97.8%  98.2%  95.1%  96.5%  89.7%  95.1%  92.5%  92.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  96.6%  89.8%  98.2%  99.1%  92.4%  99.0%  97.9%  98.9%  95.9%  97.3%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  94.3%  92.3%  97.5%  95.9%  98.1%  93.6%  96.7%  98.2%  95.1%  96.5%  90.7%  95.1%  92.5%  93.9%  94.6%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.2% 
	 96.1%  97.2%  97.3%  99.1%  97.6% 100.0%  96.6%  91.5%  98.2%  98.2%  92.4%  99.0%  99.0%  98.9%  95.9%  96.4%  96.9%  96.3%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  94.3%  93.2%  96.7%  95.9%  98.1%  94.4%  97.8%  98.2%  95.1%  96.5%  92.8%  95.1%  90.6%  97.4%  97.7%  93.0%   0.0%   0.0%   0.0% 	Avg.: 96.4% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  96.6%  90.7%  98.2%  98.2%  91.4%  99.0%  97.9%  98.9%  95.9%  96.4%  96.9%  95.4%  91.4%  99.0%  98.1%  97.5%  97.2%  99.1%  97.2%  94.3%  93.2%  97.5%  95.9%  98.1%  93.6%  98.9%  98.2%  95.1%  96.5%  90.7%  95.1%  88.7%  95.7%  96.9%  94.7%  92.9%   0.0%   0.0% 	Avg.: 96.1% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  96.6%  90.7%  98.2%  99.1%  92.4%  99.0%  96.9%  98.9%  95.9%  96.4%  96.9%  96.3%  90.5%  99.0%  98.1%  97.5%  97.2%  99.1%  96.3%  94.3%  93.2%  97.5%  95.9%  98.1%  93.6%  96.7%  98.2%  95.1%  96.5%  92.8%  95.1%  89.6%  94.8%  96.9%  95.6%  93.8%  98.1%   0.0% 	Avg.: 96.2% 
	 96.1%  97.2%  97.3%  99.1%  97.6%  99.2%  96.6%  91.5%  98.2%  99.1%  91.4%  99.0%  96.9%  98.9%  95.9%  96.4%  96.9%  96.3%  91.4%  99.0%  98.1%  96.6%  97.2%  99.1%  97.2%  94.3%  93.2%  97.5%  95.9%  97.1%  93.6%  98.9%  98.2%  95.1%  96.5%  91.8%  95.1%  89.6%  94.8%  96.2%  94.7%  91.1%  98.1%  94.8% 	Avg.: 96.1% 
************************************************************************************************************
TAg Acc
	 77.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.7% 
	 79.9%  83.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 81.5% 
	 85.5%  72.0%  69.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.7% 
	 81.0%  88.8%  62.5%  70.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.7% 
	 88.3%  90.7%  82.1%  68.5%  71.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.2% 
	 83.2%  78.5%  67.9%  63.0%  73.2%  71.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.9% 
	 82.7%  81.3%  67.9%  80.6%  80.3%  62.3%  78.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.1% 
	 79.3%  92.5%  80.4%  73.1%  84.3%  75.4%  55.1%  55.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.5% 
	 83.2%  92.5%  73.2%  83.3%  89.0%  73.0%  55.1%  46.6%  70.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.1% 
	 83.8%  85.0%  79.5%  74.1%  87.4%  82.0%  57.6%  54.2%  42.5%  59.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.6% 
	 80.4%  90.7%  76.8%  80.6%  88.2%  80.3%  69.5%  62.7%  51.3%  44.0%  59.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.2% 
	 82.1%  82.2%  74.1%  73.1%  87.4%  86.1%  69.5%  66.1%  63.7%  47.7%  52.4%  62.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.5% 
	 79.3%  89.7%  76.8%  76.9%  87.4%  90.2%  70.3%  65.3%  66.4%  56.0%  54.3%  47.0%  48.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.8% 
	 83.8%  85.0%  71.4%  73.1%  87.4%  81.1%  68.6%  67.8%  68.1%  58.7%  64.8%  58.0%  51.5%  62.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.2% 
	 79.3%  85.0%  75.9%  63.9%  85.8%  88.5%  71.2%  69.5%  61.1%  67.9%  68.6%  65.0%  52.6%  46.8%  54.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 81.0%  87.9%  74.1%  67.6%  85.8%  82.0%  71.2%  62.7%  62.8%  65.1%  67.6%  68.0%  57.7%  57.4%  63.3%  60.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.6% 
	 80.4%  86.0%  76.8%  66.7%  83.5%  87.7%  70.3%  57.6%  62.8%  74.3%  72.4%  65.0%  72.2%  64.9%  56.1%  45.5%  58.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.4% 
	 79.3%  85.0%  73.2%  78.7%  85.0%  86.1%  69.5%  69.5%  65.5%  60.6%  71.4%  71.0%  75.3%  62.8%  72.4%  56.4%  39.6%  69.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.6% 
	 81.6%  84.1%  67.9%  76.9%  85.0%  82.8%  72.0%  66.9%  62.8%  71.6%  76.2%  74.0%  70.1%  67.0%  75.5%  54.5%  45.8%  66.1%  64.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.8% 
	 82.7%  84.1%  69.6%  70.4%  84.3%  86.9%  73.7%  70.3%  61.9%  64.2%  74.3%  72.0%  71.1%  75.5%  73.5%  65.5%  58.3%  68.8%  50.0%  71.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.4% 
	 80.4%  77.6%  73.2%  78.7%  82.7%  84.4%  66.1%  67.8%  63.7%  67.0%  69.5%  72.0%  78.4%  72.3%  74.5%  65.5%  61.5%  70.6%  59.5%  54.6%  66.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.8% 
	 77.7%  82.2%  73.2%  70.4%  78.7%  89.3%  66.1%  69.5%  61.1%  68.8%  72.4%  73.0%  72.2%  76.6%  82.7%  69.1%  62.5%  72.5%  60.3%  59.8%  61.9%  70.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.4% 
	 76.0%  83.2%  72.3%  73.1%  77.2%  83.6%  72.0%  65.3%  66.4%  67.9%  74.3%  75.0%  72.2%  72.3%  72.4%  75.5%  59.4%  78.0%  61.2%  71.1%  67.6%  52.5%  68.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.2% 
	 79.3%  82.2%  71.4%  73.1%  76.4%  87.7%  72.0%  66.1%  63.7%  68.8%  73.3%  74.0%  72.2%  79.8%  68.4%  74.5%  66.7%  76.1%  65.5%  69.1%  61.9%  55.1%  59.6%  74.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.3% 
	 80.4%  76.6%  69.6%  81.5%  81.1%  86.1%  68.6%  63.6%  60.2%  71.6%  70.5%  75.0%  70.1%  77.7%  72.4%  78.2%  66.7%  78.9%  68.1%  71.1%  71.4%  55.1%  65.1%  55.0%  58.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.9% 
	 77.7%  82.2%  71.4%  76.9%  80.3%  82.8%  69.5%  63.6%  64.6%  66.1%  70.5%  75.0%  71.1%  73.4%  71.4%  75.5%  68.8%  80.7%  69.0%  73.2%  69.5%  64.4%  70.6%  73.4%  39.8%  60.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.8% 
	 81.6%  81.3%  69.6%  76.9%  78.7%  82.8%  66.9%  70.3%  66.4%  66.1%  74.3%  77.0%  71.1%  77.7%  71.4%  78.2%  66.7%  80.7%  70.7%  72.2%  63.8%  63.6%  67.9%  76.1%  52.8%  32.4%  70.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.7% 
	 76.5%  76.6%  74.1%  75.0%  78.7%  78.7%  66.1%  62.7%  63.7%  68.8%  73.3%  75.0%  73.2%  69.1%  69.4%  78.2%  68.8%  78.9%  67.2%  68.0%  69.5%  59.3%  69.7%  78.9%  49.1%  46.7%  62.4%  75.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.7% 
	 80.4%  78.5%  70.5%  68.5%  79.5%  79.5%  64.4%  60.2%  70.8%  67.0%  69.5%  75.0%  68.0%  81.9%  72.4%  77.3%  68.8%  81.7%  66.4%  76.3%  74.3%  60.2%  69.7%  80.7%  50.9%  50.5%  69.2%  64.2%  69.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.6% 
	 81.6%  83.2%  71.4%  75.9%  80.3%  81.1%  63.6%  67.8%  66.4%  71.6%  69.5%  74.0%  69.1%  73.4%  69.4%  75.5%  68.8%  81.7%  69.8%  69.1%  69.5%  60.2%  70.6%  83.5%  52.8%  61.0%  71.8%  63.3%  48.4%  70.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.5% 
	 78.2%  79.4%  72.3%  75.9%  79.5%  77.9%  62.7%  65.3%  62.8%  63.3%  71.4%  71.0%  71.1%  73.4%  70.4%  77.3%  67.7%  78.0%  67.2%  70.1%  64.8%  58.5%  74.3%  81.7%  53.7%  59.0%  71.8%  62.5%  51.6%  49.5%  69.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.8% 
	 77.1%  80.4%  68.8%  69.4%  82.7%  82.0%  65.3%  63.6%  66.4%  69.7%  76.2%  72.0%  70.1%  79.8%  73.5%  75.5%  68.8%  83.5%  67.2%  68.0%  71.4%  66.1%  70.6%  82.6%  49.1%  57.1%  73.5%  70.0%  56.6%  61.0%  48.8%  52.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.3% 
	 78.2%  81.3%  71.4%  68.5%  78.7%  77.9%  60.2%  58.5%  63.7%  67.9%  74.3%  71.0%  70.1%  79.8%  74.5%  75.5%  68.8%  81.7%  67.2%  70.1%  70.5%  65.3%  69.7%  82.6%  53.7%  57.1%  65.8%  75.0%  59.0%  64.8%  50.4%  44.4%  70.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.7% 
	 75.4%  77.6%  67.0%  76.9%  82.7%  83.6%  63.6%  63.6%  61.9%  69.7%  74.3%  72.0%  69.1%  78.7%  69.4%  80.0%  68.8%  83.5%  72.4%  75.3%  68.6%  66.1%  73.4%  83.5%  55.6%  60.0%  70.1%  70.8%  59.0%  65.7%  55.2%  55.6%  54.4%  74.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.9% 
	 75.4%  76.6%  70.5%  67.6%  82.7%  81.1%  64.4%  63.6%  65.5%  66.1%  76.2%  72.0%  69.1%  76.6%  68.4%  77.3%  63.5%  81.7%  68.1%  80.4%  70.5%  61.9%  73.4%  79.8%  58.3%  59.0%  70.9%  72.5%  57.4%  64.8%  57.6%  62.2%  53.5%  71.5%  69.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.4% 
	 78.2%  76.6%  69.6%  75.9%  80.3%  79.5%  63.6%  61.0%  68.1%  62.4%  73.3%  69.0%  70.1%  81.9%  69.4%  78.2%  66.7%  83.5%  68.1%  67.0%  67.6%  64.4%  72.5%  82.6%  56.5%  65.7%  72.6%  75.0%  55.7%  70.5%  63.2%  65.6%  53.5%  71.5%  62.8%  48.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.2% 
	 75.4%  77.6%  68.8%  72.2%  78.0%  81.1%  64.4%  64.4%  67.3%  67.9%  75.2%  72.0%  70.1%  74.5%  71.4%  78.2%  67.7%  80.7%  66.4%  70.1%  70.5%  62.7%  75.2%  84.4%  58.3%  65.7%  70.9%  73.3%  59.0%  70.5%  62.4%  71.1%  60.5%  73.2%  64.6%  30.9%  55.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 73.7%  81.3%  67.9%  65.7%  80.3%  82.8%  60.2%  63.6%  59.3%  62.4%  70.5%  73.0%  71.1%  73.4%  70.4%  78.2%  67.7%  80.7%  68.1%  74.2%  68.6%  63.6%  75.2%  84.4%  59.3%  60.0%  71.8%  75.8%  60.7%  72.4%  62.4%  67.8%  57.0%  69.9%  67.3%  33.0%  48.0%  60.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.9% 
	 74.9%  76.6%  70.5%  69.4%  78.0%  81.1%  63.6%  65.3%  69.9%  61.5%  74.3%  71.0%  68.0%  74.5%  69.4%  60.9%  71.9%  79.8%  68.1%  77.3%  69.5%  68.6%  65.1%  82.6%  57.4%  64.8%  70.9%  73.3%  59.0%  72.4%  63.2%  68.9%  56.1%  77.2%  63.7%  38.1%  47.1%  48.1%  70.4%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 75.4%  75.7%  67.9%  73.1%  78.0%  82.0%  61.9%  65.3%  63.7%  63.3%  71.4%  72.0%  66.0%  83.0%  73.5%  62.7%  69.8%  79.8%  68.1%  72.2%  68.6%  66.1%  73.4%  82.6%  57.4%  61.9%  69.2%  72.5%  59.8%  72.4%  58.4%  71.1%  57.0%  77.2%  63.7%  42.3%  52.9%  54.7%  57.4%  76.9%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 74.9%  75.7%  69.6%  69.4%  78.0%  82.8%  62.7%  64.4%  64.6%  60.6%  73.3%  73.0%  68.0%  76.6%  70.4%  74.5%  67.7%  76.1%  63.8%  73.2%  68.6%  62.7%  70.6%  81.7%  55.6%  62.9%  70.9%  74.2%  59.0%  74.3%  63.2%  72.2%  58.8%  79.7%  68.1%  43.3%  55.9%  56.6%  65.2%  63.1%  63.2%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 73.2%  75.7%  69.6%  69.4%  75.6%  81.1%  63.6%  64.4%  62.8%  64.2%  69.5%  72.0%  70.1%  73.4%  70.4%  75.5%  69.8%  79.8%  64.7%  74.2%  72.4%  63.6%  68.8%  83.5%  57.4%  63.8%  70.9%  73.3%  60.7%  76.2%  60.8%  67.8%  59.6%  75.6%  67.3%  46.4%  52.9%  61.3%  58.3%  73.8%  50.0%  58.9%   0.0%   0.0% 	Avg.: 67.7% 
	 73.2%  75.7%  68.8%  68.5%  71.7%  80.3%  61.9%  62.7%  65.5%  62.4%  72.4%  69.0%  64.9%  77.7%  72.4%  66.4%  63.5%  76.1%  68.1%  74.2%  63.8%  68.6%  74.3%  82.6%  55.6%  63.8%  68.4%  72.5%  59.0%  72.4%  57.6%  72.2%  57.9%  77.2%  66.4%  44.3%  56.9%  58.5%  54.8%  74.6%  53.5%  46.4%  67.6%   0.0% 	Avg.: 66.6% 
	 74.9%  74.8%  67.0%  70.4%  77.2%  79.5%  61.0%  59.3%  66.4%  63.3%  70.5%  71.0%  71.1%  75.5%  66.3%  68.2%  72.9%  74.3%  66.4%  76.3%  63.8%  62.7%  73.4%  83.5%  59.3%  61.0%  69.2%  75.0%  59.0%  70.5%  58.4%  74.4%  56.1%  69.1%  64.6%  45.4%  55.9%  54.7%  57.4%  72.3%  57.0%  33.9%  49.1%  66.1% 	Avg.: 65.9% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	-14.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:-14.0% 
	  0.0%  -9.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.7% 
	  0.0%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.6% 
	  0.0%   0.0%  -1.8%  -2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.1% 
	 -0.6%   0.0%   0.0%  -0.9% -10.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.3% 
	  0.6%   0.9%   0.0%  -0.9%   0.0% -13.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.2% 
	  0.6%   0.0%   0.0%   0.0%   0.0%  -0.8%  -5.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.8% 
	 -0.6%   0.0%  -0.9%  -0.9%  -0.8%   0.0%  -1.7% -13.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.3% 
	  0.6%  -1.9%   0.0%  -1.9%   0.0%  -0.8%  -0.8%  -0.8%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.8% 
	  1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%  -1.7%   0.0%  -6.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.7% 
	 -0.6%   1.9%   0.0%   0.0%   0.8%   0.0%  -2.5%   0.0%   0.0%  -0.9%  -3.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.5% 
	  0.0%   1.9%   0.0%   0.0%   0.8%   0.0%   0.8%   0.8%  -0.9%   0.0%  -1.9%  -3.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.1% 
	 -0.6%   0.0%   0.0%   0.0%   0.8%   0.0%   1.7%   0.8%   0.0%   0.0%   1.0%   0.0%  -3.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.6%   0.0%  -0.9%   0.0%   0.8%   0.0%  -0.8%   0.8%   0.0%  -0.9%   0.0%  -1.0%  -2.1%  -4.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.6% 
	 -0.6%   0.0%  -0.9%   0.0%   1.6%   0.0%   0.0%   1.7%   0.9%   0.0%   0.0%   0.0%   1.0%   0.0% -11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.5% 
	  0.0%   0.0%   0.9%  -0.9%   1.6%   0.0%   1.7%   1.7%   0.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0% -10.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.4% 
	  0.0%   0.0%   1.8%   0.0%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   1.0%   1.8%  -3.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.0%   0.9%   1.8%   0.9%   1.6%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.8%   0.0%  -2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	 -1.1%   0.9%   1.8%   0.0%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%  -1.8%   0.0%  -0.9%  -6.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	  0.6%   0.9%   1.8%  -0.9%   1.6%   0.0%   1.7%   0.8%   0.0%   0.0%   1.0%   0.0%   1.0%   0.0%   1.0%   2.7%   0.0%   0.0%  -0.9%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.6%   0.9%   1.8%   0.0%   1.6%   0.0%   0.8%  -0.8%  -0.9%  -0.9%   0.0%   0.0%   1.0%   0.0%   1.0%   5.5%   0.0%   0.0%   2.6%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.1%   0.9%   1.8%   0.9%   1.6%   0.0%   0.0%  -0.8%   0.9%   0.0%   0.0%   0.0%   1.0%   0.0%   1.0%   2.7%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.1%   0.9%   1.8%   0.9%   1.6%   0.0%   1.7%   1.7%   0.0%  -0.9%   0.0%   0.0%   1.0%   0.0%   1.0%   3.6%   0.0%   0.0%   1.7%   0.0%   0.0%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	 -0.6%   0.9%   1.8%   0.0%   1.6%   0.0%   0.0%   0.8%   0.0%   0.0%  -1.0%   0.0%   1.0%   0.0%   1.0%   3.6%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.9%   0.0%   0.0%   0.8%   0.0%   0.0%   0.8%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   1.0%   2.7%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%  -0.9%   0.0%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.0%   0.9%   1.8%   0.0%   1.6%   0.0%   0.0%   1.7%  -0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   1.0%   2.7%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%   0.0%  -0.9%  -0.9%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.0%   0.9%   1.8%   0.0%   0.8%   0.0%   0.0%   1.7%  -0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   1.0%   3.6%   0.0%  -0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.0%   0.9%   1.8%   0.0%   0.8%   0.0%   1.7%   2.5%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   1.0%   3.6%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%  -4.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.0%   0.9%   0.9%   0.0%   0.8%   0.0%   0.8%   1.7%   0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   1.0%   2.7%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.0%  -0.9%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.0%   0.9%   0.0%   0.8%   0.0%   0.8%   2.5%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   1.0%   2.7%   1.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	 -0.6%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   2.5%   0.0%   0.9%   0.0%   0.0%  -1.0%   0.0%   1.0%   2.7%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   1.0%   0.9%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.0%   0.0%   0.0%   0.8%   0.0%   2.5%   3.4%   0.0%   0.9%   0.0%   0.0%   1.0%   0.0%   1.0%   3.6%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%  -0.9%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%  -1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   3.4%   0.0%   0.9%   1.0%   0.0%   1.0%   0.0%   1.0%   2.7%   1.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.9%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.0%  -0.8%   2.5%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.0%   3.6%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.9%   0.0%   0.8%   0.0%   0.0%   0.0%  -1.1%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   2.5%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   1.0%   2.7%   1.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.0%   0.0%  -0.8%   0.0%  -1.0%   0.8%   2.2%   0.0%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.0%   0.8%   1.7%   0.0%   0.9%   1.0%   0.0%   1.0%   0.0%   1.0%   2.7%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   1.0%  -0.9%   0.8%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.9%   0.9%   0.0%   0.8%   0.0%   0.8%   2.5%   0.0%   0.9%  -1.0%   0.0%   1.0%  -1.1%   1.0%   2.7%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   1.0%   1.7%   0.8%   0.0%   0.0%   0.8%   2.2%   0.0%   0.0%   0.0%  -2.1%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.0%   0.8%   2.5%   0.9%   0.9%   0.0%   0.0%   1.0%   0.0%   1.0%   3.6%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   1.9%   0.9%   0.8%   0.0%   0.0%   1.6%   1.1%   0.0%   0.0%   0.0%   3.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.0%   0.0%   2.5%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   2.7%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.9%   1.7%   0.0%   0.0%   0.0%   0.8%   2.2%   0.0%   0.0%   0.0%   2.1%   0.0%   0.0%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.6%   0.9%   0.9%   0.0%   0.8%  -0.8%   0.0%   0.8%   0.9%   0.9%   0.0%   0.0%  -1.0%   0.0%   1.0%   3.6%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.9%   0.9%   0.8%   0.0%   0.0%   0.0%   1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   1.9%  -3.5%  -3.1%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.8%   0.0%   1.7%   0.9%   0.9%   1.0%   0.0%   1.0%   0.0%   1.0%   3.6%   0.0%   1.8%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   1.9%   0.9%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   2.1%   0.0%   3.8%   1.7%   0.8%  -1.8%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.8%   0.0%   1.7%   0.9%   0.0%   0.0%   0.0%   2.1%   0.0%   1.0%   3.6%   0.0%   0.9%   1.7%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.9%   0.9%   0.0%   0.0%   0.0%   0.8%   2.2%   0.0%   0.0%   0.0%   0.0%   0.0%   2.8%   2.6%   0.8%  -0.9%  -0.9%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.9%   0.9%   0.0%   0.8%   0.8%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   2.1%   0.0%   1.0%   3.6%   0.0%   0.9%   0.9%   0.0%   0.0%   0.8%   0.9%   0.0%   0.0%   1.9%   0.9%   0.0%   0.0%   1.0%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   2.8%   2.6%   1.5%   0.9%   2.7%   0.0%   0.0% 	Avg.:  0.8% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -2.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.2% 
	 -5.6%  11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.8% 
	  4.5%  -5.6%   7.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.0% 
	 -2.8%  -1.9% -12.5%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -3.8% 
	  5.0%  12.1%  14.3%   7.4%  -1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	  5.6%   9.3%  14.3% -10.2%  -7.1%   9.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.5% 
	  8.9%  -1.9%   1.8%   7.4%  -3.9%  -4.1%  22.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.4% 
	  5.0%   0.0%   8.9%  -2.8%  -4.7%   2.5%  22.9%   9.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	  4.5%   7.5%   2.7%   9.3%   1.6%  -6.6%  20.3%   1.7%  28.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.7% 
	  7.8%   1.9%   5.4%   2.8%   0.8%   1.6%   8.5%  -6.8%  19.5%  15.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.7% 
	  6.1%  10.3%   8.0%  10.2%   1.6%  -4.1%   8.5%  -3.4%   7.1%  11.9%   6.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.7% 
	  8.9%   2.8%   5.4%   6.5%   1.6%  -4.1%   7.6%   0.8%   4.4%   3.7%   4.8%  15.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.8% 
	  4.5%   7.5%  10.7%  10.2%   1.6%   9.0%   9.3%  -1.7%   2.7%   0.9%  -5.7%   4.0%  -3.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.8% 
	  8.9%   7.5%   6.2%  19.4%   3.1%   1.6%   6.8%  -1.7%   9.7%  -8.3%  -3.8%  -3.0%  -1.0%  16.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.4% 
	  7.3%   4.7%   8.0%  15.7%   3.1%   8.2%   6.8%   6.8%   8.0%   2.8%   1.0%  -3.0%  -5.2%   5.3%  -9.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.0% 
	  7.8%   6.5%   5.4%  16.7%   5.5%   2.5%   7.6%  11.9%   8.0%  -6.4%  -3.8%   3.0% -14.4%  -2.1%   7.1%  14.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.4% 
	  8.9%   7.5%   8.9%   4.6%   3.9%   4.1%   8.5%   0.0%   5.3%  13.8%   1.0%  -3.0%  -3.1%   2.1%  -9.2%   3.6%  18.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.5% 
	  6.7%   8.4%  14.3%   6.5%   3.9%   7.4%   5.9%   2.5%   8.0%   2.8%  -3.8%  -3.0%   5.2%  -2.1%  -3.1%   5.5%  12.5%   3.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.5% 
	  5.6%   8.4%  12.5%  13.0%   4.7%   3.3%   4.2%  -0.8%   8.8%  10.1%   1.9%   2.0%   4.1%  -8.5%   2.0%  -5.5%   0.0%   0.9%  14.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.3% 
	  7.8%  15.0%   8.9%   4.6%   6.3%   5.7%  11.9%   2.5%   7.1%   7.3%   6.7%   2.0%  -3.1%   3.2%   1.0%   0.0%  -3.1%  -0.9%   5.2%  16.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	 10.6%  10.3%   8.9%  13.0%  10.2%   0.8%  11.9%   0.8%   9.7%   5.5%   3.8%   1.0%   6.2%  -1.1%  -7.1%  -3.6%  -1.0%  -1.8%   4.3%  11.3%   4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.7% 
	 12.3%   9.3%   9.8%  10.2%  11.8%   6.6%   5.9%   5.1%   4.4%   6.4%   1.9%  -1.0%   6.2%   4.3%  10.2%  -6.4%   3.1%  -5.5%   3.4%   0.0%  -1.0%  17.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	  8.9%  10.3%  10.7%  10.2%  12.6%   2.5%   5.9%   4.2%   7.1%   5.5%   2.9%   1.0%   6.2%  -3.2%  14.3%   0.9%  -4.2%   1.8%  -0.9%   2.1%   5.7%  15.3%   9.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	  7.8%  15.9%  12.5%   1.9%   7.9%   4.1%   9.3%   6.8%  10.6%   2.8%   5.7%   0.0%   8.2%   2.1%  10.2%  -2.7%   0.0%  -0.9%  -2.6%   0.0%  -3.8%  15.3%   3.7%  19.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	 10.6%  10.3%  10.7%   6.5%   8.7%   7.4%   8.5%   6.8%   6.2%   8.3%   5.7%   0.0%   7.2%   6.4%  11.2%   2.7%  -2.1%  -1.8%  -0.9%  -2.1%   1.9%   5.9%  -1.8%   0.9%  18.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.4% 
	  6.7%  11.2%  12.5%   6.5%  10.2%   7.4%  11.0%   0.0%   4.4%   8.3%   1.9%  -2.0%   7.2%   2.1%  11.2%   0.0%   2.1%   0.0%  -1.7%   1.0%   7.6%   6.8%   2.8%  -1.8%   5.6%  27.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.7% 
	 11.7%  15.9%   8.0%   8.3%  10.2%  11.5%  11.9%   7.6%   7.1%   5.5%   2.9%   2.0%   5.2%  10.6%  13.3%   0.0%   0.0%   1.8%   3.4%   5.2%   1.9%  11.0%   0.9%  -2.8%   9.3%  13.3%   8.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.8% 
	  7.8%  14.0%  11.6%  14.8%   9.4%  10.7%  13.6%  10.2%   0.0%   7.3%   6.7%   2.0%  10.3%  -2.1%  10.2%   0.9%   0.0%  -0.9%   4.3%  -3.1%  -2.9%  10.2%   0.9%  -1.8%   7.4%   9.5%   1.7%  10.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  6.7%   9.3%  10.7%   7.4%   8.7%   9.0%  14.4%   2.5%   4.4%   2.8%   6.7%   3.0%   9.3%   8.5%  13.3%   2.7%   0.0%   0.0%   0.9%   7.2%   4.8%  10.2%   0.0%  -2.8%   5.6%  -1.0%  -0.9%  11.7%  21.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	 10.1%  13.1%   9.8%   7.4%   9.4%  12.3%  15.3%   5.1%   8.0%  11.0%   4.8%   6.0%   7.2%   8.5%  12.2%   0.9%   1.0%   3.7%   3.4%   6.2%   9.5%  11.9%  -3.7%   1.8%   4.6%   1.9%   0.0%  12.5%  18.0%  21.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.8% 
	 11.2%  12.1%  13.4%  13.9%   6.3%   8.2%  12.7%   6.8%   4.4%   4.6%   0.0%   5.0%   8.2%   2.1%   9.2%   2.7%   0.0%  -1.8%   3.4%   8.2%   2.9%   4.2%   3.7%   0.9%   9.3%   3.8%  -1.7%   5.0%  13.1%   9.5%  20.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.5% 
	 10.1%  11.2%  10.7%  14.8%  10.2%  12.3%  17.8%  11.9%   7.1%   6.4%   1.9%   6.0%   8.2%   2.1%   8.2%   2.7%   0.0%   1.8%   3.4%   6.2%   3.8%   5.1%   4.6%   0.9%   4.6%   3.8%   7.7%   0.0%  10.7%   5.7%  19.2%   7.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.1% 
	 12.8%  15.0%  15.2%   6.5%   6.3%   6.6%  14.4%   6.8%   8.8%   4.6%   1.9%   5.0%   9.3%   3.2%  13.3%  -1.8%   0.0%   0.0%  -1.7%   1.0%   5.7%   4.2%   0.9%   0.0%   2.8%   1.0%   3.4%   4.2%  10.7%   4.8%  14.4%  -3.3%  15.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	 12.8%  15.9%  11.6%  15.7%   6.3%   9.0%  13.6%   6.8%   5.3%   8.3%   0.0%   5.0%   9.3%   5.3%  14.3%   2.7%   5.2%   1.8%   4.3%  -4.1%   3.8%   8.5%   0.9%   3.7%   0.0%   1.9%   2.6%   2.5%  12.3%   5.7%  12.0%  -6.7%  16.7%   2.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	 10.1%  15.9%  12.5%   7.4%   8.7%  10.7%  14.4%   9.3%   2.7%  11.9%   2.9%   8.0%   8.2%   0.0%  13.3%   1.8%   2.1%   0.0%   4.3%  13.4%   6.7%   5.9%   1.8%   0.9%   1.9%  -4.8%   0.9%   0.0%  13.9%   0.0%   6.4%  -3.3%  16.7%   2.4%   7.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	 12.8%  15.0%  13.4%  11.1%  11.0%   9.0%  13.6%   5.9%   3.5%   6.4%   1.0%   5.0%   8.2%   7.4%  11.2%   1.8%   1.0%   2.8%   6.0%  10.3%   3.8%   7.6%  -0.9%  -0.9%   0.0%   0.0%   2.6%   1.7%  10.7%   0.0%   7.2%  -5.6%   9.6%   0.8%   5.3%  17.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.0% 
	 14.5%  11.2%  14.3%  17.6%   8.7%   7.4%  17.8%   6.8%  11.5%  11.9%   5.7%   4.0%   7.2%   8.5%  12.2%   1.8%   1.0%   2.8%   4.3%   6.2%   5.7%   6.8%   0.0%   0.0%  -0.9%   5.7%   1.7%  -0.8%   9.0%  -1.9%   7.2%   3.3%  13.2%   4.1%   2.7%  15.5%   7.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	 13.4%  15.9%  11.6%  13.9%  11.0%   9.0%  14.4%   5.1%   0.9%  12.8%   1.9%   6.0%  10.3%   7.4%  13.3%  19.1%  -3.1%   3.7%   4.3%   3.1%   4.8%   1.7%  10.1%   1.8%   1.9%   1.0%   2.6%   2.5%  10.7%   0.0%   6.4%   2.2%  14.0%  -3.3%   6.2%  10.3%   8.8%  12.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.1% 
	 12.8%  16.8%  14.3%  10.2%  11.0%   8.2%  16.1%   5.1%   7.1%  11.0%   4.8%   5.0%  12.4%  -1.1%   9.2%  17.3%   2.1%   3.7%   4.3%   8.2%   5.7%   4.2%   1.8%   1.8%   1.9%   3.8%   4.3%   3.3%   9.8%   0.0%  11.2%   0.0%  13.2%   0.0%   6.2%   6.2%   2.9%   5.7%  13.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.0% 
	 13.4%  16.8%  12.5%  13.9%  11.0%   7.4%  15.3%   5.9%   6.2%  13.8%   2.9%   4.0%  10.3%   6.4%  12.2%   5.5%   4.2%   7.3%   8.6%   7.2%   5.7%   7.6%   4.6%   2.8%   3.7%   2.9%   2.6%   1.7%  10.7%  -1.9%   6.4%  -1.1%  11.4%  -2.4%   1.8%   5.2%   0.0%   3.8%   5.2%  13.8%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.7% 
	 15.1%  16.8%  12.5%  13.9%  13.4%   9.0%  14.4%   5.9%   8.0%  10.1%   6.7%   5.0%   8.2%   9.6%  12.2%   4.5%   2.1%   3.7%   7.8%   6.2%   1.9%   6.8%   6.4%   0.9%   1.9%   1.9%   2.6%   2.5%   9.0%  -1.9%   8.8%   4.4%  10.5%   4.1%   2.7%   2.1%   2.9%  -0.9%  12.2%   3.1%  13.2%   0.0%   0.0%   0.0% 	Avg.:  6.8% 
	 15.1%  16.8%  13.4%  14.8%  17.3%   9.8%  16.1%   7.6%   5.3%  11.9%   3.8%   8.0%  13.4%   5.3%  10.2%  13.6%   8.3%   7.3%   4.3%   6.2%  10.5%   1.7%   0.9%   1.8%   3.7%   1.9%   5.1%   3.3%  10.7%   3.8%  12.0%   0.0%  12.3%   2.4%   3.5%   4.1%  -1.0%   2.8%  15.7%   2.3%   9.6%  12.5%   0.0%   0.0% 	Avg.:  7.8% 
	 13.4%  17.8%  15.2%  13.0%  11.8%  10.7%  16.9%  11.0%   4.4%  11.0%   5.7%   6.0%   7.2%   7.4%  16.3%  11.8%  -1.0%   9.2%   6.0%   4.1%  10.5%   7.6%   1.8%   0.9%   0.0%   4.8%   4.3%   0.8%  10.7%   5.7%  11.2%  -2.2%  14.0%  10.6%   5.3%   3.1%   1.0%   6.6%  13.0%   4.6%   6.1%  25.0%  18.5%   0.0% 	Avg.:  8.4% 
************************************************************************************************************
[Elapsed time = 0.4 h]
Done!

f1_score_micro: 0.6595485051860891
f1_score_macro: 0.618256544258318
              precision    recall  f1-score   support

           0       0.47      0.78      0.58         9
           1       0.75      0.75      0.75         4
           2       1.00      0.50      0.67         4
           3       0.57      1.00      0.73         4
           4       0.67      1.00      0.80         4
           5       0.50      0.25      0.33         4
           6       0.54      0.78      0.64         9
           7       1.00      1.00      1.00         4
           8       0.00      0.00      0.00         4
           9       0.73      0.89      0.80         9
          10       0.80      0.89      0.84         9
          11       1.00      0.25      0.40         4
          12       0.80      1.00      0.89         4
          13       0.67      0.80      0.73         5
          14       0.67      0.80      0.73         5
          15       1.00      1.00      1.00         4
          16       1.00      1.00      1.00         4
          17       0.00      0.00      0.00         4
          18       1.00      0.89      0.94         9
          19       0.25      0.25      0.25         4
          20       1.00      1.00      1.00         5
          21       0.12      0.33      0.18         9
          22       0.29      0.50      0.36         4
          23       0.80      0.89      0.84         9
          24       1.00      0.80      0.89         5
          25       0.80      1.00      0.89         4
          26       1.00      1.00      1.00         4
          27       0.40      0.40      0.40         5
          28       0.23      0.60      0.33         5
          29       1.00      1.00      1.00         5
          30       0.60      0.75      0.67         4
          31       1.00      1.00      1.00         4
          32       0.67      1.00      0.80         4
          33       0.62      1.00      0.77         5
          34       0.67      1.00      0.80         4
          35       1.00      1.00      1.00         4
          36       0.60      0.75      0.67         4
          37       1.00      1.00      1.00         4
          38       0.00      0.00      0.00         4
          39       0.67      1.00      0.80         4
          40       1.00      1.00      1.00         4
          41       1.00      1.00      1.00         9
          42       1.00      1.00      1.00         9
          43       0.67      0.89      0.76         9
          44       0.00      0.00      0.00         4
          45       0.60      0.75      0.67         4
          46       1.00      1.00      1.00         5
          47       0.80      1.00      0.89         4
          48       0.80      0.80      0.80         5
          49       0.07      0.11      0.08         9
          50       0.00      0.00      0.00         4
          51       1.00      1.00      1.00         4
          52       1.00      0.75      0.86         4
          53       0.70      0.78      0.74         9
          54       1.00      0.78      0.88         9
          55       0.83      0.56      0.67         9
          56       0.80      1.00      0.89         4
          57       0.80      1.00      0.89         4
          58       1.00      0.78      0.88         9
          59       0.00      0.00      0.00         4
          60       0.00      0.00      0.00         4
          61       1.00      0.75      0.86         4
          62       0.40      0.80      0.53         5
          63       0.71      1.00      0.83         5
          64       1.00      1.00      1.00         5
          65       0.90      1.00      0.95         9
          66       1.00      0.89      0.94         9
          67       1.00      1.00      1.00         4
          68       1.00      0.75      0.86         4
          69       1.00      0.40      0.57         5
          70       0.00      0.00      0.00         4
          71       1.00      0.40      0.57         5
          72       0.75      0.60      0.67         5
          73       0.00      0.00      0.00         5
          74       1.00      0.89      0.94         9
          75       0.50      0.25      0.33         4
          76       0.00      0.00      0.00         4
          77       0.00      0.00      0.00         4
          78       0.44      1.00      0.62         4
          79       0.67      0.80      0.73         5
          80       0.67      0.50      0.57         4
          81       0.75      0.75      0.75         4
          82       0.73      0.89      0.80         9
          83       1.00      0.60      0.75         5
          84       0.80      1.00      0.89         4
          85       0.43      0.75      0.55         4
          86       0.50      0.25      0.33         4
          87       0.90      1.00      0.95         9
          88       1.00      0.89      0.94         9
          89       0.67      1.00      0.80         4
          90       0.00      0.00      0.00         4
          91       0.89      0.89      0.89         9
          92       0.10      0.50      0.16         4
          93       0.57      0.80      0.67         5
          94       0.38      0.56      0.45         9
          95       0.57      1.00      0.73         4
          96       0.38      0.33      0.35         9
          97       0.12      0.50      0.19         4
          98       1.00      0.75      0.86         4
          99       0.57      1.00      0.73         4
         100       0.33      0.22      0.27         9
         101       0.67      1.00      0.80         4
         102       0.71      1.00      0.83         5
         103       1.00      0.40      0.57         5
         104       0.90      1.00      0.95         9
         105       0.73      0.89      0.80         9
         106       1.00      1.00      1.00         4
         107       0.38      0.75      0.50         4
         108       1.00      0.75      0.86         4
         109       0.90      1.00      0.95         9
         110       0.25      0.25      0.25         4
         111       1.00      1.00      1.00         9
         112       1.00      1.00      1.00         9
         113       0.82      1.00      0.90         9
         114       0.67      1.00      0.80         4
         115       0.75      1.00      0.86         9
         116       0.86      0.67      0.75         9
         117       1.00      0.25      0.40         4
         118       0.82      1.00      0.90         9
         119       0.90      1.00      0.95         9
         120       0.80      1.00      0.89         4
         121       0.80      0.89      0.84         9
         122       0.67      1.00      0.80         4
         123       1.00      1.00      1.00         5
         124       1.00      1.00      1.00         4
         125       1.00      1.00      1.00         4
         126       0.00      0.00      0.00         4
         127       0.82      1.00      0.90         9
         128       0.62      1.00      0.77         5
         129       1.00      1.00      1.00         9
         130       0.00      0.00      0.00         4
         131       0.00      0.00      0.00         9
         132       0.10      0.75      0.18         4
         133       0.80      1.00      0.89         4
         134       0.67      0.44      0.53         9
         135       0.75      0.75      0.75         4
         136       1.00      0.89      0.94         9
         137       0.57      1.00      0.73         4
         138       0.75      0.60      0.67         5
         139       1.00      0.80      0.89         5
         140       0.71      1.00      0.83         5
         141       0.00      0.00      0.00         4
         142       0.25      0.25      0.25         4
         143       0.75      0.75      0.75         4
         144       0.90      1.00      0.95         9
         145       1.00      1.00      1.00         4
         146       1.00      1.00      1.00         4
         147       0.00      0.00      0.00         9
         148       0.67      0.89      0.76         9
         149       0.00      0.00      0.00         9
         150       0.88      0.78      0.82         9
         151       0.00      0.00      0.00         4
         152       0.67      1.00      0.80         4
         153       0.33      0.25      0.29         4
         154       0.60      0.75      0.67         4
         155       1.00      0.56      0.71         9
         156       0.43      0.75      0.55         4
         157       1.00      1.00      1.00         5
         158       0.80      1.00      0.89         4
         159       1.00      1.00      1.00         9
         160       0.00      0.00      0.00         4
         161       1.00      0.50      0.67         4
         162       0.00      0.00      0.00         9
         163       1.00      1.00      1.00         5
         164       1.00      0.80      0.89         5
         165       1.00      1.00      1.00         5
         166       1.00      0.88      0.93         8
         167       0.20      0.25      0.22         4
         168       0.58      0.78      0.67         9
         169       0.67      0.44      0.53         9
         170       0.00      0.00      0.00         9
         171       1.00      0.75      0.86         4
         172       0.75      0.75      0.75         4
         173       0.00      0.00      0.00         4
         174       0.60      0.75      0.67         4
         175       0.10      0.25      0.14         4
         176       0.62      0.56      0.59         9
         177       0.00      0.00      0.00         4
         178       0.73      0.89      0.80         9
         179       0.67      1.00      0.80         4
         180       0.00      0.00      0.00         4
         181       1.00      1.00      1.00         4
         182       1.00      0.56      0.71         9
         183       0.80      1.00      0.89         4
         184       1.00      1.00      1.00         4
         185       1.00      0.71      0.83         7
         186       0.60      0.75      0.67         4
         187       0.50      0.67      0.57         9
         188       0.80      1.00      0.89         4
         189       1.00      1.00      1.00         9
         190       0.10      0.50      0.16         4
         191       0.80      1.00      0.89         4
         192       0.43      0.33      0.38         9
         193       0.10      0.25      0.14         4
         194       0.83      1.00      0.91         5
         195       0.67      1.00      0.80         4
         196       1.00      0.44      0.62         9
         197       0.75      0.60      0.67         5
         198       0.67      1.00      0.80         4
         199       0.00      0.00      0.00         9
         200       0.50      0.44      0.47         9
         201       1.00      0.75      0.86         4
         202       1.00      1.00      1.00         5
         203       1.00      0.50      0.67         4
         204       1.00      1.00      1.00         4
         205       1.00      0.50      0.67         4
         206       0.00      0.00      0.00         4
         207       0.20      0.75      0.32         4
         208       1.00      1.00      1.00         9
         209       0.13      0.44      0.21         9
         210       0.67      1.00      0.80         4
         211       0.80      0.80      0.80         5
         212       1.00      0.25      0.40         4
         213       1.00      1.00      1.00         4
         214       0.90      1.00      0.95         9
         215       0.40      1.00      0.57         4
         216       0.80      1.00      0.89         4
         217       0.40      0.22      0.29         9
         218       0.67      1.00      0.80         4
         219       0.67      0.80      0.73         5
         220       1.00      1.00      1.00         4
         221       0.67      0.40      0.50         5
         222       0.29      0.50      0.36         4
         223       0.29      0.50      0.36         4
         224       0.33      0.75      0.46         4
         225       0.90      1.00      0.95         9
         226       0.75      0.60      0.67         5
         227       0.50      1.00      0.67         4
         228       0.50      0.50      0.50         4
         229       0.50      0.33      0.40         9
         230       0.38      0.75      0.50         4
         231       0.50      0.40      0.44         5
         232       1.00      0.75      0.86         4
         233       0.83      1.00      0.91         5
         234       0.14      0.25      0.18         4
         235       0.80      0.80      0.80         5
         236       0.89      0.89      0.89         9
         237       0.00      0.00      0.00         4
         238       0.80      1.00      0.89         4
         239       1.00      0.75      0.86         4
         240       0.83      1.00      0.91         5
         241       1.00      1.00      1.00         5
         242       1.00      0.44      0.62         9
         243       1.00      1.00      1.00         4
         244       1.00      1.00      1.00         4
         245       0.50      0.60      0.55         5
         246       0.80      1.00      0.89         4
         247       1.00      1.00      1.00         4
         248       0.80      1.00      0.89         4
         249       0.80      0.80      0.80         5
         250       0.33      0.25      0.29         4
         251       0.00      0.00      0.00         4
         252       0.89      0.89      0.89         9
         253       1.00      0.25      0.40         4
         254       0.00      0.00      0.00         4
         255       1.00      1.00      1.00         9
         256       0.50      0.75      0.60         4
         257       0.00      0.00      0.00         4
         258       0.57      0.80      0.67         5
         259       0.67      1.00      0.80         4
         260       0.75      0.75      0.75         4
         261       0.82      1.00      0.90         9
         262       0.10      0.50      0.16         4
         263       0.60      0.75      0.67         4
         264       0.78      0.78      0.78         9
         265       0.33      0.20      0.25         5
         266       1.00      0.75      0.86         4
         267       0.50      0.75      0.60         4
         268       1.00      1.00      1.00         4
         269       1.00      1.00      1.00         4
         270       0.07      0.25      0.11         4
         271       0.20      0.25      0.22         4
         272       1.00      1.00      1.00         4
         273       0.67      1.00      0.80         4
         274       0.80      1.00      0.89         4
         275       0.80      1.00      0.89         4
         276       0.00      0.00      0.00         4
         277       0.75      0.75      0.75         4
         278       0.00      0.00      0.00         4
         279       0.71      1.00      0.83         5
         280       0.70      0.78      0.74         9
         281       0.80      1.00      0.89         4
         282       1.00      1.00      1.00         4
         283       1.00      0.60      0.75         5
         284       0.50      1.00      0.67         4
         285       0.60      0.75      0.67         4
         286       0.00      0.00      0.00         4
         287       1.00      1.00      1.00         4
         288       0.89      0.89      0.89         9
         289       0.80      1.00      0.89         4
         290       1.00      1.00      1.00         5
         291       1.00      1.00      1.00         4
         292       0.00      0.00      0.00         4
         293       0.83      1.00      0.91         5
         294       0.00      0.00      0.00         4
         295       0.20      0.40      0.27         5
         296       1.00      1.00      1.00         4
         297       0.75      0.75      0.75         4
         298       0.00      0.00      0.00         4
         299       0.50      0.50      0.50         4
         300       1.00      1.00      1.00         5
         301       0.50      0.75      0.60         4
         302       1.00      1.00      1.00         4
         303       1.00      1.00      1.00         4
         304       0.00      0.00      0.00         4
         305       1.00      1.00      1.00         9
         306       0.89      0.89      0.89         9
         307       0.00      0.00      0.00         4
         308       0.50      1.00      0.67         4
         309       0.33      0.25      0.29         4
         310       1.00      0.89      0.94         9
         311       0.80      1.00      0.89         4
         312       0.67      1.00      0.80         4
         313       0.00      0.00      0.00         5
         314       1.00      0.60      0.75         5
         315       1.00      1.00      1.00         4
         316       1.00      0.89      0.94         9
         317       0.60      0.60      0.60         5
         318       0.57      0.44      0.50         9
         319       1.00      1.00      1.00         5
         320       0.80      1.00      0.89         4
         321       0.50      0.75      0.60         4
         322       0.00      0.00      0.00         4
         323       0.80      0.44      0.57         9
         324       0.83      1.00      0.91         5
         325       0.57      0.89      0.70         9
         326       0.80      1.00      0.89         4
         327       1.00      1.00      1.00         4
         328       0.57      0.80      0.67         5
         329       0.40      0.50      0.44         4
         330       0.57      1.00      0.73         4
         331       1.00      0.50      0.67         4
         332       0.00      0.00      0.00         9
         333       1.00      1.00      1.00         4
         334       1.00      0.75      0.86         4
         335       0.89      0.89      0.89         9
         336       0.80      1.00      0.89         4
         337       0.80      1.00      0.89         4
         338       0.50      0.50      0.50         4
         339       0.80      1.00      0.89         4
         340       0.50      0.75      0.60         4
         341       0.33      0.25      0.29         4
         342       0.00      0.00      0.00         4
         343       1.00      1.00      1.00         4
         344       0.67      0.50      0.57         4
         345       0.14      0.25      0.18         4
         346       1.00      1.00      1.00         4
         347       0.14      1.00      0.24         4
         348       1.00      0.78      0.88         9
         349       1.00      0.25      0.40         4
         350       1.00      0.75      0.86         4
         351       0.88      0.78      0.82         9
         352       0.83      1.00      0.91         5
         353       0.50      0.75      0.60         4
         354       0.90      1.00      0.95         9
         355       1.00      0.25      0.40         4
         356       0.78      0.78      0.78         9
         357       0.75      0.75      0.75         4
         358       0.00      0.00      0.00         4
         359       1.00      1.00      1.00         5
         360       0.00      0.00      0.00         4
         361       0.60      0.67      0.63         9
         362       0.90      1.00      0.95         9
         363       1.00      1.00      1.00         5
         364       0.67      0.80      0.73         5
         365       1.00      1.00      1.00         5
         366       0.33      0.25      0.29         4
         367       1.00      0.75      0.86         4
         368       1.00      0.25      0.40         4
         369       1.00      1.00      1.00         4
         370       1.00      0.75      0.86         4
         371       0.80      1.00      0.89         4
         372       1.00      0.78      0.88         9
         373       1.00      1.00      1.00         4
         374       1.00      1.00      1.00         4
         375       0.80      1.00      0.89         4
         376       0.00      0.00      0.00         4
         377       1.00      0.60      0.75         5
         378       0.12      0.25      0.17         4
         379       0.00      0.00      0.00         9
         380       0.60      0.75      0.67         4
         381       1.00      0.80      0.89         5
         382       0.00      0.00      0.00         4
         383       1.00      1.00      1.00         9
         384       0.57      0.80      0.67         5
         385       0.57      0.80      0.67         5
         386       0.78      0.78      0.78         9
         387       1.00      1.00      1.00         9
         388       0.75      0.67      0.71         9
         389       0.73      0.89      0.80         9
         390       0.67      0.50      0.57         4
         391       1.00      0.80      0.89         5
         392       1.00      0.80      0.89         5
         393       0.25      0.25      0.25         4
         394       0.43      0.60      0.50         5
         395       0.75      0.75      0.75         4
         396       0.00      0.00      0.00         4
         397       0.78      0.78      0.78         9
         398       0.00      0.00      0.00         4
         399       0.80      0.89      0.84         9
         400       0.80      1.00      0.89         4
         401       0.80      1.00      0.89         4
         402       1.00      0.75      0.86         4
         403       1.00      0.89      0.94         9
         404       0.67      0.50      0.57         4
         405       1.00      1.00      1.00         4
         406       0.80      1.00      0.89         4
         407       0.50      0.75      0.60         4
         408       0.50      0.50      0.50         4
         409       0.75      0.75      0.75         4
         410       1.00      1.00      1.00         4
         411       0.75      0.75      0.75         4
         412       1.00      1.00      1.00         5
         413       1.00      1.00      1.00         4
         414       1.00      1.00      1.00         4
         415       0.80      0.44      0.57         9
         416       0.80      0.80      0.80         5
         417       1.00      0.60      0.75         5
         418       0.75      0.75      0.75         4
         419       0.00      0.00      0.00         4
         420       0.89      0.89      0.89         9
         421       0.80      1.00      0.89         4
         422       1.00      0.80      0.89         5
         423       0.09      0.22      0.12         9
         424       0.80      0.80      0.80         5
         425       0.80      1.00      0.89         4
         426       0.00      0.00      0.00         4
         427       0.20      0.25      0.22         4
         428       1.00      1.00      1.00         4
         429       1.00      0.75      0.86         4
         430       1.00      1.00      1.00         9
         431       0.00      0.00      0.00         4
         432       1.00      0.25      0.40         4
         433       1.00      1.00      1.00         5
         434       0.09      0.25      0.13         4
         435       1.00      1.00      1.00         5
         436       0.60      0.33      0.43         9
         437       0.75      1.00      0.86         9
         438       0.00      0.00      0.00         4
         439       1.00      0.75      0.86         4
         440       1.00      1.00      1.00         4
         441       1.00      0.78      0.88         9
         442       0.10      0.11      0.11         9
         443       1.00      1.00      1.00         4
         444       0.25      0.25      0.25         4
         445       0.86      0.67      0.75         9
         446       0.57      0.80      0.67         5
         447       1.00      0.40      0.57         5
         448       1.00      1.00      1.00         4
         449       1.00      0.25      0.40         4
         450       1.00      1.00      1.00         9
         451       0.60      0.75      0.67         4
         452       0.88      0.78      0.82         9
         453       0.00      0.00      0.00         4
         454       1.00      1.00      1.00         5
         455       0.00      0.00      0.00         4
         456       1.00      0.25      0.40         4
         457       0.82      1.00      0.90         9
         458       0.60      0.60      0.60         5
         459       0.75      0.75      0.75         4
         460       0.69      1.00      0.82         9
         461       0.43      0.75      0.55         4
         462       0.00      0.00      0.00         4
         463       0.00      0.00      0.00         4
         464       0.40      1.00      0.57         4
         465       0.83      1.00      0.91         5
         466       1.00      0.56      0.71         9
         467       0.75      0.75      0.75         4
         468       1.00      1.00      1.00         4
         469       1.00      0.89      0.94         9
         470       1.00      1.00      1.00         4
         471       1.00      1.00      1.00         5
         472       0.50      0.89      0.64         9
         473       1.00      0.25      0.40         4
         474       1.00      1.00      1.00         4
         475       0.50      0.50      0.50         4
         476       0.38      0.75      0.50         4
         477       1.00      1.00      1.00         5
         478       1.00      1.00      1.00         5
         479       0.83      1.00      0.91         5
         480       1.00      1.00      1.00         9
         481       0.67      0.50      0.57         4
         482       0.57      0.80      0.67         5
         483       0.43      0.75      0.55         4
         484       1.00      0.75      0.86         4
         485       0.60      0.75      0.67         4
         486       1.00      1.00      1.00         5
         487       1.00      0.14      0.25         7
         488       1.00      1.00      1.00         9
         489       1.00      1.00      1.00         5
         490       1.00      0.78      0.88         9
         491       0.43      0.75      0.55         4
         492       0.57      1.00      0.73         4
         493       0.82      1.00      0.90         9
         494       1.00      0.56      0.71         9
         495       0.00      0.00      0.00         4
         496       0.22      0.50      0.31         4
         497       1.00      1.00      1.00         4
         498       0.88      0.88      0.88         8
         499       0.00      0.00      0.00         4
         500       0.33      0.25      0.29         4
         501       1.00      1.00      1.00         5
         502       1.00      1.00      1.00         9
         503       0.00      0.00      0.00         4
         504       0.71      1.00      0.83         5
         505       0.82      1.00      0.90         9
         506       0.00      0.00      0.00         4
         507       1.00      0.60      0.75         5
         508       0.57      0.80      0.67         5
         509       1.00      1.00      1.00         4
         510       1.00      0.75      0.86         4
         511       0.00      0.00      0.00         4
         512       0.67      0.50      0.57         4
         513       0.25      0.11      0.15         9
         514       1.00      0.75      0.86         4
         515       0.50      0.40      0.44         5
         516       0.50      1.00      0.67         4
         517       0.00      0.00      0.00         4
         518       0.00      0.00      0.00         4
         519       0.67      1.00      0.80         4
         520       0.00      0.00      0.00         9
         521       0.90      1.00      0.95         9
         522       0.60      0.75      0.67         4
         523       1.00      1.00      1.00         5
         524       0.83      1.00      0.91         5
         525       1.00      0.89      0.94         9
         526       0.50      0.44      0.47         9
         527       0.00      0.00      0.00         5
         528       0.50      0.75      0.60         4
         529       0.75      0.75      0.75         4
         530       1.00      1.00      1.00         4
         531       0.75      0.75      0.75         4
         532       0.33      0.25      0.29         4
         533       0.50      0.60      0.55         5
         534       0.00      0.00      0.00         4
         535       1.00      1.00      1.00         9
         536       1.00      0.67      0.80         9
         537       0.80      0.80      0.80         5
         538       0.64      0.78      0.70         9
         539       0.78      0.78      0.78         9
         540       0.33      0.25      0.29         4
         541       0.00      0.00      0.00         4
         542       0.75      0.75      0.75         4
         543       1.00      0.67      0.80         9
         544       1.00      1.00      1.00         4
         545       1.00      0.75      0.86         4
         546       0.50      0.80      0.62         5
         547       0.00      0.00      0.00         4
         548       0.60      0.75      0.67         4
         549       0.67      1.00      0.80         4
         550       0.82      1.00      0.90         9
         551       0.33      0.25      0.29         4
         552       0.58      0.78      0.67         9
         553       0.30      0.75      0.43         4
         554       0.80      0.80      0.80         5
         555       0.00      0.00      0.00         4
         556       0.00      0.00      0.00         4
         557       1.00      0.50      0.67         4
         558       1.00      1.00      1.00         4
         559       1.00      1.00      1.00         9
         560       0.80      1.00      0.89         4
         561       0.00      0.00      0.00         9
         562       1.00      0.89      0.94         9
         563       0.60      0.60      0.60         5
         564       1.00      1.00      1.00         9
         565       0.88      0.78      0.82         9
         566       0.83      1.00      0.91         5
         567       1.00      0.75      0.86         4
         568       1.00      0.40      0.57         5
         569       1.00      1.00      1.00         5
         570       0.80      1.00      0.89         4
         571       0.57      1.00      0.73         4
         572       1.00      1.00      1.00         9
         573       0.67      0.89      0.76         9
         574       0.00      0.00      0.00         4
         575       0.71      1.00      0.83         5
         576       0.00      0.00      0.00         9
         577       0.89      0.89      0.89         9
         578       0.09      0.22      0.12         9
         579       1.00      0.78      0.88         9
         580       0.17      0.25      0.20         4
         581       1.00      0.89      0.94         9
         582       0.00      0.00      0.00         4
         583       0.25      0.25      0.25         4
         584       0.00      0.00      0.00         4
         585       0.75      0.75      0.75         4
         586       0.78      0.78      0.78         9
         587       0.80      1.00      0.89         4
         588       0.00      0.00      0.00         4
         589       1.00      0.75      0.86         4
         590       1.00      1.00      1.00         4
         591       1.00      0.67      0.80         9
         592       1.00      1.00      1.00         5
         593       1.00      0.89      0.94         9
         594       0.00      0.00      0.00         4
         595       0.00      0.00      0.00         4
         596       0.67      1.00      0.80         4
         597       0.83      1.00      0.91         5
         598       0.80      1.00      0.89         4
         599       1.00      1.00      1.00         4
         600       0.62      1.00      0.77         5
         601       0.00      0.00      0.00         4
         602       0.67      1.00      0.80         4
         603       1.00      0.67      0.80         9
         604       0.50      0.80      0.62         5
         605       0.17      0.25      0.20         4
         606       0.60      0.75      0.67         4
         607       0.67      0.44      0.53         9
         608       0.43      0.75      0.55         4
         609       1.00      1.00      1.00         9
         610       1.00      1.00      1.00         5
         611       1.00      0.80      0.89         5
         612       0.40      0.50      0.44         4
         613       0.78      0.78      0.78         9
         614       0.00      0.00      0.00         4
         615       0.75      0.33      0.46         9
         616       0.50      0.33      0.40         9
         617       0.29      0.50      0.36         4
         618       1.00      0.60      0.75         5
         619       1.00      0.89      0.94         9
         620       1.00      0.75      0.86         4
         621       0.70      0.78      0.74         9
         622       1.00      0.60      0.75         5
         623       1.00      0.80      0.89         5
         624       1.00      0.75      0.86         4
         625       1.00      0.75      0.86         4
         626       0.62      1.00      0.77         5
         627       1.00      0.89      0.94         9
         628       0.40      0.67      0.50         9
         629       0.83      1.00      0.91         5
         630       0.00      0.00      0.00         9
         631       0.75      0.33      0.46         9
         632       1.00      1.00      1.00         4
         633       0.00      0.00      0.00         4
         634       0.67      1.00      0.80         4
         635       0.60      0.75      0.67         4
         636       1.00      1.00      1.00         4
         637       0.80      1.00      0.89         4
         638       1.00      0.75      0.86         4
         639       0.00      0.00      0.00         4
         640       1.00      0.75      0.86         4
         641       0.80      1.00      0.89         4
         642       0.00      0.00      0.00         4
         643       1.00      1.00      1.00         5
         644       1.00      0.80      0.89         5
         645       1.00      0.56      0.71         9
         646       1.00      1.00      1.00         5
         647       0.71      1.00      0.83         5
         648       0.00      0.00      0.00         4
         649       1.00      0.75      0.86         4
         650       1.00      1.00      1.00         4
         651       0.80      0.80      0.80         5
         652       0.80      1.00      0.89         4
         653       1.00      0.75      0.86         4
         654       0.67      0.50      0.57         4
         655       1.00      1.00      1.00         4
         656       0.00      0.00      0.00         4
         657       0.00      0.00      0.00         4
         658       0.00      0.00      0.00         4
         659       0.60      0.33      0.43         9
         660       1.00      0.33      0.50         9
         661       1.00      0.78      0.88         9
         662       0.80      0.50      0.62         8
         663       0.75      0.75      0.75         4
         664       0.20      0.25      0.22         4
         665       0.00      0.00      0.00         4
         666       1.00      0.75      0.86         4
         667       0.90      1.00      0.95         9
         668       0.00      0.00      0.00         4
         669       0.80      0.89      0.84         9
         670       1.00      1.00      1.00         9
         671       0.00      0.00      0.00         4
         672       0.80      1.00      0.89         4
         673       1.00      1.00      1.00         4
         674       0.64      1.00      0.78         9
         675       0.75      0.75      0.75         4
         676       0.88      0.78      0.82         9
         677       0.60      0.75      0.67         4
         678       0.88      0.78      0.82         9
         679       0.33      0.20      0.25         5
         680       0.80      0.44      0.57         9
         681       0.00      0.00      0.00         9
         682       0.29      0.40      0.33         5
         683       1.00      0.75      0.86         4
         684       1.00      1.00      1.00         4
         685       1.00      1.00      1.00         4
         686       0.00      0.00      0.00         4
         687       1.00      1.00      1.00         9
         688       0.45      1.00      0.62         5
         689       0.60      0.75      0.67         4
         690       0.90      1.00      0.95         9
         691       1.00      0.67      0.80         9
         692       0.50      0.50      0.50         4
         693       0.67      1.00      0.80         4
         694       1.00      0.78      0.88         9
         695       0.80      1.00      0.89         4
         696       1.00      1.00      1.00         4
         697       0.00      0.00      0.00         9
         698       1.00      0.80      0.89         5
         699       0.33      0.25      0.29         4
         700       0.64      1.00      0.78         9
         701       1.00      0.75      0.86         4
         702       0.50      0.75      0.60         4
         703       1.00      0.25      0.40         4
         704       1.00      0.44      0.62         9
         705       1.00      0.75      0.86         4
         706       0.67      1.00      0.80         4
         707       0.50      0.25      0.33         4
         708       0.71      0.56      0.63         9
         709       0.00      0.00      0.00         4
         710       1.00      1.00      1.00         4
         711       1.00      0.60      0.75         5
         712       0.67      0.80      0.73         5
         713       0.69      1.00      0.82         9
         714       0.80      1.00      0.89         4
         715       0.00      0.00      0.00         9
         716       0.83      0.56      0.67         9
         717       0.44      1.00      0.62         4
         718       0.25      0.20      0.22         5
         719       0.00      0.00      0.00         4
         720       0.33      0.50      0.40         4
         721       1.00      0.75      0.86         4
         722       0.13      0.22      0.17         9
         723       1.00      1.00      1.00         5
         724       0.00      0.00      0.00         4
         725       0.00      0.00      0.00         4
         726       0.50      0.50      0.50         4
         727       0.00      0.00      0.00         4
         728       0.00      0.00      0.00         4
         729       0.60      0.75      0.67         4
         730       0.67      1.00      0.80         4
         731       1.00      1.00      1.00         4
         732       0.50      0.50      0.50         4
         733       0.60      0.75      0.67         4
         734       1.00      0.56      0.71         9
         735       0.33      0.25      0.29         4
         736       0.00      0.00      0.00         4
         737       0.00      0.00      0.00         4
         738       0.43      0.75      0.55         4
         739       0.75      0.75      0.75         4
         740       0.67      0.50      0.57         4
         741       0.88      0.78      0.82         9
         742       0.00      0.00      0.00         4
         743       0.00      0.00      0.00         5
         744       1.00      1.00      1.00         4
         745       0.43      0.75      0.55         4
         746       1.00      1.00      1.00         4
         747       0.60      0.75      0.67         4
         748       0.82      1.00      0.90         9
         749       0.00      0.00      0.00         4
         750       0.89      0.89      0.89         9
         751       0.00      0.00      0.00         4
         752       1.00      0.40      0.57         5
         753       0.33      0.75      0.46         4
         754       0.00      0.00      0.00         4
         755       1.00      0.75      0.86         4
         756       0.60      0.75      0.67         4
         757       1.00      0.56      0.71         9
         758       0.00      0.00      0.00         4
         759       0.00      0.00      0.00         9
         760       0.60      0.67      0.63         9
         761       1.00      0.75      0.86         4
         762       1.00      1.00      1.00         5
         763       0.67      0.40      0.50         5
         764       0.00      0.00      0.00         4
         765       0.00      0.00      0.00         4
         766       1.00      0.71      0.83         7
         767       0.33      0.75      0.46         4
         768       0.67      1.00      0.80         4
         769       0.57      1.00      0.73         4
         770       0.55      0.67      0.60         9
         771       1.00      1.00      1.00         4
         772       1.00      0.25      0.40         4
         773       1.00      0.80      0.89         5
         774       1.00      0.25      0.40         4
         775       0.75      0.75      0.75         4
         776       0.00      0.00      0.00         4
         777       0.00      0.00      0.00         4
         778       0.67      1.00      0.80         4
         779       0.29      0.50      0.36         4
         780       0.30      0.75      0.43         4
         781       1.00      0.33      0.50         9
         782       0.25      0.25      0.25         4
         783       0.89      0.89      0.89         9
         784       0.75      0.33      0.46         9
         785       0.60      0.75      0.67         4
         786       1.00      0.89      0.94         9
         787       0.83      0.56      0.67         9
         788       0.00      0.00      0.00         4
         789       0.88      0.78      0.82         9
         790       1.00      1.00      1.00         4
         791       0.00      0.00      0.00         4
         792       1.00      0.50      0.67         4
         793       0.90      1.00      0.95         9
         794       1.00      1.00      1.00         9
         795       0.47      1.00      0.64         9
         796       0.71      1.00      0.83         5
         797       0.57      1.00      0.73         4
         798       0.80      1.00      0.89         4
         799       0.67      0.67      0.67         9
         800       1.00      1.00      1.00         9
         801       1.00      0.40      0.57         5
         802       0.53      1.00      0.69         9
         803       1.00      0.89      0.94         9
         804       0.00      0.00      0.00         4
         805       0.20      0.25      0.22         4
         806       0.50      0.17      0.25         6
         807       0.57      0.44      0.50         9
         808       0.71      1.00      0.83         5
         809       1.00      0.75      0.86         4
         810       1.00      1.00      1.00         9
         811       0.75      0.75      0.75         4
         812       0.00      0.00      0.00         4
         813       0.50      0.33      0.40         9
         814       0.67      0.89      0.76         9
         815       1.00      1.00      1.00         4
         816       0.75      0.75      0.75         4
         817       1.00      0.75      0.86         4
         818       1.00      1.00      1.00         4
         819       0.00      0.00      0.00         9
         820       0.86      0.67      0.75         9
         821       0.67      0.40      0.50         5
         822       1.00      0.75      0.86         4
         823       0.00      0.00      0.00         5
         824       1.00      1.00      1.00         4
         825       1.00      0.67      0.80         9
         826       1.00      1.00      1.00         5
         827       1.00      0.78      0.88         9
         828       0.00      0.00      0.00         4
         829       0.50      1.00      0.67         4
         830       0.00      0.00      0.00         9
         831       0.67      1.00      0.80         4
         832       0.00      0.00      0.00         4
         833       1.00      0.40      0.57         5
         834       0.75      0.33      0.46         9
         835       1.00      1.00      1.00         4
         836       0.86      0.67      0.75         9
         837       0.00      0.00      0.00         4
         838       0.00      0.00      0.00         4
         839       0.00      0.00      0.00         4
         840       1.00      1.00      1.00         4
         841       0.75      0.75      0.75         4
         842       0.00      0.00      0.00         4
         843       1.00      0.11      0.20         9
         844       0.00      0.00      0.00         4
         845       0.00      0.00      0.00         4
         846       0.00      0.00      0.00         9
         847       1.00      0.75      0.86         4
         848       0.67      0.44      0.53         9
         849       0.00      0.00      0.00         4
         850       1.00      0.67      0.80         9
         851       0.00      0.00      0.00         4
         852       0.00      0.00      0.00         5
         853       1.00      0.80      0.89         5
         854       1.00      1.00      1.00         4
         855       0.50      0.25      0.33         4
         856       0.25      0.22      0.24         9
         857       0.00      0.00      0.00         4
         858       1.00      1.00      1.00         5
         859       0.00      0.00      0.00         4
         860       1.00      1.00      1.00         4
         861       1.00      0.78      0.88         9
         862       0.90      1.00      0.95         9
         863       0.00      0.00      0.00         4
         864       0.89      0.89      0.89         9
         865       0.80      1.00      0.89         4
         866       0.00      0.00      0.00         9
         867       1.00      0.75      0.86         4
         868       0.00      0.00      0.00         4
         869       0.50      0.25      0.33         4
         870       0.00      0.00      0.00         4
         871       0.00      0.00      0.00         5
         872       0.00      0.00      0.00         4
         873       1.00      1.00      1.00         5
         874       0.60      1.00      0.75         9
         875       0.40      0.50      0.44         4
         876       0.26      0.78      0.39         9
         877       0.26      0.78      0.39         9
         878       0.30      0.75      0.43         4
         879       0.75      1.00      0.86         9
         880       1.00      0.25      0.40         4
         881       0.26      0.89      0.40         9
         882       1.00      0.25      0.40         4
         883       0.20      0.25      0.22         4
         884       0.89      0.89      0.89         9
         885       0.67      0.50      0.57         4
         886       0.00      0.00      0.00         4
         887       0.44      0.89      0.59         9
         888       0.00      0.00      0.00         4
         889       0.60      0.75      0.67         4
         890       0.00      0.00      0.00         4
         891       0.15      1.00      0.26         4
         892       1.00      0.75      0.86         4
         893       0.00      0.00      0.00         4

    accuracy                           0.66      4917
   macro avg       0.64      0.64      0.62      4917
weighted avg       0.66      0.66      0.64      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
