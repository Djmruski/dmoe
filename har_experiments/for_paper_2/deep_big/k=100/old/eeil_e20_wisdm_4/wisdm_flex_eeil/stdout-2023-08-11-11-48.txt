	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [439, 213, 569, 597, 210, 401, 89, 125, 694, 55, 348, 663, 350, 845, 552, 481, 80, 183, 426, 59, 584, 271, 854, 707, 686, 331, 273, 655, 63, 387, 560, 433, 524, 769, 293, 555, 313, 156, 100, 200, 599, 294, 871, 502, 767, 668, 878, 44, 487, 212, 618, 688, 362, 263, 408, 412, 375, 462, 424, 133, 478, 112, 188, 309, 389, 300, 826, 19, 484, 879, 32, 529, 652, 303, 27, 111, 10, 87, 660, 531, 104, 733, 509, 277, 402, 56, 458, 785, 427, 262, 296, 461, 473, 495, 604, 638, 407, 768, 842, 297, 751, 451, 829, 338, 716, 522, 292, 616, 580, 888, 523, 208, 415, 749, 94, 290, 628, 470, 548, 867, 744, 739, 851, 852, 809, 562, 857, 889, 279, 777, 76, 284, 225, 792, 880, 122, 700, 381, 143, 835, 17, 388, 713, 391, 649, 118, 66, 241, 573, 781, 258, 243, 513, 206, 761, 85, 782, 54, 272, 170, 0, 865, 302, 257, 825, 438, 192, 418, 637, 337, 774, 712, 222, 636, 285, 605, 226, 824, 884, 681, 339, 690, 577, 18, 349, 21, 692, 24, 52, 786, 471, 783, 39, 280, 566, 204, 252, 167, 846, 84, 466, 149, 617, 71, 64, 708, 662, 306, 530, 234, 180, 613, 448, 793, 7, 270, 107, 392, 165, 444, 634, 892, 869, 557, 812, 174, 521, 203, 811, 822, 784, 307, 370, 15, 754, 22, 475, 476, 259, 199, 472, 806, 568, 235, 494, 729, 807, 397, 368, 31, 886, 611, 850, 46, 312, 731, 34, 164, 6, 48, 33, 556, 317, 441, 383, 328, 816, 178, 369, 155, 876, 346, 160, 221, 658, 365, 416, 640, 314, 468, 480, 664, 515, 453, 834, 390, 254, 817, 319, 50, 120, 578, 714, 406, 405, 305, 532, 614, 755, 839, 465, 304, 437, 186, 724, 191, 643, 730, 105, 97, 810, 559, 157, 140, 382, 82, 699, 703, 347, 78, 218, 128, 520, 435, 81, 189, 827, 801, 2, 37, 514, 669, 116, 102, 414, 464, 593, 873, 800, 396, 796, 477, 422, 726, 702, 141, 163, 310, 535, 603, 249, 144, 166, 115, 345, 511, 25, 486, 184, 512, 537, 106, 794, 651, 153, 820, 443, 706, 763, 135, 69, 325, 740, 747, 394, 420, 725, 553, 99, 698, 385, 773, 526, 855, 454, 791, 376, 108, 756, 624, 138, 629, 868, 449, 766, 885, 474, 434, 400, 86, 332, 403, 563, 220, 870, 152, 250, 404, 685, 421, 539, 579, 281, 23, 877, 172, 831, 748, 585, 832, 674, 341, 623, 561, 147, 844, 134, 177, 866, 661, 176, 323, 787, 35, 790, 770, 411, 642, 101, 728, 150, 467, 683, 833, 679, 447, 697, 540, 456, 75, 194, 65, 506, 626, 4, 357, 93, 343, 752, 419, 425, 517, 384, 398, 493, 841, 492, 883, 442, 355, 764, 324, 843, 762, 680, 352, 380, 538, 53, 788, 103, 246, 161, 77, 693, 247, 550, 14, 527, 533, 342, 639, 710, 641, 814, 821, 483, 320, 667, 596, 847, 503, 232, 372, 429, 594, 490, 554, 98, 779, 797, 858, 267, 336, 154, 505, 659, 90, 276, 625, 326, 70, 450, 159, 862, 610, 275, 802, 224, 547, 129, 373, 187, 813, 463, 124, 581, 223, 742, 38, 815, 818, 576, 137, 195, 872, 542, 148, 516, 329, 545, 169, 360, 541, 371, 431, 704, 327, 595, 543, 874, 518, 497, 863, 356, 590, 344, 358, 244, 68, 732, 40, 734, 644, 780, 215, 567, 759, 722, 632, 1, 498, 409, 240, 395, 670, 228, 705, 856, 158, 136, 127, 315, 565, 849, 491, 536, 479, 62, 428, 260, 819, 245, 237, 359, 482, 182, 551, 366, 49, 445, 73, 142, 682, 193, 738, 575, 67, 264, 798, 485, 717, 675, 544, 508, 452, 601, 564, 823, 110, 269, 776, 647, 500, 602, 231, 162, 501, 5, 719, 430, 507, 298, 340, 753, 592, 528, 282, 179, 121, 645, 671, 109, 130, 440, 727, 72, 364, 238, 622, 316, 619, 488, 657, 736, 131, 181, 745, 227, 168, 36, 673, 460, 631, 146, 803, 691, 772, 861, 278, 890, 79, 113, 13, 335, 286, 334, 760, 308, 860, 635, 230, 650, 836, 12, 173, 504, 363, 145, 864, 214, 881, 377, 455, 123, 211, 171, 771, 633, 379, 295, 3, 837, 840, 646, 615, 30, 549, 291, 741, 695, 606, 721, 51, 197, 875, 893, 119, 574, 114, 570, 654, 737, 830, 274, 266, 588, 546, 190, 735, 61, 253, 656, 175, 256, 209, 95, 88, 775, 689, 229, 20, 746, 261, 496, 393, 255, 42, 283, 399, 510, 29, 139, 378, 58, 720, 715, 353, 665, 205, 202, 83, 621, 233, 11, 457, 268, 9, 287, 201, 361, 74, 242, 289, 236, 620, 28, 799, 459, 47, 583, 216, 410, 374, 853, 882, 887, 795, 351, 45, 676, 789, 630, 57, 687, 322, 589, 627, 525, 333, 684, 330, 587, 711, 723, 386, 778, 43, 301, 251, 469, 311, 678, 757, 859, 701, 151, 489, 423, 718, 16, 765, 582, 321, 60, 709, 677, 743, 808, 750, 612, 91, 653, 609, 207, 666, 265, 413, 696, 600, 432, 248, 558, 196, 838, 804, 572, 608, 828, 288, 26, 436, 607, 198, 519, 96, 92, 239, 117, 891, 672, 8, 758, 219, 185, 354, 805, 499, 299, 446, 648, 598, 217, 848, 534, 318, 417, 367, 126, 132, 571, 41, 586, 591]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.1s loss=2.579, TAw acc= 38.4% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.163, TAw acc= 48.8% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.892, TAw acc= 57.6% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.658, TAw acc= 64.8% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.477, TAw acc= 70.4% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.336, TAw acc= 75.2% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.196, TAw acc= 75.2% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.106, TAw acc= 80.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
| Selected 288 train exemplars, time=  0.0s
288
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.113 | TAw acc= 81.9%, forg=  0.0%| TAg acc= 81.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=3.434, TAw acc= 36.7% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=2.789, TAw acc= 41.8% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.401, TAw acc= 53.2% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.124, TAw acc= 55.7% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.915, TAw acc= 59.5% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.753, TAw acc= 62.0% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.626, TAw acc= 64.6% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.504, TAw acc= 67.1% | *
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.504, TAw acc= 67.1% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.503, TAw acc= 67.1% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.502, TAw acc= 67.1% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.501, TAw acc= 67.1% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.500, TAw acc= 67.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
| Selected 448 train exemplars, time=  0.0s
448
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.191 | TAw acc= 88.3%, forg= -6.4%| TAg acc= 76.0%, forg=  5.8% <<<
>>> Test on task  1 : loss=1.429 | TAw acc= 82.4%, forg=  0.0%| TAg acc= 75.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=3.484, TAw acc= 42.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.659, TAw acc= 48.8% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=2.238, TAw acc= 66.2% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.903, TAw acc= 75.0% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.686, TAw acc= 82.5% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.493, TAw acc= 87.5% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.367, TAw acc= 90.0% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.224, TAw acc= 90.0% | *
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.223, TAw acc= 90.0% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.222, TAw acc= 90.0% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.221, TAw acc= 90.0% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.220, TAw acc= 90.0% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.219, TAw acc= 90.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
| Selected 608 train exemplars, time=  0.0s
608
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.116 | TAw acc= 88.9%, forg= -0.6%| TAg acc= 78.9%, forg=  2.9% <<<
>>> Test on task  1 : loss=1.682 | TAw acc= 85.2%, forg= -2.8%| TAg acc= 54.6%, forg= 21.3% <<<
>>> Test on task  2 : loss=1.157 | TAw acc= 91.7%, forg=  0.0%| TAg acc= 87.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.898, TAw acc= 52.7% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=2.667, TAw acc= 68.9% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.972, TAw acc= 77.0% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.576, TAw acc= 77.0% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.336, TAw acc= 87.8% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.167, TAw acc= 89.2% | *
| Epoch   7, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.051, TAw acc= 86.5% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=0.999, TAw acc= 91.9% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=0.997, TAw acc= 91.9% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=0.996, TAw acc= 91.9% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=0.994, TAw acc= 91.9% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=0.993, TAw acc= 91.9% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.991, TAw acc= 91.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
| Selected 768 train exemplars, time=  0.0s
768
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.102 | TAw acc= 87.7%, forg=  1.2%| TAg acc= 67.8%, forg= 14.0% <<<
>>> Test on task  1 : loss=1.510 | TAw acc= 88.9%, forg= -3.7%| TAg acc= 67.6%, forg=  8.3% <<<
>>> Test on task  2 : loss=1.227 | TAw acc= 95.4%, forg= -3.7%| TAg acc= 67.9%, forg= 19.3% <<<
>>> Test on task  3 : loss=1.027 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 85.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=4.069, TAw acc= 53.8% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.904, TAw acc= 59.3% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.231, TAw acc= 72.5% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=1.834, TAw acc= 74.7% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.586, TAw acc= 84.6% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.3s loss=1.402, TAw acc= 85.7% | *
| Epoch   7, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=1.267, TAw acc= 94.5% | *
| Epoch   8, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=1.168, TAw acc= 94.5% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=1.167, TAw acc= 94.5% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.166, TAw acc= 94.5% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.165, TAw acc= 94.5% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.164, TAw acc= 94.5% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.163, TAw acc= 94.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 910 train exemplars, time=  0.0s
910
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.048 | TAw acc= 87.7%, forg=  1.2%| TAg acc= 77.2%, forg=  4.7% <<<
>>> Test on task  1 : loss=1.472 | TAw acc= 92.6%, forg= -3.7%| TAg acc= 63.9%, forg= 12.0% <<<
>>> Test on task  2 : loss=1.169 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 77.1%, forg= 10.1% <<<
>>> Test on task  3 : loss=1.117 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 68.6%, forg= 16.7% <<<
>>> Test on task  4 : loss=1.002 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 74.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.3s loss=4.465, TAw acc= 31.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.204, TAw acc= 48.8% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=2.492, TAw acc= 58.8% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=2.007, TAw acc= 66.2% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.1s loss=1.707, TAw acc= 67.5% | *
| Epoch   6, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.471, TAw acc= 82.5% | *
| Epoch   7, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=1.302, TAw acc= 82.5% | *
| Epoch   8, time=  0.7s | Train: skip eval | Valid: time=  0.3s loss=1.199, TAw acc= 83.8% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=1.197, TAw acc= 83.8% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.196, TAw acc= 86.2% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.1s loss=1.194, TAw acc= 86.2% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.193, TAw acc= 86.2% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=1.191, TAw acc= 86.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1050 train exemplars, time=  0.0s
1050
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.002 | TAw acc= 87.1%, forg=  1.8%| TAg acc= 81.3%, forg=  0.6% <<<
>>> Test on task  1 : loss=1.356 | TAw acc= 88.0%, forg=  4.6%| TAg acc= 65.7%, forg= 10.2% <<<
>>> Test on task  2 : loss=1.089 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=1.027 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 76.5%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.146 | TAw acc= 95.9%, forg= -3.3%| TAg acc= 76.0%, forg= -1.7% <<<
>>> Test on task  5 : loss=1.080 | TAw acc= 91.7%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=4.306, TAw acc= 56.2% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.1s loss=2.836, TAw acc= 64.0% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.125, TAw acc= 76.4% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.729, TAw acc= 82.0% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.426, TAw acc= 91.0% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.4s loss=1.223, TAw acc= 93.3% | *
| Epoch   7, time=  0.7s | Train: skip eval | Valid: time=  0.3s loss=1.077, TAw acc= 97.8% | *
| Epoch   8, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.957, TAw acc= 97.8% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.1s loss=0.956, TAw acc= 97.8% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=0.955, TAw acc= 97.8% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.3s loss=0.954, TAw acc= 97.8% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.953, TAw acc= 97.8% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=0.952, TAw acc= 97.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1190 train exemplars, time=  0.0s
1190
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.000 | TAw acc= 90.1%, forg= -1.2%| TAg acc= 78.9%, forg=  2.9% <<<
>>> Test on task  1 : loss=1.340 | TAw acc= 90.7%, forg=  1.9%| TAg acc= 67.6%, forg=  8.3% <<<
>>> Test on task  2 : loss=0.948 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 84.4%, forg=  2.8% <<<
>>> Test on task  3 : loss=0.960 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 75.5%, forg=  9.8% <<<
>>> Test on task  4 : loss=1.081 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 70.2%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.333 | TAw acc= 95.4%, forg= -3.7%| TAg acc= 64.8%, forg= 12.0% <<<
>>> Test on task  6 : loss=1.011 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=4.658, TAw acc= 52.3% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=2.772, TAw acc= 56.8% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.984, TAw acc= 68.2% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.601, TAw acc= 83.0% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.374, TAw acc= 87.5% | *
| Epoch   6, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.201, TAw acc= 92.0% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.3s loss=1.080, TAw acc= 87.5% | *
| Epoch   8, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.983, TAw acc= 93.2% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.1s loss=0.983, TAw acc= 93.2% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=0.982, TAw acc= 93.2% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=0.982, TAw acc= 93.2% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=0.981, TAw acc= 93.2% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=0.981, TAw acc= 93.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1330 train exemplars, time=  0.0s
1330
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.044 | TAw acc= 89.5%, forg=  0.6%| TAg acc= 76.6%, forg=  5.3% <<<
>>> Test on task  1 : loss=1.419 | TAw acc= 93.5%, forg= -0.9%| TAg acc= 59.3%, forg= 16.7% <<<
>>> Test on task  2 : loss=1.018 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 79.8%, forg=  7.3% <<<
>>> Test on task  3 : loss=0.906 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 78.4%, forg=  6.9% <<<
>>> Test on task  4 : loss=1.114 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 71.1%, forg=  5.0% <<<
>>> Test on task  5 : loss=1.140 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 75.9%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.205 | TAw acc= 99.2%, forg= -0.8%| TAg acc= 63.9%, forg= 15.1% <<<
>>> Test on task  7 : loss=0.955 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 87.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=4.497, TAw acc= 55.7% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.551, TAw acc= 69.3% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.931, TAw acc= 81.8% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.553, TAw acc= 84.1% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.1s loss=1.316, TAw acc= 88.6% | *
| Epoch   6, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.153, TAw acc= 88.6% | *
| Epoch   7, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=1.043, TAw acc= 86.4% | *
| Epoch   8, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=0.898, TAw acc= 87.5% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.1s loss=0.898, TAw acc= 87.5% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.1s loss=0.898, TAw acc= 87.5% |
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=0.897, TAw acc= 87.5% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=0.897, TAw acc= 87.5% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=0.897, TAw acc= 87.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1470 train exemplars, time=  0.0s
1470
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.995 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 78.9%, forg=  2.9% <<<
>>> Test on task  1 : loss=1.419 | TAw acc= 92.6%, forg=  0.9%| TAg acc= 59.3%, forg= 16.7% <<<
>>> Test on task  2 : loss=1.008 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 72.5%, forg= 14.7% <<<
>>> Test on task  3 : loss=0.886 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 73.5%, forg= 11.8% <<<
>>> Test on task  4 : loss=0.944 | TAw acc= 96.7%, forg= -0.8%| TAg acc= 80.2%, forg= -4.1% <<<
>>> Test on task  5 : loss=1.165 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 72.2%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.062 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 74.8%, forg=  4.2% <<<
>>> Test on task  7 : loss=1.264 | TAw acc= 95.7%, forg= -2.6%| TAg acc= 70.9%, forg= 16.2% <<<
>>> Test on task  8 : loss=1.021 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 82.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=5.477, TAw acc= 36.4% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=3.424, TAw acc= 53.2% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=2.437, TAw acc= 68.8% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.902, TAw acc= 84.4% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.1s loss=1.556, TAw acc= 87.0% | *
| Epoch   6, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.380, TAw acc= 89.6% | *
| Epoch   7, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.213, TAw acc= 93.5% | *
| Epoch   8, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.117, TAw acc= 93.5% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.1s loss=1.115, TAw acc= 93.5% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.1s loss=1.113, TAw acc= 93.5% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.3s loss=1.111, TAw acc= 93.5% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.109, TAw acc= 93.5% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=1.107, TAw acc= 94.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1610 train exemplars, time=  0.0s
1610
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.101 | TAw acc= 88.9%, forg=  1.2%| TAg acc= 74.3%, forg=  7.6% <<<
>>> Test on task  1 : loss=1.305 | TAw acc= 94.4%, forg= -0.9%| TAg acc= 67.6%, forg=  8.3% <<<
>>> Test on task  2 : loss=0.921 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 78.9%, forg=  8.3% <<<
>>> Test on task  3 : loss=0.860 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 78.4%, forg=  6.9% <<<
>>> Test on task  4 : loss=0.920 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 82.6%, forg= -2.5% <<<
>>> Test on task  5 : loss=1.084 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 82.4%, forg= -5.6% <<<
>>> Test on task  6 : loss=0.933 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.0%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.188 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 77.8%, forg=  9.4% <<<
>>> Test on task  8 : loss=1.381 | TAw acc= 92.4%, forg= -2.5%| TAg acc= 63.6%, forg= 18.6% <<<
>>> Test on task  9 : loss=1.076 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 79.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.3s loss=4.790, TAw acc= 47.5% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.943, TAw acc= 67.5% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.052, TAw acc= 78.8% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.699, TAw acc= 83.8% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.522, TAw acc= 85.0% | *
| Epoch   6, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.401, TAw acc= 87.5% | *
| Epoch   7, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.287, TAw acc= 90.0% | *
| Epoch   8, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.235, TAw acc= 92.5% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.3s loss=1.234, TAw acc= 92.5% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.1s loss=1.233, TAw acc= 92.5% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.232, TAw acc= 92.5% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.231, TAw acc= 92.5% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.231, TAw acc= 92.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1750 train exemplars, time=  0.0s
1750
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.102 | TAw acc= 89.5%, forg=  0.6%| TAg acc= 72.5%, forg=  9.4% <<<
>>> Test on task  1 : loss=1.365 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 65.7%, forg= 10.2% <<<
>>> Test on task  2 : loss=0.906 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 79.8%, forg=  7.3% <<<
>>> Test on task  3 : loss=0.885 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 73.5%, forg= 11.8% <<<
>>> Test on task  4 : loss=0.924 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.0%, forg=  1.7% <<<
>>> Test on task  5 : loss=1.024 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 81.5%, forg=  0.9% <<<
>>> Test on task  6 : loss=0.848 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.2%, forg= -4.2% <<<
>>> Test on task  7 : loss=1.093 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=1.222 | TAw acc= 91.5%, forg=  0.8%| TAg acc= 73.7%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.332 | TAw acc= 97.1%, forg= -2.9%| TAg acc= 63.5%, forg= 16.3% <<<
>>> Test on task 10 : loss=1.136 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 68.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=5.628, TAw acc= 37.1% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=3.455, TAw acc= 41.4% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.381, TAw acc= 85.7% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.839, TAw acc= 90.0% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.3s loss=1.612, TAw acc= 88.6% | *
| Epoch   6, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.382, TAw acc= 94.3% | *
| Epoch   7, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.263, TAw acc= 95.7% | *
| Epoch   8, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.205, TAw acc= 94.3% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.205, TAw acc= 92.9% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 92.9% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 92.9% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.4s loss=1.204, TAw acc= 92.9% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=1.203, TAw acc= 92.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1890 train exemplars, time=  0.0s
1890
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.102 | TAw acc= 88.3%, forg=  1.8%| TAg acc= 71.9%, forg=  9.9% <<<
>>> Test on task  1 : loss=1.300 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 70.4%, forg=  5.6% <<<
>>> Test on task  2 : loss=0.899 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 78.0%, forg=  9.2% <<<
>>> Test on task  3 : loss=0.859 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 75.5%, forg=  9.8% <<<
>>> Test on task  4 : loss=0.924 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 79.3%, forg=  3.3% <<<
>>> Test on task  5 : loss=1.030 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 76.9%, forg=  5.6% <<<
>>> Test on task  6 : loss=0.847 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.2%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.033 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 79.5%, forg=  7.7% <<<
>>> Test on task  8 : loss=1.201 | TAw acc= 95.8%, forg= -3.4%| TAg acc= 80.5%, forg=  1.7% <<<
>>> Test on task  9 : loss=1.235 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.473 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 61.8%, forg=  6.4% <<<
>>> Test on task 11 : loss=1.241 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 79.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=5.366, TAw acc= 38.7% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.3s loss=3.187, TAw acc= 53.3% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=2.281, TAw acc= 68.0% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.1s loss=1.787, TAw acc= 74.7% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 81.3% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.335, TAw acc= 82.7% | *
| Epoch   7, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.303, TAw acc= 81.3% | *
| Epoch   8, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.197, TAw acc= 81.3% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.3s loss=1.197, TAw acc= 81.3% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.1s loss=1.197, TAw acc= 81.3% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.197, TAw acc= 81.3% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.3s loss=1.197, TAw acc= 81.3% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.197, TAw acc= 81.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2030 train exemplars, time=  0.0s
2030
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.049 | TAw acc= 88.3%, forg=  1.8%| TAg acc= 78.4%, forg=  3.5% <<<
>>> Test on task  1 : loss=1.359 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 65.7%, forg= 10.2% <<<
>>> Test on task  2 : loss=0.944 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 68.8%, forg= 18.3% <<<
>>> Test on task  3 : loss=0.868 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 77.5%, forg=  7.8% <<<
>>> Test on task  4 : loss=0.888 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.061 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 75.0%, forg=  7.4% <<<
>>> Test on task  6 : loss=0.770 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 84.0%, forg= -0.8% <<<
>>> Test on task  7 : loss=1.008 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 76.9%, forg= 10.3% <<<
>>> Test on task  8 : loss=1.104 | TAw acc= 94.9%, forg=  0.8%| TAg acc= 79.7%, forg=  2.5% <<<
>>> Test on task  9 : loss=1.140 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.3%, forg= 13.5% <<<
>>> Test on task 10 : loss=1.369 | TAw acc= 94.5%, forg= -0.9%| TAg acc= 65.5%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.449 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 65.3%, forg= 14.3% <<<
>>> Test on task 12 : loss=1.163 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 78.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=4.810, TAw acc= 60.0% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.1s loss=2.791, TAw acc= 77.6% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.981, TAw acc= 83.5% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.3s loss=1.465, TAw acc= 82.4% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.1s loss=1.261, TAw acc= 83.5% | *
| Epoch   6, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=1.166, TAw acc= 89.4% | *
| Epoch   7, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=1.016, TAw acc= 90.6% | *
| Epoch   8, time=  1.2s | Train: skip eval | Valid: time=  0.1s loss=0.987, TAw acc= 91.8% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.3s loss=0.986, TAw acc= 91.8% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.984, TAw acc= 91.8% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.1s loss=0.983, TAw acc= 91.8% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.981, TAw acc= 91.8% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=0.980, TAw acc= 91.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2170 train exemplars, time=  0.0s
2170
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.145 | TAw acc= 88.9%, forg=  1.2%| TAg acc= 70.2%, forg= 11.7% <<<
>>> Test on task  1 : loss=1.366 | TAw acc= 95.4%, forg= -0.9%| TAg acc= 63.9%, forg= 12.0% <<<
>>> Test on task  2 : loss=0.903 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=0.908 | TAw acc= 90.2%, forg=  3.9%| TAg acc= 72.5%, forg= 12.7% <<<
>>> Test on task  4 : loss=0.913 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.070 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 75.9%, forg=  6.5% <<<
>>> Test on task  6 : loss=0.852 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 80.7%, forg=  3.4% <<<
>>> Test on task  7 : loss=0.979 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.993 | TAw acc= 96.6%, forg= -0.8%| TAg acc= 83.9%, forg= -1.7% <<<
>>> Test on task  9 : loss=1.213 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 65.4%, forg= 14.4% <<<
>>> Test on task 10 : loss=1.252 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg= -7.3% <<<
>>> Test on task 11 : loss=1.382 | TAw acc= 91.8%, forg= -2.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.317 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 71.8%, forg=  6.8% <<<
>>> Test on task 13 : loss=1.076 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 77.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.3s loss=5.768, TAw acc= 36.8% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=3.560, TAw acc= 48.7% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.1s loss=2.607, TAw acc= 86.8% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.3s loss=2.063, TAw acc= 94.7% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.3s loss=1.756, TAw acc= 98.7% | *
| Epoch   6, time=  1.2s | Train: skip eval | Valid: time=  0.1s loss=1.560, TAw acc= 98.7% | *
| Epoch   7, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.435, TAw acc= 98.7% | *
| Epoch   8, time=  1.8s | Train: skip eval | Valid: time=  0.3s loss=1.417, TAw acc= 97.4% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.412, TAw acc= 97.4% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.4s loss=1.408, TAw acc= 97.4% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.403, TAw acc= 97.4% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.1s loss=1.399, TAw acc= 97.4% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=1.395, TAw acc= 97.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2306 train exemplars, time=  0.0s
2306
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.139 | TAw acc= 88.3%, forg=  1.8%| TAg acc= 75.4%, forg=  6.4% <<<
>>> Test on task  1 : loss=1.359 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 68.5%, forg=  7.4% <<<
>>> Test on task  2 : loss=0.896 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 77.1%, forg= 10.1% <<<
>>> Test on task  3 : loss=0.900 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 73.5%, forg= 11.8% <<<
>>> Test on task  4 : loss=0.890 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 79.3%, forg=  3.3% <<<
>>> Test on task  5 : loss=1.049 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 80.6%, forg=  1.9% <<<
>>> Test on task  6 : loss=0.837 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.8%, forg=  4.2% <<<
>>> Test on task  7 : loss=0.967 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 80.3%, forg=  6.8% <<<
>>> Test on task  8 : loss=0.992 | TAw acc= 97.5%, forg= -0.8%| TAg acc= 79.7%, forg=  4.2% <<<
>>> Test on task  9 : loss=1.156 | TAw acc= 98.1%, forg= -1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.215 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 70.9%, forg=  4.5% <<<
>>> Test on task 11 : loss=1.294 | TAw acc= 92.9%, forg= -1.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.311 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 72.8%, forg=  5.8% <<<
>>> Test on task 13 : loss=1.294 | TAw acc= 95.6%, forg= -3.5%| TAg acc= 64.9%, forg= 12.3% <<<
>>> Test on task 14 : loss=1.189 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=6.040, TAw acc= 28.2% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=3.810, TAw acc= 43.7% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.611, TAw acc= 67.6% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.3s loss=1.884, TAw acc= 90.1% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.1s loss=1.529, TAw acc= 97.2% | *
| Epoch   6, time=  1.5s | Train: skip eval | Valid: time=  0.3s loss=1.387, TAw acc= 97.2% | *
| Epoch   7, time=  2.1s | Train: skip eval | Valid: time=  0.1s loss=1.252, TAw acc= 98.6% | *
| Epoch   8, time=  1.2s | Train: skip eval | Valid: time=  0.1s loss=1.169, TAw acc= 98.6% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.4s loss=1.167, TAw acc= 98.6% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=1.165, TAw acc= 98.6% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=1.163, TAw acc= 98.6% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.3s loss=1.161, TAw acc= 98.6% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.159, TAw acc= 98.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2426 train exemplars, time=  0.0s
2426
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.138 | TAw acc= 88.3%, forg=  1.8%| TAg acc= 74.3%, forg=  7.6% <<<
>>> Test on task  1 : loss=1.409 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 63.0%, forg= 13.0% <<<
>>> Test on task  2 : loss=0.861 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=0.892 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 77.5%, forg=  7.8% <<<
>>> Test on task  4 : loss=0.891 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.046 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 78.7%, forg=  3.7% <<<
>>> Test on task  6 : loss=0.801 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.036 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 75.2%, forg= 12.0% <<<
>>> Test on task  8 : loss=0.932 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 83.9%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.084 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 10 : loss=1.223 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.283 | TAw acc= 90.8%, forg=  2.0%| TAg acc= 62.2%, forg= 17.3% <<<
>>> Test on task 12 : loss=1.189 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 74.8%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.229 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 64.0%, forg= 13.2% <<<
>>> Test on task 14 : loss=1.379 | TAw acc= 95.2%, forg= -1.0%| TAg acc= 63.5%, forg= 13.5% <<<
>>> Test on task 15 : loss=1.077 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=6.241, TAw acc= 45.0% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.3s loss=3.364, TAw acc= 62.5% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.048, TAw acc= 87.5% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.634, TAw acc= 91.2% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.380, TAw acc= 92.5% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.278, TAw acc= 96.2% | *
| Epoch   7, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.241, TAw acc= 96.2% | *
| Epoch   8, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.074, TAw acc= 97.5% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.072, TAw acc= 97.5% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.071, TAw acc= 97.5% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.069, TAw acc= 97.5% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.067, TAw acc= 97.5% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.066, TAw acc= 97.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2546 train exemplars, time=  0.0s
2546
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.156 | TAw acc= 88.9%, forg=  1.2%| TAg acc= 76.0%, forg=  5.8% <<<
>>> Test on task  1 : loss=1.474 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 63.9%, forg= 12.0% <<<
>>> Test on task  2 : loss=0.978 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg= 13.8% <<<
>>> Test on task  3 : loss=0.961 | TAw acc= 91.2%, forg=  2.9%| TAg acc= 74.5%, forg= 10.8% <<<
>>> Test on task  4 : loss=0.882 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  5 : loss=1.009 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 80.6%, forg=  1.9% <<<
>>> Test on task  6 : loss=0.755 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.970 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 81.2%, forg=  6.0% <<<
>>> Test on task  8 : loss=0.922 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 85.6%, forg= -1.7% <<<
>>> Test on task  9 : loss=1.137 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.201 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.272 | TAw acc= 90.8%, forg=  2.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.167 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 79.6%, forg= -1.0% <<<
>>> Test on task 13 : loss=1.162 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.337 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 63.5%, forg= 13.5% <<<
>>> Test on task 15 : loss=1.282 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 62.6%, forg= 16.2% <<<
>>> Test on task 16 : loss=1.111 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 67.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=5.683, TAw acc= 45.8% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=3.006, TAw acc= 62.7% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.082, TAw acc= 79.5% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.616, TAw acc= 92.8% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.337, TAw acc= 95.2% | *
| Epoch   6, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.288, TAw acc= 95.2% | *
| Epoch   7, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.158, TAw acc= 97.6% | *
| Epoch   8, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.101, TAw acc= 95.2% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=1.099, TAw acc= 95.2% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.098, TAw acc= 95.2% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.097, TAw acc= 95.2% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.1s loss=1.096, TAw acc= 95.2% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.1s loss=1.095, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2666 train exemplars, time=  0.0s
2666
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.241 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 71.9%, forg=  9.9% <<<
>>> Test on task  1 : loss=1.439 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 66.7%, forg=  9.3% <<<
>>> Test on task  2 : loss=0.987 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 67.9%, forg= 19.3% <<<
>>> Test on task  3 : loss=0.983 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 67.6%, forg= 17.6% <<<
>>> Test on task  4 : loss=0.830 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.037 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 79.6%, forg=  2.8% <<<
>>> Test on task  6 : loss=0.834 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=0.907 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 83.8%, forg=  3.4% <<<
>>> Test on task  8 : loss=0.919 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 78.8%, forg=  6.8% <<<
>>> Test on task  9 : loss=1.219 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.153 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 77.3%, forg= -1.8% <<<
>>> Test on task 11 : loss=1.213 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.139 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 78.6%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.079 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 74.6%, forg=  2.6% <<<
>>> Test on task 14 : loss=1.254 | TAw acc= 94.2%, forg=  1.0%| TAg acc= 68.3%, forg=  8.7% <<<
>>> Test on task 15 : loss=1.169 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 68.7%, forg= 10.1% <<<
>>> Test on task 16 : loss=1.449 | TAw acc= 96.3%, forg= -1.8%| TAg acc= 55.0%, forg= 11.9% <<<
>>> Test on task 17 : loss=0.733 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=5.505, TAw acc= 30.8% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=3.554, TAw acc= 66.7% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.1s loss=2.495, TAw acc= 79.5% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.844, TAw acc= 88.5% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.1s loss=1.554, TAw acc= 93.6% | *
| Epoch   6, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.388, TAw acc= 93.6% | *
| Epoch   7, time=  1.4s | Train: skip eval | Valid: time=  0.1s loss=1.271, TAw acc= 94.9% | *
| Epoch   8, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.203, TAw acc= 94.9% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.202, TAw acc= 94.9% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.1s loss=1.201, TAw acc= 94.9% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.200, TAw acc= 94.9% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.1s loss=1.199, TAw acc= 94.9% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.198, TAw acc= 94.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2786 train exemplars, time=  0.0s
2786
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.204 | TAw acc= 88.3%, forg=  1.8%| TAg acc= 73.1%, forg=  8.8% <<<
>>> Test on task  1 : loss=1.430 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 62.0%, forg= 13.9% <<<
>>> Test on task  2 : loss=0.921 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 74.3%, forg= 12.8% <<<
>>> Test on task  3 : loss=0.994 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 74.5%, forg= 10.8% <<<
>>> Test on task  4 : loss=0.853 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.157 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.810 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.926 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 81.2%, forg=  6.0% <<<
>>> Test on task  8 : loss=0.869 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 84.7%, forg=  0.8% <<<
>>> Test on task  9 : loss=1.167 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 72.1%, forg=  7.7% <<<
>>> Test on task 10 : loss=1.144 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 79.1%, forg= -1.8% <<<
>>> Test on task 11 : loss=1.207 | TAw acc= 90.8%, forg=  2.0%| TAg acc= 61.2%, forg= 18.4% <<<
>>> Test on task 12 : loss=1.097 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 77.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.058 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.290 | TAw acc= 96.2%, forg= -1.0%| TAg acc= 62.5%, forg= 14.4% <<<
>>> Test on task 15 : loss=1.050 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.8%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.336 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 62.4%, forg=  4.6% <<<
>>> Test on task 17 : loss=1.057 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 58.0%, forg= 23.2% <<<
>>> Test on task 18 : loss=0.989 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 80.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=5.259, TAw acc= 37.6% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.720, TAw acc= 84.9% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.1s loss=1.696, TAw acc= 98.9% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.336, TAw acc= 98.9% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 98.9% | *
| Epoch   6, time=  1.6s | Train: skip eval | Valid: time=  0.1s loss=1.161, TAw acc= 98.9% | *
| Epoch   7, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.087, TAw acc= 98.9% | *
| Epoch   8, time=  1.5s | Train: skip eval | Valid: time=  0.1s loss=1.080, TAw acc= 98.9% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=1.079, TAw acc= 98.9% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=1.077, TAw acc= 98.9% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.076, TAw acc= 98.9% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.074, TAw acc= 98.9% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.072, TAw acc= 98.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2906 train exemplars, time=  0.1s
2906
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.232 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 71.9%, forg=  9.9% <<<
>>> Test on task  1 : loss=1.555 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 60.2%, forg= 15.7% <<<
>>> Test on task  2 : loss=0.971 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=0.949 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 70.6%, forg= 14.7% <<<
>>> Test on task  4 : loss=0.863 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 79.3%, forg=  3.3% <<<
>>> Test on task  5 : loss=1.113 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 78.7%, forg=  3.7% <<<
>>> Test on task  6 : loss=0.869 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.960 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 83.8%, forg=  3.4% <<<
>>> Test on task  8 : loss=0.830 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.1%, forg=  2.5% <<<
>>> Test on task  9 : loss=1.238 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.119 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 79.1%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.182 | TAw acc= 90.8%, forg=  2.0%| TAg acc= 66.3%, forg= 13.3% <<<
>>> Test on task 12 : loss=1.104 | TAw acc= 88.3%, forg=  2.9%| TAg acc= 77.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.066 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 73.7%, forg=  3.5% <<<
>>> Test on task 14 : loss=1.267 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 66.3%, forg= 10.6% <<<
>>> Test on task 15 : loss=0.961 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.8%, forg=  2.0% <<<
>>> Test on task 16 : loss=1.251 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 66.1%, forg=  0.9% <<<
>>> Test on task 17 : loss=0.896 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.2%, forg=  8.0% <<<
>>> Test on task 18 : loss=1.314 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 64.2%, forg= 16.0% <<<
>>> Test on task 19 : loss=1.018 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 78.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=5.595, TAw acc= 53.5% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=3.085, TAw acc= 62.8% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=2.244, TAw acc= 76.7% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.742, TAw acc= 82.6% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.467, TAw acc= 84.9% | *
| Epoch   6, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.292, TAw acc= 89.5% | *
| Epoch   7, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=1.227, TAw acc= 90.7% | *
| Epoch   8, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.126, TAw acc= 84.9% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.1s loss=1.127, TAw acc= 84.9% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.128, TAw acc= 84.9% |
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.129, TAw acc= 84.9% |
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 84.9% |
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.131, TAw acc= 84.9% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3026 train exemplars, time=  0.0s
3026
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.343 | TAw acc= 87.7%, forg=  2.3%| TAg acc= 66.7%, forg= 15.2% <<<
>>> Test on task  1 : loss=1.474 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 63.0%, forg= 13.0% <<<
>>> Test on task  2 : loss=0.977 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 15.6% <<<
>>> Test on task  3 : loss=1.009 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 73.5%, forg= 11.8% <<<
>>> Test on task  4 : loss=0.842 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 82.6%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.073 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 78.7%, forg=  3.7% <<<
>>> Test on task  6 : loss=0.887 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 74.8%, forg=  9.2% <<<
>>> Test on task  7 : loss=0.955 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 80.3%, forg=  6.8% <<<
>>> Test on task  8 : loss=0.842 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.1%, forg=  2.5% <<<
>>> Test on task  9 : loss=1.266 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 65.4%, forg= 14.4% <<<
>>> Test on task 10 : loss=1.119 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 78.2%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.198 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 68.4%, forg= 11.2% <<<
>>> Test on task 12 : loss=1.109 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 78.6%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.124 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 71.9%, forg=  5.3% <<<
>>> Test on task 14 : loss=1.332 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 63.5%, forg= 13.5% <<<
>>> Test on task 15 : loss=0.932 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 77.8%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.219 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 65.1%, forg=  1.8% <<<
>>> Test on task 17 : loss=0.909 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.6%, forg= 11.6% <<<
>>> Test on task 18 : loss=1.143 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.6%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.440 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 57.7%, forg= 20.3% <<<
>>> Test on task 20 : loss=0.811 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 82.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=5.929, TAw acc= 48.1% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=3.105, TAw acc= 71.6% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.3s loss=2.016, TAw acc= 93.8% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 97.5% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.420, TAw acc= 98.8% | *
| Epoch   6, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.247, TAw acc= 97.5% | *
| Epoch   7, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.188, TAw acc= 98.8% | *
| Epoch   8, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.157, TAw acc= 98.8% | *
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.1s loss=1.154, TAw acc= 98.8% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.152, TAw acc= 98.8% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=1.151, TAw acc= 98.8% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 98.8% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.147, TAw acc= 98.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3146 train exemplars, time=  0.0s
3146
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.297 | TAw acc= 87.7%, forg=  2.3%| TAg acc= 70.8%, forg= 11.1% <<<
>>> Test on task  1 : loss=1.543 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 61.1%, forg= 14.8% <<<
>>> Test on task  2 : loss=1.014 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 72.5%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.021 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 69.6%, forg= 15.7% <<<
>>> Test on task  4 : loss=0.902 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.146 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 74.1%, forg=  8.3% <<<
>>> Test on task  6 : loss=0.813 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=0.883 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.796 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.9%, forg=  1.7% <<<
>>> Test on task  9 : loss=1.215 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 66.3%, forg= 13.5% <<<
>>> Test on task 10 : loss=1.121 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 80.0%, forg= -0.9% <<<
>>> Test on task 11 : loss=1.182 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 66.3%, forg= 13.3% <<<
>>> Test on task 12 : loss=1.103 | TAw acc= 88.3%, forg=  2.9%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.039 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 74.6%, forg=  2.6% <<<
>>> Test on task 14 : loss=1.244 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.854 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.8%, forg= -4.0% <<<
>>> Test on task 16 : loss=1.178 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 69.7%, forg= -2.8% <<<
>>> Test on task 17 : loss=0.860 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.4%, forg=  9.8% <<<
>>> Test on task 18 : loss=1.087 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 74.5%, forg=  5.7% <<<
>>> Test on task 19 : loss=1.491 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 58.5%, forg= 19.5% <<<
>>> Test on task 20 : loss=1.214 | TAw acc= 97.4%, forg= -2.6%| TAg acc= 61.7%, forg= 20.9% <<<
>>> Test on task 21 : loss=1.038 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 75.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=5.007, TAw acc= 39.5% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.439, TAw acc= 71.6% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=1.556, TAw acc= 85.2% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=1.295, TAw acc= 93.8% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.121, TAw acc= 96.3% | *
| Epoch   6, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=0.984, TAw acc= 96.3% | *
| Epoch   7, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=0.964, TAw acc= 93.8% | *
| Epoch   8, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=0.853, TAw acc= 97.5% | *
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.1s loss=0.854, TAw acc= 97.5% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.1s loss=0.854, TAw acc= 97.5% |
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.855, TAw acc= 97.5% |
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.855, TAw acc= 96.3% |
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.855, TAw acc= 96.3% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3266 train exemplars, time=  0.0s
3266
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.312 | TAw acc= 87.7%, forg=  2.3%| TAg acc= 69.0%, forg= 12.9% <<<
>>> Test on task  1 : loss=1.610 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 61.1%, forg= 14.8% <<<
>>> Test on task  2 : loss=0.947 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=1.025 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 65.7%, forg= 19.6% <<<
>>> Test on task  4 : loss=0.888 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.124 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 76.9%, forg=  5.6% <<<
>>> Test on task  6 : loss=0.828 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=0.939 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.781 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.1%, forg=  2.5% <<<
>>> Test on task  9 : loss=1.206 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 72.1%, forg=  7.7% <<<
>>> Test on task 10 : loss=1.098 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 80.9%, forg= -0.9% <<<
>>> Test on task 11 : loss=1.169 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 64.3%, forg= 15.3% <<<
>>> Test on task 12 : loss=1.133 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 77.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.008 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 72.8%, forg=  4.4% <<<
>>> Test on task 14 : loss=1.179 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 73.1%, forg=  3.8% <<<
>>> Test on task 15 : loss=0.827 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.8%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.171 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 67.9%, forg=  1.8% <<<
>>> Test on task 17 : loss=0.850 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 74.1%, forg=  7.1% <<<
>>> Test on task 18 : loss=1.120 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.9%, forg= 12.3% <<<
>>> Test on task 19 : loss=1.396 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 60.2%, forg= 17.9% <<<
>>> Test on task 20 : loss=1.130 | TAw acc= 98.3%, forg= -0.9%| TAg acc= 68.7%, forg= 13.9% <<<
>>> Test on task 21 : loss=1.452 | TAw acc= 97.2%, forg= -1.8%| TAg acc= 56.9%, forg= 18.3% <<<
>>> Test on task 22 : loss=0.919 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 83.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=5.802, TAw acc= 41.1% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.1s loss=2.500, TAw acc= 74.4% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.1s loss=1.649, TAw acc= 92.2% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.393, TAw acc= 96.7% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.1s loss=1.204, TAw acc= 97.8% | *
| Epoch   6, time=  2.0s | Train: skip eval | Valid: time=  0.1s loss=1.115, TAw acc= 97.8% | *
| Epoch   7, time=  2.0s | Train: skip eval | Valid: time=  0.1s loss=1.013, TAw acc= 96.7% | *
| Epoch   8, time=  2.0s | Train: skip eval | Valid: time=  0.1s loss=0.984, TAw acc= 96.7% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=0.982, TAw acc= 97.8% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.980, TAw acc= 97.8% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.1s loss=0.979, TAw acc= 97.8% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.978, TAw acc= 97.8% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.978, TAw acc= 97.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3386 train exemplars, time=  0.0s
3386
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.354 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 64.9%, forg= 17.0% <<<
>>> Test on task  1 : loss=1.668 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 57.4%, forg= 18.5% <<<
>>> Test on task  2 : loss=1.034 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 75.2%, forg= 11.9% <<<
>>> Test on task  3 : loss=1.051 | TAw acc= 91.2%, forg=  2.9%| TAg acc= 67.6%, forg= 17.6% <<<
>>> Test on task  4 : loss=0.863 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.093 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 78.7%, forg=  3.7% <<<
>>> Test on task  6 : loss=0.800 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.942 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.813 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 82.2%, forg=  3.4% <<<
>>> Test on task  9 : loss=1.316 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.141 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 78.2%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.154 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 66.3%, forg= 13.3% <<<
>>> Test on task 12 : loss=1.164 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=0.982 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 75.4%, forg=  1.8% <<<
>>> Test on task 14 : loss=1.222 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 73.1%, forg=  3.8% <<<
>>> Test on task 15 : loss=0.859 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 75.8%, forg=  7.1% <<<
>>> Test on task 16 : loss=1.207 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.7%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.865 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 74.1%, forg=  7.1% <<<
>>> Test on task 18 : loss=1.040 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 71.7%, forg=  8.5% <<<
>>> Test on task 19 : loss=1.343 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 20 : loss=1.132 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.6%, forg= 13.0% <<<
>>> Test on task 21 : loss=1.417 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 60.6%, forg= 14.7% <<<
>>> Test on task 22 : loss=1.274 | TAw acc= 94.6%, forg=  1.8%| TAg acc= 69.4%, forg= 14.4% <<<
>>> Test on task 23 : loss=0.852 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 75.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.1s loss=5.385, TAw acc= 48.1% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.1s loss=2.597, TAw acc= 76.5% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.550, TAw acc= 95.1% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.1s loss=1.255, TAw acc= 95.1% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.1% | *
| Epoch   6, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=0.998, TAw acc= 93.8% | *
| Epoch   7, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.962, TAw acc= 96.3% | *
| Epoch   8, time=  2.2s | Train: skip eval | Valid: time=  0.1s loss=0.950, TAw acc= 95.1% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.1s loss=0.947, TAw acc= 95.1% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.945, TAw acc= 95.1% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.942, TAw acc= 95.1% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 95.1% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.1s loss=0.938, TAw acc= 95.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3506 train exemplars, time=  0.0s
3506
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.384 | TAw acc= 85.4%, forg=  4.7%| TAg acc= 69.0%, forg= 12.9% <<<
>>> Test on task  1 : loss=1.654 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 60.2%, forg= 15.7% <<<
>>> Test on task  2 : loss=1.014 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 75.2%, forg= 11.9% <<<
>>> Test on task  3 : loss=1.039 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 69.6%, forg= 15.7% <<<
>>> Test on task  4 : loss=0.852 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.0%, forg=  1.7% <<<
>>> Test on task  5 : loss=1.119 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 76.9%, forg=  5.6% <<<
>>> Test on task  6 : loss=0.806 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.2%, forg=  0.8% <<<
>>> Test on task  7 : loss=0.913 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.780 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.9%, forg=  1.7% <<<
>>> Test on task  9 : loss=1.275 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 10 : loss=1.141 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 75.5%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.197 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 66.3%, forg= 13.3% <<<
>>> Test on task 12 : loss=1.249 | TAw acc= 86.4%, forg=  4.9%| TAg acc= 70.9%, forg=  8.7% <<<
>>> Test on task 13 : loss=1.016 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 74.6%, forg=  2.6% <<<
>>> Test on task 14 : loss=1.272 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.842 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 81.8%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.112 | TAw acc= 98.2%, forg= -1.8%| TAg acc= 74.3%, forg= -4.6% <<<
>>> Test on task 17 : loss=0.772 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.9%, forg=  5.4% <<<
>>> Test on task 18 : loss=1.008 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 71.7%, forg=  8.5% <<<
>>> Test on task 19 : loss=1.359 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 60.2%, forg= 17.9% <<<
>>> Test on task 20 : loss=1.053 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.7%, forg=  7.0% <<<
>>> Test on task 21 : loss=1.267 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.291 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 67.6%, forg= 16.2% <<<
>>> Test on task 23 : loss=1.400 | TAw acc=100.0%, forg= -2.5%| TAg acc= 55.8%, forg= 20.0% <<<
>>> Test on task 24 : loss=0.764 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.1s loss=5.828, TAw acc= 34.9% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.1s loss=3.480, TAw acc= 62.7% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=2.285, TAw acc= 84.3% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.573, TAw acc= 94.0% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.349, TAw acc= 91.6% | *
| Epoch   6, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.381, TAw acc= 96.4% |
| Epoch   7, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 94.0% | *
| Epoch   8, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.148, TAw acc= 92.8% | *
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.148, TAw acc= 94.0% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.148, TAw acc= 94.0% |
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.148, TAw acc= 94.0% |
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 94.0% |
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 94.0% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3626 train exemplars, time=  0.0s
3626
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.401 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 69.6%, forg= 12.3% <<<
>>> Test on task  1 : loss=1.643 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 60.2%, forg= 15.7% <<<
>>> Test on task  2 : loss=1.006 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 74.3%, forg= 12.8% <<<
>>> Test on task  3 : loss=1.131 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 66.7%, forg= 18.6% <<<
>>> Test on task  4 : loss=0.934 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.0%, forg=  1.7% <<<
>>> Test on task  5 : loss=1.159 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.805 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=0.886 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.848 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 80.5%, forg=  5.1% <<<
>>> Test on task  9 : loss=1.344 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.242 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 71.8%, forg=  9.1% <<<
>>> Test on task 11 : loss=1.284 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 66.3%, forg= 13.3% <<<
>>> Test on task 12 : loss=1.162 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.199 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.235 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.3%, forg=  9.6% <<<
>>> Test on task 15 : loss=0.853 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 80.8%, forg=  2.0% <<<
>>> Test on task 16 : loss=1.176 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 72.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=0.799 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.0%, forg=  6.2% <<<
>>> Test on task 18 : loss=0.977 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 76.4%, forg=  3.8% <<<
>>> Test on task 19 : loss=1.376 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 60.2%, forg= 17.9% <<<
>>> Test on task 20 : loss=1.152 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.7%, forg=  7.0% <<<
>>> Test on task 21 : loss=1.266 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.197 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 74.8%, forg=  9.0% <<<
>>> Test on task 23 : loss=1.404 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 57.5%, forg= 18.3% <<<
>>> Test on task 24 : loss=1.168 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 64.2%, forg= 17.4% <<<
>>> Test on task 25 : loss=1.060 | TAw acc= 93.7%, forg=  0.0%| TAg acc= 76.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.1s loss=7.203, TAw acc= 37.1% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.1s loss=3.479, TAw acc= 71.4% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.824, TAw acc= 88.6% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.1s loss=1.515, TAw acc= 91.4% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.1s loss=1.223, TAw acc= 91.4% | *
| Epoch   6, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.257, TAw acc= 92.9% |
| Epoch   7, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.726, TAw acc= 92.9% |
| Epoch   8, time=  2.4s | Train: skip eval | Valid: time=  0.1s loss=1.102, TAw acc= 91.4% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.1s loss=1.103, TAw acc= 91.4% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.1s loss=1.104, TAw acc= 91.4% |
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.104, TAw acc= 91.4% |
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.1s loss=1.105, TAw acc= 91.4% |
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.106, TAw acc= 91.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3746 train exemplars, time=  0.0s
3746
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.384 | TAw acc= 88.3%, forg=  1.8%| TAg acc= 66.1%, forg= 15.8% <<<
>>> Test on task  1 : loss=1.654 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 62.0%, forg= 13.9% <<<
>>> Test on task  2 : loss=1.028 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 72.5%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.057 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 68.6%, forg= 16.7% <<<
>>> Test on task  4 : loss=0.962 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.163 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.811 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.2%, forg=  0.8% <<<
>>> Test on task  7 : loss=0.931 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.832 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 80.5%, forg=  5.1% <<<
>>> Test on task  9 : loss=1.337 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 72.1%, forg=  7.7% <<<
>>> Test on task 10 : loss=1.222 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 78.2%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.238 | TAw acc= 93.9%, forg= -1.0%| TAg acc= 71.4%, forg=  8.2% <<<
>>> Test on task 12 : loss=1.149 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 79.6%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.089 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.243 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.3%, forg=  9.6% <<<
>>> Test on task 15 : loss=0.904 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 81.8%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.167 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 17 : loss=0.782 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 78.6%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.186 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.1%, forg= 15.1% <<<
>>> Test on task 19 : loss=1.337 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 62.6%, forg= 15.4% <<<
>>> Test on task 20 : loss=1.084 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.8%, forg=  7.8% <<<
>>> Test on task 21 : loss=1.235 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.300 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 67.6%, forg= 16.2% <<<
>>> Test on task 23 : loss=1.302 | TAw acc= 97.5%, forg=  2.5%| TAg acc= 62.5%, forg= 13.3% <<<
>>> Test on task 24 : loss=0.929 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task 25 : loss=1.573 | TAw acc= 92.8%, forg=  0.9%| TAg acc= 43.2%, forg= 33.3% <<<
>>> Test on task 26 : loss=0.846 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 82.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=5.121, TAw acc= 69.8% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.136, TAw acc= 85.4% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.1s loss=1.202, TAw acc=100.0% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.005, TAw acc=100.0% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.3s loss=0.783, TAw acc=100.0% | *
| Epoch   6, time=  3.9s | Train: skip eval | Valid: time=  0.1s loss=0.807, TAw acc=100.0% |
| Epoch   7, time=  3.1s | Train: skip eval | Valid: time=  0.3s loss=0.846, TAw acc=100.0% |
| Epoch   8, time=  3.3s | Train: skip eval | Valid: time=  0.3s loss=0.728, TAw acc=100.0% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.726, TAw acc=100.0% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.3s loss=0.724, TAw acc=100.0% | *
| Epoch   3, time=  3.3s | Train: skip eval | Valid: time=  0.3s loss=0.723, TAw acc=100.0% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.3s loss=0.721, TAw acc=100.0% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.1s loss=0.720, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3866 train exemplars, time=  0.0s
3866
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.460 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 64.3%, forg= 17.5% <<<
>>> Test on task  1 : loss=1.754 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 60.2%, forg= 15.7% <<<
>>> Test on task  2 : loss=0.955 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=1.129 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 70.6%, forg= 14.7% <<<
>>> Test on task  4 : loss=0.953 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.134 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 72.2%, forg= 10.2% <<<
>>> Test on task  6 : loss=0.831 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.8%, forg=  4.2% <<<
>>> Test on task  7 : loss=0.938 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.807 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 82.2%, forg=  3.4% <<<
>>> Test on task  9 : loss=1.362 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 72.1%, forg=  7.7% <<<
>>> Test on task 10 : loss=1.215 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 79.1%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.232 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 72.4%, forg=  7.1% <<<
>>> Test on task 12 : loss=1.130 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 76.7%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.151 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 70.2%, forg=  7.0% <<<
>>> Test on task 14 : loss=1.283 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.819 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 84.8%, forg= -2.0% <<<
>>> Test on task 16 : loss=1.226 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.8%, forg=  5.5% <<<
>>> Test on task 17 : loss=0.791 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 78.6%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.065 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 63.2%, forg= 17.0% <<<
>>> Test on task 19 : loss=1.351 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 20 : loss=1.060 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.7%, forg=  7.0% <<<
>>> Test on task 21 : loss=1.218 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.187 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 80.2%, forg=  3.6% <<<
>>> Test on task 23 : loss=1.358 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 59.2%, forg= 16.7% <<<
>>> Test on task 24 : loss=1.020 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 63.3%, forg= 18.3% <<<
>>> Test on task 25 : loss=1.647 | TAw acc= 93.7%, forg=  0.0%| TAg acc= 39.6%, forg= 36.9% <<<
>>> Test on task 26 : loss=1.211 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task 27 : loss=0.859 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 87.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.3s loss=6.821, TAw acc= 54.8% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.1s loss=3.258, TAw acc= 79.5% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.924, TAw acc= 90.4% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.531, TAw acc= 93.2% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.232, TAw acc= 93.2% | *
| Epoch   6, time=  3.1s | Train: skip eval | Valid: time=  0.3s loss=1.218, TAw acc= 95.9% | *
| Epoch   7, time=  3.6s | Train: skip eval | Valid: time=  0.3s loss=1.151, TAw acc= 94.5% | *
| Epoch   8, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.154, TAw acc= 93.2% |
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.1s loss=1.150, TAw acc= 94.5% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.150, TAw acc= 94.5% | *
| Epoch   3, time=  3.3s | Train: skip eval | Valid: time=  0.3s loss=1.149, TAw acc= 94.5% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 94.5% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.1s loss=1.149, TAw acc= 95.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3986 train exemplars, time=  0.0s
3986
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.446 | TAw acc= 86.0%, forg=  4.1%| TAg acc= 63.7%, forg= 18.1% <<<
>>> Test on task  1 : loss=1.745 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 58.3%, forg= 17.6% <<<
>>> Test on task  2 : loss=0.990 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 72.5%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.121 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 71.6%, forg= 13.7% <<<
>>> Test on task  4 : loss=0.969 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.184 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.839 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.901 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 85.5%, forg=  1.7% <<<
>>> Test on task  8 : loss=0.810 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 82.2%, forg=  3.4% <<<
>>> Test on task  9 : loss=1.390 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.3%, forg= 12.5% <<<
>>> Test on task 10 : loss=1.293 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 73.6%, forg=  7.3% <<<
>>> Test on task 11 : loss=1.253 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.118 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 77.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.125 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.314 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 68.3%, forg=  8.7% <<<
>>> Test on task 15 : loss=0.810 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 83.8%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.167 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 17 : loss=0.785 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 78.6%, forg=  2.7% <<<
>>> Test on task 18 : loss=0.973 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 74.5%, forg=  5.7% <<<
>>> Test on task 19 : loss=1.300 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 66.7%, forg= 11.4% <<<
>>> Test on task 20 : loss=1.038 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.8%, forg=  7.8% <<<
>>> Test on task 21 : loss=1.276 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.176 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 75.7%, forg=  8.1% <<<
>>> Test on task 23 : loss=1.383 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 61.7%, forg= 14.2% <<<
>>> Test on task 24 : loss=0.985 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task 25 : loss=1.467 | TAw acc= 94.6%, forg= -0.9%| TAg acc= 49.5%, forg= 27.0% <<<
>>> Test on task 26 : loss=1.181 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 63.9%, forg= 18.6% <<<
>>> Test on task 27 : loss=1.298 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 69.3%, forg= 18.1% <<<
>>> Test on task 28 : loss=1.043 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 74.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.3s loss=5.629, TAw acc= 53.4% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=3.027, TAw acc= 62.5% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=2.015, TAw acc= 84.1% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.3s loss=1.617, TAw acc= 95.5% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.3s loss=1.497, TAw acc= 95.5% | *
| Epoch   6, time=  3.7s | Train: skip eval | Valid: time=  0.3s loss=1.398, TAw acc= 95.5% | *
| Epoch   7, time=  4.3s | Train: skip eval | Valid: time=  0.3s loss=1.301, TAw acc= 95.5% | *
| Epoch   8, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.220, TAw acc= 95.5% | *
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.1s loss=1.222, TAw acc= 95.5% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 95.5% |
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.1s loss=1.224, TAw acc= 95.5% |
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.3s loss=1.225, TAw acc= 95.5% |
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.3s loss=1.225, TAw acc= 95.5% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4106 train exemplars, time=  0.2s
4106
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.453 | TAw acc= 87.7%, forg=  2.3%| TAg acc= 69.0%, forg= 12.9% <<<
>>> Test on task  1 : loss=1.824 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 57.4%, forg= 18.5% <<<
>>> Test on task  2 : loss=1.027 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 71.6%, forg= 15.6% <<<
>>> Test on task  3 : loss=1.147 | TAw acc= 91.2%, forg=  2.9%| TAg acc= 64.7%, forg= 20.6% <<<
>>> Test on task  4 : loss=0.966 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.158 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.951 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 74.8%, forg=  9.2% <<<
>>> Test on task  7 : loss=0.943 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.1%, forg=  5.1% <<<
>>> Test on task  8 : loss=0.772 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.9%, forg=  1.7% <<<
>>> Test on task  9 : loss=1.458 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.258 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.209 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 68.4%, forg= 11.2% <<<
>>> Test on task 12 : loss=1.145 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 74.8%, forg=  4.9% <<<
>>> Test on task 13 : loss=1.077 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 71.1%, forg=  6.1% <<<
>>> Test on task 14 : loss=1.340 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 65.4%, forg= 11.5% <<<
>>> Test on task 15 : loss=0.817 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 83.8%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.153 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 73.4%, forg=  0.9% <<<
>>> Test on task 17 : loss=0.831 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 77.7%, forg=  3.6% <<<
>>> Test on task 18 : loss=0.974 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.6%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.386 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 62.6%, forg= 15.4% <<<
>>> Test on task 20 : loss=1.121 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg= 13.0% <<<
>>> Test on task 21 : loss=1.229 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.7%, forg=  5.5% <<<
>>> Test on task 22 : loss=1.185 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 80.2%, forg=  3.6% <<<
>>> Test on task 23 : loss=1.397 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 59.2%, forg= 16.7% <<<
>>> Test on task 24 : loss=0.882 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task 25 : loss=1.519 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 46.8%, forg= 29.7% <<<
>>> Test on task 26 : loss=1.344 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 61.9%, forg= 20.6% <<<
>>> Test on task 27 : loss=1.325 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 67.7%, forg= 19.7% <<<
>>> Test on task 28 : loss=1.433 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 49.0%, forg= 25.0% <<<
>>> Test on task 29 : loss=1.034 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 73.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=5.976, TAw acc= 47.3% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.3s loss=2.601, TAw acc= 73.0% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.3s loss=1.406, TAw acc= 93.2% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.3s loss=1.209, TAw acc= 94.6% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.001, TAw acc= 95.9% | *
| Epoch   6, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.016, TAw acc= 94.6% |
| Epoch   7, time=  4.0s | Train: skip eval | Valid: time=  0.1s loss=0.830, TAw acc= 97.3% | *
| Epoch   8, time=  4.0s | Train: skip eval | Valid: time=  0.1s loss=1.061, TAw acc= 97.3% |
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=0.832, TAw acc= 97.3% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.3s loss=0.834, TAw acc= 97.3% |
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.3s loss=0.835, TAw acc= 97.3% |
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.3s loss=0.837, TAw acc= 97.3% |
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.3s loss=0.838, TAw acc= 97.3% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4226 train exemplars, time=  0.0s
4226
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.492 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 64.9%, forg= 17.0% <<<
>>> Test on task  1 : loss=1.907 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 56.5%, forg= 19.4% <<<
>>> Test on task  2 : loss=1.016 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 78.0%, forg=  9.2% <<<
>>> Test on task  3 : loss=1.243 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 64.7%, forg= 20.6% <<<
>>> Test on task  4 : loss=0.992 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 82.6%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.213 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 72.2%, forg= 10.2% <<<
>>> Test on task  6 : loss=0.940 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 78.2%, forg=  5.9% <<<
>>> Test on task  7 : loss=0.950 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.773 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 84.7%, forg=  0.8% <<<
>>> Test on task  9 : loss=1.466 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.2%, forg=  9.6% <<<
>>> Test on task 10 : loss=1.258 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 76.4%, forg=  4.5% <<<
>>> Test on task 11 : loss=1.295 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 67.3%, forg= 12.2% <<<
>>> Test on task 12 : loss=1.186 | TAw acc= 90.3%, forg=  1.0%| TAg acc= 76.7%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.082 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.292 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 71.2%, forg=  5.8% <<<
>>> Test on task 15 : loss=0.779 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 83.8%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.272 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.0%, forg=  7.3% <<<
>>> Test on task 17 : loss=0.809 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.2%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.010 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.8%, forg= 10.4% <<<
>>> Test on task 19 : loss=1.383 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 20 : loss=1.084 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 68.7%, forg= 13.9% <<<
>>> Test on task 21 : loss=1.222 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.7%, forg=  5.5% <<<
>>> Test on task 22 : loss=1.196 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 77.5%, forg=  6.3% <<<
>>> Test on task 23 : loss=1.378 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 56.7%, forg= 19.2% <<<
>>> Test on task 24 : loss=0.892 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task 25 : loss=1.461 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 48.6%, forg= 27.9% <<<
>>> Test on task 26 : loss=1.260 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 61.9%, forg= 20.6% <<<
>>> Test on task 27 : loss=1.260 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 71.7%, forg= 15.7% <<<
>>> Test on task 28 : loss=1.317 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.0%, forg= 11.0% <<<
>>> Test on task 29 : loss=1.518 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 52.1%, forg= 21.4% <<<
>>> Test on task 30 : loss=1.129 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 76.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.1s loss=6.949, TAw acc= 36.4% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.1s loss=3.381, TAw acc= 93.9% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.1s loss=1.927, TAw acc= 98.5% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.1s loss=1.450, TAw acc= 98.5% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.1s loss=1.504, TAw acc= 98.5% |
| Epoch   6, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.350, TAw acc= 98.5% | *
| Epoch   7, time=  3.3s | Train: skip eval | Valid: time=  0.1s loss=1.230, TAw acc= 98.5% | *
| Epoch   8, time=  3.4s | Train: skip eval | Valid: time=  0.1s loss=1.230, TAw acc= 98.5% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.1s loss=1.226, TAw acc= 98.5% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 98.5% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.1s loss=1.219, TAw acc= 98.5% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.1s loss=1.216, TAw acc= 98.5% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.1s loss=1.213, TAw acc= 98.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4346 train exemplars, time=  0.0s
4346
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.474 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 65.5%, forg= 16.4% <<<
>>> Test on task  1 : loss=1.889 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 58.3%, forg= 17.6% <<<
>>> Test on task  2 : loss=1.061 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg= 13.8% <<<
>>> Test on task  3 : loss=1.240 | TAw acc= 91.2%, forg=  2.9%| TAg acc= 61.8%, forg= 23.5% <<<
>>> Test on task  4 : loss=0.956 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.0%, forg=  1.7% <<<
>>> Test on task  5 : loss=1.168 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 74.1%, forg=  8.3% <<<
>>> Test on task  6 : loss=0.889 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=0.955 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.746 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 84.7%, forg=  0.8% <<<
>>> Test on task  9 : loss=1.430 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.328 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.326 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 64.3%, forg= 15.3% <<<
>>> Test on task 12 : loss=1.196 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 72.8%, forg=  6.8% <<<
>>> Test on task 13 : loss=1.084 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 67.5%, forg=  9.6% <<<
>>> Test on task 14 : loss=1.347 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.3%, forg=  9.6% <<<
>>> Test on task 15 : loss=0.787 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg= -1.0% <<<
>>> Test on task 16 : loss=1.271 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.0%, forg=  7.3% <<<
>>> Test on task 17 : loss=0.821 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 83.0%, forg= -1.8% <<<
>>> Test on task 18 : loss=0.994 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.7%, forg=  8.5% <<<
>>> Test on task 19 : loss=1.331 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 66.7%, forg= 11.4% <<<
>>> Test on task 20 : loss=1.085 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.9%, forg=  8.7% <<<
>>> Test on task 21 : loss=1.250 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.8%, forg=  6.4% <<<
>>> Test on task 22 : loss=1.204 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 76.6%, forg=  7.2% <<<
>>> Test on task 23 : loss=1.365 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 60.0%, forg= 15.8% <<<
>>> Test on task 24 : loss=0.841 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task 25 : loss=1.528 | TAw acc= 95.5%, forg= -0.9%| TAg acc= 53.2%, forg= 23.4% <<<
>>> Test on task 26 : loss=1.213 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task 27 : loss=1.260 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 72.4%, forg= 15.0% <<<
>>> Test on task 28 : loss=1.255 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 61.0%, forg= 13.0% <<<
>>> Test on task 29 : loss=1.462 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 53.0%, forg= 20.5% <<<
>>> Test on task 30 : loss=1.496 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 55.9%, forg= 20.6% <<<
>>> Test on task 31 : loss=1.176 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 65.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=5.794, TAw acc= 54.1% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.1s loss=3.020, TAw acc= 74.1% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.876, TAw acc= 85.9% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=1.518, TAw acc= 92.9% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.241, TAw acc= 91.8% | *
| Epoch   6, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.158, TAw acc= 92.9% | *
| Epoch   7, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 96.5% | *
| Epoch   8, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.040, TAw acc= 95.3% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.1s loss=1.039, TAw acc= 95.3% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.037, TAw acc= 95.3% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.036, TAw acc= 95.3% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.1s loss=1.035, TAw acc= 95.3% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=1.033, TAw acc= 95.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4466 train exemplars, time=  0.0s
4466
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.512 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 62.6%, forg= 19.3% <<<
>>> Test on task  1 : loss=1.889 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 59.3%, forg= 16.7% <<<
>>> Test on task  2 : loss=1.090 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 15.6% <<<
>>> Test on task  3 : loss=1.366 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 60.8%, forg= 24.5% <<<
>>> Test on task  4 : loss=0.981 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.0%, forg=  1.7% <<<
>>> Test on task  5 : loss=1.224 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 72.2%, forg= 10.2% <<<
>>> Test on task  6 : loss=0.915 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.935 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.850 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 81.4%, forg=  4.2% <<<
>>> Test on task  9 : loss=1.395 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.274 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 80.0%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.347 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 64.3%, forg= 15.3% <<<
>>> Test on task 12 : loss=1.139 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.137 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 70.2%, forg=  7.0% <<<
>>> Test on task 14 : loss=1.279 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 71.2%, forg=  5.8% <<<
>>> Test on task 15 : loss=0.766 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.249 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.6%, forg=  2.8% <<<
>>> Test on task 17 : loss=0.898 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 80.4%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.034 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.8%, forg=  9.4% <<<
>>> Test on task 19 : loss=1.343 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 65.0%, forg= 13.0% <<<
>>> Test on task 20 : loss=1.131 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.3%, forg= 11.3% <<<
>>> Test on task 21 : loss=1.286 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.8%, forg=  6.4% <<<
>>> Test on task 22 : loss=1.169 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 78.4%, forg=  5.4% <<<
>>> Test on task 23 : loss=1.406 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 60.0%, forg= 15.8% <<<
>>> Test on task 24 : loss=0.791 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task 25 : loss=1.584 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 46.8%, forg= 29.7% <<<
>>> Test on task 26 : loss=1.202 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 68.0%, forg= 14.4% <<<
>>> Test on task 27 : loss=1.356 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 70.1%, forg= 17.3% <<<
>>> Test on task 28 : loss=1.300 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 62.0%, forg= 12.0% <<<
>>> Test on task 29 : loss=1.513 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 53.8%, forg= 19.7% <<<
>>> Test on task 30 : loss=1.478 | TAw acc= 97.1%, forg= -1.0%| TAg acc= 58.8%, forg= 17.6% <<<
>>> Test on task 31 : loss=1.468 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 55.9%, forg=  9.7% <<<
>>> Test on task 32 : loss=1.099 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 80.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=7.533, TAw acc= 33.8% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.1s loss=4.186, TAw acc= 57.7% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.1s loss=2.483, TAw acc= 69.0% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.879, TAw acc= 78.9% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=1.663, TAw acc= 87.3% | *
| Epoch   6, time=  3.5s | Train: skip eval | Valid: time=  0.1s loss=1.574, TAw acc= 80.3% | *
| Epoch   7, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.446, TAw acc= 91.5% | *
| Epoch   8, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=1.444, TAw acc= 93.0% | *
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=1.441, TAw acc= 93.0% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.1s loss=1.437, TAw acc= 93.0% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.1s loss=1.434, TAw acc= 93.0% | *
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.431, TAw acc= 93.0% | *
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.1s loss=1.428, TAw acc= 93.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4586 train exemplars, time=  0.0s
4586
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.538 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 65.5%, forg= 16.4% <<<
>>> Test on task  1 : loss=1.916 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 57.4%, forg= 18.5% <<<
>>> Test on task  2 : loss=1.052 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=1.322 | TAw acc= 91.2%, forg=  2.9%| TAg acc= 57.8%, forg= 27.5% <<<
>>> Test on task  4 : loss=0.984 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.250 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 71.3%, forg= 11.1% <<<
>>> Test on task  6 : loss=0.921 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.0%, forg=  5.0% <<<
>>> Test on task  7 : loss=0.984 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.767 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 84.7%, forg=  0.8% <<<
>>> Test on task  9 : loss=1.482 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.318 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 76.4%, forg=  4.5% <<<
>>> Test on task 11 : loss=1.371 | TAw acc= 92.9%, forg=  1.0%| TAg acc= 67.3%, forg= 12.2% <<<
>>> Test on task 12 : loss=1.158 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 77.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.093 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 70.2%, forg=  7.0% <<<
>>> Test on task 14 : loss=1.349 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 68.3%, forg=  8.7% <<<
>>> Test on task 15 : loss=0.789 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.225 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=0.841 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.1%, forg=  0.9% <<<
>>> Test on task 18 : loss=0.975 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.6%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.345 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 69.1%, forg=  8.9% <<<
>>> Test on task 20 : loss=1.111 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.9%, forg=  8.7% <<<
>>> Test on task 21 : loss=1.296 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.098 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 80.2%, forg=  3.6% <<<
>>> Test on task 23 : loss=1.475 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 57.5%, forg= 18.3% <<<
>>> Test on task 24 : loss=0.873 | TAw acc=100.0%, forg=  0.0%| TAg acc= 69.7%, forg= 11.9% <<<
>>> Test on task 25 : loss=1.515 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 49.5%, forg= 27.0% <<<
>>> Test on task 26 : loss=1.214 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 66.0%, forg= 16.5% <<<
>>> Test on task 27 : loss=1.281 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 71.7%, forg= 15.7% <<<
>>> Test on task 28 : loss=1.248 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.0%, forg= 10.0% <<<
>>> Test on task 29 : loss=1.490 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 56.4%, forg= 17.1% <<<
>>> Test on task 30 : loss=1.471 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 60.8%, forg= 15.7% <<<
>>> Test on task 31 : loss=1.359 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 60.2%, forg=  5.4% <<<
>>> Test on task 32 : loss=1.461 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 59.1%, forg= 20.9% <<<
>>> Test on task 33 : loss=1.213 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 61.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=6.349, TAw acc= 47.9% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=3.304, TAw acc= 74.6% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.1s loss=1.835, TAw acc= 87.3% | *
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.1s loss=1.393, TAw acc= 94.4% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.1s loss=1.546, TAw acc= 88.7% |
| Epoch   6, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.346, TAw acc= 88.7% | *
| Epoch   7, time=  3.7s | Train: skip eval | Valid: time=  0.1s loss=1.414, TAw acc= 90.1% |
| Epoch   8, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.240, TAw acc= 95.8% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.3s loss=1.244, TAw acc= 95.8% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.248, TAw acc= 95.8% |
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.3s loss=1.251, TAw acc= 95.8% |
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.3s loss=1.254, TAw acc= 95.8% |
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.4s loss=1.256, TAw acc= 94.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4706 train exemplars, time=  0.0s
4706
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.575 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 64.3%, forg= 17.5% <<<
>>> Test on task  1 : loss=1.955 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 57.4%, forg= 18.5% <<<
>>> Test on task  2 : loss=1.068 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=1.376 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 59.8%, forg= 25.5% <<<
>>> Test on task  4 : loss=1.056 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.221 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 74.1%, forg=  8.3% <<<
>>> Test on task  6 : loss=0.908 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=0.975 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.825 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.1%, forg=  2.5% <<<
>>> Test on task  9 : loss=1.474 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.391 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 74.5%, forg=  6.4% <<<
>>> Test on task 11 : loss=1.343 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 70.4%, forg=  9.2% <<<
>>> Test on task 12 : loss=1.151 | TAw acc= 89.3%, forg=  1.9%| TAg acc= 76.7%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.099 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 69.3%, forg=  7.9% <<<
>>> Test on task 14 : loss=1.408 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.3%, forg=  9.6% <<<
>>> Test on task 15 : loss=0.765 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 88.9%, forg= -3.0% <<<
>>> Test on task 16 : loss=1.274 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.0%, forg=  7.3% <<<
>>> Test on task 17 : loss=0.789 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.1%, forg=  0.9% <<<
>>> Test on task 18 : loss=1.124 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.1%, forg= 15.1% <<<
>>> Test on task 19 : loss=1.325 | TAw acc= 94.3%, forg=  1.6%| TAg acc= 69.9%, forg=  8.1% <<<
>>> Test on task 20 : loss=1.089 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.9%, forg=  8.7% <<<
>>> Test on task 21 : loss=1.333 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.204 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 76.6%, forg=  7.2% <<<
>>> Test on task 23 : loss=1.455 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 65.0%, forg= 10.8% <<<
>>> Test on task 24 : loss=0.786 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task 25 : loss=1.522 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 48.6%, forg= 27.9% <<<
>>> Test on task 26 : loss=1.102 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 72.2%, forg= 10.3% <<<
>>> Test on task 27 : loss=1.326 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 70.1%, forg= 17.3% <<<
>>> Test on task 28 : loss=1.283 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.0%, forg= 10.0% <<<
>>> Test on task 29 : loss=1.516 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 57.3%, forg= 16.2% <<<
>>> Test on task 30 : loss=1.411 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 64.7%, forg= 11.8% <<<
>>> Test on task 31 : loss=1.297 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 60.2%, forg=  5.4% <<<
>>> Test on task 32 : loss=1.450 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 60.0%, forg= 20.0% <<<
>>> Test on task 33 : loss=1.499 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 57.1%, forg=  4.1% <<<
>>> Test on task 34 : loss=1.427 | TAw acc= 89.9%, forg=  0.0%| TAg acc= 64.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.3s loss=5.959, TAw acc= 44.7% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.3s loss=2.541, TAw acc= 71.8% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.3s loss=1.556, TAw acc= 91.8% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 96.5% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.124, TAw acc= 94.1% | *
| Epoch   6, time=  5.1s | Train: skip eval | Valid: time=  0.3s loss=0.969, TAw acc= 95.3% | *
| Epoch   7, time=  5.3s | Train: skip eval | Valid: time=  0.3s loss=0.949, TAw acc= 96.5% | *
| Epoch   8, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.843, TAw acc= 97.6% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.842, TAw acc= 97.6% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=0.841, TAw acc= 97.6% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.3s loss=0.840, TAw acc= 96.5% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.3s loss=0.840, TAw acc= 96.5% | *
| Epoch   5, time=  5.9s | Train: skip eval | Valid: time=  0.3s loss=0.839, TAw acc= 96.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4826 train exemplars, time=  0.1s
4826
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.626 | TAw acc= 85.4%, forg=  4.7%| TAg acc= 60.8%, forg= 21.1% <<<
>>> Test on task  1 : loss=1.996 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 55.6%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.108 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 72.5%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.366 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 63.7%, forg= 21.6% <<<
>>> Test on task  4 : loss=1.016 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.245 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 75.9%, forg=  6.5% <<<
>>> Test on task  6 : loss=0.918 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.8%, forg=  4.2% <<<
>>> Test on task  7 : loss=0.985 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.861 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 81.4%, forg=  4.2% <<<
>>> Test on task  9 : loss=1.473 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.351 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.500 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.167 | TAw acc= 86.4%, forg=  4.9%| TAg acc= 72.8%, forg=  6.8% <<<
>>> Test on task 13 : loss=1.086 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 70.2%, forg=  7.0% <<<
>>> Test on task 14 : loss=1.330 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.749 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 87.9%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.271 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.9%, forg=  6.4% <<<
>>> Test on task 17 : loss=0.834 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 80.4%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.073 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 66.0%, forg= 14.2% <<<
>>> Test on task 19 : loss=1.427 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 65.0%, forg= 13.0% <<<
>>> Test on task 20 : loss=1.063 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.8%, forg=  7.8% <<<
>>> Test on task 21 : loss=1.336 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.145 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 79.3%, forg=  4.5% <<<
>>> Test on task 23 : loss=1.484 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 60.8%, forg= 15.0% <<<
>>> Test on task 24 : loss=0.805 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task 25 : loss=1.637 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 50.5%, forg= 26.1% <<<
>>> Test on task 26 : loss=1.123 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task 27 : loss=1.309 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 71.7%, forg= 15.7% <<<
>>> Test on task 28 : loss=1.183 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.0%, forg= 10.0% <<<
>>> Test on task 29 : loss=1.529 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 57.3%, forg= 16.2% <<<
>>> Test on task 30 : loss=1.552 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 61.8%, forg= 14.7% <<<
>>> Test on task 31 : loss=1.309 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 64.5%, forg=  1.1% <<<
>>> Test on task 32 : loss=1.405 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 64.3%, forg= 15.7% <<<
>>> Test on task 33 : loss=1.616 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 46.9%, forg= 14.3% <<<
>>> Test on task 34 : loss=1.747 | TAw acc= 88.9%, forg=  1.0%| TAg acc= 52.5%, forg= 12.1% <<<
>>> Test on task 35 : loss=1.033 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 79.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=5.631, TAw acc= 58.1% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.3s loss=2.539, TAw acc= 74.4% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.3s loss=1.692, TAw acc= 91.9% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 96.5% | *
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=1.406, TAw acc= 95.3% |
| Epoch   6, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.355, TAw acc= 95.3% |
| Epoch   7, time=  5.3s | Train: skip eval | Valid: time=  0.3s loss=1.242, TAw acc= 96.5% | *
| Epoch   8, time=  5.4s | Train: skip eval | Valid: time=  0.3s loss=1.173, TAw acc= 96.5% | *
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.1s loss=1.171, TAw acc= 96.5% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.169, TAw acc= 96.5% | *
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.3s loss=1.169, TAw acc= 97.7% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.3s loss=1.168, TAw acc= 97.7% | *
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.3s loss=1.168, TAw acc= 97.7% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4946 train exemplars, time=  0.0s
4946
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.661 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 60.8%, forg= 21.1% <<<
>>> Test on task  1 : loss=2.000 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 56.5%, forg= 19.4% <<<
>>> Test on task  2 : loss=1.098 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 74.3%, forg= 12.8% <<<
>>> Test on task  3 : loss=1.410 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 63.7%, forg= 21.6% <<<
>>> Test on task  4 : loss=1.117 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  5 : loss=1.392 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.6%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.008 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 77.3%, forg=  6.7% <<<
>>> Test on task  7 : loss=1.004 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 81.2%, forg=  6.0% <<<
>>> Test on task  8 : loss=0.895 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 77.1%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.436 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.382 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 73.6%, forg=  7.3% <<<
>>> Test on task 11 : loss=1.454 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 65.3%, forg= 14.3% <<<
>>> Test on task 12 : loss=1.208 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 72.8%, forg=  6.8% <<<
>>> Test on task 13 : loss=1.219 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 66.7%, forg= 10.5% <<<
>>> Test on task 14 : loss=1.350 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.749 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.263 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 17 : loss=0.791 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 82.1%, forg=  0.9% <<<
>>> Test on task 18 : loss=1.084 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 68.9%, forg= 11.3% <<<
>>> Test on task 19 : loss=1.547 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 60.2%, forg= 17.9% <<<
>>> Test on task 20 : loss=1.040 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.9%, forg=  8.7% <<<
>>> Test on task 21 : loss=1.427 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 61.5%, forg= 13.8% <<<
>>> Test on task 22 : loss=1.202 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 76.6%, forg=  7.2% <<<
>>> Test on task 23 : loss=1.547 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 61.7%, forg= 14.2% <<<
>>> Test on task 24 : loss=0.781 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.9%, forg=  2.8% <<<
>>> Test on task 25 : loss=1.631 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 47.7%, forg= 28.8% <<<
>>> Test on task 26 : loss=1.137 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task 27 : loss=1.385 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 69.3%, forg= 18.1% <<<
>>> Test on task 28 : loss=1.212 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.0%, forg= 10.0% <<<
>>> Test on task 29 : loss=1.515 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 59.8%, forg= 13.7% <<<
>>> Test on task 30 : loss=1.464 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 65.7%, forg= 10.8% <<<
>>> Test on task 31 : loss=1.305 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 64.5%, forg=  1.1% <<<
>>> Test on task 32 : loss=1.472 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 61.7%, forg= 18.3% <<<
>>> Test on task 33 : loss=1.402 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 53.1%, forg=  8.2% <<<
>>> Test on task 34 : loss=1.725 | TAw acc= 89.9%, forg=  0.0%| TAg acc= 53.5%, forg= 11.1% <<<
>>> Test on task 35 : loss=1.362 | TAw acc= 95.7%, forg= -0.9%| TAg acc= 64.3%, forg= 14.8% <<<
>>> Test on task 36 : loss=1.031 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 75.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=5.403, TAw acc= 63.6% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.3s loss=1.988, TAw acc= 87.5% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.3s loss=1.071, TAw acc= 96.6% | *
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.1s loss=0.839, TAw acc= 97.7% | *
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.894, TAw acc= 97.7% |
| Epoch   6, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.710, TAw acc= 97.7% | *
| Epoch   7, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=0.838, TAw acc= 96.6% |
| Epoch   8, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.744, TAw acc= 97.7% |
| Epoch   1, time=  4.5s | Train: skip eval | Valid: time=  0.1s loss=0.713, TAw acc= 97.7% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=0.716, TAw acc= 97.7% |
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=0.718, TAw acc= 97.7% |
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.1s loss=0.721, TAw acc= 97.7% |
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=0.724, TAw acc= 97.7% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5066 train exemplars, time=  0.0s
5066
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.719 | TAw acc= 86.0%, forg=  4.1%| TAg acc= 61.4%, forg= 20.5% <<<
>>> Test on task  1 : loss=1.998 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 59.3%, forg= 16.7% <<<
>>> Test on task  2 : loss=1.096 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 76.1%, forg= 11.0% <<<
>>> Test on task  3 : loss=1.380 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 58.8%, forg= 26.5% <<<
>>> Test on task  4 : loss=1.071 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.339 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 70.4%, forg= 12.0% <<<
>>> Test on task  6 : loss=0.957 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 78.2%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.008 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.9%, forg=  4.3% <<<
>>> Test on task  8 : loss=0.877 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 82.2%, forg=  3.4% <<<
>>> Test on task  9 : loss=1.605 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 63.5%, forg= 16.3% <<<
>>> Test on task 10 : loss=1.342 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.495 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 62.2%, forg= 17.3% <<<
>>> Test on task 12 : loss=1.192 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 73.8%, forg=  5.8% <<<
>>> Test on task 13 : loss=1.120 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 70.2%, forg=  7.0% <<<
>>> Test on task 14 : loss=1.368 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.789 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 82.8%, forg=  6.1% <<<
>>> Test on task 16 : loss=1.260 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=0.797 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 83.9%, forg= -0.9% <<<
>>> Test on task 18 : loss=1.081 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 68.9%, forg= 11.3% <<<
>>> Test on task 19 : loss=1.423 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 66.7%, forg= 11.4% <<<
>>> Test on task 20 : loss=1.031 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.4%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.386 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 69.7%, forg=  5.5% <<<
>>> Test on task 22 : loss=1.256 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 75.7%, forg=  8.1% <<<
>>> Test on task 23 : loss=1.486 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 63.3%, forg= 12.5% <<<
>>> Test on task 24 : loss=0.855 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task 25 : loss=1.696 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 48.6%, forg= 27.9% <<<
>>> Test on task 26 : loss=1.249 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 69.1%, forg= 13.4% <<<
>>> Test on task 27 : loss=1.389 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 69.3%, forg= 18.1% <<<
>>> Test on task 28 : loss=1.243 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 62.0%, forg= 12.0% <<<
>>> Test on task 29 : loss=1.520 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 62.4%, forg= 11.1% <<<
>>> Test on task 30 : loss=1.512 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 64.7%, forg= 11.8% <<<
>>> Test on task 31 : loss=1.174 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 68.8%, forg= -3.2% <<<
>>> Test on task 32 : loss=1.511 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 62.6%, forg= 17.4% <<<
>>> Test on task 33 : loss=1.467 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 55.1%, forg=  6.1% <<<
>>> Test on task 34 : loss=1.791 | TAw acc= 88.9%, forg=  1.0%| TAg acc= 56.6%, forg=  8.1% <<<
>>> Test on task 35 : loss=1.336 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 60.9%, forg= 18.3% <<<
>>> Test on task 36 : loss=1.587 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 47.8%, forg= 27.8% <<<
>>> Test on task 37 : loss=0.800 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 83.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=5.803, TAw acc= 50.7% | *
| Epoch   2, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=2.867, TAw acc= 77.3% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.1s loss=1.798, TAw acc= 89.3% | *
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.1s loss=1.544, TAw acc= 92.0% | *
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.1s loss=1.433, TAw acc= 96.0% | *
| Epoch   6, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 90.7% | *
| Epoch   7, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 92.0% | *
| Epoch   8, time=  4.7s | Train: skip eval | Valid: time=  0.1s loss=1.275, TAw acc= 90.7% | *
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.267, TAw acc= 90.7% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.1s loss=1.260, TAw acc= 90.7% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.254, TAw acc= 90.7% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.1s loss=1.248, TAw acc= 90.7% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.1s loss=1.243, TAw acc= 90.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5186 train exemplars, time=  0.0s
5186
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.626 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 62.6%, forg= 19.3% <<<
>>> Test on task  1 : loss=2.043 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 55.6%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.192 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 70.6%, forg= 16.5% <<<
>>> Test on task  3 : loss=1.471 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 55.9%, forg= 29.4% <<<
>>> Test on task  4 : loss=1.111 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.342 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.980 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 82.4%, forg=  1.7% <<<
>>> Test on task  7 : loss=0.986 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 84.6%, forg=  2.6% <<<
>>> Test on task  8 : loss=0.901 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 78.0%, forg=  7.6% <<<
>>> Test on task  9 : loss=1.519 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.2%, forg=  9.6% <<<
>>> Test on task 10 : loss=1.336 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 76.4%, forg=  4.5% <<<
>>> Test on task 11 : loss=1.473 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 62.2%, forg= 17.3% <<<
>>> Test on task 12 : loss=1.157 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.142 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 70.2%, forg=  7.0% <<<
>>> Test on task 14 : loss=1.361 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.774 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.285 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 65.1%, forg=  9.2% <<<
>>> Test on task 17 : loss=0.849 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.2%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.137 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.1%, forg= 15.1% <<<
>>> Test on task 19 : loss=1.499 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 63.4%, forg= 14.6% <<<
>>> Test on task 20 : loss=1.048 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 76.5%, forg=  6.1% <<<
>>> Test on task 21 : loss=1.437 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.202 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 77.5%, forg=  6.3% <<<
>>> Test on task 23 : loss=1.483 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 64.2%, forg= 11.7% <<<
>>> Test on task 24 : loss=0.742 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task 25 : loss=1.668 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 47.7%, forg= 28.8% <<<
>>> Test on task 26 : loss=1.112 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 75.3%, forg=  7.2% <<<
>>> Test on task 27 : loss=1.402 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 70.9%, forg= 16.5% <<<
>>> Test on task 28 : loss=1.263 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 63.0%, forg= 11.0% <<<
>>> Test on task 29 : loss=1.609 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 59.0%, forg= 14.5% <<<
>>> Test on task 30 : loss=1.489 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 62.7%, forg= 13.7% <<<
>>> Test on task 31 : loss=1.192 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 63.4%, forg=  5.4% <<<
>>> Test on task 32 : loss=1.402 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 68.7%, forg= 11.3% <<<
>>> Test on task 33 : loss=1.445 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 52.0%, forg=  9.2% <<<
>>> Test on task 34 : loss=1.661 | TAw acc= 90.9%, forg= -1.0%| TAg acc= 60.6%, forg=  4.0% <<<
>>> Test on task 35 : loss=1.311 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 67.8%, forg= 11.3% <<<
>>> Test on task 36 : loss=1.516 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 50.4%, forg= 25.2% <<<
>>> Test on task 37 : loss=1.097 | TAw acc= 98.3%, forg= -0.8%| TAg acc= 74.6%, forg=  9.3% <<<
>>> Test on task 38 : loss=1.161 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 64.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=4.905, TAw acc= 60.4% | *
| Epoch   2, time=  4.7s | Train: skip eval | Valid: time=  0.1s loss=2.495, TAw acc= 68.3% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.772, TAw acc= 79.2% | *
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.315, TAw acc= 86.1% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.112, TAw acc= 91.1% | *
| Epoch   6, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=1.084, TAw acc= 88.1% | *
| Epoch   7, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.982, TAw acc= 89.1% | *
| Epoch   8, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.990, TAw acc= 93.1% |
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.1s loss=0.980, TAw acc= 90.1% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=0.978, TAw acc= 90.1% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.977, TAw acc= 90.1% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=0.976, TAw acc= 90.1% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.975, TAw acc= 89.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5306 train exemplars, time=  0.0s
5306
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.680 | TAw acc= 86.0%, forg=  4.1%| TAg acc= 62.0%, forg= 19.9% <<<
>>> Test on task  1 : loss=2.093 | TAw acc= 91.7%, forg=  3.7%| TAg acc= 54.6%, forg= 21.3% <<<
>>> Test on task  2 : loss=1.142 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg= 13.8% <<<
>>> Test on task  3 : loss=1.421 | TAw acc= 92.2%, forg=  2.0%| TAg acc= 62.7%, forg= 22.5% <<<
>>> Test on task  4 : loss=1.106 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.347 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 69.4%, forg= 13.0% <<<
>>> Test on task  6 : loss=0.999 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.080 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.1%, forg=  5.1% <<<
>>> Test on task  8 : loss=0.899 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 78.8%, forg=  6.8% <<<
>>> Test on task  9 : loss=1.587 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 66.3%, forg= 13.5% <<<
>>> Test on task 10 : loss=1.403 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 71.8%, forg=  9.1% <<<
>>> Test on task 11 : loss=1.551 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.171 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.121 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 67.5%, forg=  9.6% <<<
>>> Test on task 14 : loss=1.367 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 71.2%, forg=  5.8% <<<
>>> Test on task 15 : loss=0.737 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 86.9%, forg=  2.0% <<<
>>> Test on task 16 : loss=1.260 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.7%, forg=  4.6% <<<
>>> Test on task 17 : loss=0.825 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 79.5%, forg=  4.5% <<<
>>> Test on task 18 : loss=1.079 | TAw acc=100.0%, forg= -0.9%| TAg acc= 72.6%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.459 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 66.7%, forg= 11.4% <<<
>>> Test on task 20 : loss=1.081 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 75.7%, forg=  7.0% <<<
>>> Test on task 21 : loss=1.364 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 70.6%, forg=  4.6% <<<
>>> Test on task 22 : loss=1.235 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.7%, forg=  8.1% <<<
>>> Test on task 23 : loss=1.584 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 60.8%, forg= 15.0% <<<
>>> Test on task 24 : loss=0.765 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task 25 : loss=1.650 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 52.3%, forg= 24.3% <<<
>>> Test on task 26 : loss=1.247 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task 27 : loss=1.490 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 65.4%, forg= 22.0% <<<
>>> Test on task 28 : loss=1.269 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 64.0%, forg= 10.0% <<<
>>> Test on task 29 : loss=1.559 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 59.8%, forg= 13.7% <<<
>>> Test on task 30 : loss=1.456 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  9.8% <<<
>>> Test on task 31 : loss=1.190 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 65.6%, forg=  3.2% <<<
>>> Test on task 32 : loss=1.481 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 60.9%, forg= 19.1% <<<
>>> Test on task 33 : loss=1.493 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 53.1%, forg=  8.2% <<<
>>> Test on task 34 : loss=1.751 | TAw acc= 87.9%, forg=  3.0%| TAg acc= 58.6%, forg=  6.1% <<<
>>> Test on task 35 : loss=1.294 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 67.0%, forg= 12.2% <<<
>>> Test on task 36 : loss=1.577 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 52.2%, forg= 23.5% <<<
>>> Test on task 37 : loss=1.146 | TAw acc= 97.5%, forg=  0.8%| TAg acc= 67.8%, forg= 16.1% <<<
>>> Test on task 38 : loss=1.527 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 51.9%, forg= 12.5% <<<
>>> Test on task 39 : loss=0.760 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 78.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=7.396, TAw acc= 44.0% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.1s loss=3.563, TAw acc= 66.7% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.1s loss=1.934, TAw acc= 88.0% | *
| Epoch   4, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.520, TAw acc= 97.3% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.1s loss=1.396, TAw acc= 93.3% | *
| Epoch   6, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.239, TAw acc= 98.7% | *
| Epoch   7, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.233, TAw acc= 98.7% | *
| Epoch   8, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.186, TAw acc=100.0% | *
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=1.180, TAw acc=100.0% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc=100.0% | *
| Epoch   3, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.170, TAw acc=100.0% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.1s loss=1.165, TAw acc=100.0% | *
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=1.161, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5426 train exemplars, time=  0.0s
5426
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.715 | TAw acc= 87.7%, forg=  2.3%| TAg acc= 62.6%, forg= 19.3% <<<
>>> Test on task  1 : loss=2.140 | TAw acc= 91.7%, forg=  3.7%| TAg acc= 54.6%, forg= 21.3% <<<
>>> Test on task  2 : loss=1.172 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 74.3%, forg= 12.8% <<<
>>> Test on task  3 : loss=1.360 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 61.8%, forg= 23.5% <<<
>>> Test on task  4 : loss=1.109 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 80.2%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.381 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 71.3%, forg= 11.1% <<<
>>> Test on task  6 : loss=1.012 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 81.5%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.079 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.1%, forg=  5.1% <<<
>>> Test on task  8 : loss=0.867 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 78.0%, forg=  7.6% <<<
>>> Test on task  9 : loss=1.574 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=1.327 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.559 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 60.2%, forg= 19.4% <<<
>>> Test on task 12 : loss=1.117 | TAw acc= 86.4%, forg=  4.9%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task 13 : loss=1.177 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 68.4%, forg=  8.8% <<<
>>> Test on task 14 : loss=1.377 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 15 : loss=0.730 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.353 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.9%, forg=  6.4% <<<
>>> Test on task 17 : loss=0.790 | TAw acc=100.0%, forg= -0.9%| TAg acc= 82.1%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.058 | TAw acc=100.0%, forg=  0.0%| TAg acc= 69.8%, forg= 10.4% <<<
>>> Test on task 19 : loss=1.485 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 20 : loss=1.020 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 77.4%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.508 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 64.2%, forg= 11.0% <<<
>>> Test on task 22 : loss=1.214 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 78.4%, forg=  5.4% <<<
>>> Test on task 23 : loss=1.582 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 63.3%, forg= 12.5% <<<
>>> Test on task 24 : loss=0.779 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task 25 : loss=1.641 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 51.4%, forg= 25.2% <<<
>>> Test on task 26 : loss=1.232 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 71.1%, forg= 11.3% <<<
>>> Test on task 27 : loss=1.451 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 66.9%, forg= 20.5% <<<
>>> Test on task 28 : loss=1.291 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 62.0%, forg= 12.0% <<<
>>> Test on task 29 : loss=1.680 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 59.0%, forg= 14.5% <<<
>>> Test on task 30 : loss=1.470 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  9.8% <<<
>>> Test on task 31 : loss=1.115 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.596 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 58.3%, forg= 21.7% <<<
>>> Test on task 33 : loss=1.442 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 51.0%, forg= 10.2% <<<
>>> Test on task 34 : loss=1.698 | TAw acc= 87.9%, forg=  3.0%| TAg acc= 63.6%, forg=  1.0% <<<
>>> Test on task 35 : loss=1.288 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 68.7%, forg= 10.4% <<<
>>> Test on task 36 : loss=1.622 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 55.7%, forg= 20.0% <<<
>>> Test on task 37 : loss=1.101 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.3%, forg= 13.6% <<<
>>> Test on task 38 : loss=1.388 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 60.6%, forg=  3.8% <<<
>>> Test on task 39 : loss=1.095 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 73.5%, forg=  5.3% <<<
>>> Test on task 40 : loss=1.175 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 68.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.2s | Train: skip eval | Valid: time=  0.1s loss=6.729, TAw acc= 61.3% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.1s loss=3.201, TAw acc= 76.2% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=2.234, TAw acc= 88.8% | *
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.935, TAw acc= 90.0% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.1s loss=1.600, TAw acc= 96.2% | *
| Epoch   6, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.492, TAw acc= 97.5% | *
| Epoch   7, time=  5.5s | Train: skip eval | Valid: time=  0.1s loss=1.399, TAw acc= 96.2% | *
| Epoch   8, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.420, TAw acc= 96.2% |
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.1s loss=1.399, TAw acc= 96.2% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.398, TAw acc= 96.2% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.1s loss=1.397, TAw acc= 96.2% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.397, TAw acc= 96.2% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.1s loss=1.396, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5546 train exemplars, time=  0.0s
5546
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.787 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 59.6%, forg= 22.2% <<<
>>> Test on task  1 : loss=2.071 | TAw acc= 91.7%, forg=  3.7%| TAg acc= 58.3%, forg= 17.6% <<<
>>> Test on task  2 : loss=1.190 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 15.6% <<<
>>> Test on task  3 : loss=1.413 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 60.8%, forg= 24.5% <<<
>>> Test on task  4 : loss=1.137 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 78.5%, forg=  4.1% <<<
>>> Test on task  5 : loss=1.398 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 69.4%, forg= 13.0% <<<
>>> Test on task  6 : loss=1.017 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 77.3%, forg=  6.7% <<<
>>> Test on task  7 : loss=1.083 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 81.2%, forg=  6.0% <<<
>>> Test on task  8 : loss=0.947 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 78.8%, forg=  6.8% <<<
>>> Test on task  9 : loss=1.613 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.385 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 77.3%, forg=  3.6% <<<
>>> Test on task 11 : loss=1.570 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.165 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 76.7%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.158 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 67.5%, forg=  9.6% <<<
>>> Test on task 14 : loss=1.388 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 74.0%, forg=  2.9% <<<
>>> Test on task 15 : loss=0.707 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 89.9%, forg= -1.0% <<<
>>> Test on task 16 : loss=1.405 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 63.3%, forg= 11.0% <<<
>>> Test on task 17 : loss=0.836 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 79.5%, forg=  4.5% <<<
>>> Test on task 18 : loss=1.072 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.8%, forg=  9.4% <<<
>>> Test on task 19 : loss=1.464 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 65.9%, forg= 12.2% <<<
>>> Test on task 20 : loss=1.091 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.7%, forg=  7.0% <<<
>>> Test on task 21 : loss=1.410 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 22 : loss=1.222 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.7%, forg=  8.1% <<<
>>> Test on task 23 : loss=1.606 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 60.0%, forg= 15.8% <<<
>>> Test on task 24 : loss=0.763 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task 25 : loss=1.761 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 51.4%, forg= 25.2% <<<
>>> Test on task 26 : loss=1.185 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 68.0%, forg= 14.4% <<<
>>> Test on task 27 : loss=1.467 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 68.5%, forg= 18.9% <<<
>>> Test on task 28 : loss=1.371 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 59.0%, forg= 15.0% <<<
>>> Test on task 29 : loss=1.649 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 59.8%, forg= 13.7% <<<
>>> Test on task 30 : loss=1.582 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  9.8% <<<
>>> Test on task 31 : loss=1.135 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 69.9%, forg= -1.1% <<<
>>> Test on task 32 : loss=1.443 | TAw acc= 93.9%, forg=  0.9%| TAg acc= 67.8%, forg= 12.2% <<<
>>> Test on task 33 : loss=1.443 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 53.1%, forg=  8.2% <<<
>>> Test on task 34 : loss=1.652 | TAw acc= 89.9%, forg=  1.0%| TAg acc= 63.6%, forg=  1.0% <<<
>>> Test on task 35 : loss=1.372 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 68.7%, forg= 10.4% <<<
>>> Test on task 36 : loss=1.566 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 55.7%, forg= 20.0% <<<
>>> Test on task 37 : loss=1.063 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.1%, forg=  6.8% <<<
>>> Test on task 38 : loss=1.410 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 59.6%, forg=  4.8% <<<
>>> Test on task 39 : loss=1.048 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 74.2%, forg=  4.5% <<<
>>> Test on task 40 : loss=1.455 | TAw acc= 94.2%, forg= -1.0%| TAg acc= 59.2%, forg=  9.7% <<<
>>> Test on task 41 : loss=1.117 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 64.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=3.282, TAw acc= 71.6% | *
| Epoch   2, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.716, TAw acc= 82.4% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.1s loss=0.965, TAw acc= 91.2% | *
| Epoch   4, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.823, TAw acc= 89.2% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.1s loss=0.765, TAw acc= 90.2% | *
| Epoch   6, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=0.690, TAw acc= 89.2% | *
| Epoch   7, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=0.720, TAw acc= 89.2% |
| Epoch   8, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.699, TAw acc= 89.2% |
| Epoch   1, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.691, TAw acc= 89.2% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.692, TAw acc= 89.2% |
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.693, TAw acc= 89.2% |
| Epoch   4, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.694, TAw acc= 89.2% |
| Epoch   5, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.696, TAw acc= 88.2% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5666 train exemplars, time=  0.1s
5666
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.751 | TAw acc= 86.5%, forg=  3.5%| TAg acc= 62.0%, forg= 19.9% <<<
>>> Test on task  1 : loss=2.134 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 55.6%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.239 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 67.9%, forg= 19.3% <<<
>>> Test on task  3 : loss=1.450 | TAw acc= 91.2%, forg=  2.9%| TAg acc= 58.8%, forg= 26.5% <<<
>>> Test on task  4 : loss=1.181 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 74.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.398 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 73.1%, forg=  9.3% <<<
>>> Test on task  6 : loss=1.034 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 79.8%, forg=  4.2% <<<
>>> Test on task  7 : loss=1.081 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 82.1%, forg=  5.1% <<<
>>> Test on task  8 : loss=0.965 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 77.1%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.566 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.374 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 72.7%, forg=  8.2% <<<
>>> Test on task 11 : loss=1.792 | TAw acc= 90.8%, forg=  3.1%| TAg acc= 63.3%, forg= 16.3% <<<
>>> Test on task 12 : loss=1.165 | TAw acc= 87.4%, forg=  3.9%| TAg acc= 77.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.171 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 71.1%, forg=  6.1% <<<
>>> Test on task 14 : loss=1.491 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 68.3%, forg=  8.7% <<<
>>> Test on task 15 : loss=0.782 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 83.8%, forg=  6.1% <<<
>>> Test on task 16 : loss=1.328 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.8%, forg=  5.5% <<<
>>> Test on task 17 : loss=0.883 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 76.8%, forg=  7.1% <<<
>>> Test on task 18 : loss=1.211 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 63.2%, forg= 17.0% <<<
>>> Test on task 19 : loss=1.522 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 20 : loss=1.100 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.3%, forg=  4.3% <<<
>>> Test on task 21 : loss=1.417 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 64.2%, forg= 11.0% <<<
>>> Test on task 22 : loss=1.252 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.7%, forg=  8.1% <<<
>>> Test on task 23 : loss=1.606 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 60.0%, forg= 15.8% <<<
>>> Test on task 24 : loss=0.832 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task 25 : loss=1.853 | TAw acc= 92.8%, forg=  2.7%| TAg acc= 47.7%, forg= 28.8% <<<
>>> Test on task 26 : loss=1.198 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 69.1%, forg= 13.4% <<<
>>> Test on task 27 : loss=1.561 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 68.5%, forg= 18.9% <<<
>>> Test on task 28 : loss=1.332 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 62.0%, forg= 12.0% <<<
>>> Test on task 29 : loss=1.707 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 58.1%, forg= 15.4% <<<
>>> Test on task 30 : loss=1.589 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 65.7%, forg= 10.8% <<<
>>> Test on task 31 : loss=1.145 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 67.7%, forg=  2.2% <<<
>>> Test on task 32 : loss=1.504 | TAw acc= 93.9%, forg=  0.9%| TAg acc= 64.3%, forg= 15.7% <<<
>>> Test on task 33 : loss=1.419 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 57.1%, forg=  4.1% <<<
>>> Test on task 34 : loss=1.648 | TAw acc= 90.9%, forg=  0.0%| TAg acc= 65.7%, forg= -1.0% <<<
>>> Test on task 35 : loss=1.425 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 65.2%, forg= 13.9% <<<
>>> Test on task 36 : loss=1.451 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 56.5%, forg= 19.1% <<<
>>> Test on task 37 : loss=1.056 | TAw acc= 97.5%, forg=  0.8%| TAg acc= 73.7%, forg= 10.2% <<<
>>> Test on task 38 : loss=1.371 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 60.6%, forg=  3.8% <<<
>>> Test on task 39 : loss=1.055 | TAw acc= 95.5%, forg= -0.8%| TAg acc= 75.8%, forg=  3.0% <<<
>>> Test on task 40 : loss=1.396 | TAw acc= 95.1%, forg= -1.0%| TAg acc= 57.3%, forg= 11.7% <<<
>>> Test on task 41 : loss=1.644 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 43.0%, forg= 21.5% <<<
>>> Test on task 42 : loss=0.811 | TAw acc= 91.0%, forg=  0.0%| TAg acc= 77.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=6.269, TAw acc= 46.3% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.1s loss=2.566, TAw acc= 76.8% | *
| Epoch   3, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.538, TAw acc= 87.8% | *
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.1s loss=1.273, TAw acc= 92.7% | *
| Epoch   5, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.104, TAw acc= 95.1% | *
| Epoch   6, time=  6.0s | Train: skip eval | Valid: time=  0.1s loss=1.001, TAw acc= 96.3% | *
| Epoch   7, time=  5.9s | Train: skip eval | Valid: time=  0.1s loss=0.970, TAw acc= 96.3% | *
| Epoch   8, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.956, TAw acc= 97.6% | *
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.951, TAw acc= 97.6% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.947, TAw acc= 97.6% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.944, TAw acc= 97.6% | *
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.941, TAw acc= 97.6% | *
| Epoch   5, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 97.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 24
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5786 train exemplars, time=  0.0s
5786
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.729 | TAw acc= 87.1%, forg=  2.9%| TAg acc= 62.0%, forg= 19.9% <<<
>>> Test on task  1 : loss=2.185 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 55.6%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.232 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 15.6% <<<
>>> Test on task  3 : loss=1.438 | TAw acc= 93.1%, forg=  1.0%| TAg acc= 61.8%, forg= 23.5% <<<
>>> Test on task  4 : loss=1.227 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  5 : loss=1.420 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.2%, forg= 10.2% <<<
>>> Test on task  6 : loss=1.044 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 77.3%, forg=  6.7% <<<
>>> Test on task  7 : loss=1.088 | TAw acc= 94.9%, forg=  0.9%| TAg acc= 83.8%, forg=  3.4% <<<
>>> Test on task  8 : loss=0.901 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 78.0%, forg=  7.6% <<<
>>> Test on task  9 : loss=1.601 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.372 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 73.6%, forg=  7.3% <<<
>>> Test on task 11 : loss=1.704 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 62.2%, forg= 17.3% <<<
>>> Test on task 12 : loss=1.207 | TAw acc= 86.4%, forg=  4.9%| TAg acc= 71.8%, forg=  7.8% <<<
>>> Test on task 13 : loss=1.238 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 66.7%, forg= 10.5% <<<
>>> Test on task 14 : loss=1.413 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 68.3%, forg=  8.7% <<<
>>> Test on task 15 : loss=0.796 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 85.9%, forg=  4.0% <<<
>>> Test on task 16 : loss=1.420 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 65.1%, forg=  9.2% <<<
>>> Test on task 17 : loss=0.897 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.7%, forg=  6.2% <<<
>>> Test on task 18 : loss=1.018 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 74.5%, forg=  5.7% <<<
>>> Test on task 19 : loss=1.503 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 20 : loss=1.108 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.4%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.431 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.257 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.5%, forg=  6.3% <<<
>>> Test on task 23 : loss=1.640 | TAw acc= 96.7%, forg=  3.3%| TAg acc= 62.5%, forg= 13.3% <<<
>>> Test on task 24 : loss=0.834 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task 25 : loss=1.892 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 47.7%, forg= 28.8% <<<
>>> Test on task 26 : loss=1.319 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 67.0%, forg= 15.5% <<<
>>> Test on task 27 : loss=1.515 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 70.1%, forg= 17.3% <<<
>>> Test on task 28 : loss=1.278 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 68.0%, forg=  6.0% <<<
>>> Test on task 29 : loss=1.672 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 61.5%, forg= 12.0% <<<
>>> Test on task 30 : loss=1.541 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 65.7%, forg= 10.8% <<<
>>> Test on task 31 : loss=1.128 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 69.9%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.575 | TAw acc= 93.9%, forg=  0.9%| TAg acc= 63.5%, forg= 16.5% <<<
>>> Test on task 33 : loss=1.445 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 54.1%, forg=  7.1% <<<
>>> Test on task 34 : loss=1.677 | TAw acc= 88.9%, forg=  2.0%| TAg acc= 63.6%, forg=  2.0% <<<
>>> Test on task 35 : loss=1.375 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 67.8%, forg= 11.3% <<<
>>> Test on task 36 : loss=1.650 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 53.9%, forg= 21.7% <<<
>>> Test on task 37 : loss=1.068 | TAw acc= 97.5%, forg=  0.8%| TAg acc= 76.3%, forg=  7.6% <<<
>>> Test on task 38 : loss=1.280 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 64.4%, forg=  0.0% <<<
>>> Test on task 39 : loss=0.992 | TAw acc= 93.9%, forg=  1.5%| TAg acc= 76.5%, forg=  2.3% <<<
>>> Test on task 40 : loss=1.376 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 62.1%, forg=  6.8% <<<
>>> Test on task 41 : loss=1.500 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 48.6%, forg= 15.9% <<<
>>> Test on task 42 : loss=1.174 | TAw acc= 88.8%, forg=  2.2%| TAg acc= 64.2%, forg= 13.4% <<<
>>> Test on task 43 : loss=0.903 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 75.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_4/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 81.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 81.9% 
	 88.3%  82.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 85.4% 
	 88.9%  85.2%  91.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 88.6% 
	 87.7%  88.9%  95.4%  94.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.5% 
	 87.7%  92.6%  95.4%  93.1%  92.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.3% 
	 87.1%  88.0%  95.4%  93.1%  95.9%  91.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.9% 
	 90.1%  90.7%  96.3%  93.1%  95.9%  95.4%  98.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 89.5%  93.5%  96.3%  93.1%  95.9%  95.4%  99.2%  93.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.5% 
	 90.1%  92.6%  95.4%  92.2%  96.7%  95.4%  99.2%  95.7%  89.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.1% 
	 88.9%  94.4%  95.4%  93.1%  96.7%  95.4%  99.2%  94.0%  92.4%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.4% 
	 89.5%  94.4%  95.4%  92.2%  96.7%  95.4%  99.2%  94.9%  91.5%  97.1%  93.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.5% 
	 88.3%  93.5%  95.4%  93.1%  96.7%  95.4%  99.2%  94.0%  95.8%  97.1%  93.6%  89.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 88.3%  93.5%  95.4%  93.1%  95.9%  95.4%  99.2%  94.0%  94.9%  97.1%  94.5%  89.8%  91.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.0% 
	 88.9%  95.4%  95.4%  90.2%  96.7%  95.4%  99.2%  94.0%  96.6%  97.1%  94.5%  91.8%  90.3%  92.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.1% 
	 88.3%  93.5%  95.4%  92.2%  95.9%  95.4%  99.2%  94.0%  97.5%  98.1%  94.5%  92.9%  89.3%  95.6%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.4% 
	 88.3%  94.4%  95.4%  92.2%  96.7%  95.4%  99.2%  94.0%  96.6%  97.1%  94.5%  90.8%  89.3%  95.6%  95.2%  98.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.5% 
	 88.9%  93.5%  95.4%  91.2%  96.7%  95.4%  99.2%  94.0%  96.6%  97.1%  94.5%  90.8%  91.3%  95.6%  95.2%  98.0%  94.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 87.1%  92.6%  95.4%  92.2%  96.7%  95.4%  99.2%  94.0%  96.6%  97.1%  94.5%  91.8%  89.3%  95.6%  94.2%  97.0%  96.3%  99.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 88.3%  94.4%  95.4%  93.1%  96.7%  95.4%  99.2%  94.9%  96.6%  97.1%  94.5%  90.8%  87.4%  95.6%  96.2%  98.0%  95.4%  99.1%  98.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.1%  94.4%  96.3%  93.1%  96.7%  95.4%  99.2%  94.0%  97.5%  97.1%  94.5%  90.8%  88.3%  95.6%  96.2%  98.0%  95.4%  99.1%  97.2%  95.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.7%  94.4%  95.4%  93.1%  96.7%  95.4%  99.2%  94.0%  97.5%  97.1%  94.5%  91.8%  90.3%  95.6%  96.2%  98.0%  95.4%  99.1%  98.1%  95.9%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 87.7%  95.4%  95.4%  93.1%  96.7%  95.4%  99.2%  94.0%  97.5%  97.1%  94.5%  91.8%  88.3%  95.6%  96.2%  98.0%  95.4%  99.1%  97.2%  95.1%  97.4%  95.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 87.7%  95.4%  95.4%  93.1%  95.9%  95.4%  99.2%  94.0%  97.5%  97.1%  93.6%  91.8%  90.3%  96.5%  96.2%  98.0%  95.4%  99.1%  98.1%  95.1%  98.3%  97.2%  96.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 87.1%  94.4%  95.4%  91.2%  95.9%  95.4%  99.2%  94.0%  97.5%  97.1%  94.5%  91.8%  87.4%  96.5%  95.2%  99.0%  96.3%  99.1%  98.1%  95.1%  97.4%  97.2%  94.6%  97.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 85.4%  95.4%  95.4%  94.1%  95.9%  95.4%  99.2%  94.0%  97.5%  97.1%  93.6%  91.8%  86.4%  95.6%  95.2%  99.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  97.3% 100.0% 100.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 86.5%  95.4%  95.4%  93.1%  95.9%  95.4%  99.2%  94.0%  97.5%  97.1%  93.6%  91.8%  90.3%  95.6%  95.2%  99.0%  97.2%  99.1%  99.1%  95.1%  98.3%  96.3%  96.4%  99.2%  99.1%  93.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 88.3%  94.4%  95.4%  92.2%  96.7%  95.4%  99.2%  94.0%  97.5%  97.1%  94.5%  93.9%  90.3%  95.6%  95.2%  99.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  96.4%  97.5%  99.1%  92.8%  97.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 87.1%  92.6%  95.4%  93.1%  96.7%  94.4%  99.2%  94.0%  97.5%  97.1%  94.5%  93.9%  89.3%  95.6%  95.2%  99.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  96.4%  96.7%  99.1%  93.7%  97.9%  97.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 86.0%  93.5%  95.4%  93.1%  95.9%  95.4%  99.2%  94.0%  97.5%  97.1%  94.5%  91.8%  90.3%  95.6%  95.2%  99.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  96.4%  96.7%  99.1%  94.6%  97.9%  97.6%  97.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 87.7%  92.6%  96.3%  91.2%  95.9%  94.4%  99.2%  94.9%  97.5%  97.1%  94.5%  91.8%  89.3%  95.6%  95.2%  99.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  96.4%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  96.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 86.5%  92.6%  95.4%  93.1%  96.7%  94.4%  99.2%  94.9%  97.5%  97.1%  94.5%  91.8%  90.3%  95.6%  95.2%  99.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  96.4%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  96.6%  96.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 86.5%  93.5%  95.4%  91.2%  96.7%  93.5%  99.2%  94.9%  97.5%  97.1%  94.5%  91.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  97.3%  96.7% 100.0%  95.5%  97.9%  97.6%  98.0%  96.6%  96.1%  98.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 86.5%  92.6%  95.4%  93.1%  96.7%  95.4%  99.2%  94.9%  97.5%  97.1%  94.5%  91.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  96.4%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  97.4%  97.1%  97.8%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 87.1%  93.5%  95.4%  91.2%  96.7%  94.4%  99.2%  94.9%  97.5%  97.1%  94.5%  92.9%  89.3%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  97.4%  97.2%  96.4%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  97.4%  97.1%  98.9%  94.8%  96.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 86.5%  93.5%  96.3%  93.1%  96.7%  93.5%  99.2%  94.9%  97.5%  97.1%  94.5%  91.8%  89.3%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  94.3%  97.4%  97.2%  97.3%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  96.6%  97.1%  98.9%  94.8%  96.9%  89.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 85.4%  93.5%  95.4%  93.1%  96.7%  94.4%  99.2%  94.9%  97.5%  97.1%  94.5%  91.8%  86.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  97.3%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  96.6%  97.1%  98.9%  94.8%  95.9%  88.9%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 87.1%  93.5%  95.4%  93.1%  96.7%  95.4%  98.3%  94.9%  97.5%  97.1%  94.5%  91.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  97.2%  97.3%  96.7% 100.0%  93.7%  97.9%  97.6%  98.0%  96.6%  97.1%  98.9%  94.8%  95.9%  89.9%  95.7%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 86.0%  92.6%  95.4%  93.1%  96.7%  95.4%  98.3%  94.9%  97.5%  97.1%  94.5%  91.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  96.3%  96.4%  96.7% 100.0%  94.6%  97.9%  97.6%  99.0%  97.4%  96.1%  98.9%  94.8%  96.9%  88.9%  94.8%  96.5%  97.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 87.1%  93.5%  95.4%  93.1%  96.7%  95.4%  98.3%  94.9%  97.5%  97.1%  94.5%  91.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  97.4%  96.3%  96.4%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  97.4%  97.1%  98.9%  94.8%  96.9%  90.9%  94.8%  95.7%  98.3%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 86.0%  91.7%  95.4%  92.2%  96.7%  95.4%  98.3%  94.9%  97.5%  97.1%  94.5%  93.9%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1% 100.0%  95.1%  97.4%  96.3%  97.3%  96.7% 100.0%  93.7%  97.9%  97.6%  98.0%  97.4%  97.1%  98.9%  94.8%  96.9%  87.9%  94.8%  95.7%  97.5%  94.2%  94.7%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 87.7%  91.7%  95.4%  93.1%  96.7%  96.3%  98.3%  94.9%  97.5%  97.1%  94.5%  91.8%  86.4%  95.6%  95.2%  98.0%  98.2% 100.0% 100.0%  95.1%  97.4%  97.2%  97.3%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  96.6%  97.1%  98.9%  94.8%  96.9%  87.9%  94.8%  95.7%  98.3%  93.3%  93.9%  93.2%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 86.5%  91.7%  95.4%  93.1%  96.7%  95.4%  98.3%  94.9%  97.5%  97.1%  94.5%  91.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  96.3%  97.3%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  97.4%  97.1%  98.9%  93.9%  98.0%  89.9%  94.8%  94.8%  98.3%  93.3%  93.9%  94.2%  98.1%   0.0%   0.0% 	Avg.: 95.7% 
	 86.5%  92.6%  95.4%  91.2%  96.7%  96.3%  98.3%  94.9%  97.5%  97.1%  94.5%  90.8%  87.4%  95.6%  95.2%  98.0%  98.2%  99.1%  99.1%  95.1%  98.3%  96.3%  97.3%  96.7% 100.0%  92.8%  97.9%  97.6%  98.0%  96.6%  97.1%  98.9%  93.9%  98.0%  90.9%  94.8%  95.7%  97.5%  93.3%  95.5%  95.1%  97.2%  91.0%   0.0% 	Avg.: 95.6% 
	 87.1%  93.5%  95.4%  93.1%  96.7%  96.3%  98.3%  94.9%  97.5%  97.1%  94.5%  91.8%  86.4%  95.6%  95.2%  98.0%  98.2% 100.0%  99.1%  95.1%  98.3%  96.3%  97.3%  96.7% 100.0%  94.6%  97.9%  97.6%  98.0%  96.6%  97.1%  98.9%  93.9%  98.0%  88.9%  94.8%  95.7%  97.5%  93.3%  93.9%  95.1%  97.2%  88.8%  94.6% 	Avg.: 95.6% 
************************************************************************************************************
TAg Acc
	 81.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 81.9% 
	 76.0%  75.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.0% 
	 78.9%  54.6%  87.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.6% 
	 67.8%  67.6%  67.9%  85.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.2% 
	 77.2%  63.9%  77.1%  68.6%  74.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.2% 
	 81.3%  65.7%  76.1%  76.5%  76.0%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.4% 
	 78.9%  67.6%  84.4%  75.5%  70.2%  64.8%  79.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.4% 
	 76.6%  59.3%  79.8%  78.4%  71.1%  75.9%  63.9%  87.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.0% 
	 78.9%  59.3%  72.5%  73.5%  80.2%  72.2%  74.8%  70.9%  82.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.8% 
	 74.3%  67.6%  78.9%  78.4%  82.6%  82.4%  79.0%  77.8%  63.6%  79.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.4% 
	 72.5%  65.7%  79.8%  73.5%  81.0%  81.5%  83.2%  82.9%  73.7%  63.5%  68.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.0% 
	 71.9%  70.4%  78.0%  75.5%  79.3%  76.9%  83.2%  79.5%  80.5%  69.2%  61.8%  79.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.5% 
	 78.4%  65.7%  68.8%  77.5%  81.8%  75.0%  84.0%  76.9%  79.7%  66.3%  65.5%  65.3%  78.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.1% 
	 70.2%  63.9%  76.1%  72.5%  81.8%  75.9%  80.7%  82.9%  83.9%  65.4%  75.5%  63.3%  71.8%  77.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.4% 
	 75.4%  68.5%  77.1%  73.5%  79.3%  80.6%  79.8%  80.3%  79.7%  69.2%  70.9%  63.3%  72.8%  64.9%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.2% 
	 74.3%  63.0%  76.1%  77.5%  80.2%  78.7%  81.5%  75.2%  83.9%  73.1%  75.5%  62.2%  74.8%  64.0%  63.5%  78.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.9% 
	 76.0%  63.9%  73.4%  74.5%  77.7%  80.6%  82.4%  81.2%  85.6%  69.2%  75.5%  63.3%  79.6%  69.3%  63.5%  62.6%  67.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.2% 
	 71.9%  66.7%  67.9%  67.6%  81.8%  79.6%  81.5%  83.8%  78.8%  69.2%  77.3%  63.3%  78.6%  74.6%  68.3%  68.7%  55.0%  81.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.1% 
	 73.1%  62.0%  74.3%  74.5%  80.2%  73.1%  82.4%  81.2%  84.7%  72.1%  79.1%  61.2%  77.7%  69.3%  62.5%  75.8%  62.4%  58.0%  80.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.8% 
	 71.9%  60.2%  76.1%  70.6%  79.3%  78.7%  82.4%  83.8%  83.1%  71.2%  79.1%  66.3%  77.7%  73.7%  66.3%  76.8%  66.1%  73.2%  64.2%  78.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.9% 
	 66.7%  63.0%  71.6%  73.5%  82.6%  78.7%  74.8%  80.3%  83.1%  65.4%  78.2%  68.4%  78.6%  71.9%  63.5%  77.8%  65.1%  69.6%  72.6%  57.7%  82.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.7% 
	 70.8%  61.1%  72.5%  69.6%  81.8%  74.1%  81.5%  84.6%  83.9%  66.3%  80.0%  66.3%  75.7%  74.6%  70.2%  82.8%  69.7%  71.4%  74.5%  58.5%  61.7%  75.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.0% 
	 69.0%  61.1%  76.1%  65.7%  80.2%  76.9%  81.5%  82.9%  83.1%  72.1%  80.9%  64.3%  77.7%  72.8%  73.1%  82.8%  67.9%  74.1%  67.9%  60.2%  68.7%  56.9%  83.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.0% 
	 64.9%  57.4%  75.2%  67.6%  81.8%  78.7%  82.4%  82.9%  82.2%  69.2%  78.2%  66.3%  75.7%  75.4%  73.1%  75.8%  69.7%  74.1%  71.7%  64.2%  69.6%  60.6%  69.4%  75.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.6% 
	 69.0%  60.2%  75.2%  69.6%  81.0%  76.9%  83.2%  84.6%  83.9%  73.1%  75.5%  66.3%  70.9%  74.6%  70.2%  81.8%  74.3%  75.9%  71.7%  60.2%  75.7%  66.1%  67.6%  55.8%  81.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.0% 
	 69.6%  60.2%  74.3%  66.7%  81.0%  73.1%  81.5%  84.6%  80.5%  71.2%  71.8%  66.3%  75.7%  69.3%  67.3%  80.8%  72.5%  75.0%  76.4%  60.2%  75.7%  67.0%  74.8%  57.5%  64.2%  76.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.1% 
	 66.1%  62.0%  72.5%  68.6%  80.2%  73.1%  83.2%  82.9%  80.5%  72.1%  78.2%  71.4%  79.6%  69.3%  67.3%  81.8%  69.7%  78.6%  65.1%  62.6%  74.8%  66.1%  67.6%  62.5%  73.4%  43.2%  82.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.7% 
	 64.3%  60.2%  76.1%  70.6%  81.8%  72.2%  79.8%  84.6%  82.2%  72.1%  79.1%  72.4%  76.7%  70.2%  70.2%  84.8%  68.8%  78.6%  63.2%  64.2%  75.7%  66.1%  80.2%  59.2%  63.3%  39.6%  70.1%  87.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.9% 
	 63.7%  58.3%  72.5%  71.6%  81.8%  73.1%  82.4%  85.5%  82.2%  67.3%  73.6%  63.3%  77.7%  69.3%  68.3%  83.8%  69.7%  78.6%  74.5%  66.7%  74.8%  67.0%  75.7%  61.7%  71.6%  49.5%  63.9%  69.3%  74.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.4% 
	 69.0%  57.4%  71.6%  64.7%  81.8%  73.1%  74.8%  82.1%  83.9%  69.2%  75.5%  68.4%  74.8%  71.1%  65.4%  83.8%  73.4%  77.7%  72.6%  62.6%  69.6%  69.7%  80.2%  59.2%  74.3%  46.8%  61.9%  67.7%  49.0%  73.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.2% 
	 64.9%  56.5%  78.0%  64.7%  82.6%  72.2%  78.2%  82.9%  84.7%  70.2%  76.4%  67.3%  76.7%  69.3%  71.2%  83.8%  67.0%  81.2%  69.8%  64.2%  68.7%  69.7%  77.5%  56.7%  74.3%  48.6%  61.9%  71.7%  63.0%  52.1%  76.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.4% 
	 65.5%  58.3%  73.4%  61.8%  81.0%  74.1%  81.5%  82.9%  84.7%  71.2%  75.5%  64.3%  72.8%  67.5%  67.3%  85.9%  67.0%  83.0%  71.7%  66.7%  73.9%  68.8%  76.6%  60.0%  77.1%  53.2%  70.1%  72.4%  61.0%  53.0%  55.9%  65.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.1% 
	 62.6%  59.3%  71.6%  60.8%  81.0%  72.2%  82.4%  84.6%  81.4%  71.2%  80.0%  64.3%  75.7%  70.2%  71.2%  85.9%  71.6%  80.4%  70.8%  65.0%  71.3%  68.8%  78.4%  60.0%  76.1%  46.8%  68.0%  70.1%  62.0%  53.8%  58.8%  55.9%  80.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.1% 
	 65.5%  57.4%  76.1%  57.8%  81.8%  71.3%  79.0%  82.9%  84.7%  71.2%  76.4%  67.3%  77.7%  70.2%  68.3%  85.9%  72.5%  82.1%  72.6%  69.1%  73.9%  67.0%  80.2%  57.5%  69.7%  49.5%  66.0%  71.7%  64.0%  56.4%  60.8%  60.2%  59.1%  61.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.6% 
	 64.3%  57.4%  76.1%  59.8%  80.2%  74.1%  81.5%  84.6%  83.1%  71.2%  74.5%  70.4%  76.7%  69.3%  67.3%  88.9%  67.0%  82.1%  65.1%  69.9%  73.9%  67.0%  76.6%  65.0%  78.0%  48.6%  72.2%  70.1%  64.0%  57.3%  64.7%  60.2%  60.0%  57.1%  64.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.8% 
	 60.8%  55.6%  72.5%  63.7%  81.8%  75.9%  79.8%  84.6%  81.4%  71.2%  75.5%  63.3%  72.8%  70.2%  70.2%  87.9%  67.9%  80.4%  66.0%  65.0%  74.8%  67.0%  79.3%  60.8%  76.1%  50.5%  70.1%  71.7%  64.0%  57.3%  61.8%  64.5%  64.3%  46.9%  52.5%  79.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.1% 
	 60.8%  56.5%  74.3%  63.7%  77.7%  67.6%  77.3%  81.2%  77.1%  71.2%  73.6%  65.3%  72.8%  66.7%  70.2%  85.9%  69.7%  82.1%  68.9%  60.2%  73.9%  61.5%  76.6%  61.7%  78.9%  47.7%  70.1%  69.3%  64.0%  59.8%  65.7%  64.5%  61.7%  53.1%  53.5%  64.3%  75.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 61.4%  59.3%  76.1%  58.8%  81.8%  70.4%  78.2%  82.9%  82.2%  63.5%  75.5%  62.2%  73.8%  70.2%  70.2%  82.8%  72.5%  83.9%  68.9%  66.7%  77.4%  69.7%  75.7%  63.3%  74.3%  48.6%  69.1%  69.3%  62.0%  62.4%  64.7%  68.8%  62.6%  55.1%  56.6%  60.9%  47.8%  83.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.8% 
	 62.6%  55.6%  70.6%  55.9%  80.2%  73.1%  82.4%  84.6%  78.0%  70.2%  76.4%  62.2%  75.7%  70.2%  70.2%  85.9%  65.1%  81.2%  65.1%  63.4%  76.5%  67.0%  77.5%  64.2%  77.1%  47.7%  75.3%  70.9%  63.0%  59.0%  62.7%  63.4%  68.7%  52.0%  60.6%  67.8%  50.4%  74.6%  64.4%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.5% 
	 62.0%  54.6%  73.4%  62.7%  81.8%  69.4%  81.5%  82.1%  78.8%  66.3%  71.8%  63.3%  75.7%  67.5%  71.2%  86.9%  69.7%  79.5%  72.6%  66.7%  75.7%  70.6%  75.7%  60.8%  76.1%  52.3%  70.1%  65.4%  64.0%  59.8%  66.7%  65.6%  60.9%  53.1%  58.6%  67.0%  52.2%  67.8%  51.9%  78.8%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.3% 
	 62.6%  54.6%  74.3%  61.8%  80.2%  71.3%  81.5%  82.1%  78.0%  71.2%  75.5%  60.2%  75.7%  68.4%  70.2%  85.9%  67.9%  82.1%  69.8%  64.2%  77.4%  64.2%  78.4%  63.3%  77.1%  51.4%  71.1%  66.9%  62.0%  59.0%  66.7%  68.8%  58.3%  51.0%  63.6%  68.7%  55.7%  70.3%  60.6%  73.5%  68.9%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 59.6%  58.3%  71.6%  60.8%  78.5%  69.4%  77.3%  81.2%  78.8%  69.2%  77.3%  63.3%  76.7%  67.5%  74.0%  89.9%  63.3%  79.5%  70.8%  65.9%  75.7%  67.0%  75.7%  60.0%  78.0%  51.4%  68.0%  68.5%  59.0%  59.8%  66.7%  69.9%  67.8%  53.1%  63.6%  68.7%  55.7%  77.1%  59.6%  74.2%  59.2%  64.5%   0.0%   0.0% 	Avg.: 68.5% 
	 62.0%  55.6%  67.9%  58.8%  74.4%  73.1%  79.8%  82.1%  77.1%  69.2%  72.7%  63.3%  77.7%  71.1%  68.3%  83.8%  68.8%  76.8%  63.2%  64.2%  78.3%  64.2%  75.7%  60.0%  74.3%  47.7%  69.1%  68.5%  62.0%  58.1%  65.7%  67.7%  64.3%  57.1%  65.7%  65.2%  56.5%  73.7%  60.6%  75.8%  57.3%  43.0%  77.6%   0.0% 	Avg.: 67.4% 
	 62.0%  55.6%  71.6%  61.8%  77.7%  72.2%  77.3%  83.8%  78.0%  69.2%  73.6%  62.2%  71.8%  66.7%  68.3%  85.9%  65.1%  77.7%  74.5%  64.2%  77.4%  66.1%  77.5%  62.5%  75.2%  47.7%  67.0%  70.1%  68.0%  61.5%  65.7%  69.9%  63.5%  54.1%  63.6%  67.8%  53.9%  76.3%  64.4%  76.5%  62.1%  48.6%  64.2%  75.7% 	Avg.: 68.1% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -6.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -6.4% 
	 -0.6%  -2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.7% 
	  1.2%  -3.7%  -3.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.1% 
	  1.2%  -3.7%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.4% 
	  1.8%   4.6%   0.0%   1.0%  -3.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	 -1.2%   1.9%  -0.9%   1.0%   0.0%  -3.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.5% 
	  0.6%  -0.9%   0.0%   1.0%   0.0%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.0% 
	  0.0%   0.9%   0.9%   2.0%  -0.8%   0.0%   0.0%  -2.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  1.2%  -0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   1.7%  -2.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.6%   0.0%   0.9%   2.0%   0.0%   0.0%   0.0%   0.9%   0.8%  -2.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.8%   0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   1.7%  -3.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.8%   0.9%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.8%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.2%  -0.9%   0.9%   3.9%   0.0%   0.0%   0.0%   1.7%  -0.8%   0.0%   0.0%  -2.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.8%   1.9%   0.9%   2.0%   0.8%   0.0%   0.0%   1.7%  -0.8%  -1.0%   0.0%  -1.0%   1.9%  -3.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.8%   0.9%   0.9%   2.0%   0.0%   0.0%   0.0%   1.7%   0.8%   1.0%   0.0%   2.0%   1.9%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.2%   1.9%   0.9%   2.9%   0.0%   0.0%   0.0%   1.7%   0.8%   1.0%   0.0%   2.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  2.9%   2.8%   0.9%   2.0%   0.0%   0.0%   0.0%   1.7%   0.8%   1.0%   0.0%   1.0%   1.9%   0.0%   1.0%   1.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  1.8%   0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   0.9%   0.8%   1.0%   0.0%   2.0%   3.9%   0.0%  -1.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  2.9%   0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   1.7%   0.0%   1.0%   0.0%   2.0%   2.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  2.3%   0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   1.7%   0.0%   1.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  2.3%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   1.7%   0.0%   1.0%   0.0%   1.0%   2.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.8%  -2.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  2.3%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.0%   1.0%   0.9%   1.0%   1.0%  -0.9%   0.0%   0.0%   0.9%   0.0%   0.0%   0.8%  -0.9%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  2.9%   0.9%   0.9%   2.9%   0.8%   0.0%   0.0%   1.7%   0.0%   1.0%   0.0%   1.0%   3.9%   0.0%   1.0%  -1.0%   0.0%   0.0%   0.0%   0.8%   0.9%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  4.7%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   1.7%   0.0%   1.0%   0.9%   1.0%   4.9%   0.9%   1.0%   0.0%  -1.8%   0.0%  -0.9%   0.8%   0.0%   0.0%  -0.9%  -2.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  3.5%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.0%   1.0%   0.9%   1.0%   1.0%   0.9%   1.0%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%   0.9%   0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.8%   0.9%   0.9%   2.0%   0.0%   0.0%   0.0%   1.7%   0.0%   1.0%   0.0%  -1.0%   1.0%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.9%   2.5%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  2.9%   2.8%   0.9%   1.0%   0.0%   0.9%   0.0%   1.7%   0.0%   1.0%   0.0%   0.0%   1.9%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.9%   3.3%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  4.1%   1.9%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.0%   1.0%   0.0%   2.0%   1.0%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.9%   3.3%   0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  2.3%   2.8%   0.0%   2.9%   0.8%   0.9%   0.0%   0.9%   0.0%   1.0%   0.0%   2.0%   1.9%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.9%   3.3%   0.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  3.5%   2.8%   0.9%   1.0%   0.0%   0.9%   0.0%   0.9%   0.0%   1.0%   0.0%   2.0%   1.0%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.9%   3.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  3.5%   1.9%   0.9%   2.9%   0.0%   1.9%   0.0%   0.9%   0.0%   1.0%   0.0%   2.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   3.3%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  3.5%   2.8%   0.9%   1.0%   0.0%   0.0%   0.0%   0.9%   0.0%   1.0%   0.0%   2.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.9%   3.3%   0.0%   0.9%   0.0%   0.0%   0.0%  -0.9%  -1.0%   1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  2.9%   1.9%   0.9%   2.9%   0.0%   0.9%   0.0%   0.9%   0.0%   1.0%   0.0%   1.0%   1.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.9%   0.0%   0.9%   3.3%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  3.5%   1.9%   0.0%   1.0%   0.0%   1.9%   0.0%   0.9%   0.0%   1.0%   0.0%   2.0%   1.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   1.6%   0.9%   0.0%   0.0%   3.3%   0.0%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  4.7%   1.9%   0.9%   1.0%   0.0%   0.9%   0.0%   0.9%   0.0%   1.0%   0.0%   2.0%   4.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   3.3%   0.0%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  2.9%   1.9%   0.9%   1.0%   0.0%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   2.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   3.3%   0.0%   1.8%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   1.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  4.1%   2.8%   0.9%   1.0%   0.0%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   2.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.9%   0.9%   3.3%   0.0%   0.9%   0.0%   0.0%  -1.0%   0.0%   1.0%   0.0%   0.0%   0.0%   1.0%   0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  2.9%   1.9%   0.9%   1.0%   0.0%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   2.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.9%   0.9%   0.9%   3.3%   0.0%   0.9%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%  -1.0%   0.9%   0.9%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  4.1%   3.7%   0.9%   2.0%   0.0%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   0.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.0%  -0.9%   0.8%   0.9%   0.9%   0.0%   3.3%   0.0%   1.8%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   3.0%   0.9%   0.9%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  2.3%   3.7%   0.9%   1.0%   0.0%  -0.9%   0.8%   0.9%   0.0%   1.0%   0.0%   2.0%   4.9%   0.9%   1.0%   1.0%   0.0%  -0.9%   0.0%   0.8%   0.9%   0.0%   0.0%   3.3%   0.0%   0.9%   0.0%   0.0%   1.0%   0.9%   0.0%   0.0%   0.0%   0.0%   3.0%   0.9%   0.9%   0.0%   1.0%   0.8%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  3.5%   3.7%   0.9%   1.0%   0.0%   0.9%   0.8%   0.9%   0.0%   1.0%   0.0%   2.0%   3.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.9%   0.8%   0.0%   0.9%   0.0%   3.3%   0.0%   0.9%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.9%  -1.0%   1.0%   0.9%   1.7%   0.0%   1.0%   0.8%  -1.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  3.5%   2.8%   0.9%   2.9%   0.0%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   3.1%   3.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.9%   0.8%   0.0%   0.9%   0.0%   3.3%   0.0%   2.7%   0.0%   0.0%   1.0%   0.9%   0.0%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   0.8%   1.0%  -0.8%  -1.0%   0.9%   0.0%   0.0% 	Avg.:  0.9% 
	  2.9%   1.9%   0.9%   1.0%   0.0%   0.0%   0.8%   0.9%   0.0%   1.0%   0.0%   2.0%   4.9%   0.9%   1.0%   1.0%   0.0%   0.0%   0.9%   0.8%   0.0%   0.9%   0.0%   3.3%   0.0%   0.9%   0.0%   0.0%   1.0%   0.9%   0.0%   0.0%   0.9%   0.0%   2.0%   0.9%   0.9%   0.8%   1.0%   1.5%   0.0%   0.9%   2.2%   0.0% 	Avg.:  0.9% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  5.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  2.9%  21.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 12.1% 
	 14.0%   8.3%  19.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 13.9% 
	  4.7%  12.0%  10.1%  16.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.9% 
	  0.6%  10.2%  11.0%   8.8%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  2.9%   8.3%   2.8%   9.8%   5.8%  12.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	  5.3%  16.7%   7.3%   6.9%   5.0%   0.9%  15.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.2% 
	  2.9%  16.7%  14.7%  11.8%  -4.1%   4.6%   4.2%  16.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	  7.6%   8.3%   8.3%   6.9%  -2.5%  -5.6%   0.0%   9.4%  18.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.7% 
	  9.4%  10.2%   7.3%  11.8%   1.7%   0.9%  -4.2%   4.3%   8.5%  16.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.6% 
	  9.9%   5.6%   9.2%   9.8%   3.3%   5.6%   0.0%   7.7%   1.7%  10.6%   6.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	  3.5%  10.2%  18.3%   7.8%   0.8%   7.4%  -0.8%  10.3%   2.5%  13.5%   2.7%  14.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	 11.7%  12.0%  11.0%  12.7%   0.8%   6.5%   3.4%   4.3%  -1.7%  14.4%  -7.3%  16.3%   6.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.0% 
	  6.4%   7.4%  10.1%  11.8%   3.3%   1.9%   4.2%   6.8%   4.2%  10.6%   4.5%  16.3%   5.8%  12.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	  7.6%  13.0%  11.0%   7.8%   2.5%   3.7%   2.5%  12.0%   0.0%   6.7%   0.0%  17.3%   3.9%  13.2%  13.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.6% 
	  5.8%  12.0%  13.8%  10.8%   5.0%   1.9%   1.7%   6.0%  -1.7%  10.6%   0.0%  16.3%  -1.0%   7.9%  13.5%  16.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.4% 
	  9.9%   9.3%  19.3%  17.6%   0.8%   2.8%   2.5%   3.4%   6.8%  10.6%  -1.8%  16.3%   1.0%   2.6%   8.7%  10.1%  11.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.8% 
	  8.8%  13.9%  12.8%  10.8%   2.5%   9.3%   1.7%   6.0%   0.8%   7.7%  -1.8%  18.4%   1.9%   7.9%  14.4%   3.0%   4.6%  23.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.1% 
	  9.9%  15.7%  11.0%  14.7%   3.3%   3.7%   1.7%   3.4%   2.5%   8.7%   0.0%  13.3%   1.9%   3.5%  10.6%   2.0%   0.9%   8.0%  16.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	 15.2%  13.0%  15.6%  11.8%   0.0%   3.7%   9.2%   6.8%   2.5%  14.4%   0.9%  11.2%   1.0%   5.3%  13.5%   1.0%   1.8%  11.6%   7.5%  20.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.3% 
	 11.1%  14.8%  14.7%  15.7%   0.8%   8.3%   2.5%   2.6%   1.7%  13.5%  -0.9%  13.3%   3.9%   2.6%   6.7%  -4.0%  -2.8%   9.8%   5.7%  19.5%  20.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.6% 
	 12.9%  14.8%  11.0%  19.6%   2.5%   5.6%   2.5%   4.3%   2.5%   7.7%  -0.9%  15.3%   1.9%   4.4%   3.8%   0.0%   1.8%   7.1%  12.3%  17.9%  13.9%  18.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.2% 
	 17.0%  18.5%  11.9%  17.6%   0.8%   3.7%   1.7%   4.3%   3.4%  10.6%   2.7%  13.3%   3.9%   1.8%   3.8%   7.1%   0.0%   7.1%   8.5%  13.8%  13.0%  14.7%  14.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 12.9%  15.7%  11.9%  15.7%   1.7%   5.6%   0.8%   2.6%   1.7%   6.7%   5.5%  13.3%   8.7%   2.6%   6.7%   1.0%  -4.6%   5.4%   8.5%  17.9%   7.0%   9.2%  16.2%  20.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.0% 
	 12.3%  15.7%  12.8%  18.6%   1.7%   9.3%   2.5%   2.6%   5.1%   8.7%   9.1%  13.3%   3.9%   7.9%   9.6%   2.0%   1.8%   6.2%   3.8%  17.9%   7.0%   8.3%   9.0%  18.3%  17.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.0% 
	 15.8%  13.9%  14.7%  16.7%   2.5%   9.3%   0.8%   4.3%   5.1%   7.7%   2.7%   8.2%   0.0%   7.9%   9.6%   1.0%   4.6%   2.7%  15.1%  15.4%   7.8%   9.2%  16.2%  13.3%   8.3%  33.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.5% 
	 17.5%  15.7%  11.0%  14.7%   0.8%  10.2%   4.2%   2.6%   3.4%   7.7%   1.8%   7.1%   2.9%   7.0%   6.7%  -2.0%   5.5%   2.7%  17.0%  13.8%   7.0%   9.2%   3.6%  16.7%  18.3%  36.9%  12.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 18.1%  17.6%  14.7%  13.7%   0.8%   9.3%   1.7%   1.7%   3.4%  12.5%   7.3%  16.3%   1.9%   7.9%   8.7%   1.0%   4.6%   2.7%   5.7%  11.4%   7.8%   8.3%   8.1%  14.2%  10.1%  27.0%  18.6%  18.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 12.9%  18.5%  15.6%  20.6%   0.8%   9.3%   9.2%   5.1%   1.7%  10.6%   5.5%  11.2%   4.9%   6.1%  11.5%   1.0%   0.9%   3.6%   7.5%  15.4%  13.0%   5.5%   3.6%  16.7%   7.3%  29.7%  20.6%  19.7%  25.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.8% 
	 17.0%  19.4%   9.2%  20.6%   0.0%  10.2%   5.9%   4.3%   0.8%   9.6%   4.5%  12.2%   2.9%   7.9%   5.8%   1.0%   7.3%   0.0%  10.4%  13.8%  13.9%   5.5%   6.3%  19.2%   7.3%  27.9%  20.6%  15.7%  11.0%  21.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.4% 
	 16.4%  17.6%  13.8%  23.5%   1.7%   8.3%   2.5%   4.3%   0.8%   8.7%   5.5%  15.3%   6.8%   9.6%   9.6%  -1.0%   7.3%  -1.8%   8.5%  11.4%   8.7%   6.4%   7.2%  15.8%   4.6%  23.4%  12.4%  15.0%  13.0%  20.5%  20.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.2% 
	 19.3%  16.7%  15.6%  24.5%   1.7%  10.2%   1.7%   2.6%   4.2%   8.7%   0.9%  15.3%   3.9%   7.0%   5.8%   0.0%   2.8%   2.7%   9.4%  13.0%  11.3%   6.4%   5.4%  15.8%   5.5%  29.7%  14.4%  17.3%  12.0%  19.7%  17.6%   9.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.3% 
	 16.4%  18.5%  11.0%  27.5%   0.8%  11.1%   5.0%   4.3%   0.8%   8.7%   4.5%  12.2%   1.9%   7.0%   8.7%   0.0%   1.8%   0.9%   7.5%   8.9%   8.7%   8.3%   3.6%  18.3%  11.9%  27.0%  16.5%  15.7%  10.0%  17.1%  15.7%   5.4%  20.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.2% 
	 17.5%  18.5%  11.0%  25.5%   2.5%   8.3%   2.5%   2.6%   2.5%   8.7%   6.4%   9.2%   2.9%   7.9%   9.6%  -3.0%   7.3%   0.9%  15.1%   8.1%   8.7%   8.3%   7.2%  10.8%   3.7%  27.9%  10.3%  17.3%  10.0%  16.2%  11.8%   5.4%  20.0%   4.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.6% 
	 21.1%  20.4%  14.7%  21.6%   0.8%   6.5%   4.2%   2.6%   4.2%   8.7%   5.5%  16.3%   6.8%   7.0%   6.7%   1.0%   6.4%   2.7%  14.2%  13.0%   7.8%   8.3%   4.5%  15.0%   5.5%  26.1%  12.4%  15.7%  10.0%  16.2%  14.7%   1.1%  15.7%  14.3%  12.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.4% 
	 21.1%  19.4%  12.8%  21.6%   5.0%  14.8%   6.7%   6.0%   8.5%   8.7%   7.3%  14.3%   6.8%  10.5%   6.7%   3.0%   4.6%   0.9%  11.3%  17.9%   8.7%  13.8%   7.2%  14.2%   2.8%  28.8%  12.4%  18.1%  10.0%  13.7%  10.8%   1.1%  18.3%   8.2%  11.1%  14.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.2% 
	 20.5%  16.7%  11.0%  26.5%   0.8%  12.0%   5.9%   4.3%   3.4%  16.3%   5.5%  17.3%   5.8%   7.0%   6.7%   6.1%   1.8%  -0.9%  11.3%  11.4%   5.2%   5.5%   8.1%  12.5%   7.3%  27.9%  13.4%  18.1%  12.0%  11.1%  11.8%  -3.2%  17.4%   6.1%   8.1%  18.3%  27.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.7% 
	 19.3%  20.4%  16.5%  29.4%   2.5%   9.3%   1.7%   2.6%   7.6%   9.6%   4.5%  17.3%   3.9%   7.0%   6.7%   3.0%   9.2%   2.7%  15.1%  14.6%   6.1%   8.3%   6.3%  11.7%   4.6%  28.8%   7.2%  16.5%  11.0%  14.5%  13.7%   5.4%  11.3%   9.2%   4.0%  11.3%  25.2%   9.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.7% 
	 19.9%  21.3%  13.8%  22.5%   0.8%  13.0%   2.5%   5.1%   6.8%  13.5%   9.1%  16.3%   3.9%   9.6%   5.8%   2.0%   4.6%   4.5%   7.5%  11.4%   7.0%   4.6%   8.1%  15.0%   5.5%  24.3%  12.4%  22.0%  10.0%  13.7%   9.8%   3.2%  19.1%   8.2%   6.1%  12.2%  23.5%  16.1%  12.5%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.0% 
	 19.3%  21.3%  12.8%  23.5%   2.5%  11.1%   2.5%   5.1%   7.6%   8.7%   5.5%  19.4%   3.9%   8.8%   6.7%   3.0%   6.4%   1.8%  10.4%  13.8%   5.2%  11.0%   5.4%  12.5%   4.6%  25.2%  11.3%  20.5%  12.0%  14.5%   9.8%   0.0%  21.7%  10.2%   1.0%  10.4%  20.0%  13.6%   3.8%   5.3%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.3% 
	 22.2%  17.6%  15.6%  24.5%   4.1%  13.0%   6.7%   6.0%   6.8%  10.6%   3.6%  16.3%   2.9%   9.6%   2.9%  -1.0%  11.0%   4.5%   9.4%  12.2%   7.0%   8.3%   8.1%  15.8%   3.7%  25.2%  14.4%  18.9%  15.0%  13.7%   9.8%  -1.1%  12.2%   8.2%   1.0%  10.4%  20.0%   6.8%   4.8%   4.5%   9.7%   0.0%   0.0%   0.0% 	Avg.: 10.1% 
	 19.9%  20.4%  19.3%  26.5%   8.3%   9.3%   4.2%   5.1%   8.5%  10.6%   8.2%  16.3%   1.9%   6.1%   8.7%   6.1%   5.5%   7.1%  17.0%  13.8%   4.3%  11.0%   8.1%  15.8%   7.3%  28.8%  13.4%  18.9%  12.0%  15.4%  10.8%   2.2%  15.7%   4.1%  -1.0%  13.9%  19.1%  10.2%   3.8%   3.0%  11.7%  21.5%   0.0%   0.0% 	Avg.: 11.3% 
	 19.9%  20.4%  15.6%  23.5%   5.0%  10.2%   6.7%   3.4%   7.6%  10.6%   7.3%  17.3%   7.8%  10.5%   8.7%   4.0%   9.2%   6.2%   5.7%  13.8%   5.2%   9.2%   6.3%  13.3%   6.4%  28.8%  15.5%  17.3%   6.0%  12.0%  10.8%   0.0%  16.5%   7.1%   2.0%  11.3%  21.7%   7.6%   0.0%   2.3%   6.8%  15.9%  13.4%   0.0% 	Avg.: 10.4% 
************************************************************************************************************
[Elapsed time = 0.5 h]
Done!

f1_score_micro: 0.681919869839333
f1_score_macro: 0.6511953986824375
              precision    recall  f1-score   support

           0       0.73      0.89      0.80         9
           1       0.00      0.00      0.00         4
           2       0.67      1.00      0.80         4
           3       0.57      1.00      0.73         4
           4       0.07      0.25      0.11         4
           5       1.00      1.00      1.00         4
           6       0.82      1.00      0.90         9
           7       1.00      1.00      1.00         4
           8       0.60      0.75      0.67         4
           9       1.00      1.00      1.00         5
          10       0.60      0.60      0.60         5
          11       1.00      0.75      0.86         4
          12       0.50      0.80      0.62         5
          13       0.71      1.00      0.83         5
          14       0.80      0.80      0.80         5
          15       0.80      1.00      0.89         4
          16       1.00      1.00      1.00         4
          17       1.00      0.50      0.67         4
          18       0.44      1.00      0.62         4
          19       0.00      0.00      0.00         4
          20       0.50      0.50      0.50         4
          21       0.29      0.50      0.36         4
          22       0.14      0.25      0.18         4
          23       0.25      0.33      0.29         9
          24       0.62      0.56      0.59         9
          25       0.10      0.25      0.14         4
          26       0.75      0.75      0.75         4
          27       0.08      0.11      0.09         9
          28       0.00      0.00      0.00         4
          29       0.55      0.67      0.60         9
          30       0.00      0.00      0.00         4
          31       0.75      0.75      0.75         4
          32       0.67      0.50      0.57         4
          33       0.50      0.50      0.50         4
          34       0.43      0.60      0.50         5
          35       0.00      0.00      0.00         4
          36       0.00      0.00      0.00         4
          37       0.75      0.75      0.75         4
          38       0.83      1.00      0.91         5
          39       0.00      0.00      0.00         4
          40       0.75      0.75      0.75         4
          41       0.60      0.33      0.43         9
          42       0.82      1.00      0.90         9
          43       0.67      0.40      0.50         5
          44       0.50      0.25      0.33         4
          45       0.75      0.75      0.75         4
          46       0.75      0.75      0.75         4
          47       1.00      0.78      0.88         9
          48       1.00      0.89      0.94         9
          49       0.00      0.00      0.00         4
          50       0.80      1.00      0.89         4
          51       0.44      0.44      0.44         9
          52       0.14      0.50      0.22         4
          53       0.00      0.00      0.00         4
          54       0.20      0.50      0.29         4
          55       0.00      0.00      0.00         4
          56       1.00      0.75      0.86         4
          57       0.80      1.00      0.89         4
          58       0.57      0.67      0.62         6
          59       0.82      1.00      0.90         9
          60       0.60      0.75      0.67         4
          61       0.11      0.25      0.15         4
          62       0.57      0.44      0.50         9
          63       0.00      0.00      0.00         4
          64       0.90      1.00      0.95         9
          65       1.00      0.60      0.75         5
          66       1.00      0.25      0.40         4
          67       1.00      1.00      1.00         4
          68       1.00      1.00      1.00         4
          69       1.00      0.75      0.86         4
          70       0.80      0.89      0.84         9
          71       0.90      1.00      0.95         9
          72       0.60      0.60      0.60         5
          73       1.00      1.00      1.00         4
          74       1.00      1.00      1.00         9
          75       0.00      0.00      0.00         4
          76       0.00      0.00      0.00         4
          77       1.00      0.75      0.86         4
          78       0.00      0.00      0.00         9
          79       1.00      0.75      0.86         4
          80       0.71      1.00      0.83         5
          81       1.00      1.00      1.00         4
          82       0.06      0.25      0.10         4
          83       0.44      1.00      0.62         4
          84       1.00      0.80      0.89         5
          85       1.00      0.75      0.86         4
          86       0.00      0.00      0.00         4
          87       1.00      1.00      1.00         9
          88       0.00      0.00      0.00         4
          89       0.12      0.50      0.19         4
          90       1.00      0.89      0.94         9
          91       0.00      0.00      0.00         4
          92       0.67      1.00      0.80         4
          93       0.80      1.00      0.89         4
          94       0.75      0.60      0.67         5
          95       0.86      0.67      0.75         9
          96       0.00      0.00      0.00         4
          97       1.00      1.00      1.00         9
          98       0.80      0.80      0.80         5
          99       0.40      1.00      0.57         4
         100       0.80      0.80      0.80         5
         101       1.00      0.40      0.57         5
         102       1.00      1.00      1.00         4
         103       1.00      0.89      0.94         9
         104       0.57      1.00      0.73         4
         105       0.58      0.78      0.67         9
         106       1.00      0.78      0.88         9
         107       0.50      1.00      0.67         4
         108       0.57      1.00      0.73         4
         109       1.00      0.89      0.94         9
         110       0.82      1.00      0.90         9
         111       0.00      0.00      0.00         4
         112       0.67      0.40      0.50         5
         113       0.83      1.00      0.91         5
         114       1.00      0.78      0.88         9
         115       1.00      1.00      1.00         9
         116       0.43      0.75      0.55         4
         117       0.75      0.75      0.75         4
         118       0.71      1.00      0.83         5
         119       0.60      0.75      0.67         4
         120       0.75      0.75      0.75         4
         121       0.50      0.78      0.61         9
         122       0.57      0.80      0.67         5
         123       0.67      0.50      0.57         4
         124       0.11      0.25      0.15         4
         125       1.00      0.50      0.67         4
         126       0.25      0.50      0.33         4
         127       0.38      0.33      0.35         9
         128       1.00      0.25      0.40         4
         129       0.50      0.25      0.33         4
         130       1.00      1.00      1.00         9
         131       0.67      1.00      0.80         4
         132       0.67      1.00      0.80         4
         133       1.00      1.00      1.00         5
         134       1.00      1.00      1.00         4
         135       1.00      0.75      0.86         4
         136       1.00      0.75      0.86         4
         137       1.00      0.89      0.94         9
         138       0.88      0.78      0.82         9
         139       1.00      1.00      1.00         9
         140       1.00      1.00      1.00         4
         141       0.88      0.78      0.82         9
         142       0.80      0.57      0.67         7
         143       0.64      0.78      0.70         9
         144       0.80      1.00      0.89         4
         145       1.00      0.75      0.86         4
         146       1.00      1.00      1.00         4
         147       0.75      0.33      0.46         9
         148       1.00      0.50      0.67         4
         149       1.00      0.75      0.86         4
         150       0.50      0.25      0.33         4
         151       1.00      0.89      0.94         9
         152       1.00      0.80      0.89         5
         153       0.67      1.00      0.80         4
         154       1.00      0.75      0.86         4
         155       1.00      0.75      0.86         4
         156       1.00      1.00      1.00         4
         157       1.00      1.00      1.00         4
         158       1.00      0.89      0.94         9
         159       1.00      1.00      1.00         4
         160       1.00      1.00      1.00         4
         161       0.75      0.75      0.75         4
         162       0.83      1.00      0.91         5
         163       1.00      0.50      0.67         4
         164       0.75      0.75      0.75         4
         165       1.00      1.00      1.00         9
         166       1.00      1.00      1.00         9
         167       0.50      0.25      0.33         4
         168       0.73      0.89      0.80         9
         169       0.80      0.44      0.57         9
         170       0.80      1.00      0.89         4
         171       1.00      1.00      1.00         5
         172       0.75      0.67      0.71         9
         173       1.00      1.00      1.00         9
         174       1.00      1.00      1.00         4
         175       0.04      0.11      0.06         9
         176       0.40      1.00      0.57         4
         177       1.00      1.00      1.00         4
         178       1.00      0.89      0.94         9
         179       1.00      1.00      1.00         4
         180       0.80      0.89      0.84         9
         181       1.00      0.40      0.57         5
         182       0.14      0.25      0.18         4
         183       0.83      1.00      0.91         5
         184       0.50      0.67      0.57         9
         185       0.80      1.00      0.89         4
         186       0.50      0.60      0.55         5
         187       0.82      1.00      0.90         9
         188       1.00      0.75      0.86         4
         189       1.00      0.89      0.94         9
         190       0.88      0.78      0.82         9
         191       0.80      1.00      0.89         4
         192       1.00      0.75      0.86         4
         193       0.67      1.00      0.80         4
         194       1.00      1.00      1.00         4
         195       1.00      0.25      0.40         4
         196       1.00      1.00      1.00         4
         197       0.57      0.44      0.50         9
         198       0.60      0.60      0.60         5
         199       1.00      0.75      0.86         4
         200       0.80      1.00      0.89         4
         201       1.00      1.00      1.00         5
         202       0.75      0.75      0.75         4
         203       0.80      1.00      0.89         4
         204       0.00      0.00      0.00         4
         205       0.10      0.11      0.11         9
         206       1.00      1.00      1.00         5
         207       0.75      0.75      0.75         4
         208       1.00      1.00      1.00         4
         209       0.75      0.75      0.75         4
         210       1.00      0.50      0.67         4
         211       0.80      0.80      0.80         5
         212       0.86      0.67      0.75         9
         213       0.82      1.00      0.90         9
         214       1.00      1.00      1.00         5
         215       1.00      0.78      0.88         9
         216       1.00      0.75      0.86         4
         217       0.86      0.67      0.75         9
         218       0.60      0.75      0.67         4
         219       1.00      1.00      1.00         5
         220       1.00      0.75      0.86         4
         221       0.60      0.60      0.60         5
         222       0.67      1.00      0.80         4
         223       0.00      0.00      0.00         4
         224       0.83      1.00      0.91         5
         225       0.88      0.78      0.82         9
         226       0.86      0.67      0.75         9
         227       1.00      0.40      0.57         5
         228       1.00      1.00      1.00         4
         229       0.44      1.00      0.62         4
         230       0.80      0.89      0.84         9
         231       0.08      0.25      0.12         4
         232       0.67      1.00      0.80         4
         233       0.10      0.25      0.14         4
         234       0.00      0.00      0.00         4
         235       0.80      1.00      0.89         4
         236       0.43      0.75      0.55         4
         237       1.00      0.75      0.86         4
         238       0.00      0.00      0.00         4
         239       1.00      1.00      1.00         5
         240       0.44      0.80      0.57         5
         241       0.00      0.00      0.00         4
         242       0.12      0.25      0.17         4
         243       0.38      0.75      0.50         4
         244       0.89      0.89      0.89         9
         245       0.57      1.00      0.73         4
         246       0.00      0.00      0.00         4
         247       0.60      0.75      0.67         4
         248       0.80      1.00      0.89         4
         249       1.00      1.00      1.00         4
         250       0.47      0.78      0.58         9
         251       0.10      0.11      0.11         9
         252       1.00      0.75      0.86         4
         253       0.80      0.80      0.80         5
         254       0.00      0.00      0.00         4
         255       1.00      1.00      1.00         4
         256       0.90      1.00      0.95         9
         257       0.00      0.00      0.00         4
         258       0.80      1.00      0.89         4
         259       1.00      1.00      1.00         5
         260       0.80      1.00      0.89         4
         261       0.00      0.00      0.00         4
         262       0.25      0.20      0.22         5
         263       0.50      0.67      0.57         9
         264       0.67      1.00      0.80         4
         265       0.67      1.00      0.80         4
         266       0.90      1.00      0.95         9
         267       0.67      0.50      0.57         4
         268       1.00      0.75      0.86         4
         269       0.83      1.00      0.91         5
         270       0.67      1.00      0.80         4
         271       1.00      1.00      1.00         9
         272       0.00      0.00      0.00         4
         273       1.00      0.25      0.40         4
         274       0.08      0.11      0.09         9
         275       0.75      0.60      0.67         5
         276       0.00      0.00      0.00         9
         277       0.90      1.00      0.95         9
         278       0.50      0.75      0.60         4
         279       0.00      0.00      0.00         4
         280       1.00      0.75      0.86         4
         281       1.00      0.80      0.89         5
         282       1.00      0.60      0.75         5
         283       0.50      0.25      0.33         4
         284       0.89      0.89      0.89         9
         285       0.80      0.89      0.84         9
         286       0.75      0.75      0.75         4
         287       0.44      1.00      0.62         4
         288       1.00      1.00      1.00         9
         289       0.80      1.00      0.89         4
         290       0.80      1.00      0.89         4
         291       0.50      0.75      0.60         4
         292       1.00      1.00      1.00         5
         293       0.33      0.25      0.29         4
         294       0.75      0.75      0.75         4
         295       0.80      1.00      0.89         4
         296       0.00      0.00      0.00         4
         297       0.75      0.75      0.75         4
         298       0.00      0.00      0.00         4
         299       0.88      0.78      0.82         9
         300       1.00      1.00      1.00         5
         301       0.50      0.60      0.55         5
         302       0.50      0.33      0.40         9
         303       1.00      1.00      1.00         4
         304       1.00      1.00      1.00         4
         305       0.71      0.56      0.63         9
         306       0.71      1.00      0.83         5
         307       1.00      1.00      1.00         4
         308       0.80      1.00      0.89         4
         309       1.00      1.00      1.00         5
         310       0.00      0.00      0.00         4
         311       0.00      0.00      0.00         4
         312       0.67      1.00      0.80         4
         313       0.89      0.89      0.89         9
         314       0.80      1.00      0.89         4
         315       1.00      0.75      0.86         4
         316       1.00      1.00      1.00         5
         317       0.57      1.00      0.73         4
         318       1.00      1.00      1.00         4
         319       1.00      1.00      1.00         4
         320       1.00      0.60      0.75         5
         321       0.80      1.00      0.89         4
         322       0.25      0.50      0.33         4
         323       0.67      1.00      0.80         4
         324       0.90      1.00      0.95         9
         325       1.00      1.00      1.00         9
         326       0.50      1.00      0.67         4
         327       0.57      0.80      0.67         5
         328       0.80      1.00      0.89         4
         329       1.00      0.75      0.86         4
         330       1.00      0.33      0.50         9
         331       1.00      1.00      1.00         4
         332       0.43      0.75      0.55         4
         333       1.00      1.00      1.00         5
         334       0.50      0.25      0.33         4
         335       0.50      0.25      0.33         4
         336       0.83      1.00      0.91         5
         337       0.67      1.00      0.80         4
         338       1.00      0.75      0.86         4
         339       1.00      1.00      1.00         9
         340       1.00      0.25      0.40         4
         341       0.20      0.25      0.22         4
         342       0.50      0.50      0.50         4
         343       1.00      1.00      1.00         4
         344       0.90      1.00      0.95         9
         345       1.00      1.00      1.00         9
         346       0.00      0.00      0.00         4
         347       0.00      0.00      0.00         4
         348       0.33      0.33      0.33         9
         349       0.83      0.56      0.67         9
         350       0.83      1.00      0.91         5
         351       0.83      1.00      0.91         5
         352       1.00      0.80      0.89         5
         353       0.00      0.00      0.00         4
         354       1.00      1.00      1.00         5
         355       0.00      0.00      0.00         4
         356       1.00      1.00      1.00         4
         357       0.80      0.89      0.84         9
         358       1.00      0.75      0.86         4
         359       0.60      0.75      0.67         4
         360       0.82      1.00      0.90         9
         361       1.00      1.00      1.00         5
         362       0.67      1.00      0.80         4
         363       0.75      0.75      0.75         4
         364       0.75      0.60      0.67         5
         365       0.80      0.89      0.84         9
         366       1.00      1.00      1.00         9
         367       0.00      0.00      0.00         9
         368       0.80      0.80      0.80         5
         369       1.00      0.75      0.86         4
         370       0.56      1.00      0.71         5
         371       1.00      1.00      1.00         4
         372       0.67      0.40      0.50         5
         373       1.00      1.00      1.00         5
         374       0.56      1.00      0.72         9
         375       0.67      0.50      0.57         4
         376       0.75      0.75      0.75         4
         377       1.00      0.67      0.80         9
         378       1.00      0.75      0.86         4
         379       0.67      1.00      0.80         4
         380       1.00      1.00      1.00         4
         381       0.80      1.00      0.89         4
         382       0.60      0.75      0.67         4
         383       0.14      0.25      0.18         4
         384       0.75      0.75      0.75         4
         385       0.70      0.78      0.74         9
         386       1.00      0.25      0.40         4
         387       1.00      1.00      1.00         4
         388       0.00      0.00      0.00         4
         389       1.00      1.00      1.00         4
         390       1.00      0.75      0.86         4
         391       0.89      0.89      0.89         9
         392       0.60      0.67      0.63         9
         393       1.00      0.80      0.89         5
         394       1.00      0.44      0.62         9
         395       1.00      0.89      0.94         9
         396       0.80      1.00      0.89         4
         397       0.50      0.25      0.33         4
         398       0.67      0.40      0.50         5
         399       0.60      0.75      0.67         4
         400       1.00      0.50      0.67         4
         401       0.40      1.00      0.57         4
         402       0.50      0.60      0.55         5
         403       0.80      0.44      0.57         9
         404       0.58      0.78      0.67         9
         405       1.00      0.50      0.67         4
         406       0.60      0.75      0.67         4
         407       1.00      0.60      0.75         5
         408       1.00      0.33      0.50         9
         409       0.60      0.67      0.63         9
         410       1.00      0.44      0.62         9
         411       0.53      0.89      0.67         9
         412       1.00      1.00      1.00         4
         413       0.80      1.00      0.89         4
         414       1.00      1.00      1.00         4
         415       0.73      0.89      0.80         9
         416       1.00      1.00      1.00         4
         417       0.38      0.75      0.50         4
         418       0.70      0.78      0.74         9
         419       1.00      1.00      1.00         4
         420       1.00      1.00      1.00         4
         421       0.89      0.89      0.89         9
         422       1.00      1.00      1.00         4
         423       0.00      0.00      0.00         4
         424       0.67      0.67      0.67         9
         425       1.00      0.75      0.86         4
         426       0.80      1.00      0.89         4
         427       0.80      1.00      0.89         4
         428       0.75      0.75      0.75         4
         429       0.30      0.33      0.32         9
         430       0.67      1.00      0.80         4
         431       0.82      1.00      0.90         9
         432       1.00      0.33      0.50         9
         433       1.00      1.00      1.00         4
         434       1.00      0.80      0.89         5
         435       0.73      0.89      0.80         9
         436       0.00      0.00      0.00         4
         437       0.88      0.78      0.82         9
         438       0.60      0.75      0.67         4
         439       0.83      0.56      0.67         9
         440       0.43      0.75      0.55         4
         441       0.67      0.50      0.57         4
         442       1.00      1.00      1.00         4
         443       1.00      1.00      1.00         4
         444       0.80      1.00      0.89         4
         445       0.75      0.60      0.67         5
         446       1.00      1.00      1.00         5
         447       0.75      0.67      0.71         9
         448       0.33      0.25      0.29         4
         449       0.90      1.00      0.95         9
         450       1.00      0.60      0.75         5
         451       0.00      0.00      0.00         4
         452       0.00      0.00      0.00         4
         453       1.00      0.25      0.40         4
         454       1.00      0.75      0.86         4
         455       0.00      0.00      0.00         4
         456       0.90      1.00      0.95         9
         457       0.80      0.80      0.80         5
         458       0.67      1.00      0.80         4
         459       1.00      0.75      0.86         4
         460       1.00      1.00      1.00         4
         461       0.75      0.75      0.75         4
         462       1.00      1.00      1.00         4
         463       1.00      0.80      0.89         5
         464       1.00      1.00      1.00         5
         465       1.00      0.56      0.71         9
         466       0.73      0.89      0.80         9
         467       0.43      0.75      0.55         4
         468       0.17      0.20      0.18         5
         469       1.00      0.75      0.86         4
         470       0.67      0.80      0.73         5
         471       0.82      1.00      0.90         9
         472       1.00      0.78      0.88         9
         473       0.43      0.60      0.50         5
         474       0.80      1.00      0.89         4
         475       1.00      0.60      0.75         5
         476       1.00      0.50      0.67         4
         477       0.73      0.89      0.80         9
         478       1.00      1.00      1.00         5
         479       1.00      0.78      0.88         9
         480       1.00      0.50      0.67         4
         481       1.00      1.00      1.00         5
         482       0.00      0.00      0.00         4
         483       0.80      1.00      0.89         4
         484       1.00      0.89      0.94         9
         485       1.00      0.56      0.71         9
         486       0.80      0.80      0.80         5
         487       0.00      0.00      0.00         4
         488       0.00      0.00      0.00         4
         489       0.60      0.75      0.67         4
         490       0.83      0.56      0.67         9
         491       0.88      0.78      0.82         9
         492       0.00      0.00      0.00         9
         493       0.60      0.60      0.60         5
         494       1.00      1.00      1.00         5
         495       1.00      1.00      1.00         9
         496       0.67      0.50      0.57         4
         497       0.50      0.25      0.33         4
         498       0.50      0.75      0.60         4
         499       1.00      0.40      0.57         5
         500       0.83      1.00      0.91         5
         501       1.00      1.00      1.00         4
         502       1.00      0.33      0.50         9
         503       1.00      0.75      0.86         4
         504       0.33      0.25      0.29         4
         505       0.75      1.00      0.86         9
         506       0.82      1.00      0.90         9
         507       0.50      0.75      0.60         4
         508       0.90      1.00      0.95         9
         509       0.50      1.00      0.67         4
         510       0.67      0.40      0.50         5
         511       0.00      0.00      0.00         4
         512       0.67      1.00      0.80         4
         513       0.57      1.00      0.73         4
         514       1.00      1.00      1.00         4
         515       0.00      0.00      0.00         4
         516       0.00      0.00      0.00         9
         517       1.00      1.00      1.00         9
         518       1.00      1.00      1.00         4
         519       0.50      0.50      0.50         4
         520       0.50      0.25      0.33         4
         521       1.00      0.25      0.40         4
         522       0.00      0.00      0.00         4
         523       0.00      0.00      0.00         4
         524       1.00      0.60      0.75         5
         525       0.00      0.00      0.00         9
         526       0.80      1.00      0.89         4
         527       1.00      0.75      0.86         4
         528       0.67      0.22      0.33         9
         529       0.50      0.75      0.60         4
         530       0.80      1.00      0.89         4
         531       0.50      0.56      0.53         9
         532       1.00      0.75      0.86         4
         533       0.71      0.56      0.63         9
         534       0.50      0.20      0.29         5
         535       1.00      1.00      1.00         4
         536       0.67      0.50      0.57         4
         537       0.50      0.25      0.33         4
         538       1.00      0.80      0.89         5
         539       1.00      1.00      1.00         4
         540       0.75      0.75      0.75         4
         541       1.00      0.75      0.86         8
         542       0.60      0.75      0.67         4
         543       0.57      1.00      0.73         4
         544       0.90      1.00      0.95         9
         545       0.75      0.75      0.75         4
         546       1.00      0.22      0.36         9
         547       0.75      0.75      0.75         4
         548       1.00      1.00      1.00         4
         549       0.80      1.00      0.89         4
         550       1.00      0.75      0.86         4
         551       0.80      1.00      0.89         4
         552       0.00      0.00      0.00         4
         553       0.17      0.20      0.18         5
         554       0.83      1.00      0.91         5
         555       0.60      0.33      0.43         9
         556       0.00      0.00      0.00         9
         557       0.60      0.75      0.67         4
         558       1.00      1.00      1.00         4
         559       1.00      0.80      0.89         5
         560       0.75      0.75      0.75         4
         561       0.20      0.25      0.22         4
         562       1.00      0.88      0.93         8
         563       0.88      0.78      0.82         9
         564       0.75      0.75      0.75         4
         565       1.00      0.89      0.94         9
         566       0.78      0.78      0.78         9
         567       0.00      0.00      0.00         4
         568       0.67      0.40      0.50         5
         569       0.80      1.00      0.89         4
         570       0.80      1.00      0.89         4
         571       1.00      1.00      1.00         9
         572       0.88      0.78      0.82         9
         573       0.89      0.89      0.89         9
         574       1.00      1.00      1.00         4
         575       1.00      1.00      1.00         4
         576       1.00      0.50      0.67         4
         577       0.17      1.00      0.29         4
         578       0.75      0.75      0.75         4
         579       0.75      0.75      0.75         4
         580       1.00      1.00      1.00         5
         581       1.00      1.00      1.00         5
         582       0.00      0.00      0.00         4
         583       0.80      0.89      0.84         9
         584       1.00      0.80      0.89         5
         585       0.86      0.67      0.75         9
         586       0.75      0.75      0.75         4
         587       0.22      0.44      0.30         9
         588       0.00      0.00      0.00         4
         589       0.00      0.00      0.00         4
         590       0.60      0.75      0.67         4
         591       1.00      1.00      1.00         4
         592       0.75      0.60      0.67         5
         593       0.60      0.60      0.60         5
         594       0.80      0.80      0.80         5
         595       1.00      0.20      0.33         5
         596       0.75      0.67      0.71         9
         597       0.50      0.50      0.50         4
         598       0.00      0.00      0.00         4
         599       0.67      0.50      0.57         4
         600       0.00      0.00      0.00         4
         601       0.75      0.75      0.75         4
         602       1.00      0.78      0.88         9
         603       1.00      1.00      1.00         4
         604       0.00      0.00      0.00         4
         605       1.00      0.75      0.86         4
         606       0.82      1.00      0.90         9
         607       1.00      0.25      0.40         4
         608       1.00      0.78      0.88         9
         609       0.78      0.78      0.78         9
         610       0.57      0.44      0.50         9
         611       0.75      0.75      0.75         4
         612       1.00      0.56      0.71         9
         613       0.67      1.00      0.80         4
         614       0.56      0.56      0.56         9
         615       0.62      0.56      0.59         9
         616       0.40      0.50      0.44         4
         617       0.71      1.00      0.83         5
         618       0.00      0.00      0.00         4
         619       0.67      0.50      0.57         4
         620       0.86      0.67      0.75         9
         621       0.80      1.00      0.89         4
         622       1.00      0.75      0.86         4
         623       1.00      0.67      0.80         9
         624       0.00      0.00      0.00         4
         625       0.67      0.50      0.57         4
         626       0.67      1.00      0.80         4
         627       1.00      0.75      0.86         4
         628       0.80      1.00      0.89         4
         629       0.00      0.00      0.00         4
         630       0.57      1.00      0.73         4
         631       0.80      1.00      0.89         4
         632       0.80      1.00      0.89         4
         633       0.44      0.80      0.57         5
         634       0.83      1.00      0.91         5
         635       0.67      0.50      0.57         4
         636       0.00      0.00      0.00         4
         637       0.80      1.00      0.89         4
         638       0.83      1.00      0.91         5
         639       1.00      1.00      1.00         4
         640       0.00      0.00      0.00         4
         641       0.00      0.00      0.00         4
         642       0.62      1.00      0.77         5
         643       0.64      1.00      0.78         9
         644       0.67      1.00      0.80         4
         645       0.78      0.78      0.78         9
         646       0.60      0.75      0.67         4
         647       0.80      1.00      0.89         4
         648       0.33      0.25      0.29         4
         649       1.00      0.75      0.86         4
         650       0.60      0.75      0.67         4
         651       0.43      0.75      0.55         4
         652       0.00      0.00      0.00         4
         653       1.00      0.75      0.86         4
         654       0.57      0.89      0.70         9
         655       0.80      1.00      0.89         4
         656       0.75      0.75      0.75         4
         657       0.00      0.00      0.00         4
         658       1.00      0.89      0.94         9
         659       1.00      0.25      0.40         4
         660       0.83      0.56      0.67         9
         661       0.56      0.71      0.63         7
         662       0.75      0.67      0.71         9
         663       0.00      0.00      0.00         9
         664       0.62      0.56      0.59         9
         665       0.80      1.00      0.89         4
         666       1.00      0.75      0.86         4
         667       0.75      0.60      0.67         5
         668       1.00      0.75      0.86         4
         669       0.57      0.80      0.67         5
         670       0.67      1.00      0.80         4
         671       1.00      0.75      0.86         4
         672       0.00      0.00      0.00         4
         673       0.80      1.00      0.89         4
         674       1.00      0.80      0.89         5
         675       0.50      0.75      0.60         4
         676       0.43      0.33      0.38         9
         677       1.00      0.25      0.40         4
         678       0.75      0.75      0.75         4
         679       1.00      0.75      0.86         4
         680       1.00      0.80      0.89         5
         681       1.00      0.75      0.86         4
         682       0.00      0.00      0.00         4
         683       0.00      0.00      0.00         4
         684       1.00      1.00      1.00         4
         685       0.80      1.00      0.89         4
         686       1.00      1.00      1.00         4
         687       0.00      0.00      0.00         4
         688       0.00      0.00      0.00         4
         689       0.00      0.00      0.00         4
         690       0.67      0.89      0.76         9
         691       0.67      0.50      0.57         4
         692       0.00      0.00      0.00         5
         693       0.70      0.78      0.74         9
         694       0.00      0.00      0.00         4
         695       0.57      1.00      0.73         4
         696       0.80      1.00      0.89         4
         697       0.00      0.00      0.00         4
         698       0.88      0.78      0.82         9
         699       1.00      1.00      1.00         5
         700       0.00      0.00      0.00         4
         701       0.60      0.75      0.67         4
         702       0.40      0.50      0.44         4
         703       0.00      0.00      0.00         4
         704       0.88      0.78      0.82         9
         705       0.00      0.00      0.00         4
         706       0.00      0.00      0.00         4
         707       1.00      0.75      0.86         4
         708       1.00      1.00      1.00         4
         709       0.50      0.50      0.50         4
         710       0.83      1.00      0.91         5
         711       1.00      1.00      1.00         5
         712       0.75      1.00      0.86         9
         713       0.60      0.60      0.60         5
         714       1.00      0.60      0.75         5
         715       1.00      0.80      0.89         5
         716       0.80      1.00      0.89         4
         717       1.00      0.50      0.67         4
         718       1.00      1.00      1.00         9
         719       0.58      0.78      0.67         9
         720       0.57      0.80      0.67         5
         721       0.00      0.00      0.00         9
         722       0.75      0.75      0.75         4
         723       1.00      1.00      1.00         5
         724       0.73      0.89      0.80         9
         725       0.67      1.00      0.80         4
         726       1.00      0.67      0.80         9
         727       0.57      0.80      0.67         5
         728       0.00      0.00      0.00         4
         729       0.00      0.00      0.00         4
         730       0.43      0.75      0.55         4
         731       0.50      0.50      0.50         4
         732       0.67      0.67      0.67         9
         733       1.00      1.00      1.00         4
         734       1.00      0.11      0.20         9
         735       0.33      0.60      0.43         5
         736       0.86      0.67      0.75         9
         737       1.00      1.00      1.00         5
         738       0.86      0.67      0.75         9
         739       0.73      0.89      0.80         9
         740       0.00      0.00      0.00         4
         741       0.67      0.80      0.73         5
         742       0.00      0.00      0.00         9
         743       0.00      0.00      0.00         4
         744       0.67      0.50      0.57         4
         745       0.00      0.00      0.00         4
         746       1.00      1.00      1.00         5
         747       1.00      0.75      0.86         4
         748       1.00      1.00      1.00         4
         749       0.57      0.44      0.50         9
         750       1.00      1.00      1.00         4
         751       0.80      0.80      0.80         5
         752       0.25      0.75      0.38         4
         753       0.00      0.00      0.00         4
         754       0.80      0.80      0.80         5
         755       1.00      0.40      0.57         5
         756       1.00      0.80      0.89         5
         757       1.00      1.00      1.00         9
         758       1.00      1.00      1.00         4
         759       1.00      0.62      0.77         8
         760       0.00      0.00      0.00         4
         761       1.00      1.00      1.00         4
         762       1.00      0.56      0.71         9
         763       0.40      0.50      0.44         4
         764       1.00      0.89      0.94         9
         765       0.89      0.89      0.89         9
         766       1.00      1.00      1.00         4
         767       0.43      0.75      0.55         4
         768       1.00      1.00      1.00         4
         769       0.80      0.80      0.80         5
         770       1.00      0.75      0.86         4
         771       1.00      0.78      0.88         9
         772       0.89      0.89      0.89         9
         773       1.00      0.50      0.67         4
         774       0.00      0.00      0.00         4
         775       0.20      0.50      0.29         4
         776       0.25      0.20      0.22         5
         777       1.00      1.00      1.00         4
         778       0.60      0.75      0.67         4
         779       1.00      0.80      0.89         5
         780       0.00      0.00      0.00         4
         781       0.73      0.89      0.80         9
         782       1.00      0.78      0.88         9
         783       1.00      0.89      0.94         9
         784       1.00      0.75      0.86         4
         785       1.00      0.25      0.40         4
         786       1.00      1.00      1.00         4
         787       1.00      0.60      0.75         5
         788       0.00      0.00      0.00         4
         789       0.90      1.00      0.95         9
         790       0.50      0.50      0.50         4
         791       0.71      1.00      0.83         5
         792       0.00      0.00      0.00         4
         793       0.75      0.75      0.75         4
         794       1.00      0.50      0.67         4
         795       0.67      1.00      0.80         4
         796       0.71      0.56      0.63         9
         797       1.00      1.00      1.00         5
         798       0.50      0.25      0.33         4
         799       1.00      1.00      1.00         9
         800       0.88      0.78      0.82         9
         801       0.89      0.89      0.89         9
         802       1.00      0.75      0.86         4
         803       1.00      1.00      1.00         5
         804       0.86      0.67      0.75         9
         805       1.00      1.00      1.00         9
         806       0.71      0.56      0.63         9
         807       0.80      0.89      0.84         9
         808       0.75      0.75      0.75         4
         809       0.80      1.00      0.89         4
         810       0.29      0.22      0.25         9
         811       1.00      1.00      1.00         4
         812       0.78      0.78      0.78         9
         813       1.00      1.00      1.00         4
         814       1.00      1.00      1.00         4
         815       1.00      0.50      0.67         4
         816       0.75      0.75      0.75         4
         817       0.73      0.89      0.80         9
         818       0.50      0.25      0.33         4
         819       0.80      0.80      0.80         5
         820       0.50      0.75      0.60         4
         821       0.00      0.00      0.00         4
         822       0.82      1.00      0.90         9
         823       0.00      0.00      0.00         4
         824       0.00      0.00      0.00         4
         825       1.00      0.80      0.89         5
         826       0.67      0.80      0.73         5
         827       0.67      0.22      0.33         9
         828       0.90      1.00      0.95         9
         829       0.67      1.00      0.80         4
         830       0.00      0.00      0.00         4
         831       1.00      1.00      1.00         4
         832       0.00      0.00      0.00         4
         833       0.75      0.75      0.75         4
         834       0.00      0.00      0.00         4
         835       0.14      0.11      0.12         9
         836       1.00      0.75      0.86         4
         837       0.80      0.89      0.84         9
         838       0.00      0.00      0.00         4
         839       1.00      0.25      0.40         4
         840       1.00      0.75      0.86         4
         841       0.90      1.00      0.95         9
         842       0.89      0.89      0.89         9
         843       0.00      0.00      0.00         9
         844       1.00      1.00      1.00         4
         845       0.67      0.50      0.57         4
         846       0.50      0.25      0.33         4
         847       0.00      0.00      0.00         4
         848       0.50      0.25      0.33         4
         849       0.00      0.00      0.00         5
         850       0.67      1.00      0.80         4
         851       1.00      0.75      0.86         4
         852       0.00      0.00      0.00         4
         853       1.00      0.80      0.89         5
         854       0.75      1.00      0.86         9
         855       0.00      0.00      0.00         4
         856       0.33      0.14      0.20         7
         857       0.11      0.22      0.14         9
         858       1.00      0.50      0.67         4
         859       0.89      0.89      0.89         9
         860       0.90      1.00      0.95         9
         861       0.67      0.50      0.57         4
         862       0.00      0.00      0.00         9
         863       1.00      0.75      0.86         4
         864       0.00      0.00      0.00         4
         865       1.00      0.89      0.94         9
         866       1.00      0.78      0.88         9
         867       1.00      0.89      0.94         9
         868       1.00      0.40      0.57         5
         869       0.67      0.89      0.76         9
         870       0.80      1.00      0.89         4
         871       1.00      1.00      1.00         9
         872       0.00      0.00      0.00         4
         873       1.00      1.00      1.00         4
         874       0.50      0.25      0.33         4
         875       0.67      0.40      0.50         5
         876       0.00      0.00      0.00         4
         877       0.50      0.25      0.33         4
         878       1.00      0.75      0.86         4
         879       0.18      0.50      0.27         4
         880       0.62      1.00      0.77         5
         881       0.45      1.00      0.62         5
         882       0.43      1.00      0.60         9
         883       0.50      0.25      0.33         4
         884       0.36      1.00      0.53         4
         885       0.80      1.00      0.89         4
         886       0.27      0.80      0.40         5
         887       1.00      0.60      0.75         5
         888       0.82      1.00      0.90         9
         889       0.50      0.50      0.50         4
         890       0.56      1.00      0.72         9
         891       0.58      0.78      0.67         9
         892       0.62      0.89      0.73         9
         893       0.83      1.00      0.91         5

    accuracy                           0.68      4917
   macro avg       0.68      0.67      0.65      4917
weighted avg       0.70      0.68      0.67      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
