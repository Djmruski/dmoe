	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [752, 246, 683, 363, 445, 557, 128, 388, 442, 144, 104, 160, 544, 453, 406, 701, 482, 311, 748, 888, 264, 22, 652, 547, 410, 671, 73, 54, 659, 189, 545, 739, 51, 754, 865, 647, 808, 97, 814, 237, 403, 314, 245, 309, 155, 432, 316, 100, 676, 718, 38, 231, 596, 230, 229, 176, 421, 3, 560, 802, 427, 371, 498, 546, 1, 747, 767, 670, 27, 838, 102, 32, 628, 704, 63, 119, 90, 775, 439, 733, 706, 448, 530, 698, 607, 351, 76, 483, 864, 790, 185, 350, 673, 446, 592, 686, 877, 173, 142, 271, 434, 744, 730, 213, 182, 337, 384, 788, 525, 858, 392, 653, 848, 711, 740, 687, 863, 95, 280, 43, 725, 379, 644, 55, 892, 789, 824, 471, 678, 266, 251, 583, 609, 349, 553, 288, 619, 378, 179, 516, 116, 501, 153, 795, 202, 675, 258, 252, 526, 460, 474, 702, 629, 856, 566, 777, 642, 157, 12, 517, 347, 46, 205, 431, 359, 467, 726, 196, 42, 89, 551, 890, 79, 362, 690, 372, 548, 834, 390, 253, 180, 589, 122, 572, 382, 239, 509, 881, 394, 811, 373, 478, 338, 512, 400, 761, 696, 299, 663, 440, 357, 414, 639, 542, 355, 822, 335, 375, 595, 884, 625, 386, 792, 401, 415, 395, 710, 206, 593, 826, 666, 524, 470, 334, 93, 444, 336, 107, 319, 853, 285, 667, 839, 821, 632, 321, 197, 662, 455, 328, 221, 113, 616, 305, 25, 463, 154, 438, 130, 805, 85, 776, 24, 849, 75, 159, 735, 443, 691, 859, 44, 487, 241, 405, 626, 885, 114, 418, 700, 887, 528, 614, 110, 608, 420, 140, 366, 9, 496, 837, 611, 646, 15, 7, 787, 794, 212, 103, 290, 650, 339, 597, 893, 697, 538, 668, 30, 131, 138, 354, 168, 688, 265, 91, 515, 327, 303, 845, 780, 533, 497, 491, 617, 712, 45, 836, 121, 411, 188, 558, 169, 324, 192, 677, 554, 635, 222, 585, 200, 770, 603, 801, 481, 166, 606, 360, 331, 866, 244, 745, 723, 58, 724, 199, 267, 580, 577, 715, 685, 4, 146, 404, 20, 218, 279, 461, 383, 809, 578, 255, 170, 141, 322, 385, 654, 473, 250, 53, 310, 829, 399, 458, 622, 552, 618, 494, 276, 274, 340, 708, 83, 604, 126, 490, 277, 232, 705, 734, 692, 348, 756, 361, 269, 565, 571, 664, 29, 10, 78, 135, 323, 695, 226, 508, 825, 549, 171, 472, 681, 655, 86, 728, 630, 559, 233, 115, 813, 342, 408, 124, 376, 247, 751, 599, 684, 98, 719, 150, 259, 529, 665, 835, 177, 204, 649, 125, 797, 364, 869, 217, 870, 161, 407, 294, 743, 172, 803, 429, 519, 326, 717, 586, 65, 416, 52, 661, 37, 591, 435, 304, 152, 656, 449, 462, 257, 465, 87, 503, 291, 273, 111, 64, 855, 441, 507, 156, 293, 570, 477, 236, 377, 137, 500, 419, 8, 332, 325, 831, 511, 26, 396, 613, 541, 18, 413, 426, 256, 223, 254, 755, 873, 219, 768, 875, 127, 764, 249, 31, 736, 850, 193, 823, 165, 840, 149, 145, 234, 486, 28, 430, 521, 709, 68, 451, 300, 534, 120, 210, 492, 187, 640, 488, 502, 518, 19, 0, 50, 201, 861, 296, 59, 612, 60, 298, 489, 750, 759, 722, 136, 876, 307, 568, 883, 539, 381, 641, 2, 643, 669, 112, 139, 82, 732, 282, 262, 815, 567, 72, 308, 148, 164, 833, 224, 633, 186, 23, 56, 346, 261, 208, 707, 225, 819, 118, 40, 313, 283, 693, 634, 784, 584, 804, 682, 857, 184, 5, 49, 867, 753, 368, 742, 352, 862, 41, 872, 34, 720, 129, 380, 563, 620, 278, 424, 807, 454, 827, 16, 329, 143, 11, 228, 573, 564, 601, 398, 737, 203, 555, 674, 270, 191, 637, 523, 741, 729, 816, 842, 109, 80, 39, 243, 151, 757, 84, 798, 452, 301, 358, 397, 852, 574, 70, 778, 174, 211, 758, 240, 598, 602, 791, 227, 74, 284, 779, 514, 475, 579, 167, 195, 581, 672, 485, 466, 561, 868, 297, 621, 425, 456, 318, 263, 374, 889, 476, 576, 657, 480, 468, 588, 450, 631, 134, 860, 295, 389, 513, 582, 214, 344, 880, 535, 272, 746, 268, 499, 699, 178, 844, 638, 772, 181, 317, 198, 147, 714, 531, 731, 716, 367, 117, 242, 800, 47, 123, 99, 330, 600, 13, 175, 882, 387, 341, 71, 493, 843, 510, 457, 540, 315, 536, 365, 417, 762, 158, 92, 428, 320, 6, 832, 506, 782, 353, 88, 847, 658, 495, 235, 587, 190, 436, 36, 66, 594, 286, 760, 769, 793, 786, 447, 766, 289, 306, 818, 108, 689, 17, 605, 106, 333, 721, 886, 402, 854, 651, 437, 773, 537, 820, 878, 785, 21, 238, 828, 841, 369, 94, 812, 248, 409, 783, 216, 615, 14, 370, 627, 33, 292, 556, 680, 846, 694, 275, 209, 35, 891, 356, 343, 505, 422, 522, 67, 749, 543, 527, 610, 851, 645, 879, 774, 765, 345, 796, 81, 287, 874, 590, 132, 162, 648, 101, 260, 562, 771, 469, 810, 57, 624, 763, 393, 520, 830, 550, 623, 220, 133, 77, 713, 679, 817, 703, 660, 532, 96, 69, 312, 799, 391, 302, 871, 464, 806, 504, 194, 727, 412, 163, 183, 105, 569, 433, 61, 215, 48, 575, 423, 781, 207, 479, 62, 459, 484, 281, 636, 738]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.882, TAw acc= 47.3% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.409, TAw acc= 55.0% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.047, TAw acc= 58.9% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.761, TAw acc= 64.3% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.539, TAw acc= 69.0% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.365, TAw acc= 72.1% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.216, TAw acc= 75.2% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.100, TAw acc= 81.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 298 train exemplars, time=  0.0s
298
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.056 | TAw acc= 86.0%, forg=  0.0%| TAg acc= 86.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=3.303, TAw acc= 56.9% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=2.644, TAw acc= 62.5% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.214, TAw acc= 66.7% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.908, TAw acc= 69.4% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.660, TAw acc= 75.0% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.461, TAw acc= 84.7% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.315, TAw acc= 90.3% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.195, TAw acc= 91.7% | *
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.194, TAw acc= 91.7% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.193, TAw acc= 91.7% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.192, TAw acc= 91.7% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.191, TAw acc= 91.7% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=1.190, TAw acc= 91.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 458 train exemplars, time=  0.0s
458
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.034 | TAw acc= 91.6%, forg= -5.6%| TAg acc= 84.3%, forg=  1.7% <<<
>>> Test on task  1 : loss=1.147 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 83.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.657, TAw acc= 65.1% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.694, TAw acc= 61.6% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.058, TAw acc= 74.4% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.683, TAw acc= 81.4% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.417, TAw acc= 87.2% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.241, TAw acc= 87.2% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.083, TAw acc= 87.2% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 93.0% | *
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.957, TAw acc= 93.0% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.956, TAw acc= 94.2% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.955, TAw acc= 94.2% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.954, TAw acc= 95.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.952, TAw acc= 95.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 618 train exemplars, time=  0.0s
618
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.012 | TAw acc= 92.7%, forg= -1.1%| TAg acc= 79.2%, forg=  6.7% <<<
>>> Test on task  1 : loss=1.228 | TAw acc= 98.0%, forg=-10.0%| TAg acc= 77.0%, forg=  6.0% <<<
>>> Test on task  2 : loss=1.047 | TAw acc= 91.5%, forg=  0.0%| TAg acc= 90.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.999, TAw acc= 62.4% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.997, TAw acc= 76.5% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.540, TAw acc= 88.2% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.294, TAw acc= 90.6% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.127, TAw acc= 94.1% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.969, TAw acc= 94.1% | *
| Epoch   7, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.874, TAw acc= 94.1% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.763, TAw acc= 95.3% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.763, TAw acc= 95.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.763, TAw acc= 95.3% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.762, TAw acc= 95.3% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.762, TAw acc= 95.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.761, TAw acc= 95.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 778 train exemplars, time=  0.0s
778
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.911 | TAw acc= 94.9%, forg= -2.2%| TAg acc= 83.7%, forg=  2.2% <<<
>>> Test on task  1 : loss=1.066 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.0%, forg=  1.0% <<<
>>> Test on task  2 : loss=1.261 | TAw acc= 95.7%, forg= -4.3%| TAg acc= 78.6%, forg= 12.0% <<<
>>> Test on task  3 : loss=0.963 | TAw acc= 91.2%, forg=  0.0%| TAg acc= 81.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.779, TAw acc= 62.6% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.480, TAw acc= 63.7% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.979, TAw acc= 67.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.668, TAw acc= 78.0% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.452, TAw acc= 79.1% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 83.5% | *
| Epoch   7, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.181, TAw acc= 85.7% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.111, TAw acc= 90.1% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.108, TAw acc= 90.1% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.104, TAw acc= 90.1% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.100, TAw acc= 90.1% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.097, TAw acc= 90.1% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.094, TAw acc= 90.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 938 train exemplars, time=  0.0s
938
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.822 | TAw acc= 97.2%, forg= -2.2%| TAg acc= 89.9%, forg= -3.9% <<<
>>> Test on task  1 : loss=0.950 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 84.0%, forg= -1.0% <<<
>>> Test on task  2 : loss=1.165 | TAw acc= 97.4%, forg= -1.7%| TAg acc= 78.6%, forg= 12.0% <<<
>>> Test on task  3 : loss=1.194 | TAw acc= 96.5%, forg= -5.3%| TAg acc= 76.3%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.191 | TAw acc= 90.0%, forg=  0.0%| TAg acc= 75.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.132, TAw acc= 60.6% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.765, TAw acc= 68.7% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.046, TAw acc= 75.8% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.623, TAw acc= 82.8% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.360, TAw acc= 83.8% | *
| Epoch   6, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.164, TAw acc= 90.9% | *
| Epoch   7, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.025, TAw acc= 91.9% | *
| Epoch   8, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=0.929, TAw acc= 90.9% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=0.928, TAw acc= 90.9% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=0.927, TAw acc= 91.9% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=0.926, TAw acc= 91.9% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=0.924, TAw acc= 91.9% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=0.923, TAw acc= 91.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1098 train exemplars, time=  0.0s
1098
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.771 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 89.9%, forg=  0.0% <<<
>>> Test on task  1 : loss=0.964 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 80.0%, forg=  4.0% <<<
>>> Test on task  2 : loss=1.094 | TAw acc= 98.3%, forg= -0.9%| TAg acc= 82.1%, forg=  8.5% <<<
>>> Test on task  3 : loss=1.141 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 65.8%, forg= 15.8% <<<
>>> Test on task  4 : loss=1.316 | TAw acc= 93.3%, forg= -3.3%| TAg acc= 75.8%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.150 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 83.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.960, TAw acc= 27.6% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.1s loss=3.585, TAw acc= 36.8% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.759, TAw acc= 55.3% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.213, TAw acc= 56.6% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.898, TAw acc= 76.3% | *
| Epoch   6, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.657, TAw acc= 81.6% | *
| Epoch   7, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 82.9% | *
| Epoch   8, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.387, TAw acc= 85.5% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.386, TAw acc= 85.5% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.385, TAw acc= 85.5% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.383, TAw acc= 85.5% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.382, TAw acc= 85.5% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.381, TAw acc= 85.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1258 train exemplars, time=  0.0s
1258
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.756 | TAw acc= 95.5%, forg=  1.7%| TAg acc= 89.9%, forg=  0.0% <<<
>>> Test on task  1 : loss=0.895 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 88.0%, forg= -4.0% <<<
>>> Test on task  2 : loss=0.987 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 80.3%, forg= 10.3% <<<
>>> Test on task  3 : loss=0.941 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 84.2%, forg= -2.6% <<<
>>> Test on task  4 : loss=1.192 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 83.3%, forg= -7.5% <<<
>>> Test on task  5 : loss=1.346 | TAw acc= 93.9%, forg= -3.8%| TAg acc= 70.2%, forg= 13.0% <<<
>>> Test on task  6 : loss=1.159 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 68.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.680, TAw acc= 32.0% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=3.010, TAw acc= 50.7% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=2.308, TAw acc= 61.3% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.4s loss=1.876, TAw acc= 74.7% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.597, TAw acc= 80.0% | *
| Epoch   6, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.402, TAw acc= 81.3% | *
| Epoch   7, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 80.0% | *
| Epoch   8, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.151, TAw acc= 82.7% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.150, TAw acc= 82.7% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 82.7% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.148, TAw acc= 82.7% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.147, TAw acc= 82.7% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.146, TAw acc= 82.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1408 train exemplars, time=  0.0s
1408
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.759 | TAw acc= 97.8%, forg= -0.6%| TAg acc= 90.4%, forg= -0.6% <<<
>>> Test on task  1 : loss=0.797 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 88.0%, forg=  0.0% <<<
>>> Test on task  2 : loss=0.978 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.5%, forg= 11.1% <<<
>>> Test on task  3 : loss=0.876 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 84.2%, forg=  0.0% <<<
>>> Test on task  4 : loss=1.167 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 81.7%, forg=  1.7% <<<
>>> Test on task  5 : loss=1.183 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 79.4%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.224 | TAw acc= 95.2%, forg= -5.7%| TAg acc= 66.7%, forg=  1.9% <<<
>>> Test on task  7 : loss=1.154 | TAw acc= 87.5%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.902, TAw acc= 59.3% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.014, TAw acc= 78.0% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.367, TAw acc= 86.8% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.105, TAw acc= 93.4% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.022, TAw acc= 95.6% | *
| Epoch   6, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.878, TAw acc= 95.6% | *
| Epoch   7, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.832, TAw acc= 95.6% | *
| Epoch   8, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.739, TAw acc= 95.6% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.739, TAw acc= 95.6% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.739, TAw acc= 95.6% |
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.738, TAw acc= 95.6% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.738, TAw acc= 95.6% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.1s loss=0.738, TAw acc= 95.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1548 train exemplars, time=  0.0s
1548
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.785 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 87.1%, forg=  3.4% <<<
>>> Test on task  1 : loss=0.760 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 86.0%, forg=  2.0% <<<
>>> Test on task  2 : loss=0.941 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 80.3%, forg= 10.3% <<<
>>> Test on task  3 : loss=0.854 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.5%, forg=  1.8% <<<
>>> Test on task  4 : loss=1.124 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 85.8%, forg= -2.5% <<<
>>> Test on task  5 : loss=1.113 | TAw acc= 93.1%, forg=  0.8%| TAg acc= 79.4%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.118 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 70.5%, forg= -1.9% <<<
>>> Test on task  7 : loss=1.250 | TAw acc= 90.4%, forg= -2.9%| TAg acc= 73.1%, forg=  3.8% <<<
>>> Test on task  8 : loss=0.769 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 82.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=5.445, TAw acc= 40.0% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.732, TAw acc= 64.0% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.723, TAw acc= 73.3% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.300, TAw acc= 80.0% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.961, TAw acc= 89.3% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.767, TAw acc= 89.3% | *
| Epoch   7, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.610, TAw acc= 96.0% | *
| Epoch   8, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.500, TAw acc= 96.0% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.498, TAw acc= 96.0% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.496, TAw acc= 96.0% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.494, TAw acc= 96.0% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.492, TAw acc= 96.0% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.490, TAw acc= 96.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1688 train exemplars, time=  0.0s
1688
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.727 | TAw acc= 96.1%, forg=  1.7%| TAg acc= 89.3%, forg=  1.1% <<<
>>> Test on task  1 : loss=0.734 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 87.0%, forg=  1.0% <<<
>>> Test on task  2 : loss=0.942 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.5%, forg= 11.1% <<<
>>> Test on task  3 : loss=0.882 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.8%, forg=  4.4% <<<
>>> Test on task  4 : loss=1.067 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 85.0%, forg=  0.8% <<<
>>> Test on task  5 : loss=1.050 | TAw acc= 93.1%, forg=  0.8%| TAg acc= 78.6%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.032 | TAw acc= 92.4%, forg=  2.9%| TAg acc= 73.3%, forg= -2.9% <<<
>>> Test on task  7 : loss=1.136 | TAw acc= 92.3%, forg= -1.9%| TAg acc= 74.0%, forg=  2.9% <<<
>>> Test on task  8 : loss=1.258 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 57.9%, forg= 24.8% <<<
>>> Test on task  9 : loss=1.389 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 68.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=5.009, TAw acc= 44.4% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.113, TAw acc= 56.8% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.116, TAw acc= 80.2% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.606, TAw acc= 82.7% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.343, TAw acc= 86.4% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 92.6% | *
| Epoch   7, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.027, TAw acc= 96.3% | *
| Epoch   8, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.980, TAw acc= 97.5% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.978, TAw acc= 97.5% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.977, TAw acc= 97.5% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.975, TAw acc= 97.5% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.973, TAw acc= 97.5% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.972, TAw acc= 97.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1828 train exemplars, time=  0.0s
1828
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.812 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 82.6%, forg=  7.9% <<<
>>> Test on task  1 : loss=0.721 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 87.0%, forg=  1.0% <<<
>>> Test on task  2 : loss=0.941 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.6%, forg= 12.0% <<<
>>> Test on task  3 : loss=0.886 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.6%, forg=  9.6% <<<
>>> Test on task  4 : loss=1.116 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 80.0%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.066 | TAw acc= 92.4%, forg=  1.5%| TAg acc= 80.2%, forg=  3.1% <<<
>>> Test on task  6 : loss=0.977 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 78.1%, forg= -4.8% <<<
>>> Test on task  7 : loss=1.040 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 81.7%, forg= -4.8% <<<
>>> Test on task  8 : loss=1.193 | TAw acc= 97.5%, forg= -1.7%| TAg acc= 65.3%, forg= 17.4% <<<
>>> Test on task  9 : loss=1.448 | TAw acc= 94.1%, forg= -1.0%| TAg acc= 63.7%, forg=  4.9% <<<
>>> Test on task 10 : loss=1.019 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=5.874, TAw acc= 53.5% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.1s loss=3.610, TAw acc= 57.7% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.468, TAw acc= 77.5% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.906, TAw acc= 88.7% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.1s loss=1.597, TAw acc= 91.5% | *
| Epoch   6, time=  0.8s | Train: skip eval | Valid: time=  0.1s loss=1.381, TAw acc= 97.2% | *
| Epoch   7, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 98.6% | *
| Epoch   8, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.103, TAw acc= 98.6% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.103, TAw acc= 98.6% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.102, TAw acc= 98.6% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.102, TAw acc= 98.6% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.102, TAw acc= 98.6% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.101, TAw acc= 98.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1968 train exemplars, time=  0.0s
1968
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.815 | TAw acc= 96.1%, forg=  1.7%| TAg acc= 80.9%, forg=  9.6% <<<
>>> Test on task  1 : loss=0.720 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 88.0%, forg=  0.0% <<<
>>> Test on task  2 : loss=0.904 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 80.3%, forg= 10.3% <<<
>>> Test on task  3 : loss=0.855 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 83.3%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.020 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 85.8%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.040 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 78.6%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.022 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 75.2%, forg=  2.9% <<<
>>> Test on task  7 : loss=0.963 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 82.7%, forg= -1.0% <<<
>>> Test on task  8 : loss=1.058 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 68.6%, forg= 14.0% <<<
>>> Test on task  9 : loss=1.434 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 57.8%, forg= 10.8% <<<
>>> Test on task 10 : loss=1.215 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 61.5%, forg= 20.2% <<<
>>> Test on task 11 : loss=1.066 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=4.282, TAw acc= 51.5% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.340, TAw acc= 71.7% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.667, TAw acc= 81.8% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.303, TAw acc= 85.9% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.120, TAw acc= 90.9% | *
| Epoch   6, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.035, TAw acc= 90.9% | *
| Epoch   7, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.936, TAw acc= 90.9% | *
| Epoch   8, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.910, TAw acc= 91.9% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.909, TAw acc= 91.9% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.908, TAw acc= 91.9% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.907, TAw acc= 91.9% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.905, TAw acc= 91.9% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.904, TAw acc= 91.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2108 train exemplars, time=  0.0s
2108
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.766 | TAw acc= 96.1%, forg=  1.7%| TAg acc= 88.8%, forg=  1.7% <<<
>>> Test on task  1 : loss=0.724 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 87.0%, forg=  1.0% <<<
>>> Test on task  2 : loss=0.952 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.8%, forg= 12.8% <<<
>>> Test on task  3 : loss=0.967 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.9%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.120 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 80.8%, forg=  5.0% <<<
>>> Test on task  5 : loss=1.088 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 78.6%, forg=  4.6% <<<
>>> Test on task  6 : loss=0.977 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 75.2%, forg=  2.9% <<<
>>> Test on task  7 : loss=0.988 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 82.7%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.005 | TAw acc= 98.3%, forg= -0.8%| TAg acc= 75.2%, forg=  7.4% <<<
>>> Test on task  9 : loss=1.425 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 57.8%, forg= 10.8% <<<
>>> Test on task 10 : loss=1.031 | TAw acc=100.0%, forg= -1.8%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task 11 : loss=1.241 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 66.3%, forg=  5.1% <<<
>>> Test on task 12 : loss=0.867 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 78.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=4.709, TAw acc= 53.2% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.453, TAw acc= 67.0% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.764, TAw acc= 79.8% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.469, TAw acc= 87.2% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.430, TAw acc= 93.6% | *
| Epoch   6, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.270, TAw acc= 90.4% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.168, TAw acc= 92.6% | *
| Epoch   8, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.109, TAw acc= 94.7% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.106, TAw acc= 94.7% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.104, TAw acc= 94.7% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.101, TAw acc= 94.7% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.098, TAw acc= 94.7% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.096, TAw acc= 94.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2248 train exemplars, time=  0.0s
2248
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.780 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 84.8%, forg=  5.6% <<<
>>> Test on task  1 : loss=0.676 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 89.0%, forg= -1.0% <<<
>>> Test on task  2 : loss=1.003 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.6%, forg= 12.0% <<<
>>> Test on task  3 : loss=1.006 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.3%, forg= 14.9% <<<
>>> Test on task  4 : loss=1.128 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 79.2%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.077 | TAw acc= 93.1%, forg=  0.8%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=0.905 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 78.1%, forg=  0.0% <<<
>>> Test on task  7 : loss=0.984 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 84.6%, forg= -1.9% <<<
>>> Test on task  8 : loss=0.965 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.5%, forg=  4.1% <<<
>>> Test on task  9 : loss=1.289 | TAw acc= 97.1%, forg= -2.9%| TAg acc= 65.7%, forg=  2.9% <<<
>>> Test on task 10 : loss=0.937 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task 11 : loss=1.117 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg= -4.1% <<<
>>> Test on task 12 : loss=1.334 | TAw acc= 96.2%, forg= -0.8%| TAg acc= 57.7%, forg= 20.8% <<<
>>> Test on task 13 : loss=0.935 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 88.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=5.685, TAw acc= 31.2% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=3.993, TAw acc= 33.8% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.932, TAw acc= 64.9% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.305, TAw acc= 79.2% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.940, TAw acc= 85.7% | *
| Epoch   6, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.766, TAw acc= 85.7% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.656, TAw acc= 85.7% | *
| Epoch   8, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.575, TAw acc= 85.7% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.574, TAw acc= 85.7% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.572, TAw acc= 85.7% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.571, TAw acc= 85.7% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.570, TAw acc= 85.7% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.1s loss=1.569, TAw acc= 85.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2388 train exemplars, time=  0.0s
2388
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.723 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 87.6%, forg=  2.8% <<<
>>> Test on task  1 : loss=0.738 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 87.0%, forg=  2.0% <<<
>>> Test on task  2 : loss=1.025 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.8%, forg= 12.8% <<<
>>> Test on task  3 : loss=0.946 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.4%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.084 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 80.0%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.013 | TAw acc= 92.4%, forg=  1.5%| TAg acc= 79.4%, forg=  3.8% <<<
>>> Test on task  6 : loss=0.927 | TAw acc= 96.2%, forg= -1.0%| TAg acc= 75.2%, forg=  2.9% <<<
>>> Test on task  7 : loss=0.911 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 84.6%, forg=  0.0% <<<
>>> Test on task  8 : loss=0.997 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  9 : loss=1.289 | TAw acc= 95.1%, forg=  2.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.863 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.110 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.5%, forg= -1.0% <<<
>>> Test on task 12 : loss=1.208 | TAw acc= 94.6%, forg=  1.5%| TAg acc= 62.3%, forg= 16.2% <<<
>>> Test on task 13 : loss=1.217 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 69.6%, forg= 19.2% <<<
>>> Test on task 14 : loss=1.252 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 64.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=4.067, TAw acc= 65.3% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.251, TAw acc= 73.3% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.571, TAw acc= 83.2% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.292, TAw acc= 87.1% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.147, TAw acc= 93.1% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.024, TAw acc= 94.1% | *
| Epoch   7, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.921, TAw acc= 94.1% | *
| Epoch   8, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.882, TAw acc= 93.1% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.881, TAw acc= 93.1% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.881, TAw acc= 94.1% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=0.881, TAw acc= 94.1% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.3s loss=0.880, TAw acc= 94.1% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.880, TAw acc= 94.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2528 train exemplars, time=  0.0s
2528
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.815 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 78.7%, forg= 11.8% <<<
>>> Test on task  1 : loss=0.726 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 85.0%, forg=  4.0% <<<
>>> Test on task  2 : loss=1.019 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.1%, forg= 14.5% <<<
>>> Test on task  3 : loss=0.907 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 73.7%, forg= 10.5% <<<
>>> Test on task  4 : loss=1.238 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 75.0%, forg= 10.8% <<<
>>> Test on task  5 : loss=0.978 | TAw acc= 93.1%, forg=  0.8%| TAg acc= 81.7%, forg=  1.5% <<<
>>> Test on task  6 : loss=0.982 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 72.4%, forg=  5.7% <<<
>>> Test on task  7 : loss=0.894 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 81.7%, forg=  2.9% <<<
>>> Test on task  8 : loss=0.986 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  9 : loss=1.241 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 68.6%, forg=  0.0% <<<
>>> Test on task 10 : loss=0.882 | TAw acc=100.0%, forg=  0.0%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task 11 : loss=1.070 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.156 | TAw acc= 93.8%, forg=  2.3%| TAg acc= 61.5%, forg= 16.9% <<<
>>> Test on task 13 : loss=1.148 | TAw acc= 99.2%, forg= -1.6%| TAg acc= 73.6%, forg= 15.2% <<<
>>> Test on task 14 : loss=1.463 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 60.0%, forg=  4.8% <<<
>>> Test on task 15 : loss=0.931 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 78.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=5.762, TAw acc= 46.6% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.893, TAw acc= 67.1% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.902, TAw acc= 98.6% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.440, TAw acc= 98.6% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.277, TAw acc=100.0% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc=100.0% |
| Epoch   7, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.104, TAw acc= 97.3% | *
| Epoch   8, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.053, TAw acc=100.0% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.050, TAw acc=100.0% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.047, TAw acc=100.0% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.044, TAw acc=100.0% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.040, TAw acc=100.0% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.037, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2668 train exemplars, time=  0.0s
2668
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.747 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 85.4%, forg=  5.1% <<<
>>> Test on task  1 : loss=0.709 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 86.0%, forg=  3.0% <<<
>>> Test on task  2 : loss=1.013 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 78.6%, forg= 12.0% <<<
>>> Test on task  3 : loss=0.974 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 77.2%, forg=  7.0% <<<
>>> Test on task  4 : loss=1.166 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 80.0%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.045 | TAw acc= 93.1%, forg=  0.8%| TAg acc= 79.4%, forg=  3.8% <<<
>>> Test on task  6 : loss=0.941 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 78.1%, forg=  0.0% <<<
>>> Test on task  7 : loss=0.861 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 82.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.020 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  9 : loss=1.209 | TAw acc= 95.1%, forg=  2.0%| TAg acc= 67.6%, forg=  1.0% <<<
>>> Test on task 10 : loss=0.776 | TAw acc=100.0%, forg=  0.0%| TAg acc= 85.3%, forg= -3.7% <<<
>>> Test on task 11 : loss=0.988 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.6%, forg= -1.0% <<<
>>> Test on task 12 : loss=1.140 | TAw acc= 93.8%, forg=  2.3%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.163 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 73.6%, forg= 15.2% <<<
>>> Test on task 14 : loss=1.343 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 64.8%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.305 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 62.1%, forg= 15.9% <<<
>>> Test on task 16 : loss=1.048 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 75.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=5.750, TAw acc= 38.6% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=3.617, TAw acc= 54.3% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.568, TAw acc= 72.9% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.907, TAw acc= 85.7% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.658, TAw acc= 85.7% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.484, TAw acc= 85.7% | *
| Epoch   7, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 87.1% | *
| Epoch   8, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.235, TAw acc= 87.1% |
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.214, TAw acc= 87.1% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.215, TAw acc= 87.1% |
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.216, TAw acc= 87.1% |
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.217, TAw acc= 87.1% |
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.217, TAw acc= 87.1% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2808 train exemplars, time=  0.0s
2808
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.804 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 80.3%, forg= 10.1% <<<
>>> Test on task  1 : loss=0.749 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 88.0%, forg=  1.0% <<<
>>> Test on task  2 : loss=1.083 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 76.1%, forg= 14.5% <<<
>>> Test on task  3 : loss=1.003 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 70.2%, forg= 14.0% <<<
>>> Test on task  4 : loss=1.181 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 78.3%, forg=  7.5% <<<
>>> Test on task  5 : loss=1.061 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 76.3%, forg=  6.9% <<<
>>> Test on task  6 : loss=0.997 | TAw acc= 97.1%, forg= -1.0%| TAg acc= 73.3%, forg=  4.8% <<<
>>> Test on task  7 : loss=0.828 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 82.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=0.907 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  9 : loss=1.384 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.752 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.020 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.072 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 68.5%, forg= 10.0% <<<
>>> Test on task 13 : loss=1.037 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 75.2%, forg= 13.6% <<<
>>> Test on task 14 : loss=1.276 | TAw acc= 92.4%, forg= -1.0%| TAg acc= 68.6%, forg= -3.8% <<<
>>> Test on task 15 : loss=1.230 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 69.7%, forg=  8.3% <<<
>>> Test on task 16 : loss=1.203 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 61.8%, forg= 13.7% <<<
>>> Test on task 17 : loss=1.136 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 63.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=4.322, TAw acc= 52.0% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=2.410, TAw acc= 64.7% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.767, TAw acc= 84.3% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.430, TAw acc= 89.2% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.289, TAw acc= 91.2% | *
| Epoch   6, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.155, TAw acc= 91.2% | *
| Epoch   7, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.187, TAw acc= 88.2% |
| Epoch   8, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 91.2% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.126, TAw acc= 91.2% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.123, TAw acc= 91.2% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.120, TAw acc= 90.2% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.117, TAw acc= 90.2% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.114, TAw acc= 90.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2948 train exemplars, time=  0.0s
2948
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.808 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 79.8%, forg= 10.7% <<<
>>> Test on task  1 : loss=0.744 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 87.0%, forg=  2.0% <<<
>>> Test on task  2 : loss=1.148 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 72.6%, forg= 17.9% <<<
>>> Test on task  3 : loss=1.051 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.6%, forg=  9.6% <<<
>>> Test on task  4 : loss=1.227 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.3%, forg= 12.5% <<<
>>> Test on task  5 : loss=1.035 | TAw acc= 93.1%, forg=  0.8%| TAg acc= 78.6%, forg=  4.6% <<<
>>> Test on task  6 : loss=0.939 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 77.1%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.821 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 85.6%, forg= -1.0% <<<
>>> Test on task  8 : loss=0.973 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.9%, forg=  5.8% <<<
>>> Test on task  9 : loss=1.314 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  2.0% <<<
>>> Test on task 10 : loss=0.834 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.017 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.5%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.105 | TAw acc= 94.6%, forg=  1.5%| TAg acc= 66.9%, forg= 11.5% <<<
>>> Test on task 13 : loss=1.010 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 74.4%, forg= 14.4% <<<
>>> Test on task 14 : loss=1.283 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 63.8%, forg=  4.8% <<<
>>> Test on task 15 : loss=1.234 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 62.1%, forg= 15.9% <<<
>>> Test on task 16 : loss=1.092 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 66.7%, forg=  8.8% <<<
>>> Test on task 17 : loss=1.240 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 63.9%, forg=  0.0% <<<
>>> Test on task 18 : loss=0.935 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=6.388, TAw acc= 30.7% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=3.558, TAw acc= 73.3% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.320, TAw acc= 92.0% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.746, TAw acc= 94.7% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.440, TAw acc= 94.7% | *
| Epoch   6, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.372, TAw acc= 96.0% | *
| Epoch   7, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc= 93.3% | *
| Epoch   8, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.230, TAw acc= 94.7% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.228, TAw acc= 94.7% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.226, TAw acc= 94.7% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.224, TAw acc= 94.7% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 94.7% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.221, TAw acc= 94.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3088 train exemplars, time=  0.0s
3088
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.819 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 79.8%, forg= 10.7% <<<
>>> Test on task  1 : loss=0.744 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 83.0%, forg=  6.0% <<<
>>> Test on task  2 : loss=1.130 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.9%, forg= 13.7% <<<
>>> Test on task  3 : loss=1.064 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 73.7%, forg= 10.5% <<<
>>> Test on task  4 : loss=1.199 | TAw acc= 94.2%, forg= -0.8%| TAg acc= 73.3%, forg= 12.5% <<<
>>> Test on task  5 : loss=1.055 | TAw acc= 94.7%, forg= -0.8%| TAg acc= 76.3%, forg=  6.9% <<<
>>> Test on task  6 : loss=0.938 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 77.1%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.811 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 82.7%, forg=  2.9% <<<
>>> Test on task  8 : loss=1.080 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 67.8%, forg= 14.9% <<<
>>> Test on task  9 : loss=1.295 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 67.6%, forg=  1.0% <<<
>>> Test on task 10 : loss=0.769 | TAw acc=100.0%, forg=  0.0%| TAg acc= 80.7%, forg=  4.6% <<<
>>> Test on task 11 : loss=1.009 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.095 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 65.4%, forg= 13.1% <<<
>>> Test on task 13 : loss=1.028 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 72.8%, forg= 16.0% <<<
>>> Test on task 14 : loss=1.189 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 68.6%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.171 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 72.0%, forg=  6.1% <<<
>>> Test on task 16 : loss=1.036 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.6%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.125 | TAw acc= 94.8%, forg= -1.0%| TAg acc= 68.0%, forg= -4.1% <<<
>>> Test on task 18 : loss=1.195 | TAw acc= 89.6%, forg=  3.0%| TAg acc= 64.9%, forg= 11.9% <<<
>>> Test on task 19 : loss=1.083 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 72.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=5.233, TAw acc= 55.2% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=2.764, TAw acc= 78.2% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.807, TAw acc= 82.8% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.384, TAw acc= 87.4% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.132, TAw acc= 96.6% | *
| Epoch   6, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=0.996, TAw acc= 97.7% | *
| Epoch   7, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.924, TAw acc= 97.7% | *
| Epoch   8, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.887, TAw acc= 97.7% | *
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=0.885, TAw acc= 97.7% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.883, TAw acc= 97.7% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.882, TAw acc= 97.7% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.880, TAw acc= 97.7% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.878, TAw acc= 97.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3228 train exemplars, time=  0.0s
3228
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.789 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 84.3%, forg=  6.2% <<<
>>> Test on task  1 : loss=0.761 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 81.0%, forg=  8.0% <<<
>>> Test on task  2 : loss=1.165 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 75.2%, forg= 15.4% <<<
>>> Test on task  3 : loss=1.029 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.9%, forg= 12.3% <<<
>>> Test on task  4 : loss=1.204 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 74.2%, forg= 11.7% <<<
>>> Test on task  5 : loss=1.013 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 80.2%, forg=  3.1% <<<
>>> Test on task  6 : loss=0.998 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 73.3%, forg=  4.8% <<<
>>> Test on task  7 : loss=0.789 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 84.6%, forg=  1.0% <<<
>>> Test on task  8 : loss=0.945 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.9%, forg=  5.8% <<<
>>> Test on task  9 : loss=1.373 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  2.0% <<<
>>> Test on task 10 : loss=0.764 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.0%, forg=  7.3% <<<
>>> Test on task 11 : loss=1.006 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg=  3.1% <<<
>>> Test on task 12 : loss=1.112 | TAw acc= 95.4%, forg=  0.8%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.025 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 73.6%, forg= 15.2% <<<
>>> Test on task 14 : loss=1.178 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 68.6%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.177 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 71.2%, forg=  6.8% <<<
>>> Test on task 16 : loss=0.992 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.6%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.068 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 72.2%, forg= -4.1% <<<
>>> Test on task 18 : loss=1.253 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 59.7%, forg= 17.2% <<<
>>> Test on task 19 : loss=1.345 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 57.3%, forg= 15.5% <<<
>>> Test on task 20 : loss=0.965 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 76.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=6.057, TAw acc= 48.3% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.596, TAw acc= 77.0% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.568, TAw acc= 92.0% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.205, TAw acc= 92.0% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 95.4% | *
| Epoch   6, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.025, TAw acc= 95.4% | *
| Epoch   7, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.024, TAw acc= 94.3% | *
| Epoch   8, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 95.4% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 95.4% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.939, TAw acc= 95.4% |
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 95.4% |
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 95.4% |
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=0.941, TAw acc= 95.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3368 train exemplars, time=  0.0s
3368
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.803 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 80.9%, forg=  9.6% <<<
>>> Test on task  1 : loss=0.753 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 83.0%, forg=  6.0% <<<
>>> Test on task  2 : loss=1.208 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 75.2%, forg= 15.4% <<<
>>> Test on task  3 : loss=1.049 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.4%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.262 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 71.7%, forg= 14.2% <<<
>>> Test on task  5 : loss=1.129 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 76.3%, forg=  6.9% <<<
>>> Test on task  6 : loss=0.952 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 76.2%, forg=  1.9% <<<
>>> Test on task  7 : loss=0.852 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 80.8%, forg=  4.8% <<<
>>> Test on task  8 : loss=0.999 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.4%, forg=  8.3% <<<
>>> Test on task  9 : loss=1.314 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.743 | TAw acc=100.0%, forg=  0.0%| TAg acc= 84.4%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.013 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.159 | TAw acc= 95.4%, forg=  0.8%| TAg acc= 63.8%, forg= 14.6% <<<
>>> Test on task 13 : loss=1.030 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 74.4%, forg= 14.4% <<<
>>> Test on task 14 : loss=1.166 | TAw acc= 93.3%, forg= -1.0%| TAg acc= 69.5%, forg= -1.0% <<<
>>> Test on task 15 : loss=1.172 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 74.2%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.067 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.6%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.050 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 66.0%, forg=  6.2% <<<
>>> Test on task 18 : loss=1.122 | TAw acc= 91.8%, forg=  0.7%| TAg acc= 69.4%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.263 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 62.1%, forg= 10.7% <<<
>>> Test on task 20 : loss=1.353 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 57.8%, forg= 19.0% <<<
>>> Test on task 21 : loss=1.027 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 69.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=6.442, TAw acc= 27.6% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.988, TAw acc= 68.4% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=2.104, TAw acc= 80.3% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.595, TAw acc= 86.8% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.520, TAw acc= 84.2% | *
| Epoch   6, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.426, TAw acc= 84.2% | *
| Epoch   7, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.327, TAw acc= 84.2% | *
| Epoch   8, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.281, TAw acc= 85.5% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 85.5% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 85.5% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.273, TAw acc= 85.5% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.3s loss=1.270, TAw acc= 85.5% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 84.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3508 train exemplars, time=  0.0s
3508
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.844 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 78.1%, forg= 12.4% <<<
>>> Test on task  1 : loss=0.813 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 80.0%, forg=  9.0% <<<
>>> Test on task  2 : loss=1.211 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 76.9%, forg= 13.7% <<<
>>> Test on task  3 : loss=1.101 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.9%, forg= 12.3% <<<
>>> Test on task  4 : loss=1.317 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 69.2%, forg= 16.7% <<<
>>> Test on task  5 : loss=1.057 | TAw acc= 93.1%, forg=  1.5%| TAg acc= 77.9%, forg=  5.3% <<<
>>> Test on task  6 : loss=1.013 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 74.3%, forg=  3.8% <<<
>>> Test on task  7 : loss=0.815 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 83.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=0.933 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.3%, forg=  3.3% <<<
>>> Test on task  9 : loss=1.392 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  2.0% <<<
>>> Test on task 10 : loss=0.799 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.9%, forg=  6.4% <<<
>>> Test on task 11 : loss=0.993 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.162 | TAw acc= 95.4%, forg=  0.8%| TAg acc= 65.4%, forg= 13.1% <<<
>>> Test on task 13 : loss=1.021 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 70.4%, forg= 18.4% <<<
>>> Test on task 14 : loss=1.103 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 71.4%, forg= -1.9% <<<
>>> Test on task 15 : loss=1.131 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 75.8%, forg=  2.3% <<<
>>> Test on task 16 : loss=1.009 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.6%, forg=  4.9% <<<
>>> Test on task 17 : loss=0.980 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 71.1%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.180 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 62.7%, forg= 14.2% <<<
>>> Test on task 19 : loss=1.217 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 66.0%, forg=  6.8% <<<
>>> Test on task 20 : loss=1.316 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 63.8%, forg= 12.9% <<<
>>> Test on task 21 : loss=1.365 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 54.3%, forg= 15.5% <<<
>>> Test on task 22 : loss=1.085 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 71.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=6.374, TAw acc= 56.9% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=2.692, TAw acc= 79.2% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.574, TAw acc= 84.7% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.262, TAw acc= 86.1% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.110, TAw acc= 88.9% | *
| Epoch   6, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.029, TAw acc= 88.9% | *
| Epoch   7, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 88.9% | *
| Epoch   8, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.955, TAw acc= 90.3% |
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 88.9% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 88.9% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 88.9% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 88.9% |
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 88.9% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3648 train exemplars, time=  0.1s
3648
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.835 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 78.1%, forg= 12.4% <<<
>>> Test on task  1 : loss=0.794 | TAw acc=100.0%, forg= -1.0%| TAg acc= 81.0%, forg=  8.0% <<<
>>> Test on task  2 : loss=1.161 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 76.1%, forg= 14.5% <<<
>>> Test on task  3 : loss=1.125 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.8%, forg= 11.4% <<<
>>> Test on task  4 : loss=1.261 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 70.8%, forg= 15.0% <<<
>>> Test on task  5 : loss=1.068 | TAw acc= 95.4%, forg= -0.8%| TAg acc= 78.6%, forg=  4.6% <<<
>>> Test on task  6 : loss=0.989 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 77.1%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.853 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 83.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.046 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.4%, forg=  8.3% <<<
>>> Test on task  9 : loss=1.400 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 62.7%, forg=  5.9% <<<
>>> Test on task 10 : loss=0.765 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.6%, forg=  2.8% <<<
>>> Test on task 11 : loss=1.020 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.5%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.111 | TAw acc= 95.4%, forg=  0.8%| TAg acc= 64.6%, forg= 13.8% <<<
>>> Test on task 13 : loss=1.003 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 75.2%, forg= 13.6% <<<
>>> Test on task 14 : loss=1.202 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 69.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.122 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 75.8%, forg=  2.3% <<<
>>> Test on task 16 : loss=0.986 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.6%, forg=  5.9% <<<
>>> Test on task 17 : loss=1.021 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 72.2%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.059 | TAw acc= 91.8%, forg=  0.7%| TAg acc= 70.9%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.249 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 66.0%, forg=  6.8% <<<
>>> Test on task 20 : loss=1.241 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 64.7%, forg= 12.1% <<<
>>> Test on task 21 : loss=1.312 | TAw acc= 95.7%, forg= -1.7%| TAg acc= 59.5%, forg= 10.3% <<<
>>> Test on task 22 : loss=1.462 | TAw acc= 84.6%, forg=  6.7%| TAg acc= 50.0%, forg= 21.2% <<<
>>> Test on task 23 : loss=1.201 | TAw acc= 86.0%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=6.558, TAw acc= 53.5% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=3.318, TAw acc= 70.4% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.990, TAw acc= 87.3% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.484, TAw acc= 97.2% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.215, TAw acc= 97.2% | *
| Epoch   6, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.143, TAw acc= 97.2% | *
| Epoch   7, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.108, TAw acc= 95.8% | *
| Epoch   8, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.987, TAw acc= 97.2% | *
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=0.986, TAw acc= 97.2% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=0.985, TAw acc= 97.2% | *
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.983, TAw acc= 97.2% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=0.982, TAw acc= 97.2% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=0.981, TAw acc= 97.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3788 train exemplars, time=  0.0s
3788
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.875 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 77.0%, forg= 13.5% <<<
>>> Test on task  1 : loss=0.772 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.0%, forg=  6.0% <<<
>>> Test on task  2 : loss=1.202 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 76.9%, forg= 13.7% <<<
>>> Test on task  3 : loss=1.100 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.9%, forg= 12.3% <<<
>>> Test on task  4 : loss=1.266 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 75.0%, forg= 10.8% <<<
>>> Test on task  5 : loss=1.062 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 76.3%, forg=  6.9% <<<
>>> Test on task  6 : loss=1.049 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 75.2%, forg=  2.9% <<<
>>> Test on task  7 : loss=0.817 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 83.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=0.996 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  9 : loss=1.374 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  2.0% <<<
>>> Test on task 10 : loss=0.767 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.5%, forg=  1.8% <<<
>>> Test on task 11 : loss=0.992 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.1% <<<
>>> Test on task 12 : loss=1.129 | TAw acc= 93.8%, forg=  2.3%| TAg acc= 67.7%, forg= 10.8% <<<
>>> Test on task 13 : loss=1.000 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 72.8%, forg= 16.0% <<<
>>> Test on task 14 : loss=1.156 | TAw acc= 94.3%, forg= -1.0%| TAg acc= 72.4%, forg= -1.0% <<<
>>> Test on task 15 : loss=1.112 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 76.5%, forg=  1.5% <<<
>>> Test on task 16 : loss=0.919 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.6%, forg=  4.9% <<<
>>> Test on task 17 : loss=1.017 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 73.2%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.116 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 66.4%, forg= 10.4% <<<
>>> Test on task 19 : loss=1.171 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 70.9%, forg=  1.9% <<<
>>> Test on task 20 : loss=1.274 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 64.7%, forg= 12.1% <<<
>>> Test on task 21 : loss=1.283 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 62.9%, forg=  6.9% <<<
>>> Test on task 22 : loss=1.336 | TAw acc= 86.5%, forg=  4.8%| TAg acc= 52.9%, forg= 18.3% <<<
>>> Test on task 23 : loss=1.408 | TAw acc= 86.0%, forg=  0.0%| TAg acc= 62.0%, forg= 13.0% <<<
>>> Test on task 24 : loss=1.065 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 79.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=5.341, TAw acc= 62.9% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.661, TAw acc= 70.8% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.817, TAw acc= 88.8% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.350, TAw acc= 94.4% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.277, TAw acc= 95.5% | *
| Epoch   6, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.194, TAw acc= 95.5% | *
| Epoch   7, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=0.944, TAw acc= 94.4% | *
| Epoch   8, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.049, TAw acc= 94.4% |
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.945, TAw acc= 94.4% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=0.948, TAw acc= 94.4% |
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.951, TAw acc= 94.4% |
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.955, TAw acc= 94.4% |
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.959, TAw acc= 94.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3928 train exemplars, time=  0.0s
3928
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.895 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 77.0%, forg= 13.5% <<<
>>> Test on task  1 : loss=0.819 | TAw acc=100.0%, forg=  0.0%| TAg acc= 79.0%, forg= 10.0% <<<
>>> Test on task  2 : loss=1.302 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 71.8%, forg= 18.8% <<<
>>> Test on task  3 : loss=1.170 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.5%, forg= 16.7% <<<
>>> Test on task  4 : loss=1.506 | TAw acc= 95.0%, forg= -0.8%| TAg acc= 69.2%, forg= 16.7% <<<
>>> Test on task  5 : loss=1.072 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 76.3%, forg=  6.9% <<<
>>> Test on task  6 : loss=1.045 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 77.1%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.849 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 83.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.003 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.9%, forg=  5.8% <<<
>>> Test on task  9 : loss=1.420 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.814 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.004 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.212 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 63.8%, forg= 14.6% <<<
>>> Test on task 13 : loss=1.118 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 72.8%, forg= 16.0% <<<
>>> Test on task 14 : loss=1.234 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 69.5%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.115 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 77.3%, forg=  0.8% <<<
>>> Test on task 16 : loss=0.998 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 66.7%, forg=  8.8% <<<
>>> Test on task 17 : loss=1.041 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 75.3%, forg= -2.1% <<<
>>> Test on task 18 : loss=1.189 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 66.4%, forg= 10.4% <<<
>>> Test on task 19 : loss=1.204 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 63.1%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.191 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 68.1%, forg=  8.6% <<<
>>> Test on task 21 : loss=1.384 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 59.5%, forg= 10.3% <<<
>>> Test on task 22 : loss=1.285 | TAw acc= 89.4%, forg=  1.9%| TAg acc= 54.8%, forg= 16.3% <<<
>>> Test on task 23 : loss=1.406 | TAw acc= 85.0%, forg=  1.0%| TAg acc= 67.0%, forg=  8.0% <<<
>>> Test on task 24 : loss=1.353 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 66.3%, forg= 13.3% <<<
>>> Test on task 25 : loss=0.848 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 81.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=6.463, TAw acc= 42.3% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=3.152, TAw acc= 57.7% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.030, TAw acc= 77.5% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.657, TAw acc= 78.9% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.530, TAw acc= 84.5% | *
| Epoch   6, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.450, TAw acc= 87.3% | *
| Epoch   7, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.359, TAw acc= 85.9% | *
| Epoch   8, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.295, TAw acc= 88.7% | *
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.288, TAw acc= 88.7% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.283, TAw acc= 88.7% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 87.3% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc= 87.3% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.271, TAw acc= 87.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4068 train exemplars, time=  0.0s
4068
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.916 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 76.4%, forg= 14.0% <<<
>>> Test on task  1 : loss=0.797 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.0%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.337 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 70.1%, forg= 20.5% <<<
>>> Test on task  3 : loss=1.131 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.1%, forg= 13.2% <<<
>>> Test on task  4 : loss=1.330 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 74.2%, forg= 11.7% <<<
>>> Test on task  5 : loss=1.052 | TAw acc= 93.9%, forg=  1.5%| TAg acc= 77.1%, forg=  6.1% <<<
>>> Test on task  6 : loss=1.049 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 70.5%, forg=  7.6% <<<
>>> Test on task  7 : loss=0.814 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 84.6%, forg=  1.0% <<<
>>> Test on task  8 : loss=1.019 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.2%, forg=  7.4% <<<
>>> Test on task  9 : loss=1.503 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 59.8%, forg=  8.8% <<<
>>> Test on task 10 : loss=0.750 | TAw acc=100.0%, forg=  0.0%| TAg acc= 85.3%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.009 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.4%, forg=  5.1% <<<
>>> Test on task 12 : loss=1.148 | TAw acc= 94.6%, forg=  1.5%| TAg acc= 65.4%, forg= 13.1% <<<
>>> Test on task 13 : loss=1.077 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 71.2%, forg= 17.6% <<<
>>> Test on task 14 : loss=1.194 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.145 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 75.8%, forg=  2.3% <<<
>>> Test on task 16 : loss=1.015 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.6%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.013 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 75.3%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.025 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 69.4%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.148 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 67.0%, forg=  5.8% <<<
>>> Test on task 20 : loss=1.221 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 63.8%, forg= 12.9% <<<
>>> Test on task 21 : loss=1.288 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 66.4%, forg=  3.4% <<<
>>> Test on task 22 : loss=1.230 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 58.7%, forg= 12.5% <<<
>>> Test on task 23 : loss=1.390 | TAw acc= 87.0%, forg= -1.0%| TAg acc= 67.0%, forg=  8.0% <<<
>>> Test on task 24 : loss=1.220 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 71.4%, forg=  8.2% <<<
>>> Test on task 25 : loss=1.185 | TAw acc= 96.6%, forg= -1.7%| TAg acc= 68.1%, forg= 13.4% <<<
>>> Test on task 26 : loss=1.213 | TAw acc= 92.8%, forg=  0.0%| TAg acc= 72.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=6.406, TAw acc= 47.1% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=3.585, TAw acc= 81.4% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=2.478, TAw acc= 88.6% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=2.059, TAw acc= 87.1% | *
| Epoch   5, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.823, TAw acc= 90.0% | *
| Epoch   6, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.667, TAw acc= 91.4% | *
| Epoch   7, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.536, TAw acc= 90.0% | *
| Epoch   8, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.465, TAw acc= 90.0% | *
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.1s loss=1.470, TAw acc= 90.0% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.474, TAw acc= 90.0% |
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.477, TAw acc= 90.0% |
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.480, TAw acc= 90.0% |
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.483, TAw acc= 90.0% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4208 train exemplars, time=  0.0s
4208
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.921 | TAw acc= 96.1%, forg=  1.7%| TAg acc= 79.2%, forg= 11.2% <<<
>>> Test on task  1 : loss=0.763 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.0%, forg=  8.0% <<<
>>> Test on task  2 : loss=1.274 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.5%, forg= 17.1% <<<
>>> Test on task  3 : loss=1.180 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.4%, forg= 15.8% <<<
>>> Test on task  4 : loss=1.352 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 73.3%, forg= 12.5% <<<
>>> Test on task  5 : loss=1.057 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 77.9%, forg=  5.3% <<<
>>> Test on task  6 : loss=1.031 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 77.1%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.873 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 78.8%, forg=  6.7% <<<
>>> Test on task  8 : loss=1.182 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.1%, forg= 11.6% <<<
>>> Test on task  9 : loss=1.415 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 65.7%, forg=  2.9% <<<
>>> Test on task 10 : loss=0.743 | TAw acc=100.0%, forg=  0.0%| TAg acc= 79.8%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.001 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.1% <<<
>>> Test on task 12 : loss=1.246 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 60.8%, forg= 17.7% <<<
>>> Test on task 13 : loss=1.060 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 72.0%, forg= 16.8% <<<
>>> Test on task 14 : loss=1.234 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 68.6%, forg=  3.8% <<<
>>> Test on task 15 : loss=1.187 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 75.0%, forg=  3.0% <<<
>>> Test on task 16 : loss=0.987 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.6%, forg=  7.8% <<<
>>> Test on task 17 : loss=0.991 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 77.3%, forg= -2.1% <<<
>>> Test on task 18 : loss=1.204 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 60.4%, forg= 16.4% <<<
>>> Test on task 19 : loss=1.247 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 57.3%, forg= 15.5% <<<
>>> Test on task 20 : loss=1.245 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 66.4%, forg= 10.3% <<<
>>> Test on task 21 : loss=1.351 | TAw acc= 94.0%, forg=  1.7%| TAg acc= 61.2%, forg=  8.6% <<<
>>> Test on task 22 : loss=1.189 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 61.5%, forg=  9.6% <<<
>>> Test on task 23 : loss=1.388 | TAw acc= 83.0%, forg=  4.0%| TAg acc= 70.0%, forg=  5.0% <<<
>>> Test on task 24 : loss=1.166 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 67.3%, forg= 12.2% <<<
>>> Test on task 25 : loss=1.137 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 67.2%, forg= 14.3% <<<
>>> Test on task 26 : loss=1.467 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 54.6%, forg= 17.5% <<<
>>> Test on task 27 : loss=1.094 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 78.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=6.352, TAw acc= 53.2% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=3.001, TAw acc= 76.6% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=2.000, TAw acc= 90.9% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.1s loss=1.384, TAw acc= 97.4% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 98.7% |
| Epoch   6, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 98.7% | *
| Epoch   7, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.265, TAw acc= 98.7% | *
| Epoch   8, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.152, TAw acc= 98.7% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 97.4% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.146, TAw acc= 97.4% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.143, TAw acc= 97.4% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.140, TAw acc= 97.4% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.138, TAw acc= 97.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4348 train exemplars, time=  0.0s
4348
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.979 | TAw acc= 96.1%, forg=  1.7%| TAg acc= 74.2%, forg= 16.3% <<<
>>> Test on task  1 : loss=0.815 | TAw acc=100.0%, forg=  0.0%| TAg acc= 80.0%, forg=  9.0% <<<
>>> Test on task  2 : loss=1.292 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 72.6%, forg= 17.9% <<<
>>> Test on task  3 : loss=1.254 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.3%, forg= 14.9% <<<
>>> Test on task  4 : loss=1.374 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 71.7%, forg= 14.2% <<<
>>> Test on task  5 : loss=1.038 | TAw acc= 93.9%, forg=  1.5%| TAg acc= 78.6%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.080 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 71.4%, forg=  6.7% <<<
>>> Test on task  7 : loss=0.768 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 82.7%, forg=  2.9% <<<
>>> Test on task  8 : loss=1.019 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.7%, forg=  5.0% <<<
>>> Test on task  9 : loss=1.456 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 63.7%, forg=  4.9% <<<
>>> Test on task 10 : loss=0.786 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.6%, forg=  2.8% <<<
>>> Test on task 11 : loss=1.072 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg=  3.1% <<<
>>> Test on task 12 : loss=1.166 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 66.9%, forg= 11.5% <<<
>>> Test on task 13 : loss=1.053 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 72.0%, forg= 16.8% <<<
>>> Test on task 14 : loss=1.165 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 70.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.154 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 75.0%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.017 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 66.7%, forg=  8.8% <<<
>>> Test on task 17 : loss=0.976 | TAw acc= 92.8%, forg=  2.1%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.123 | TAw acc= 91.8%, forg=  0.7%| TAg acc= 67.9%, forg=  9.0% <<<
>>> Test on task 19 : loss=1.185 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 64.1%, forg=  8.7% <<<
>>> Test on task 20 : loss=1.188 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 67.2%, forg=  9.5% <<<
>>> Test on task 21 : loss=1.354 | TAw acc= 93.1%, forg=  2.6%| TAg acc= 62.1%, forg=  7.8% <<<
>>> Test on task 22 : loss=1.162 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 61.5%, forg=  9.6% <<<
>>> Test on task 23 : loss=1.335 | TAw acc= 87.0%, forg=  0.0%| TAg acc= 72.0%, forg=  3.0% <<<
>>> Test on task 24 : loss=1.186 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 68.4%, forg= 11.2% <<<
>>> Test on task 25 : loss=1.128 | TAw acc= 95.0%, forg=  1.7%| TAg acc= 68.9%, forg= 12.6% <<<
>>> Test on task 26 : loss=1.415 | TAw acc= 93.8%, forg= -1.0%| TAg acc= 51.5%, forg= 20.6% <<<
>>> Test on task 27 : loss=1.576 | TAw acc= 96.9%, forg= -2.1%| TAg acc= 52.6%, forg= 25.8% <<<
>>> Test on task 28 : loss=1.154 | TAw acc=100.0%, forg=  0.0%| TAg acc= 64.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=6.897, TAw acc= 59.0% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=3.216, TAw acc= 73.1% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.877, TAw acc= 88.5% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 92.3% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.240, TAw acc= 94.9% | *
| Epoch   6, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.103, TAw acc= 96.2% | *
| Epoch   7, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.023, TAw acc= 98.7% | *
| Epoch   8, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.923, TAw acc= 97.4% | *
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=0.922, TAw acc= 97.4% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=0.922, TAw acc= 97.4% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=0.922, TAw acc= 97.4% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.922, TAw acc= 96.2% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.922, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4488 train exemplars, time=  0.0s
4488
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.943 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 76.4%, forg= 14.0% <<<
>>> Test on task  1 : loss=0.806 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.0%, forg= 12.0% <<<
>>> Test on task  2 : loss=1.308 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.5%, forg= 17.1% <<<
>>> Test on task  3 : loss=1.179 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.6%, forg=  9.6% <<<
>>> Test on task  4 : loss=1.370 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 71.7%, forg= 14.2% <<<
>>> Test on task  5 : loss=1.094 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 76.3%, forg=  6.9% <<<
>>> Test on task  6 : loss=1.053 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 75.2%, forg=  2.9% <<<
>>> Test on task  7 : loss=0.844 | TAw acc= 89.4%, forg=  2.9%| TAg acc= 82.7%, forg=  2.9% <<<
>>> Test on task  8 : loss=1.033 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.4%, forg=  8.3% <<<
>>> Test on task  9 : loss=1.502 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 62.7%, forg=  5.9% <<<
>>> Test on task 10 : loss=0.748 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.5%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.114 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.1% <<<
>>> Test on task 12 : loss=1.174 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 65.4%, forg= 13.1% <<<
>>> Test on task 13 : loss=1.110 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 71.2%, forg= 17.6% <<<
>>> Test on task 14 : loss=1.183 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 72.4%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.145 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 78.0%, forg=  0.0% <<<
>>> Test on task 16 : loss=0.952 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 73.5%, forg=  2.0% <<<
>>> Test on task 17 : loss=0.999 | TAw acc= 92.8%, forg=  2.1%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.087 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 64.9%, forg= 11.9% <<<
>>> Test on task 19 : loss=1.119 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 68.0%, forg=  4.9% <<<
>>> Test on task 20 : loss=1.245 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 69.0%, forg=  7.8% <<<
>>> Test on task 21 : loss=1.336 | TAw acc= 93.1%, forg=  2.6%| TAg acc= 59.5%, forg= 10.3% <<<
>>> Test on task 22 : loss=1.177 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 56.7%, forg= 14.4% <<<
>>> Test on task 23 : loss=1.401 | TAw acc= 87.0%, forg=  0.0%| TAg acc= 72.0%, forg=  3.0% <<<
>>> Test on task 24 : loss=1.137 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.4%, forg=  9.2% <<<
>>> Test on task 25 : loss=1.166 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 64.7%, forg= 16.8% <<<
>>> Test on task 26 : loss=1.361 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 58.8%, forg= 13.4% <<<
>>> Test on task 27 : loss=1.520 | TAw acc= 97.9%, forg= -1.0%| TAg acc= 58.8%, forg= 19.6% <<<
>>> Test on task 28 : loss=1.426 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 60.0%, forg=  4.8% <<<
>>> Test on task 29 : loss=0.971 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.2%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=6.412, TAw acc= 56.3% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=2.764, TAw acc= 66.7% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.558, TAw acc= 81.6% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.147, TAw acc= 88.5% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.092, TAw acc= 86.2% | *
| Epoch   6, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.004, TAw acc= 93.1% | *
| Epoch   7, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.982, TAw acc= 94.3% | *
| Epoch   8, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.965, TAw acc= 92.0% | *
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.960, TAw acc= 92.0% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.956, TAw acc= 92.0% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.952, TAw acc= 92.0% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.948, TAw acc= 92.0% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.944, TAw acc= 92.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4628 train exemplars, time=  0.0s
4628
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.990 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 74.2%, forg= 16.3% <<<
>>> Test on task  1 : loss=0.852 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.0%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.346 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.5%, forg= 17.1% <<<
>>> Test on task  3 : loss=1.212 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.4%, forg= 15.8% <<<
>>> Test on task  4 : loss=1.424 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 68.3%, forg= 17.5% <<<
>>> Test on task  5 : loss=1.086 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 77.1%, forg=  6.1% <<<
>>> Test on task  6 : loss=1.076 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 74.3%, forg=  3.8% <<<
>>> Test on task  7 : loss=0.867 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 81.7%, forg=  3.8% <<<
>>> Test on task  8 : loss=1.015 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.4%, forg=  8.3% <<<
>>> Test on task  9 : loss=1.588 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 62.7%, forg=  5.9% <<<
>>> Test on task 10 : loss=0.733 | TAw acc=100.0%, forg=  0.0%| TAg acc= 86.2%, forg= -0.9% <<<
>>> Test on task 11 : loss=1.100 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.1% <<<
>>> Test on task 12 : loss=1.274 | TAw acc= 92.3%, forg=  3.8%| TAg acc= 59.2%, forg= 19.2% <<<
>>> Test on task 13 : loss=1.119 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.4%, forg= 22.4% <<<
>>> Test on task 14 : loss=1.317 | TAw acc= 92.4%, forg=  1.9%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.161 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 78.8%, forg= -0.8% <<<
>>> Test on task 16 : loss=1.044 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 65.7%, forg=  9.8% <<<
>>> Test on task 17 : loss=1.003 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 77.3%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.046 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 69.4%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.157 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 69.9%, forg=  2.9% <<<
>>> Test on task 20 : loss=1.244 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 63.8%, forg= 12.9% <<<
>>> Test on task 21 : loss=1.480 | TAw acc= 93.1%, forg=  2.6%| TAg acc= 56.9%, forg= 12.9% <<<
>>> Test on task 22 : loss=1.115 | TAw acc= 86.5%, forg=  4.8%| TAg acc= 60.6%, forg= 10.6% <<<
>>> Test on task 23 : loss=1.369 | TAw acc= 86.0%, forg=  1.0%| TAg acc= 73.0%, forg=  2.0% <<<
>>> Test on task 24 : loss=1.114 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.5%, forg=  5.1% <<<
>>> Test on task 25 : loss=1.128 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.2%, forg= 14.3% <<<
>>> Test on task 26 : loss=1.325 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 62.9%, forg=  9.3% <<<
>>> Test on task 27 : loss=1.627 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 61.9%, forg= 16.5% <<<
>>> Test on task 28 : loss=1.299 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 63.8%, forg=  1.0% <<<
>>> Test on task 29 : loss=1.137 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 74.8%, forg=  7.5% <<<
>>> Test on task 30 : loss=1.012 | TAw acc= 92.3%, forg=  0.0%| TAg acc= 68.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=6.521, TAw acc= 46.5% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=3.147, TAw acc= 76.1% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.732, TAw acc= 84.5% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.446, TAw acc= 91.5% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.308, TAw acc= 90.1% | *
| Epoch   6, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.162, TAw acc= 91.5% | *
| Epoch   7, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.208, TAw acc= 90.1% |
| Epoch   8, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.071, TAw acc= 98.6% | *
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.069, TAw acc= 95.8% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.067, TAw acc= 95.8% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.066, TAw acc= 95.8% | *
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.064, TAw acc= 95.8% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.063, TAw acc= 95.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4768 train exemplars, time=  0.1s
4768
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.025 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 73.6%, forg= 16.9% <<<
>>> Test on task  1 : loss=0.809 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.0%, forg= 12.0% <<<
>>> Test on task  2 : loss=1.371 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 70.1%, forg= 20.5% <<<
>>> Test on task  3 : loss=1.208 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.1%, forg= 13.2% <<<
>>> Test on task  4 : loss=1.390 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 73.3%, forg= 12.5% <<<
>>> Test on task  5 : loss=1.092 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.098 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 74.3%, forg=  3.8% <<<
>>> Test on task  7 : loss=0.851 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 77.9%, forg=  7.7% <<<
>>> Test on task  8 : loss=1.161 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.9%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.452 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 66.7%, forg=  2.0% <<<
>>> Test on task 10 : loss=0.744 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.6%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.113 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.264 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 63.8%, forg= 14.6% <<<
>>> Test on task 13 : loss=1.076 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 69.6%, forg= 19.2% <<<
>>> Test on task 14 : loss=1.325 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 67.6%, forg=  4.8% <<<
>>> Test on task 15 : loss=1.148 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 76.5%, forg=  2.3% <<<
>>> Test on task 16 : loss=0.920 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.6%, forg=  3.9% <<<
>>> Test on task 17 : loss=1.033 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.077 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 67.9%, forg=  9.0% <<<
>>> Test on task 19 : loss=1.155 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 66.0%, forg=  6.8% <<<
>>> Test on task 20 : loss=1.284 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 57.8%, forg= 19.0% <<<
>>> Test on task 21 : loss=1.334 | TAw acc= 93.1%, forg=  2.6%| TAg acc= 63.8%, forg=  6.0% <<<
>>> Test on task 22 : loss=1.279 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 57.7%, forg= 13.5% <<<
>>> Test on task 23 : loss=1.404 | TAw acc= 86.0%, forg=  1.0%| TAg acc= 72.0%, forg=  3.0% <<<
>>> Test on task 24 : loss=1.156 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 69.4%, forg= 10.2% <<<
>>> Test on task 25 : loss=1.143 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.5%, forg= 16.0% <<<
>>> Test on task 26 : loss=1.341 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 60.8%, forg= 11.3% <<<
>>> Test on task 27 : loss=1.578 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 62.9%, forg= 15.5% <<<
>>> Test on task 28 : loss=1.261 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 60.0%, forg=  4.8% <<<
>>> Test on task 29 : loss=1.110 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 74.8%, forg=  7.5% <<<
>>> Test on task 30 : loss=1.294 | TAw acc= 93.2%, forg= -0.9%| TAg acc= 53.8%, forg= 14.5% <<<
>>> Test on task 31 : loss=1.215 | TAw acc= 92.8%, forg=  0.0%| TAg acc= 62.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=4.764, TAw acc= 59.8% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=2.632, TAw acc= 76.1% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.678, TAw acc= 82.6% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.399, TAw acc= 90.2% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.270, TAw acc= 91.3% | *
| Epoch   6, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.091, TAw acc= 92.4% | *
| Epoch   7, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.167, TAw acc= 88.0% |
| Epoch   8, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.056, TAw acc= 92.4% | *
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 92.4% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.058, TAw acc= 92.4% |
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.059, TAw acc= 92.4% |
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.060, TAw acc= 93.5% |
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.062, TAw acc= 93.5% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4908 train exemplars, time=  0.0s
4908
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.079 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 70.8%, forg= 19.7% <<<
>>> Test on task  1 : loss=0.881 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.0%, forg= 12.0% <<<
>>> Test on task  2 : loss=1.343 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 72.6%, forg= 17.9% <<<
>>> Test on task  3 : loss=1.231 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.4%, forg= 15.8% <<<
>>> Test on task  4 : loss=1.458 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 68.3%, forg= 17.5% <<<
>>> Test on task  5 : loss=1.033 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 77.1%, forg=  6.1% <<<
>>> Test on task  6 : loss=1.087 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 74.3%, forg=  3.8% <<<
>>> Test on task  7 : loss=0.963 | TAw acc= 89.4%, forg=  2.9%| TAg acc= 74.0%, forg= 11.5% <<<
>>> Test on task  8 : loss=1.078 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.2%, forg=  7.4% <<<
>>> Test on task  9 : loss=1.542 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 63.7%, forg=  4.9% <<<
>>> Test on task 10 : loss=0.742 | TAw acc=100.0%, forg=  0.0%| TAg acc= 84.4%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.100 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.1% <<<
>>> Test on task 12 : loss=1.324 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 57.7%, forg= 20.8% <<<
>>> Test on task 13 : loss=1.132 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 69.6%, forg= 19.2% <<<
>>> Test on task 14 : loss=1.253 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 72.4%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.338 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 68.2%, forg= 10.6% <<<
>>> Test on task 16 : loss=1.021 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 66.7%, forg=  8.8% <<<
>>> Test on task 17 : loss=1.047 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 73.2%, forg=  4.1% <<<
>>> Test on task 18 : loss=1.085 | TAw acc= 91.8%, forg=  0.7%| TAg acc= 70.9%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.206 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 63.1%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.298 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 61.2%, forg= 15.5% <<<
>>> Test on task 21 : loss=1.423 | TAw acc= 92.2%, forg=  3.4%| TAg acc= 60.3%, forg=  9.5% <<<
>>> Test on task 22 : loss=1.125 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 64.4%, forg=  6.7% <<<
>>> Test on task 23 : loss=1.381 | TAw acc= 86.0%, forg=  1.0%| TAg acc= 76.0%, forg= -1.0% <<<
>>> Test on task 24 : loss=1.129 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.4%, forg=  9.2% <<<
>>> Test on task 25 : loss=1.138 | TAw acc= 97.5%, forg= -0.8%| TAg acc= 68.1%, forg= 13.4% <<<
>>> Test on task 26 : loss=1.364 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 62.9%, forg=  9.3% <<<
>>> Test on task 27 : loss=1.600 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.8%, forg= 19.6% <<<
>>> Test on task 28 : loss=1.291 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.9%, forg=  2.9% <<<
>>> Test on task 29 : loss=1.099 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 77.6%, forg=  4.7% <<<
>>> Test on task 30 : loss=1.239 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 56.4%, forg= 12.0% <<<
>>> Test on task 31 : loss=1.418 | TAw acc= 90.7%, forg=  2.1%| TAg acc= 56.7%, forg=  6.2% <<<
>>> Test on task 32 : loss=1.070 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 74.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=7.725, TAw acc= 39.7% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=4.282, TAw acc= 61.5% | *
| Epoch   3, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=2.549, TAw acc= 88.5% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.3s loss=1.862, TAw acc= 92.3% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.579, TAw acc= 93.6% | *
| Epoch   6, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 93.6% | *
| Epoch   7, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.370, TAw acc= 92.3% |
| Epoch   8, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.351, TAw acc= 94.9% |
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 93.6% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 93.6% |
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 93.6% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 93.6% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 93.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5048 train exemplars, time=  0.0s
5048
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.084 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 71.3%, forg= 19.1% <<<
>>> Test on task  1 : loss=0.852 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.0%, forg= 12.0% <<<
>>> Test on task  2 : loss=1.403 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 72.6%, forg= 17.9% <<<
>>> Test on task  3 : loss=1.331 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg= 17.5% <<<
>>> Test on task  4 : loss=1.385 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 72.5%, forg= 13.3% <<<
>>> Test on task  5 : loss=1.068 | TAw acc= 93.9%, forg=  1.5%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.142 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 72.4%, forg=  5.7% <<<
>>> Test on task  7 : loss=0.900 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 78.8%, forg=  6.7% <<<
>>> Test on task  8 : loss=1.101 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.6%, forg=  9.1% <<<
>>> Test on task  9 : loss=1.597 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 63.7%, forg=  4.9% <<<
>>> Test on task 10 : loss=0.729 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.5%, forg=  2.8% <<<
>>> Test on task 11 : loss=1.111 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg=  3.1% <<<
>>> Test on task 12 : loss=1.266 | TAw acc= 92.3%, forg=  3.8%| TAg acc= 63.8%, forg= 14.6% <<<
>>> Test on task 13 : loss=1.127 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 71.2%, forg= 17.6% <<<
>>> Test on task 14 : loss=1.276 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.190 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 78.0%, forg=  0.8% <<<
>>> Test on task 16 : loss=0.966 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 67.6%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.053 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 76.3%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.050 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 72.4%, forg=  4.5% <<<
>>> Test on task 19 : loss=1.278 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 58.3%, forg= 14.6% <<<
>>> Test on task 20 : loss=1.305 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 62.9%, forg= 13.8% <<<
>>> Test on task 21 : loss=1.390 | TAw acc= 93.1%, forg=  2.6%| TAg acc= 62.9%, forg=  6.9% <<<
>>> Test on task 22 : loss=1.239 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 61.5%, forg=  9.6% <<<
>>> Test on task 23 : loss=1.401 | TAw acc= 84.0%, forg=  3.0%| TAg acc= 72.0%, forg=  4.0% <<<
>>> Test on task 24 : loss=1.151 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 69.4%, forg= 10.2% <<<
>>> Test on task 25 : loss=1.246 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 61.3%, forg= 20.2% <<<
>>> Test on task 26 : loss=1.424 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 60.8%, forg= 11.3% <<<
>>> Test on task 27 : loss=1.661 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 60.8%, forg= 17.5% <<<
>>> Test on task 28 : loss=1.313 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.0%, forg=  3.8% <<<
>>> Test on task 29 : loss=1.087 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 79.4%, forg=  2.8% <<<
>>> Test on task 30 : loss=1.235 | TAw acc= 94.0%, forg= -0.9%| TAg acc= 58.1%, forg= 10.3% <<<
>>> Test on task 31 : loss=1.417 | TAw acc= 92.8%, forg=  0.0%| TAg acc= 60.8%, forg=  2.1% <<<
>>> Test on task 32 : loss=1.481 | TAw acc= 91.8%, forg=  0.8%| TAg acc= 60.7%, forg= 13.9% <<<
>>> Test on task 33 : loss=1.406 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 58.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=5.666, TAw acc= 65.9% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=2.575, TAw acc= 78.8% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.931, TAw acc= 87.1% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.669, TAw acc= 88.2% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.520, TAw acc= 87.1% | *
| Epoch   6, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.393, TAw acc= 89.4% | *
| Epoch   7, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.343, TAw acc= 91.8% | *
| Epoch   8, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.355, TAw acc= 91.8% |
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.341, TAw acc= 91.8% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.340, TAw acc= 91.8% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.339, TAw acc= 91.8% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.338, TAw acc= 91.8% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.337, TAw acc= 91.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5188 train exemplars, time=  0.0s
5188
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.093 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 75.8%, forg= 14.6% <<<
>>> Test on task  1 : loss=0.839 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.0%, forg= 13.0% <<<
>>> Test on task  2 : loss=1.405 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 71.8%, forg= 18.8% <<<
>>> Test on task  3 : loss=1.297 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg= 17.5% <<<
>>> Test on task  4 : loss=1.518 | TAw acc= 93.3%, forg=  1.7%| TAg acc= 67.5%, forg= 18.3% <<<
>>> Test on task  5 : loss=1.085 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 74.8%, forg=  8.4% <<<
>>> Test on task  6 : loss=1.145 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 74.3%, forg=  3.8% <<<
>>> Test on task  7 : loss=0.938 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 78.8%, forg=  6.7% <<<
>>> Test on task  8 : loss=1.227 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.1%, forg= 11.6% <<<
>>> Test on task  9 : loss=1.482 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 62.7%, forg=  5.9% <<<
>>> Test on task 10 : loss=0.783 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.6%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.109 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 12 : loss=1.213 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 67.7%, forg= 10.8% <<<
>>> Test on task 13 : loss=1.216 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 67.2%, forg= 21.6% <<<
>>> Test on task 14 : loss=1.314 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.357 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 68.9%, forg=  9.8% <<<
>>> Test on task 16 : loss=1.052 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 64.7%, forg= 10.8% <<<
>>> Test on task 17 : loss=1.091 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 77.3%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.120 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 69.4%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.243 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 63.1%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.316 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 62.1%, forg= 14.7% <<<
>>> Test on task 21 : loss=1.517 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 60.3%, forg=  9.5% <<<
>>> Test on task 22 : loss=1.211 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 61.5%, forg=  9.6% <<<
>>> Test on task 23 : loss=1.388 | TAw acc= 85.0%, forg=  2.0%| TAg acc= 71.0%, forg=  5.0% <<<
>>> Test on task 24 : loss=1.168 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 68.4%, forg= 11.2% <<<
>>> Test on task 25 : loss=1.160 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 66.4%, forg= 15.1% <<<
>>> Test on task 26 : loss=1.311 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 64.9%, forg=  7.2% <<<
>>> Test on task 27 : loss=1.567 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 62.9%, forg= 15.5% <<<
>>> Test on task 28 : loss=1.221 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 62.9%, forg=  1.9% <<<
>>> Test on task 29 : loss=1.121 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 76.6%, forg=  5.6% <<<
>>> Test on task 30 : loss=1.278 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 53.0%, forg= 15.4% <<<
>>> Test on task 31 : loss=1.354 | TAw acc= 93.8%, forg= -1.0%| TAg acc= 61.9%, forg=  1.0% <<<
>>> Test on task 32 : loss=1.407 | TAw acc= 91.0%, forg=  1.6%| TAg acc= 64.8%, forg=  9.8% <<<
>>> Test on task 33 : loss=1.804 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 43.4%, forg= 15.1% <<<
>>> Test on task 34 : loss=1.198 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 68.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=6.228, TAw acc= 43.2% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=3.784, TAw acc= 65.4% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=2.569, TAw acc= 80.2% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.717, TAw acc= 95.1% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.487, TAw acc= 98.8% | *
| Epoch   6, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.385, TAw acc= 98.8% | *
| Epoch   7, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 98.8% | *
| Epoch   8, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.137, TAw acc= 98.8% | *
| Epoch   1, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.137, TAw acc= 98.8% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.137, TAw acc= 98.8% | *
| Epoch   3, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.136, TAw acc= 98.8% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.136, TAw acc= 98.8% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.136, TAw acc= 98.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5323 train exemplars, time=  0.0s
5323
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.078 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 72.5%, forg= 18.0% <<<
>>> Test on task  1 : loss=0.870 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.0%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.422 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 70.1%, forg= 20.5% <<<
>>> Test on task  3 : loss=1.235 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.9%, forg= 12.3% <<<
>>> Test on task  4 : loss=1.529 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 67.5%, forg= 18.3% <<<
>>> Test on task  5 : loss=1.101 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 74.0%, forg=  9.2% <<<
>>> Test on task  6 : loss=1.246 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 71.4%, forg=  6.7% <<<
>>> Test on task  7 : loss=0.908 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 79.8%, forg=  5.8% <<<
>>> Test on task  8 : loss=1.208 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.9%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.575 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.760 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  4.6% <<<
>>> Test on task 11 : loss=1.149 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.1% <<<
>>> Test on task 12 : loss=1.294 | TAw acc= 91.5%, forg=  4.6%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.202 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 68.8%, forg= 20.0% <<<
>>> Test on task 14 : loss=1.330 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 70.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.410 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 69.7%, forg=  9.1% <<<
>>> Test on task 16 : loss=1.039 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 67.6%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.038 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 77.3%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.048 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 70.1%, forg=  6.7% <<<
>>> Test on task 19 : loss=1.243 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 63.1%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.270 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 62.9%, forg= 13.8% <<<
>>> Test on task 21 : loss=1.437 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 62.9%, forg=  6.9% <<<
>>> Test on task 22 : loss=1.278 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 62.5%, forg=  8.7% <<<
>>> Test on task 23 : loss=1.444 | TAw acc= 83.0%, forg=  4.0%| TAg acc= 72.0%, forg=  4.0% <<<
>>> Test on task 24 : loss=1.101 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 73.5%, forg=  6.1% <<<
>>> Test on task 25 : loss=1.262 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 60.5%, forg= 21.0% <<<
>>> Test on task 26 : loss=1.413 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 62.9%, forg=  9.3% <<<
>>> Test on task 27 : loss=1.719 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.8%, forg= 19.6% <<<
>>> Test on task 28 : loss=1.160 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 66.7%, forg= -1.9% <<<
>>> Test on task 29 : loss=1.102 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 79.4%, forg=  2.8% <<<
>>> Test on task 30 : loss=1.166 | TAw acc= 93.2%, forg=  0.9%| TAg acc= 61.5%, forg=  6.8% <<<
>>> Test on task 31 : loss=1.332 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 63.9%, forg= -1.0% <<<
>>> Test on task 32 : loss=1.458 | TAw acc= 91.0%, forg=  1.6%| TAg acc= 59.8%, forg= 14.8% <<<
>>> Test on task 33 : loss=1.803 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 42.5%, forg= 16.0% <<<
>>> Test on task 34 : loss=1.478 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 57.0%, forg= 11.4% <<<
>>> Test on task 35 : loss=0.829 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 80.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=5.644, TAw acc= 57.1% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=2.604, TAw acc= 81.8% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.504, TAw acc= 87.0% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 92.2% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.050, TAw acc= 92.2% | *
| Epoch   6, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.069, TAw acc= 90.9% |
| Epoch   7, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.137, TAw acc= 93.5% |
| Epoch   8, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.081, TAw acc= 93.5% |
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.054, TAw acc= 92.2% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.058, TAw acc= 92.2% |
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.061, TAw acc= 92.2% |
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.064, TAw acc= 92.2% |
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.067, TAw acc= 92.2% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5443 train exemplars, time=  0.0s
5443
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.169 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 65.7%, forg= 24.7% <<<
>>> Test on task  1 : loss=0.900 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.0%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.438 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 71.8%, forg= 18.8% <<<
>>> Test on task  3 : loss=1.320 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.4%, forg= 15.8% <<<
>>> Test on task  4 : loss=1.462 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 71.7%, forg= 14.2% <<<
>>> Test on task  5 : loss=1.115 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 71.8%, forg= 11.5% <<<
>>> Test on task  6 : loss=1.195 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 70.5%, forg=  7.6% <<<
>>> Test on task  7 : loss=0.969 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 75.0%, forg= 10.6% <<<
>>> Test on task  8 : loss=1.114 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.4%, forg=  8.3% <<<
>>> Test on task  9 : loss=1.634 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 62.7%, forg=  5.9% <<<
>>> Test on task 10 : loss=0.825 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  4.6% <<<
>>> Test on task 11 : loss=1.163 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.4%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.268 | TAw acc= 90.8%, forg=  5.4%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.297 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.4%, forg= 22.4% <<<
>>> Test on task 14 : loss=1.345 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 70.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.282 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 73.5%, forg=  5.3% <<<
>>> Test on task 16 : loss=0.989 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.6%, forg=  3.9% <<<
>>> Test on task 17 : loss=1.113 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 74.2%, forg=  3.1% <<<
>>> Test on task 18 : loss=1.034 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 72.4%, forg=  4.5% <<<
>>> Test on task 19 : loss=1.305 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 58.3%, forg= 14.6% <<<
>>> Test on task 20 : loss=1.356 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task 21 : loss=1.529 | TAw acc= 93.1%, forg=  2.6%| TAg acc= 58.6%, forg= 11.2% <<<
>>> Test on task 22 : loss=1.209 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 60.6%, forg= 10.6% <<<
>>> Test on task 23 : loss=1.471 | TAw acc= 87.0%, forg=  0.0%| TAg acc= 75.0%, forg=  1.0% <<<
>>> Test on task 24 : loss=1.044 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 77.6%, forg=  2.0% <<<
>>> Test on task 25 : loss=1.233 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 68.9%, forg= 12.6% <<<
>>> Test on task 26 : loss=1.448 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 62.9%, forg=  9.3% <<<
>>> Test on task 27 : loss=1.678 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 62.9%, forg= 15.5% <<<
>>> Test on task 28 : loss=1.310 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.0%, forg=  5.7% <<<
>>> Test on task 29 : loss=1.075 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 79.4%, forg=  2.8% <<<
>>> Test on task 30 : loss=1.181 | TAw acc= 92.3%, forg=  1.7%| TAg acc= 61.5%, forg=  6.8% <<<
>>> Test on task 31 : loss=1.345 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 63.9%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.623 | TAw acc= 91.8%, forg=  0.8%| TAg acc= 63.1%, forg= 11.5% <<<
>>> Test on task 33 : loss=1.691 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 52.8%, forg=  5.7% <<<
>>> Test on task 34 : loss=1.549 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 54.4%, forg= 14.0% <<<
>>> Test on task 35 : loss=1.041 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 79.1%, forg=  0.9% <<<
>>> Test on task 36 : loss=1.268 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 65.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=5.012, TAw acc= 55.3% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=2.068, TAw acc= 85.1% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.334, TAw acc= 96.8% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.188, TAw acc= 90.4% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.026, TAw acc= 90.4% | *
| Epoch   6, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.084, TAw acc= 97.9% |
| Epoch   7, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.903, TAw acc= 90.4% | *
| Epoch   8, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.903, TAw acc= 97.9% | *
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.904, TAw acc= 97.9% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.906, TAw acc= 97.9% |
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.908, TAw acc= 97.9% |
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.909, TAw acc= 97.9% |
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.910, TAw acc= 97.9% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5563 train exemplars, time=  0.1s
5563
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.218 | TAw acc= 95.5%, forg=  2.2%| TAg acc= 68.0%, forg= 22.5% <<<
>>> Test on task  1 : loss=0.877 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.0%, forg= 13.0% <<<
>>> Test on task  2 : loss=1.454 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 71.8%, forg= 18.8% <<<
>>> Test on task  3 : loss=1.420 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 64.0%, forg= 20.2% <<<
>>> Test on task  4 : loss=1.582 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 67.5%, forg= 18.3% <<<
>>> Test on task  5 : loss=1.090 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.191 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 76.2%, forg=  1.9% <<<
>>> Test on task  7 : loss=0.956 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 79.8%, forg=  5.8% <<<
>>> Test on task  8 : loss=1.195 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.9%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.531 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 61.8%, forg=  6.9% <<<
>>> Test on task 10 : loss=0.769 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.6%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.176 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 74.5%, forg=  3.1% <<<
>>> Test on task 12 : loss=1.263 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 63.8%, forg= 14.6% <<<
>>> Test on task 13 : loss=1.356 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 63.2%, forg= 25.6% <<<
>>> Test on task 14 : loss=1.311 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.373 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 75.0%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.002 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 69.6%, forg=  5.9% <<<
>>> Test on task 17 : loss=1.061 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 76.3%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.077 | TAw acc= 89.6%, forg=  3.0%| TAg acc= 70.9%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.347 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 62.1%, forg= 10.7% <<<
>>> Test on task 20 : loss=1.340 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 62.1%, forg= 14.7% <<<
>>> Test on task 21 : loss=1.460 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 63.8%, forg=  6.0% <<<
>>> Test on task 22 : loss=1.447 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 59.6%, forg= 11.5% <<<
>>> Test on task 23 : loss=1.512 | TAw acc= 86.0%, forg=  1.0%| TAg acc= 73.0%, forg=  3.0% <<<
>>> Test on task 24 : loss=1.143 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 70.4%, forg=  9.2% <<<
>>> Test on task 25 : loss=1.196 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 66.4%, forg= 15.1% <<<
>>> Test on task 26 : loss=1.354 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 64.9%, forg=  7.2% <<<
>>> Test on task 27 : loss=1.769 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 54.6%, forg= 23.7% <<<
>>> Test on task 28 : loss=1.291 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.0%, forg=  5.7% <<<
>>> Test on task 29 : loss=1.043 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 80.4%, forg=  1.9% <<<
>>> Test on task 30 : loss=1.204 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 60.7%, forg=  7.7% <<<
>>> Test on task 31 : loss=1.374 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 60.8%, forg=  3.1% <<<
>>> Test on task 32 : loss=1.552 | TAw acc= 94.3%, forg= -1.6%| TAg acc= 63.9%, forg= 10.7% <<<
>>> Test on task 33 : loss=1.673 | TAw acc= 94.3%, forg=  0.9%| TAg acc= 57.5%, forg=  0.9% <<<
>>> Test on task 34 : loss=1.462 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 57.0%, forg= 11.4% <<<
>>> Test on task 35 : loss=0.966 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 78.2%, forg=  1.8% <<<
>>> Test on task 36 : loss=1.607 | TAw acc= 88.6%, forg=  1.0%| TAg acc= 51.4%, forg= 14.3% <<<
>>> Test on task 37 : loss=0.849 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 81.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=4.497, TAw acc= 64.6% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.986, TAw acc= 84.4% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.328, TAw acc= 96.9% | *
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.225, TAw acc= 99.0% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.931, TAw acc= 99.0% | *
| Epoch   6, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=0.922, TAw acc= 99.0% | *
| Epoch   7, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.903, TAw acc= 99.0% | *
| Epoch   8, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.800, TAw acc= 99.0% | *
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.797, TAw acc= 99.0% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=0.794, TAw acc= 99.0% | *
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.792, TAw acc= 99.0% | *
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=0.790, TAw acc= 99.0% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.788, TAw acc= 99.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5683 train exemplars, time=  0.0s
5683
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.209 | TAw acc= 95.5%, forg=  2.2%| TAg acc= 68.0%, forg= 22.5% <<<
>>> Test on task  1 : loss=0.885 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.0%, forg= 14.0% <<<
>>> Test on task  2 : loss=1.471 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.2%, forg= 21.4% <<<
>>> Test on task  3 : loss=1.365 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.4%, forg= 15.8% <<<
>>> Test on task  4 : loss=1.546 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 70.0%, forg= 15.8% <<<
>>> Test on task  5 : loss=1.133 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.205 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 74.3%, forg=  3.8% <<<
>>> Test on task  7 : loss=0.892 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 80.8%, forg=  4.8% <<<
>>> Test on task  8 : loss=1.173 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.6%, forg=  9.1% <<<
>>> Test on task  9 : loss=1.597 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.727 | TAw acc=100.0%, forg=  0.0%| TAg acc= 84.4%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.175 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 72.4%, forg=  5.1% <<<
>>> Test on task 12 : loss=1.407 | TAw acc= 93.1%, forg=  3.1%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.311 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.4%, forg= 22.4% <<<
>>> Test on task 14 : loss=1.347 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.351 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 72.0%, forg=  6.8% <<<
>>> Test on task 16 : loss=1.086 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.6%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.096 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.038 | TAw acc= 90.3%, forg=  2.2%| TAg acc= 70.1%, forg=  6.7% <<<
>>> Test on task 19 : loss=1.269 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 66.0%, forg=  6.8% <<<
>>> Test on task 20 : loss=1.323 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 61.2%, forg= 15.5% <<<
>>> Test on task 21 : loss=1.486 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 62.1%, forg=  7.8% <<<
>>> Test on task 22 : loss=1.232 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 59.6%, forg= 11.5% <<<
>>> Test on task 23 : loss=1.525 | TAw acc= 85.0%, forg=  2.0%| TAg acc= 73.0%, forg=  3.0% <<<
>>> Test on task 24 : loss=1.119 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 71.4%, forg=  8.2% <<<
>>> Test on task 25 : loss=1.393 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 61.3%, forg= 20.2% <<<
>>> Test on task 26 : loss=1.347 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 66.0%, forg=  6.2% <<<
>>> Test on task 27 : loss=1.737 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 61.9%, forg= 16.5% <<<
>>> Test on task 28 : loss=1.171 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 63.8%, forg=  2.9% <<<
>>> Test on task 29 : loss=1.045 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 76.6%, forg=  5.6% <<<
>>> Test on task 30 : loss=1.178 | TAw acc= 94.9%, forg= -0.9%| TAg acc= 59.8%, forg=  8.5% <<<
>>> Test on task 31 : loss=1.324 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 63.9%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.632 | TAw acc= 92.6%, forg=  1.6%| TAg acc= 61.5%, forg= 13.1% <<<
>>> Test on task 33 : loss=1.834 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 53.8%, forg=  4.7% <<<
>>> Test on task 34 : loss=1.520 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 51.8%, forg= 16.7% <<<
>>> Test on task 35 : loss=0.936 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 76.4%, forg=  3.6% <<<
>>> Test on task 36 : loss=1.637 | TAw acc= 88.6%, forg=  1.0%| TAg acc= 48.6%, forg= 17.1% <<<
>>> Test on task 37 : loss=1.299 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 64.0%, forg= 17.6% <<<
>>> Test on task 38 : loss=0.743 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 85.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=5.266, TAw acc= 52.1% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=2.499, TAw acc= 81.7% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.439, TAw acc= 98.6% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.974, TAw acc=100.0% | *
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.091, TAw acc= 98.6% |
| Epoch   6, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.857, TAw acc=100.0% | *
| Epoch   7, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.954, TAw acc=100.0% |
| Epoch   8, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.827, TAw acc=100.0% | *
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.823, TAw acc=100.0% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.820, TAw acc=100.0% | *
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.817, TAw acc=100.0% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.814, TAw acc=100.0% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.811, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5803 train exemplars, time=  0.0s
5803
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.239 | TAw acc= 94.9%, forg=  2.8%| TAg acc= 66.9%, forg= 23.6% <<<
>>> Test on task  1 : loss=0.891 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.0%, forg= 14.0% <<<
>>> Test on task  2 : loss=1.502 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 70.9%, forg= 19.7% <<<
>>> Test on task  3 : loss=1.345 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.3%, forg= 14.9% <<<
>>> Test on task  4 : loss=1.588 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 69.2%, forg= 16.7% <<<
>>> Test on task  5 : loss=1.109 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.251 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 73.3%, forg=  4.8% <<<
>>> Test on task  7 : loss=0.949 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 78.8%, forg=  6.7% <<<
>>> Test on task  8 : loss=1.169 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.1%, forg= 11.6% <<<
>>> Test on task  9 : loss=1.636 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 63.7%, forg=  4.9% <<<
>>> Test on task 10 : loss=0.734 | TAw acc=100.0%, forg=  0.0%| TAg acc= 85.3%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.164 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 71.4%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.425 | TAw acc= 90.8%, forg=  5.4%| TAg acc= 62.3%, forg= 16.2% <<<
>>> Test on task 13 : loss=1.276 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 67.2%, forg= 21.6% <<<
>>> Test on task 14 : loss=1.313 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 74.3%, forg= -1.9% <<<
>>> Test on task 15 : loss=1.368 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 74.2%, forg=  4.5% <<<
>>> Test on task 16 : loss=1.014 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 72.5%, forg=  2.9% <<<
>>> Test on task 17 : loss=1.131 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.047 | TAw acc= 89.6%, forg=  3.0%| TAg acc= 70.1%, forg=  6.7% <<<
>>> Test on task 19 : loss=1.298 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 62.1%, forg= 10.7% <<<
>>> Test on task 20 : loss=1.350 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 63.8%, forg= 12.9% <<<
>>> Test on task 21 : loss=1.481 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 62.1%, forg=  7.8% <<<
>>> Test on task 22 : loss=1.293 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 62.5%, forg=  8.7% <<<
>>> Test on task 23 : loss=1.543 | TAw acc= 83.0%, forg=  4.0%| TAg acc= 70.0%, forg=  6.0% <<<
>>> Test on task 24 : loss=1.113 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 74.5%, forg=  5.1% <<<
>>> Test on task 25 : loss=1.324 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.7%, forg= 16.8% <<<
>>> Test on task 26 : loss=1.413 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 61.9%, forg= 10.3% <<<
>>> Test on task 27 : loss=1.748 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 62.9%, forg= 15.5% <<<
>>> Test on task 28 : loss=1.203 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 64.8%, forg=  1.9% <<<
>>> Test on task 29 : loss=1.070 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 78.5%, forg=  3.7% <<<
>>> Test on task 30 : loss=1.165 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 64.1%, forg=  4.3% <<<
>>> Test on task 31 : loss=1.231 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 69.1%, forg= -5.2% <<<
>>> Test on task 32 : loss=1.544 | TAw acc= 91.8%, forg=  2.5%| TAg acc= 66.4%, forg=  8.2% <<<
>>> Test on task 33 : loss=1.642 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 57.5%, forg=  0.9% <<<
>>> Test on task 34 : loss=1.423 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 61.4%, forg=  7.0% <<<
>>> Test on task 35 : loss=0.871 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 80.0%, forg=  0.0% <<<
>>> Test on task 36 : loss=1.671 | TAw acc= 88.6%, forg=  1.0%| TAg acc= 51.4%, forg= 14.3% <<<
>>> Test on task 37 : loss=1.164 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 67.2%, forg= 14.4% <<<
>>> Test on task 38 : loss=1.213 | TAw acc= 97.6%, forg=  1.6%| TAg acc= 60.6%, forg= 24.4% <<<
>>> Test on task 39 : loss=1.052 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 69.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.7s | Train: skip eval | Valid: time=  0.1s loss=6.316, TAw acc= 56.4% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=2.966, TAw acc= 88.5% | *
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.623, TAw acc= 98.7% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.338, TAw acc= 98.7% | *
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 98.7% | *
| Epoch   6, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.161, TAw acc= 98.7% | *
| Epoch   7, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.220, TAw acc= 98.7% |
| Epoch   8, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.125, TAw acc= 98.7% | *
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.123, TAw acc= 98.7% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.120, TAw acc= 98.7% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.119, TAw acc= 98.7% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.117, TAw acc= 98.7% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.115, TAw acc= 98.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5923 train exemplars, time=  0.0s
5923
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.202 | TAw acc= 95.5%, forg=  2.2%| TAg acc= 70.2%, forg= 20.2% <<<
>>> Test on task  1 : loss=0.851 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.0%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.486 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 70.1%, forg= 20.5% <<<
>>> Test on task  3 : loss=1.347 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.5%, forg= 16.7% <<<
>>> Test on task  4 : loss=1.667 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 65.0%, forg= 20.8% <<<
>>> Test on task  5 : loss=1.200 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 74.8%, forg=  8.4% <<<
>>> Test on task  6 : loss=1.213 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 75.2%, forg=  2.9% <<<
>>> Test on task  7 : loss=0.986 | TAw acc= 91.3%, forg=  1.0%| TAg acc= 77.9%, forg=  7.7% <<<
>>> Test on task  8 : loss=1.416 | TAw acc= 97.5%, forg=  0.8%| TAg acc= 66.9%, forg= 15.7% <<<
>>> Test on task  9 : loss=1.576 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 65.7%, forg=  2.9% <<<
>>> Test on task 10 : loss=0.785 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 82.6%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.204 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 72.4%, forg=  5.1% <<<
>>> Test on task 12 : loss=1.391 | TAw acc= 91.5%, forg=  4.6%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.331 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.4%, forg= 22.4% <<<
>>> Test on task 14 : loss=1.362 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.370 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 74.2%, forg=  4.5% <<<
>>> Test on task 16 : loss=1.010 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.6%, forg=  3.9% <<<
>>> Test on task 17 : loss=1.092 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.086 | TAw acc= 89.6%, forg=  3.0%| TAg acc= 69.4%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.377 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 62.1%, forg= 10.7% <<<
>>> Test on task 20 : loss=1.342 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task 21 : loss=1.457 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 66.4%, forg=  3.4% <<<
>>> Test on task 22 : loss=1.255 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 57.7%, forg= 13.5% <<<
>>> Test on task 23 : loss=1.626 | TAw acc= 84.0%, forg=  3.0%| TAg acc= 70.0%, forg=  6.0% <<<
>>> Test on task 24 : loss=1.129 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.4%, forg=  7.1% <<<
>>> Test on task 25 : loss=1.220 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 68.1%, forg= 13.4% <<<
>>> Test on task 26 : loss=1.428 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 63.9%, forg=  8.2% <<<
>>> Test on task 27 : loss=1.774 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 60.8%, forg= 17.5% <<<
>>> Test on task 28 : loss=1.320 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.0%, forg=  5.7% <<<
>>> Test on task 29 : loss=1.100 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 77.6%, forg=  4.7% <<<
>>> Test on task 30 : loss=1.099 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 66.7%, forg=  1.7% <<<
>>> Test on task 31 : loss=1.204 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 70.1%, forg= -1.0% <<<
>>> Test on task 32 : loss=1.614 | TAw acc= 92.6%, forg=  1.6%| TAg acc= 66.4%, forg=  8.2% <<<
>>> Test on task 33 : loss=1.634 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 58.5%, forg=  0.0% <<<
>>> Test on task 34 : loss=1.427 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 59.6%, forg=  8.8% <<<
>>> Test on task 35 : loss=0.883 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 79.1%, forg=  0.9% <<<
>>> Test on task 36 : loss=1.809 | TAw acc= 87.6%, forg=  1.9%| TAg acc= 50.5%, forg= 15.2% <<<
>>> Test on task 37 : loss=1.204 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 68.0%, forg= 13.6% <<<
>>> Test on task 38 : loss=1.166 | TAw acc= 97.6%, forg=  1.6%| TAg acc= 63.8%, forg= 21.3% <<<
>>> Test on task 39 : loss=1.262 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 66.7%, forg=  3.0% <<<
>>> Test on task 40 : loss=1.069 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 74.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=7.806, TAw acc= 30.9% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=4.004, TAw acc= 70.6% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=2.316, TAw acc= 88.2% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.620, TAw acc= 97.1% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.404, TAw acc= 97.1% | *
| Epoch   6, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.321, TAw acc= 95.6% | *
| Epoch   7, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.266, TAw acc= 95.6% | *
| Epoch   8, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.077, TAw acc= 95.6% | *
| Epoch   1, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.076, TAw acc= 95.6% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.076, TAw acc= 95.6% | *
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.076, TAw acc= 95.6% |
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.076, TAw acc= 95.6% |
| Epoch   5, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.077, TAw acc= 95.6% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 6043 train exemplars, time=  0.0s
6043
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.281 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 68.5%, forg= 21.9% <<<
>>> Test on task  1 : loss=0.971 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.0%, forg= 21.0% <<<
>>> Test on task  2 : loss=1.553 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.2%, forg= 21.4% <<<
>>> Test on task  3 : loss=1.400 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg= 17.5% <<<
>>> Test on task  4 : loss=1.637 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 65.8%, forg= 20.0% <<<
>>> Test on task  5 : loss=1.139 | TAw acc= 93.9%, forg=  1.5%| TAg acc= 75.6%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.339 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 70.5%, forg=  7.6% <<<
>>> Test on task  7 : loss=0.908 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 83.7%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.380 | TAw acc= 97.5%, forg=  0.8%| TAg acc= 68.6%, forg= 14.0% <<<
>>> Test on task  9 : loss=1.570 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 64.7%, forg=  3.9% <<<
>>> Test on task 10 : loss=0.760 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 82.6%, forg=  3.7% <<<
>>> Test on task 11 : loss=1.184 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 71.4%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.411 | TAw acc= 92.3%, forg=  3.8%| TAg acc= 63.1%, forg= 15.4% <<<
>>> Test on task 13 : loss=1.311 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 69.6%, forg= 19.2% <<<
>>> Test on task 14 : loss=1.351 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task 15 : loss=1.332 | TAw acc= 93.9%, forg=  0.8%| TAg acc= 77.3%, forg=  1.5% <<<
>>> Test on task 16 : loss=0.970 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 74.5%, forg=  1.0% <<<
>>> Test on task 17 : loss=1.144 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 74.2%, forg=  3.1% <<<
>>> Test on task 18 : loss=1.108 | TAw acc= 89.6%, forg=  3.0%| TAg acc= 67.9%, forg=  9.0% <<<
>>> Test on task 19 : loss=1.284 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 65.0%, forg=  7.8% <<<
>>> Test on task 20 : loss=1.327 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 62.9%, forg= 13.8% <<<
>>> Test on task 21 : loss=1.553 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 60.3%, forg=  9.5% <<<
>>> Test on task 22 : loss=1.231 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 57.7%, forg= 13.5% <<<
>>> Test on task 23 : loss=1.650 | TAw acc= 84.0%, forg=  3.0%| TAg acc= 73.0%, forg=  3.0% <<<
>>> Test on task 24 : loss=1.168 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 75.5%, forg=  4.1% <<<
>>> Test on task 25 : loss=1.253 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 67.2%, forg= 14.3% <<<
>>> Test on task 26 : loss=1.424 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 61.9%, forg= 10.3% <<<
>>> Test on task 27 : loss=1.832 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 61.9%, forg= 16.5% <<<
>>> Test on task 28 : loss=1.288 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.0%, forg=  5.7% <<<
>>> Test on task 29 : loss=1.024 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 81.3%, forg=  0.9% <<<
>>> Test on task 30 : loss=1.184 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 59.8%, forg=  8.5% <<<
>>> Test on task 31 : loss=1.233 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 69.1%, forg=  1.0% <<<
>>> Test on task 32 : loss=1.588 | TAw acc= 91.8%, forg=  2.5%| TAg acc= 63.1%, forg= 11.5% <<<
>>> Test on task 33 : loss=1.756 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 58.5%, forg=  0.0% <<<
>>> Test on task 34 : loss=1.377 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 60.5%, forg=  7.9% <<<
>>> Test on task 35 : loss=0.887 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 76.4%, forg=  3.6% <<<
>>> Test on task 36 : loss=1.715 | TAw acc= 87.6%, forg=  1.9%| TAg acc= 50.5%, forg= 15.2% <<<
>>> Test on task 37 : loss=1.196 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 65.6%, forg= 16.0% <<<
>>> Test on task 38 : loss=1.058 | TAw acc= 97.6%, forg=  1.6%| TAg acc= 67.7%, forg= 17.3% <<<
>>> Test on task 39 : loss=1.256 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 66.7%, forg=  3.0% <<<
>>> Test on task 40 : loss=1.479 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 56.6%, forg= 17.9% <<<
>>> Test on task 41 : loss=1.386 | TAw acc= 93.7%, forg=  0.0%| TAg acc= 69.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=5.771, TAw acc= 63.4% | *
| Epoch   2, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=2.754, TAw acc= 84.1% | *
| Epoch   3, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.907, TAw acc= 91.5% | *
| Epoch   4, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.407, TAw acc= 92.7% | *
| Epoch   5, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 95.1% | *
| Epoch   6, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.133, TAw acc= 95.1% | *
| Epoch   7, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.059, TAw acc= 96.3% | *
| Epoch   8, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.065, TAw acc= 97.6% |
| Epoch   1, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.061, TAw acc= 96.3% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.062, TAw acc= 96.3% |
| Epoch   3, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.064, TAw acc= 96.3% |
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.065, TAw acc= 96.3% |
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.066, TAw acc= 96.3% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 6163 train exemplars, time=  0.1s
6163
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.275 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 70.2%, forg= 20.2% <<<
>>> Test on task  1 : loss=0.965 | TAw acc=100.0%, forg=  0.0%| TAg acc= 72.0%, forg= 17.0% <<<
>>> Test on task  2 : loss=1.586 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 70.1%, forg= 20.5% <<<
>>> Test on task  3 : loss=1.414 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg= 17.5% <<<
>>> Test on task  4 : loss=1.670 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 69.2%, forg= 16.7% <<<
>>> Test on task  5 : loss=1.193 | TAw acc= 94.7%, forg=  0.8%| TAg acc= 74.0%, forg=  9.2% <<<
>>> Test on task  6 : loss=1.278 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 72.4%, forg=  5.7% <<<
>>> Test on task  7 : loss=0.976 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 77.9%, forg=  7.7% <<<
>>> Test on task  8 : loss=1.393 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 66.9%, forg= 15.7% <<<
>>> Test on task  9 : loss=1.691 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 61.8%, forg=  6.9% <<<
>>> Test on task 10 : loss=0.806 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 81.7%, forg=  4.6% <<<
>>> Test on task 11 : loss=1.234 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 71.4%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.439 | TAw acc= 92.3%, forg=  3.8%| TAg acc= 59.2%, forg= 19.2% <<<
>>> Test on task 13 : loss=1.307 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 68.8%, forg= 20.0% <<<
>>> Test on task 14 : loss=1.304 | TAw acc= 95.2%, forg= -1.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task 15 : loss=1.386 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 73.5%, forg=  5.3% <<<
>>> Test on task 16 : loss=1.060 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.6%, forg=  3.9% <<<
>>> Test on task 17 : loss=1.159 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 74.2%, forg=  3.1% <<<
>>> Test on task 18 : loss=1.242 | TAw acc= 89.6%, forg=  3.0%| TAg acc= 66.4%, forg= 10.4% <<<
>>> Test on task 19 : loss=1.420 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 59.2%, forg= 13.6% <<<
>>> Test on task 20 : loss=1.347 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task 21 : loss=1.418 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 63.8%, forg=  6.0% <<<
>>> Test on task 22 : loss=1.425 | TAw acc= 87.5%, forg=  3.8%| TAg acc= 55.8%, forg= 15.4% <<<
>>> Test on task 23 : loss=1.663 | TAw acc= 83.0%, forg=  4.0%| TAg acc= 71.0%, forg=  5.0% <<<
>>> Test on task 24 : loss=1.117 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 76.5%, forg=  3.1% <<<
>>> Test on task 25 : loss=1.254 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.4%, forg= 15.1% <<<
>>> Test on task 26 : loss=1.466 | TAw acc= 90.7%, forg=  3.1%| TAg acc= 64.9%, forg=  7.2% <<<
>>> Test on task 27 : loss=1.877 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 59.8%, forg= 18.6% <<<
>>> Test on task 28 : loss=1.286 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.9%, forg=  4.8% <<<
>>> Test on task 29 : loss=1.047 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 79.4%, forg=  2.8% <<<
>>> Test on task 30 : loss=1.093 | TAw acc= 93.2%, forg=  1.7%| TAg acc= 66.7%, forg=  1.7% <<<
>>> Test on task 31 : loss=1.238 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 66.0%, forg=  4.1% <<<
>>> Test on task 32 : loss=1.571 | TAw acc= 91.8%, forg=  2.5%| TAg acc= 65.6%, forg=  9.0% <<<
>>> Test on task 33 : loss=1.699 | TAw acc= 96.2%, forg= -0.9%| TAg acc= 61.3%, forg= -2.8% <<<
>>> Test on task 34 : loss=1.418 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 63.2%, forg=  5.3% <<<
>>> Test on task 35 : loss=0.828 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 80.9%, forg= -0.9% <<<
>>> Test on task 36 : loss=1.697 | TAw acc= 87.6%, forg=  1.9%| TAg acc= 52.4%, forg= 13.3% <<<
>>> Test on task 37 : loss=1.194 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 72.0%, forg=  9.6% <<<
>>> Test on task 38 : loss=1.231 | TAw acc= 96.9%, forg=  2.4%| TAg acc= 60.6%, forg= 24.4% <<<
>>> Test on task 39 : loss=1.241 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 67.7%, forg=  2.0% <<<
>>> Test on task 40 : loss=1.419 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 60.4%, forg= 14.2% <<<
>>> Test on task 41 : loss=1.827 | TAw acc= 93.7%, forg=  0.0%| TAg acc= 40.0%, forg= 29.5% <<<
>>> Test on task 42 : loss=1.230 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.7s | Train: skip eval | Valid: time=  0.1s loss=6.811, TAw acc= 60.0% | *
| Epoch   2, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=2.970, TAw acc= 72.9% | *
| Epoch   3, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.949, TAw acc= 88.6% | *
| Epoch   4, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.614, TAw acc= 87.1% | *
| Epoch   5, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.425, TAw acc= 88.6% | *
| Epoch   6, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.534, TAw acc= 91.4% |
| Epoch   7, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 94.3% | *
| Epoch   8, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.463, TAw acc= 88.6% |
| Epoch   1, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.224, TAw acc= 92.9% | *
| Epoch   2, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.235, TAw acc= 91.4% |
| Epoch   3, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.245, TAw acc= 91.4% |
| Epoch   4, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.254, TAw acc= 91.4% |
| Epoch   5, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.262, TAw acc= 91.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 6283 train exemplars, time=  0.0s
6283
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.255 | TAw acc= 96.6%, forg=  1.1%| TAg acc= 70.2%, forg= 20.2% <<<
>>> Test on task  1 : loss=0.944 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.0%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.633 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.2%, forg= 21.4% <<<
>>> Test on task  3 : loss=1.464 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 65.8%, forg= 18.4% <<<
>>> Test on task  4 : loss=1.609 | TAw acc= 94.2%, forg=  0.8%| TAg acc= 69.2%, forg= 16.7% <<<
>>> Test on task  5 : loss=1.154 | TAw acc= 93.9%, forg=  1.5%| TAg acc= 74.8%, forg=  8.4% <<<
>>> Test on task  6 : loss=1.305 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 72.4%, forg=  5.7% <<<
>>> Test on task  7 : loss=0.979 | TAw acc= 90.4%, forg=  1.9%| TAg acc= 78.8%, forg=  6.7% <<<
>>> Test on task  8 : loss=1.296 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.2%, forg= 12.4% <<<
>>> Test on task  9 : loss=1.701 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 62.7%, forg=  5.9% <<<
>>> Test on task 10 : loss=0.796 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.7%, forg=  4.6% <<<
>>> Test on task 11 : loss=1.220 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 72.4%, forg=  5.1% <<<
>>> Test on task 12 : loss=1.431 | TAw acc= 90.8%, forg=  5.4%| TAg acc= 61.5%, forg= 16.9% <<<
>>> Test on task 13 : loss=1.352 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 67.2%, forg= 21.6% <<<
>>> Test on task 14 : loss=1.375 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 69.5%, forg=  4.8% <<<
>>> Test on task 15 : loss=1.438 | TAw acc= 93.2%, forg=  1.5%| TAg acc= 71.2%, forg=  7.6% <<<
>>> Test on task 16 : loss=1.089 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.6%, forg=  4.9% <<<
>>> Test on task 17 : loss=1.182 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 75.3%, forg=  2.1% <<<
>>> Test on task 18 : loss=1.147 | TAw acc= 91.0%, forg=  1.5%| TAg acc= 71.6%, forg=  5.2% <<<
>>> Test on task 19 : loss=1.389 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 63.1%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.421 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 61.2%, forg= 15.5% <<<
>>> Test on task 21 : loss=1.496 | TAw acc= 90.5%, forg=  5.2%| TAg acc= 62.1%, forg=  7.8% <<<
>>> Test on task 22 : loss=1.353 | TAw acc= 88.5%, forg=  2.9%| TAg acc= 58.7%, forg= 12.5% <<<
>>> Test on task 23 : loss=1.659 | TAw acc= 83.0%, forg=  4.0%| TAg acc= 71.0%, forg=  5.0% <<<
>>> Test on task 24 : loss=1.140 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.4%, forg=  7.1% <<<
>>> Test on task 25 : loss=1.253 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.4%, forg= 15.1% <<<
>>> Test on task 26 : loss=1.572 | TAw acc= 91.8%, forg=  2.1%| TAg acc= 61.9%, forg= 10.3% <<<
>>> Test on task 27 : loss=1.826 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 60.8%, forg= 17.5% <<<
>>> Test on task 28 : loss=1.256 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.9%, forg=  4.8% <<<
>>> Test on task 29 : loss=1.032 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 80.4%, forg=  1.9% <<<
>>> Test on task 30 : loss=1.219 | TAw acc= 93.2%, forg=  1.7%| TAg acc= 62.4%, forg=  6.0% <<<
>>> Test on task 31 : loss=1.264 | TAw acc= 92.8%, forg=  1.0%| TAg acc= 68.0%, forg=  2.1% <<<
>>> Test on task 32 : loss=1.577 | TAw acc= 93.4%, forg=  0.8%| TAg acc= 66.4%, forg=  8.2% <<<
>>> Test on task 33 : loss=1.748 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 57.5%, forg=  3.8% <<<
>>> Test on task 34 : loss=1.415 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 61.4%, forg=  7.0% <<<
>>> Test on task 35 : loss=0.823 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 79.1%, forg=  1.8% <<<
>>> Test on task 36 : loss=1.756 | TAw acc= 87.6%, forg=  1.9%| TAg acc= 49.5%, forg= 16.2% <<<
>>> Test on task 37 : loss=1.128 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 72.8%, forg=  8.8% <<<
>>> Test on task 38 : loss=1.065 | TAw acc= 97.6%, forg=  1.6%| TAg acc= 67.7%, forg= 17.3% <<<
>>> Test on task 39 : loss=1.282 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 64.6%, forg=  5.1% <<<
>>> Test on task 40 : loss=1.420 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 60.4%, forg= 14.2% <<<
>>> Test on task 41 : loss=1.895 | TAw acc= 93.7%, forg=  0.0%| TAg acc= 38.9%, forg= 30.5% <<<
>>> Test on task 42 : loss=1.681 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 50.0%, forg= 21.4% <<<
>>> Test on task 43 : loss=1.280 | TAw acc= 89.6%, forg=  0.0%| TAg acc= 65.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_1/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 86.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 86.0% 
	 91.6%  88.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 89.8% 
	 92.7%  98.0%  91.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.0% 
	 94.9%  98.0%  95.7%  91.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 97.2%  98.0%  97.4%  96.5%  90.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 97.2%  98.0%  98.3%  97.4%  93.3%  90.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 95.5%  98.0%  97.4%  97.4%  93.3%  93.9%  89.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 97.8%  98.0%  98.3%  98.2%  93.3%  93.9%  95.2%  87.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 96.6%  98.0%  97.4%  98.2%  93.3%  93.1%  95.2%  90.4%  95.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 96.1%  98.0%  98.3%  98.2%  93.3%  93.1%  92.4%  92.3%  95.9%  93.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 97.2%  98.0%  98.3%  98.2%  93.3%  92.4%  95.2%  90.4%  97.5%  94.1%  98.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 96.1%  98.0%  98.3%  98.2%  92.5%  93.9%  95.2%  91.3%  97.5%  94.1%  98.2%  98.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 96.1%  98.0%  98.3%  98.2%  92.5%  93.9%  95.2%  91.3%  98.3%  94.1% 100.0%  99.0%  95.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.2% 
	 97.2%  98.0%  98.3%  98.2%  92.5%  93.1%  95.2%  90.4%  98.3%  97.1% 100.0%  99.0%  96.2%  97.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.5% 
	 97.2%  98.0%  98.3%  98.2%  92.5%  92.4%  96.2%  91.3%  98.3%  95.1% 100.0%  99.0%  94.6%  97.6%  91.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 97.2%  99.0%  98.3%  98.2%  92.5%  93.1%  96.2%  91.3%  98.3%  97.1% 100.0%  99.0%  93.8%  99.2%  91.4%  94.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.2% 
	 96.6%  99.0%  97.4%  98.2%  92.5%  93.1%  96.2%  91.3%  98.3%  95.1% 100.0%  99.0%  93.8%  99.2%  91.4%  94.7%  97.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 97.2%  99.0%  97.4%  98.2%  92.5%  93.9%  97.1%  90.4%  98.3%  97.1% 100.0%  99.0%  96.2%  99.2%  92.4%  93.9%  98.0%  93.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.3% 
	 97.2%  99.0%  97.4%  98.2%  93.3%  93.1%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  94.6%  99.2%  91.4%  93.2%  97.1%  93.8%  92.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 96.6%  99.0%  98.3%  98.2%  94.2%  94.7%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  96.2%  99.2%  91.4%  93.9%  98.0%  94.8%  89.6%  95.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 97.2%  99.0%  97.4%  98.2%  94.2%  94.7%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  95.4%  99.2%  91.4%  93.9%  98.0%  94.8%  91.0%  95.1%  97.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.2% 
	 97.2%  99.0%  97.4%  98.2%  94.2%  93.9%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  95.4%  99.2%  93.3%  93.9%  98.0%  93.8%  91.8%  95.1%  97.4%  94.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.1% 
	 97.2%  99.0%  97.4%  98.2%  94.2%  93.1%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  95.4%  99.2%  93.3%  93.9%  97.1%  93.8%  90.3%  95.1%  97.4%  94.0%  91.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  95.4%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  95.4%  99.2%  93.3%  94.7%  97.1%  93.8%  91.8%  95.1%  97.4%  95.7%  84.6%  86.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 96.6% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  91.3%  98.3%  97.1% 100.0%  99.0%  93.8%  99.2%  94.3%  94.7%  97.1%  94.8%  91.0%  95.1%  97.4%  95.7%  86.5%  86.0%  96.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.6% 100.0%  97.4%  98.2%  95.0%  94.7%  96.2%  91.3%  98.3%  96.1% 100.0%  99.0%  96.2%  99.2%  93.3%  94.7%  97.1%  93.8%  91.0%  95.1%  97.4%  94.8%  89.4%  85.0%  96.9%  95.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.6% 100.0%  97.4%  98.2%  95.0%  93.9%  96.2%  91.3%  98.3%  97.1% 100.0%  99.0%  94.6%  99.2%  93.3%  94.7%  97.1%  94.8%  91.0%  95.1%  97.4%  94.0%  88.5%  87.0%  98.0%  96.6%  92.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.1% 100.0%  97.4%  98.2%  95.0%  94.7%  97.1%  90.4%  98.3%  96.1% 100.0%  99.0%  93.1%  99.2%  93.3%  93.9%  97.1%  94.8%  90.3%  95.1%  97.4%  94.0%  88.5%  83.0%  96.9%  95.8%  91.8%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 96.1% 100.0%  97.4%  98.2%  95.0%  93.9%  97.1%  90.4%  98.3%  97.1% 100.0%  99.0%  93.1%  99.2%  94.3%  93.9%  98.0%  92.8%  91.8%  95.1%  97.4%  93.1%  88.5%  87.0%  96.9%  95.0%  93.8%  96.9% 100.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  94.7%  96.2%  89.4%  98.3%  97.1% 100.0%  99.0%  93.1%  99.2%  94.3%  93.9%  98.0%  92.8%  91.0%  95.1%  97.4%  93.1%  88.5%  87.0%  98.0%  96.6%  90.7%  97.9%  99.0%  98.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  94.7%  96.2%  90.4%  98.3%  97.1% 100.0%  99.0%  92.3%  99.2%  92.4%  93.9%  98.0%  93.8%  91.0%  95.1%  97.4%  93.1%  86.5%  86.0%  98.0%  96.6%  90.7%  97.9%  99.0%  97.2%  92.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  90.4%  98.3%  97.1% 100.0%  99.0%  93.1%  99.2%  93.3%  93.9%  98.0%  93.8%  90.3%  95.1%  97.4%  93.1%  87.5%  86.0%  98.0%  96.6%  90.7%  96.9%  99.0%  97.2%  93.2%  92.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  89.4%  98.3%  96.1% 100.0%  99.0%  93.1%  99.2%  93.3%  93.9%  98.0%  93.8%  91.8%  95.1%  97.4%  92.2%  87.5%  86.0%  98.0%  97.5%  90.7%  96.9%  99.0%  97.2%  93.2%  90.7%  92.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  93.9%  97.1%  90.4%  98.3%  96.1% 100.0%  99.0%  92.3%  99.2%  93.3%  93.9%  98.0%  93.8%  90.3%  95.1%  97.4%  93.1%  88.5%  84.0%  96.9%  97.5%  90.7%  97.9%  99.0%  97.2%  94.0%  92.8%  91.8%  95.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 97.2% 100.0%  97.4%  98.2%  93.3%  94.7%  97.1%  90.4%  98.3%  96.1% 100.0%  99.0%  93.1%  99.2%  93.3%  93.9%  97.1%  93.8%  90.3%  95.1%  97.4%  90.5%  88.5%  85.0%  96.9%  96.6%  91.8%  97.9%  99.0%  97.2%  94.0%  93.8%  91.0%  95.3%  96.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 96.6% 100.0%  97.4%  98.2%  94.2%  94.7%  96.2%  90.4%  98.3%  96.1% 100.0%  99.0%  91.5%  99.2%  93.3%  93.2%  98.0%  94.8%  91.0%  95.1%  97.4%  90.5%  88.5%  83.0%  96.9%  96.6%  91.8%  96.9%  99.0%  96.3%  93.2%  92.8%  91.0%  95.3%  95.6%  99.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 97.2% 100.0%  97.4%  98.2%  94.2%  94.7%  96.2%  90.4%  98.3%  96.1% 100.0%  99.0%  90.8%  99.2%  94.3%  93.2%  98.0%  93.8%  90.3%  95.1%  97.4%  93.1%  87.5%  87.0%  96.9%  97.5%  91.8%  97.9%  99.0%  96.3%  92.3%  92.8%  91.8%  95.3%  94.7%  98.2%  89.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 95.5% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  90.4%  98.3%  96.1% 100.0%  98.0%  93.1%  99.2%  94.3%  93.2%  98.0%  94.8%  89.6%  95.1%  97.4%  90.5%  87.5%  86.0%  96.9%  96.6%  91.8%  97.9%  99.0%  96.3%  94.0%  91.8%  94.3%  94.3%  95.6%  98.2%  88.6%  98.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 95.5% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  91.3%  98.3%  96.1% 100.0%  98.0%  93.1%  99.2%  94.3%  93.2%  98.0%  94.8%  90.3%  95.1%  97.4%  90.5%  87.5%  85.0%  96.9%  97.5%  91.8%  97.9%  99.0%  96.3%  94.9%  92.8%  92.6%  95.3%  95.6%  98.2%  88.6%  97.6%  99.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 94.9% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  90.4%  98.3%  96.1% 100.0%  98.0%  90.8%  99.2%  94.3%  93.2%  97.1%  94.8%  89.6%  95.1%  97.4%  90.5%  88.5%  83.0%  96.9%  97.5%  91.8%  97.9%  99.0%  96.3%  94.0%  91.8%  91.8%  95.3%  95.6%  98.2%  88.6%  97.6%  97.6%  98.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 95.5% 100.0%  97.4%  98.2%  94.2%  94.7%  97.1%  91.3%  97.5%  96.1%  99.1%  98.0%  91.5%  99.2%  94.3%  93.2%  98.0%  93.8%  89.6%  95.1%  97.4%  90.5%  87.5%  84.0%  96.9%  97.5%  91.8%  97.9%  99.0%  96.3%  94.9%  92.8%  92.6%  95.3%  95.6%  98.2%  87.6%  98.4%  97.6%  99.0%  98.1%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 96.6% 100.0%  97.4%  98.2%  94.2%  93.9%  97.1%  90.4%  97.5%  96.1%  99.1%  98.0%  92.3%  99.2%  94.3%  93.9%  97.1%  94.8%  89.6%  95.1%  97.4%  90.5%  88.5%  84.0%  96.9%  97.5%  90.7%  97.9%  99.0%  97.2%  94.0%  92.8%  91.8%  95.3%  95.6%  98.2%  87.6%  98.4%  97.6%  99.0%  98.1%  93.7%   0.0%   0.0% 	Avg.: 95.2% 
	 96.6% 100.0%  97.4%  98.2%  94.2%  94.7%  96.2%  90.4%  98.3%  96.1%  99.1%  98.0%  92.3%  99.2%  95.2%  93.2%  98.0%  94.8%  89.6%  95.1%  97.4%  90.5%  87.5%  83.0%  96.9%  97.5%  90.7%  97.9%  99.0%  96.3%  93.2%  92.8%  91.8%  96.2%  95.6%  99.1%  87.6%  98.4%  96.9%  98.0%  98.1%  93.7%  97.3%   0.0% 	Avg.: 95.2% 
	 96.6% 100.0%  97.4%  98.2%  94.2%  93.9%  97.1%  90.4%  98.3%  96.1% 100.0%  98.0%  90.8%  99.2%  95.2%  93.2%  98.0%  93.8%  91.0%  95.1%  97.4%  90.5%  88.5%  83.0%  96.9%  97.5%  91.8%  97.9%  99.0%  96.3%  93.2%  92.8%  93.4%  96.2%  95.6%  98.2%  87.6%  98.4%  97.6%  99.0%  99.1%  93.7%  95.5%  89.6% 	Avg.: 95.1% 
************************************************************************************************************
TAg Acc
	 86.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 86.0% 
	 84.3%  83.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 83.6% 
	 79.2%  77.0%  90.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 82.3% 
	 83.7%  82.0%  78.6%  81.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 81.5% 
	 89.9%  84.0%  78.6%  76.3%  75.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.9% 
	 89.9%  80.0%  82.1%  65.8%  75.8%  83.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.5% 
	 89.9%  88.0%  80.3%  84.2%  83.3%  70.2%  68.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.7% 
	 90.4%  88.0%  79.5%  84.2%  81.7%  79.4%  66.7%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.8% 
	 87.1%  86.0%  80.3%  82.5%  85.8%  79.4%  70.5%  73.1%  82.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.8% 
	 89.3%  87.0%  79.5%  79.8%  85.0%  78.6%  73.3%  74.0%  57.9%  68.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.3% 
	 82.6%  87.0%  78.6%  74.6%  80.0%  80.2%  78.1%  81.7%  65.3%  63.7%  81.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.6% 
	 80.9%  88.0%  80.3%  83.3%  85.8%  78.6%  75.2%  82.7%  68.6%  57.8%  61.5%  71.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.2% 
	 88.8%  87.0%  77.8%  78.9%  80.8%  78.6%  75.2%  82.7%  75.2%  57.8%  75.2%  66.3%  78.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.1% 
	 84.8%  89.0%  78.6%  69.3%  79.2%  75.6%  78.1%  84.6%  78.5%  65.7%  77.1%  75.5%  57.7%  88.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.3% 
	 87.6%  87.0%  77.8%  75.4%  80.0%  79.4%  75.2%  84.6%  77.7%  64.7%  76.1%  76.5%  62.3%  69.6%  64.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.9% 
	 78.7%  85.0%  76.1%  73.7%  75.0%  81.7%  72.4%  81.7%  77.7%  68.6%  71.6%  74.5%  61.5%  73.6%  60.0%  78.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.4% 
	 85.4%  86.0%  78.6%  77.2%  80.0%  79.4%  78.1%  82.7%  77.7%  67.6%  85.3%  77.6%  63.1%  73.6%  64.8%  62.1%  75.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.2% 
	 80.3%  88.0%  76.1%  70.2%  78.3%  76.3%  73.3%  82.7%  77.7%  64.7%  81.7%  75.5%  68.5%  75.2%  68.6%  69.7%  61.8%  63.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.0% 
	 79.8%  87.0%  72.6%  74.6%  73.3%  78.6%  77.1%  85.6%  76.9%  66.7%  81.7%  76.5%  66.9%  74.4%  63.8%  62.1%  66.7%  63.9%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.0% 
	 79.8%  83.0%  76.9%  73.7%  73.3%  76.3%  77.1%  82.7%  67.8%  67.6%  80.7%  75.5%  65.4%  72.8%  68.6%  72.0%  68.6%  68.0%  64.9%  72.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.4% 
	 84.3%  81.0%  75.2%  71.9%  74.2%  80.2%  73.3%  84.6%  76.9%  66.7%  78.0%  74.5%  63.1%  73.6%  68.6%  71.2%  68.6%  72.2%  59.7%  57.3%  76.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.9% 
	 80.9%  83.0%  75.2%  75.4%  71.7%  76.3%  76.2%  80.8%  74.4%  64.7%  84.4%  75.5%  63.8%  74.4%  69.5%  74.2%  68.6%  66.0%  69.4%  62.1%  57.8%  69.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.5% 
	 78.1%  80.0%  76.9%  71.9%  69.2%  77.9%  74.3%  83.7%  79.3%  66.7%  78.9%  75.5%  65.4%  70.4%  71.4%  75.8%  70.6%  71.1%  62.7%  66.0%  63.8%  54.3%  71.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.0% 
	 78.1%  81.0%  76.1%  72.8%  70.8%  78.6%  77.1%  83.7%  74.4%  62.7%  82.6%  76.5%  64.6%  75.2%  69.5%  75.8%  69.6%  72.2%  70.9%  66.0%  64.7%  59.5%  50.0%  75.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.0% 
	 77.0%  83.0%  76.9%  71.9%  75.0%  76.3%  75.2%  83.7%  77.7%  66.7%  83.5%  73.5%  67.7%  72.8%  72.4%  76.5%  70.6%  73.2%  66.4%  70.9%  64.7%  62.9%  52.9%  62.0%  79.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.5% 
	 77.0%  79.0%  71.8%  67.5%  69.2%  76.3%  77.1%  83.7%  76.9%  64.7%  81.7%  75.5%  63.8%  72.8%  69.5%  77.3%  66.7%  75.3%  66.4%  63.1%  68.1%  59.5%  54.8%  67.0%  66.3%  81.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.2% 
	 76.4%  82.0%  70.1%  71.1%  74.2%  77.1%  70.5%  84.6%  75.2%  59.8%  85.3%  72.4%  65.4%  71.2%  71.4%  75.8%  67.6%  75.3%  69.4%  67.0%  63.8%  66.4%  58.7%  67.0%  71.4%  68.1%  72.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.5% 
	 79.2%  81.0%  73.5%  68.4%  73.3%  77.9%  77.1%  78.8%  71.1%  65.7%  79.8%  73.5%  60.8%  72.0%  68.6%  75.0%  67.6%  77.3%  60.4%  57.3%  66.4%  61.2%  61.5%  70.0%  67.3%  67.2%  54.6%  78.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.2% 
	 74.2%  80.0%  72.6%  69.3%  71.7%  78.6%  71.4%  82.7%  77.7%  63.7%  82.6%  74.5%  66.9%  72.0%  70.5%  75.0%  66.7%  75.3%  67.9%  64.1%  67.2%  62.1%  61.5%  72.0%  68.4%  68.9%  51.5%  52.6%  64.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.9% 
	 76.4%  77.0%  73.5%  74.6%  71.7%  76.3%  75.2%  82.7%  74.4%  62.7%  83.5%  73.5%  65.4%  71.2%  72.4%  78.0%  73.5%  75.3%  64.9%  68.0%  69.0%  59.5%  56.7%  72.0%  70.4%  64.7%  58.8%  58.8%  60.0%  82.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.7% 
	 74.2%  74.0%  73.5%  68.4%  68.3%  77.1%  74.3%  81.7%  74.4%  62.7%  86.2%  73.5%  59.2%  66.4%  71.4%  78.8%  65.7%  77.3%  69.4%  69.9%  63.8%  56.9%  60.6%  73.0%  74.5%  67.2%  62.9%  61.9%  63.8%  74.8%  68.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.1% 
	 73.6%  77.0%  70.1%  71.1%  73.3%  75.6%  74.3%  77.9%  71.9%  66.7%  82.6%  75.5%  63.8%  69.6%  67.6%  76.5%  71.6%  75.3%  67.9%  66.0%  57.8%  63.8%  57.7%  72.0%  69.4%  65.5%  60.8%  62.9%  60.0%  74.8%  53.8%  62.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 70.8%  77.0%  72.6%  68.4%  68.3%  77.1%  74.3%  74.0%  75.2%  63.7%  84.4%  73.5%  57.7%  69.6%  72.4%  68.2%  66.7%  73.2%  70.9%  63.1%  61.2%  60.3%  64.4%  76.0%  70.4%  68.1%  62.9%  58.8%  61.9%  77.6%  56.4%  56.7%  74.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.8% 
	 71.3%  77.0%  72.6%  66.7%  72.5%  75.6%  72.4%  78.8%  73.6%  63.7%  83.5%  74.5%  63.8%  71.2%  71.4%  78.0%  67.6%  76.3%  72.4%  58.3%  62.9%  62.9%  61.5%  72.0%  69.4%  61.3%  60.8%  60.8%  61.0%  79.4%  58.1%  60.8%  60.7%  58.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 75.8%  76.0%  71.8%  66.7%  67.5%  74.8%  74.3%  78.8%  71.1%  62.7%  82.6%  75.5%  67.7%  67.2%  71.4%  68.9%  64.7%  77.3%  69.4%  63.1%  62.1%  60.3%  61.5%  71.0%  68.4%  66.4%  64.9%  62.9%  62.9%  76.6%  53.0%  61.9%  64.8%  43.4%  68.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.9% 
	 72.5%  74.0%  70.1%  71.9%  67.5%  74.0%  71.4%  79.8%  71.9%  64.7%  81.7%  73.5%  63.1%  68.8%  70.5%  69.7%  67.6%  77.3%  70.1%  63.1%  62.9%  62.9%  62.5%  72.0%  73.5%  60.5%  62.9%  58.8%  66.7%  79.4%  61.5%  63.9%  59.8%  42.5%  57.0%  80.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.1% 
	 65.7%  74.0%  71.8%  68.4%  71.7%  71.8%  70.5%  75.0%  74.4%  62.7%  81.7%  71.4%  63.1%  66.4%  70.5%  73.5%  71.6%  74.2%  72.4%  58.3%  60.3%  58.6%  60.6%  75.0%  77.6%  68.9%  62.9%  62.9%  61.0%  79.4%  61.5%  63.9%  63.1%  52.8%  54.4%  79.1%  65.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 68.0%  76.0%  71.8%  64.0%  67.5%  75.6%  76.2%  79.8%  71.9%  61.8%  82.6%  74.5%  63.8%  63.2%  71.4%  75.0%  69.6%  76.3%  70.9%  62.1%  62.1%  63.8%  59.6%  73.0%  70.4%  66.4%  64.9%  54.6%  61.0%  80.4%  60.7%  60.8%  63.9%  57.5%  57.0%  78.2%  51.4%  81.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.1% 
	 68.0%  75.0%  69.2%  68.4%  70.0%  75.6%  74.3%  80.8%  73.6%  64.7%  84.4%  72.4%  63.1%  66.4%  71.4%  72.0%  68.6%  75.3%  70.1%  66.0%  61.2%  62.1%  59.6%  73.0%  71.4%  61.3%  66.0%  61.9%  63.8%  76.6%  59.8%  63.9%  61.5%  53.8%  51.8%  76.4%  48.6%  64.0%  85.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 66.9%  75.0%  70.9%  69.3%  69.2%  75.6%  73.3%  78.8%  71.1%  63.7%  85.3%  71.4%  62.3%  67.2%  74.3%  74.2%  72.5%  75.3%  70.1%  62.1%  63.8%  62.1%  62.5%  70.0%  74.5%  64.7%  61.9%  62.9%  64.8%  78.5%  64.1%  69.1%  66.4%  57.5%  61.4%  80.0%  51.4%  67.2%  60.6%  69.7%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.5% 
	 70.2%  74.0%  70.1%  67.5%  65.0%  74.8%  75.2%  77.9%  66.9%  65.7%  82.6%  72.4%  63.1%  66.4%  73.3%  74.2%  71.6%  75.3%  69.4%  62.1%  60.3%  66.4%  57.7%  70.0%  72.4%  68.1%  63.9%  60.8%  61.0%  77.6%  66.7%  70.1%  66.4%  58.5%  59.6%  79.1%  50.5%  68.0%  63.8%  66.7%  74.5%   0.0%   0.0%   0.0% 	Avg.: 68.3% 
	 68.5%  68.0%  69.2%  66.7%  65.8%  75.6%  70.5%  83.7%  68.6%  64.7%  82.6%  71.4%  63.1%  69.6%  70.5%  77.3%  74.5%  74.2%  67.9%  65.0%  62.9%  60.3%  57.7%  73.0%  75.5%  67.2%  61.9%  61.9%  61.0%  81.3%  59.8%  69.1%  63.1%  58.5%  60.5%  76.4%  50.5%  65.6%  67.7%  66.7%  56.6%  69.5%   0.0%   0.0% 	Avg.: 67.7% 
	 70.2%  72.0%  70.1%  66.7%  69.2%  74.0%  72.4%  77.9%  66.9%  61.8%  81.7%  71.4%  59.2%  68.8%  70.5%  73.5%  71.6%  74.2%  66.4%  59.2%  60.3%  63.8%  55.8%  71.0%  76.5%  66.4%  64.9%  59.8%  61.9%  79.4%  66.7%  66.0%  65.6%  61.3%  63.2%  80.9%  52.4%  72.0%  60.6%  67.7%  60.4%  40.0%  71.4%   0.0% 	Avg.: 67.1% 
	 70.2%  74.0%  69.2%  65.8%  69.2%  74.8%  72.4%  78.8%  70.2%  62.7%  81.7%  72.4%  61.5%  67.2%  69.5%  71.2%  70.6%  75.3%  71.6%  63.1%  61.2%  62.1%  58.7%  71.0%  72.4%  66.4%  61.9%  60.8%  61.9%  80.4%  62.4%  68.0%  66.4%  57.5%  61.4%  79.1%  49.5%  72.8%  67.7%  64.6%  60.4%  38.9%  50.0%  65.6% 	Avg.: 66.7% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -5.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -5.6% 
	 -1.1% -10.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -5.6% 
	 -2.2%   0.0%  -4.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.2% 
	 -2.2%   0.0%  -1.7%  -5.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.3% 
	  0.0%   0.0%  -0.9%  -0.9%  -3.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.0% 
	  1.7%   0.0%   0.9%   0.0%   0.0%  -3.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	 -0.6%   0.0%   0.0%  -0.9%   0.0%   0.0%  -5.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.0% 
	  1.1%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%  -2.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.0% 
	  1.7%   0.0%   0.0%   0.0%   0.0%   0.8%   2.9%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   1.5%   0.0%   1.9%  -1.7%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  1.7%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.7%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   1.0%  -0.8%   0.0%  -1.8%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.0% 
	  0.6%   0.0%   0.0%   0.0%   0.8%   0.8%   0.0%   1.9%   0.0%  -2.9%   0.0%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.6%   0.0%   0.0%   0.0%   0.8%   1.5%  -1.0%   1.0%   0.0%   2.0%   0.0%   0.0%   1.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.6%  -1.0%   0.0%   0.0%   0.8%   0.8%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   2.3%  -1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.1%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   1.0%   0.0%   2.0%   0.0%   0.0%   2.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   0.0%  -1.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%  -1.0%   0.8%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.6%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   1.5%   0.0%   1.0%   1.5%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   0.0%   0.0%   0.0%  -0.8%  -0.8%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.8%   0.0%  -1.0%   3.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.6%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   1.0%   0.8%   0.0%   0.0%   1.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%  -1.0%   0.8%   0.0%   1.0%   0.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   0.0%   0.9%   0.0%   0.0%   1.5%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.8%   1.0%   1.0%   2.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%  -1.0%   0.9%   0.0%   0.0%  -0.8%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   1.0%   1.0%   0.7%   0.0%   0.0%  -1.7%   6.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.1%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   2.3%   0.0%  -1.0%   0.0%   1.0%   0.0%   1.5%   0.0%   0.0%   0.0%   4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   0.0%   0.9%   0.0%  -0.8%   0.8%   1.0%   1.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   1.0%   1.0%   1.5%   0.0%   0.0%   0.9%   1.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   0.0%   0.9%   0.0%   0.0%   1.5%   1.0%   1.0%   0.0%   0.0%   0.0%   0.0%   1.5%   0.0%   1.0%   0.0%   1.0%   0.0%   1.5%   0.0%   0.0%   1.7%   2.9%  -1.0%  -1.0%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.7%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%   1.9%   0.0%   1.0%   0.0%   0.0%   3.1%   0.0%   1.0%   0.8%   1.0%   0.0%   2.2%   0.0%   0.0%   1.7%   2.9%   4.0%   1.0%   0.8%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.0% 
	  1.7%   0.0%   0.9%   0.0%   0.0%   1.5%   0.0%   1.9%   0.0%   0.0%   0.0%   0.0%   3.1%   0.0%   0.0%   0.8%   0.0%   2.1%   0.7%   0.0%   0.0%   2.6%   2.9%   0.0%   1.0%   1.7%  -1.0%  -2.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   0.8%   1.0%   2.9%   0.0%   0.0%   0.0%   0.0%   3.1%   0.0%   0.0%   0.8%   0.0%   2.1%   1.5%   0.0%   0.0%   2.6%   2.9%   0.0%   0.0%   0.0%   3.1%  -1.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   0.8%   1.0%   1.9%   0.0%   0.0%   0.0%   0.0%   3.8%   0.0%   1.9%   0.8%   0.0%   1.0%   1.5%   0.0%   0.0%   2.6%   4.8%   1.0%   0.0%   0.0%   3.1%   0.0%   1.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   1.9%   0.0%   0.0%   0.0%   0.0%   3.1%   0.0%   1.0%   0.8%   0.0%   1.0%   2.2%   0.0%   0.0%   2.6%   3.8%   1.0%   0.0%   0.0%   3.1%   1.0%   1.0%   0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   2.9%   0.0%   1.0%   0.0%   0.0%   3.1%   0.0%   1.0%   0.8%   0.0%   1.0%   0.7%   0.0%   0.0%   3.4%   3.8%   1.0%   0.0%  -0.8%   3.1%   1.0%   1.0%   0.9%   0.0%   2.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   1.5%   0.0%   1.9%   0.0%   1.0%   0.0%   0.0%   3.8%   0.0%   1.0%   0.8%   0.0%   1.0%   2.2%   0.0%   0.0%   2.6%   2.9%   3.0%   1.0%   0.0%   3.1%   0.0%   1.0%   0.9%  -0.9%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.6%   0.0%   0.9%   0.0%   1.7%   0.8%   0.0%   1.9%   0.0%   1.0%   0.0%   0.0%   3.1%   0.0%   1.0%   0.8%   1.0%   1.0%   2.2%   0.0%   0.0%   5.2%   2.9%   2.0%   1.0%   0.8%   2.1%   0.0%   1.0%   0.9%   0.0%  -1.0%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  1.1%   0.0%   0.9%   0.0%   0.8%   0.8%   1.0%   1.9%   0.0%   1.0%   0.0%   0.0%   4.6%   0.0%   1.0%   1.5%   0.0%   0.0%   1.5%   0.0%   0.0%   5.2%   2.9%   4.0%   1.0%   0.8%   2.1%   1.0%   1.0%   1.9%   0.9%   1.0%   1.6%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  0.6%   0.0%   0.9%   0.0%   0.8%   0.8%   1.0%   1.9%   0.0%   1.0%   0.0%   0.0%   5.4%   0.0%   0.0%   1.5%   0.0%   1.0%   2.2%   0.0%   0.0%   2.6%   3.8%   0.0%   1.0%   0.0%   2.1%   0.0%   1.0%   1.9%   1.7%   1.0%   0.8%   0.0%   1.8%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.0% 
	  2.2%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   1.9%   0.0%   1.0%   0.0%   1.0%   3.1%   0.0%   0.0%   1.5%   0.0%   0.0%   3.0%   0.0%   0.0%   5.2%   3.8%   1.0%   1.0%   0.8%   2.1%   0.0%   1.0%   1.9%   0.0%   2.1%  -1.6%   0.9%   0.9%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.0% 
	  2.2%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   1.0%   0.0%   1.0%   0.0%   1.0%   3.1%   0.0%   0.0%   1.5%   0.0%   0.0%   2.2%   0.0%   0.0%   5.2%   3.8%   2.0%   1.0%   0.0%   2.1%   0.0%   1.0%   1.9%  -0.9%   1.0%   1.6%   0.0%   0.9%   0.9%   1.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.0% 
	  2.8%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   1.9%   0.0%   1.0%   0.0%   1.0%   5.4%   0.0%   0.0%   1.5%   1.0%   0.0%   3.0%   0.0%   0.0%   5.2%   2.9%   4.0%   1.0%   0.0%   2.1%   0.0%   1.0%   1.9%   0.9%   2.1%   2.5%   0.0%   0.9%   0.9%   1.0%   0.8%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.2% 
	  2.2%   0.0%   0.9%   0.0%   0.8%   0.8%   0.0%   1.0%   0.8%   1.0%   0.9%   1.0%   4.6%   0.0%   0.0%   1.5%   0.0%   1.0%   3.0%   0.0%   0.0%   5.2%   3.8%   3.0%   1.0%   0.0%   2.1%   0.0%   1.0%   1.9%   0.0%   1.0%   1.6%   0.0%   0.9%   0.9%   1.9%   0.0%   1.6%  -1.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  1.1%   0.0%   0.9%   0.0%   0.8%   1.5%   0.0%   1.9%   0.8%   1.0%   0.9%   1.0%   3.8%   0.0%   0.0%   0.8%   1.0%   0.0%   3.0%   0.0%   0.0%   5.2%   2.9%   3.0%   1.0%   0.0%   3.1%   0.0%   1.0%   0.9%   0.9%   1.0%   2.5%   0.0%   0.9%   0.9%   1.9%   0.0%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  1.1%   0.0%   0.9%   0.0%   0.8%   0.8%   1.0%   1.9%   0.0%   1.0%   0.9%   1.0%   3.8%   0.0%  -1.0%   1.5%   0.0%   0.0%   3.0%   0.0%   0.0%   5.2%   3.8%   4.0%   1.0%   0.0%   3.1%   0.0%   1.0%   1.9%   1.7%   1.0%   2.5%  -0.9%   0.9%   0.0%   1.9%   0.0%   2.4%   1.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  1.1%   0.0%   0.9%   0.0%   0.8%   1.5%   0.0%   1.9%   0.0%   1.0%   0.0%   1.0%   5.4%   0.0%   0.0%   1.5%   0.0%   1.0%   1.5%   0.0%   0.0%   5.2%   2.9%   4.0%   1.0%   0.0%   2.1%   0.0%   1.0%   1.9%   1.7%   1.0%   0.8%   0.0%   0.9%   0.9%   1.9%   0.0%   1.6%   0.0%  -0.9%   0.0%   1.8%   0.0% 	Avg.:  1.1% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.7% 
	  6.7%   6.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.4% 
	  2.2%   1.0%  12.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	 -3.9%  -1.0%  12.0%   5.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.1% 
	  0.0%   4.0%   8.5%  15.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.7% 
	  0.0%  -4.0%  10.3%  -2.6%  -7.5%  13.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.5% 
	 -0.6%   0.0%  11.1%   0.0%   1.7%   3.8%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.6% 
	  3.4%   2.0%  10.3%   1.8%  -2.5%   3.8%  -1.9%   3.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.6% 
	  1.1%   1.0%  11.1%   4.4%   0.8%   4.6%  -2.9%   2.9%  24.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.3% 
	  7.9%   1.0%  12.0%   9.6%   5.8%   3.1%  -4.8%  -4.8%  17.4%   4.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	  9.6%   0.0%  10.3%   0.9%   0.0%   4.6%   2.9%  -1.0%  14.0%  10.8%  20.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.6% 
	  1.7%   1.0%  12.8%   5.3%   5.0%   4.6%   2.9%   0.0%   7.4%  10.8%   6.4%   5.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	  5.6%  -1.0%  12.0%  14.9%   6.7%   7.6%   0.0%  -1.9%   4.1%   2.9%   4.6%  -4.1%  20.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	  2.8%   2.0%  12.8%   8.8%   5.8%   3.8%   2.9%   0.0%   5.0%   3.9%   5.5%  -1.0%  16.2%  19.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	 11.8%   4.0%  14.5%  10.5%  10.8%   1.5%   5.7%   2.9%   5.0%   0.0%  10.1%   2.0%  16.9%  15.2%   4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.7% 
	  5.1%   3.0%  12.0%   7.0%   5.8%   3.8%   0.0%   1.9%   5.0%   1.0%  -3.7%  -1.0%  15.4%  15.2%   0.0%  15.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.4% 
	 10.1%   1.0%  14.5%  14.0%   7.5%   6.9%   4.8%   1.9%   5.0%   3.9%   3.7%   2.0%  10.0%  13.6%  -3.8%   8.3%  13.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	 10.7%   2.0%  17.9%   9.6%  12.5%   4.6%   1.0%  -1.0%   5.8%   2.0%   3.7%   1.0%  11.5%  14.4%   4.8%  15.9%   8.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.0% 
	 10.7%   6.0%  13.7%  10.5%  12.5%   6.9%   1.0%   2.9%  14.9%   1.0%   4.6%   2.0%  13.1%  16.0%   0.0%   6.1%   6.9%  -4.1%  11.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.2% 
	  6.2%   8.0%  15.4%  12.3%  11.7%   3.1%   4.8%   1.0%   5.8%   2.0%   7.3%   3.1%  15.4%  15.2%   0.0%   6.8%   6.9%  -4.1%  17.2%  15.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.7% 
	  9.6%   6.0%  15.4%   8.8%  14.2%   6.9%   1.9%   4.8%   8.3%   3.9%   0.9%   2.0%  14.6%  14.4%  -1.0%   3.8%   6.9%   6.2%   7.5%  10.7%  19.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.8% 
	 12.4%   9.0%  13.7%  12.3%  16.7%   5.3%   3.8%   1.9%   3.3%   2.0%   6.4%   2.0%  13.1%  18.4%  -1.9%   2.3%   4.9%   1.0%  14.2%   6.8%  12.9%  15.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.0% 
	 12.4%   8.0%  14.5%  11.4%  15.0%   4.6%   1.0%   1.9%   8.3%   5.9%   2.8%   1.0%  13.8%  13.6%   1.9%   2.3%   5.9%   0.0%   6.0%   6.8%  12.1%  10.3%  21.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.8% 
	 13.5%   6.0%  13.7%  12.3%  10.8%   6.9%   2.9%   1.9%   5.0%   2.0%   1.8%   4.1%  10.8%  16.0%  -1.0%   1.5%   4.9%  -1.0%  10.4%   1.9%  12.1%   6.9%  18.3%  13.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.3% 
	 13.5%  10.0%  18.8%  16.7%  16.7%   6.9%   1.0%   1.9%   5.8%   3.9%   3.7%   2.0%  14.6%  16.0%   2.9%   0.8%   8.8%  -2.1%  10.4%   9.7%   8.6%  10.3%  16.3%   8.0%  13.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.7% 
	 14.0%   7.0%  20.5%  13.2%  11.7%   6.1%   7.6%   1.0%   7.4%   8.8%   0.0%   5.1%  13.1%  17.6%   1.0%   2.3%   7.8%   0.0%   7.5%   5.8%  12.9%   3.4%  12.5%   8.0%   8.2%  13.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.3% 
	 11.2%   8.0%  17.1%  15.8%  12.5%   5.3%   1.0%   6.7%  11.6%   2.9%   5.5%   4.1%  17.7%  16.8%   3.8%   3.0%   7.8%  -2.1%  16.4%  15.5%  10.3%   8.6%   9.6%   5.0%  12.2%  14.3%  17.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.6% 
	 16.3%   9.0%  17.9%  14.9%  14.2%   4.6%   6.7%   2.9%   5.0%   4.9%   2.8%   3.1%  11.5%  16.8%   1.9%   3.0%   8.8%   2.1%   9.0%   8.7%   9.5%   7.8%   9.6%   3.0%  11.2%  12.6%  20.6%  25.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 14.0%  12.0%  17.1%   9.6%  14.2%   6.9%   2.9%   2.9%   8.3%   5.9%   1.8%   4.1%  13.1%  17.6%   0.0%   0.0%   2.0%   2.1%  11.9%   4.9%   7.8%  10.3%  14.4%   3.0%   9.2%  16.8%  13.4%  19.6%   4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	 16.3%  15.0%  17.1%  15.8%  17.5%   6.1%   3.8%   3.8%   8.3%   5.9%  -0.9%   4.1%  19.2%  22.4%   1.0%  -0.8%   9.8%   0.0%   7.5%   2.9%  12.9%  12.9%  10.6%   2.0%   5.1%  14.3%   9.3%  16.5%   1.0%   7.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.9% 
	 16.9%  12.0%  20.5%  13.2%  12.5%   7.6%   3.8%   7.7%  10.7%   2.0%   3.7%   2.0%  14.6%  19.2%   4.8%   2.3%   3.9%   2.1%   9.0%   6.8%  19.0%   6.0%  13.5%   3.0%  10.2%  16.0%  11.3%  15.5%   4.8%   7.5%  14.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.6% 
	 19.7%  12.0%  17.9%  15.8%  17.5%   6.1%   3.8%  11.5%   7.4%   4.9%   1.8%   4.1%  20.8%  19.2%   0.0%  10.6%   8.8%   4.1%   6.0%   9.7%  15.5%   9.5%   6.7%  -1.0%   9.2%  13.4%   9.3%  19.6%   2.9%   4.7%  12.0%   6.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.7% 
	 19.1%  12.0%  17.9%  17.5%  13.3%   7.6%   5.7%   6.7%   9.1%   4.9%   2.8%   3.1%  14.6%  17.6%   1.0%   0.8%   7.8%   1.0%   4.5%  14.6%  13.8%   6.9%   9.6%   4.0%  10.2%  20.2%  11.3%  17.5%   3.8%   2.8%  10.3%   2.1%  13.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.3% 
	 14.6%  13.0%  18.8%  17.5%  18.3%   8.4%   3.8%   6.7%  11.6%   5.9%   3.7%   2.0%  10.8%  21.6%   1.0%   9.8%  10.8%   0.0%   7.5%   9.7%  14.7%   9.5%   9.6%   5.0%  11.2%  15.1%   7.2%  15.5%   1.9%   5.6%  15.4%   1.0%   9.8%  15.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 18.0%  15.0%  20.5%  12.3%  18.3%   9.2%   6.7%   5.8%  10.7%   3.9%   4.6%   4.1%  15.4%  20.0%   1.9%   9.1%   7.8%   0.0%   6.7%   9.7%  13.8%   6.9%   8.7%   4.0%   6.1%  21.0%   9.3%  19.6%  -1.9%   2.8%   6.8%  -1.0%  14.8%  16.0%  11.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.7% 
	 24.7%  15.0%  18.8%  15.8%  14.2%  11.5%   7.6%  10.6%   8.3%   5.9%   4.6%   6.1%  15.4%  22.4%   1.9%   5.3%   3.9%   3.1%   4.5%  14.6%  16.4%  11.2%  10.6%   1.0%   2.0%  12.6%   9.3%  15.5%   5.7%   2.8%   6.8%   0.0%  11.5%   5.7%  14.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 22.5%  13.0%  18.8%  20.2%  18.3%   7.6%   1.9%   5.8%  10.7%   6.9%   3.7%   3.1%  14.6%  25.6%   1.0%   3.8%   5.9%   1.0%   6.0%  10.7%  14.7%   6.0%  11.5%   3.0%   9.2%  15.1%   7.2%  23.7%   5.7%   1.9%   7.7%   3.1%  10.7%   0.9%  11.4%   1.8%  14.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 22.5%  14.0%  21.4%  15.8%  15.8%   7.6%   3.8%   4.8%   9.1%   3.9%   1.8%   5.1%  15.4%  22.4%   1.0%   6.8%   6.9%   2.1%   6.7%   6.8%  15.5%   7.8%  11.5%   3.0%   8.2%  20.2%   6.2%  16.5%   2.9%   5.6%   8.5%   0.0%  13.1%   4.7%  16.7%   3.6%  17.1%  17.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 23.6%  14.0%  19.7%  14.9%  16.7%   7.6%   4.8%   6.7%  11.6%   4.9%   0.9%   6.1%  16.2%  21.6%  -1.9%   4.5%   2.9%   2.1%   6.7%  10.7%  12.9%   7.8%   8.7%   6.0%   5.1%  16.8%  10.3%  15.5%   1.9%   3.7%   4.3%  -5.2%   8.2%   0.9%   7.0%   0.0%  14.3%  14.4%  24.4%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.0% 
	 20.2%  15.0%  20.5%  16.7%  20.8%   8.4%   2.9%   7.7%  15.7%   2.9%   3.7%   5.1%  15.4%  22.4%   1.0%   4.5%   3.9%   2.1%   7.5%  10.7%  16.4%   3.4%  13.5%   6.0%   7.1%  13.4%   8.2%  17.5%   5.7%   4.7%   1.7%  -1.0%   8.2%   0.0%   8.8%   0.9%  15.2%  13.6%  21.3%   3.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 21.9%  21.0%  21.4%  17.5%  20.0%   7.6%   7.6%   1.9%  14.0%   3.9%   3.7%   6.1%  15.4%  19.2%   3.8%   1.5%   1.0%   3.1%   9.0%   7.8%  13.8%   9.5%  13.5%   3.0%   4.1%  14.3%  10.3%  16.5%   5.7%   0.9%   8.5%   1.0%  11.5%   0.0%   7.9%   3.6%  15.2%  16.0%  17.3%   3.0%  17.9%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 20.2%  17.0%  20.5%  17.5%  16.7%   9.2%   5.7%   7.7%  15.7%   6.9%   4.6%   6.1%  19.2%  20.0%   3.8%   5.3%   3.9%   3.1%  10.4%  13.6%  16.4%   6.0%  15.4%   5.0%   3.1%  15.1%   7.2%  18.6%   4.8%   2.8%   1.7%   4.1%   9.0%  -2.8%   5.3%  -0.9%  13.3%   9.6%  24.4%   2.0%  14.2%  29.5%   0.0%   0.0% 	Avg.: 10.3% 
	 20.2%  15.0%  21.4%  18.4%  16.7%   8.4%   5.7%   6.7%  12.4%   5.9%   4.6%   5.1%  16.9%  21.6%   4.8%   7.6%   4.9%   2.1%   5.2%   9.7%  15.5%   7.8%  12.5%   5.0%   7.1%  15.1%  10.3%  17.5%   4.8%   1.9%   6.0%   2.1%   8.2%   3.8%   7.0%   1.8%  16.2%   8.8%  17.3%   5.1%  14.2%  30.5%  21.4%   0.0% 	Avg.: 10.5% 
************************************************************************************************************
[Elapsed time = 0.5 h]
Done!

f1_score_micro: 0.6682936750050844
f1_score_macro: 0.6402621566655953
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         4
           1       0.80      0.80      0.80         5
           2       0.80      1.00      0.89         4
           3       0.07      0.25      0.11         4
           4       0.83      0.56      0.67         9
           5       0.11      0.50      0.17         4
           6       1.00      1.00      1.00         4
           7       0.54      0.78      0.64         9
           8       0.29      0.80      0.42         5
           9       1.00      1.00      1.00         5
          10       0.83      1.00      0.91         5
          11       0.00      0.00      0.00         4
          12       0.82      1.00      0.90         9
          13       0.50      1.00      0.67         4
          14       0.33      0.25      0.29         4
          15       0.57      0.80      0.67         5
          16       0.20      0.25      0.22         4
          17       0.08      0.25      0.12         4
          18       0.40      0.50      0.44         4
          19       1.00      0.89      0.94         9
          20       0.15      0.75      0.25         4
          21       0.80      1.00      0.89         4
          22       0.80      0.80      0.80         5
          23       0.67      1.00      0.80         4
          24       0.12      0.50      0.20         4
          25       0.75      0.75      0.75         4
          26       1.00      1.00      1.00         4
          27       1.00      0.75      0.86         4
          28       0.06      0.11      0.07         9
          29       1.00      0.89      0.94         9
          30       0.00      0.00      0.00         4
          31       0.70      0.78      0.74         9
          32       1.00      1.00      1.00         5
          33       0.14      0.50      0.22         4
          34       0.67      1.00      0.80         4
          35       0.50      0.75      0.60         4
          36       0.00      0.00      0.00         4
          37       1.00      1.00      1.00         5
          38       0.83      1.00      0.91         5
          39       1.00      1.00      1.00         4
          40       0.50      0.50      0.50         4
          41       0.67      1.00      0.80         4
          42       1.00      0.56      0.71         9
          43       0.00      0.00      0.00         4
          44       0.83      1.00      0.91         5
          45       0.50      1.00      0.67         4
          46       0.75      1.00      0.86         9
          47       0.60      0.60      0.60         5
          48       0.90      1.00      0.95         9
          49       1.00      0.75      0.86         4
          50       1.00      0.75      0.86         4
          51       0.67      0.50      0.57         4
          52       0.25      0.20      0.22         5
          53       0.43      0.75      0.55         4
          54       1.00      0.75      0.86         4
          55       0.50      0.75      0.60         4
          56       0.75      0.67      0.71         9
          57       1.00      1.00      1.00         5
          58       0.22      0.50      0.31         4
          59       1.00      1.00      1.00         4
          60       0.50      0.25      0.33         4
          61       0.50      0.80      0.62         5
          62       1.00      0.80      0.89         5
          63       1.00      0.80      0.89         5
          64       1.00      1.00      1.00         5
          65       1.00      1.00      1.00         5
          66       0.00      0.00      0.00         4
          67       0.71      0.56      0.63         9
          68       1.00      0.89      0.94         9
          69       0.64      0.78      0.70         9
          70       1.00      1.00      1.00         5
          71       1.00      0.78      0.88         9
          72       0.75      0.75      0.75         4
          73       0.00      0.00      0.00         9
          74       0.00      0.00      0.00         4
          75       0.71      1.00      0.83         5
          76       1.00      1.00      1.00         9
          77       1.00      1.00      1.00         4
          78       0.64      0.78      0.70         9
          79       1.00      1.00      1.00         4
          80       0.25      0.11      0.15         9
          81       0.80      0.89      0.84         9
          82       1.00      0.75      0.86         4
          83       0.57      1.00      0.73         4
          84       0.20      0.11      0.14         9
          85       0.20      0.25      0.22         4
          86       0.82      1.00      0.90         9
          87       0.33      0.25      0.29         4
          88       0.75      0.60      0.67         5
          89       0.71      1.00      0.83         5
          90       1.00      0.50      0.67         4
          91       0.57      0.80      0.67         5
          92       0.50      0.50      0.50         4
          93       1.00      0.50      0.67         4
          94       1.00      0.67      0.80         9
          95       0.64      0.78      0.70         9
          96       0.80      1.00      0.89         4
          97       0.75      0.75      0.75         4
          98       0.67      0.44      0.53         9
          99       0.38      0.75      0.50         4
         100       0.40      0.50      0.44         4
         101       0.75      0.75      0.75         4
         102       1.00      0.50      0.67         4
         103       0.00      0.00      0.00         4
         104       1.00      0.89      0.94         9
         105       0.56      0.56      0.56         9
         106       0.67      0.50      0.57         4
         107       0.78      0.78      0.78         9
         108       0.80      1.00      0.89         4
         109       0.00      0.00      0.00         4
         110       0.62      0.89      0.73         9
         111       1.00      0.78      0.88         9
         112       0.67      1.00      0.80         4
         113       1.00      1.00      1.00         4
         114       0.60      0.60      0.60         5
         115       0.75      0.67      0.71         9
         116       1.00      0.78      0.88         9
         117       1.00      1.00      1.00         5
         118       1.00      0.75      0.86         4
         119       0.88      0.78      0.82         9
         120       0.80      1.00      0.89         4
         121       0.14      0.25      0.18         4
         122       0.89      0.89      0.89         9
         123       1.00      0.80      0.89         5
         124       0.80      0.80      0.80         5
         125       1.00      1.00      1.00         9
         126       1.00      1.00      1.00         4
         127       0.60      0.67      0.63         9
         128       0.80      0.89      0.84         9
         129       0.67      0.80      0.73         5
         130       1.00      1.00      1.00         5
         131       0.67      0.50      0.57         4
         132       0.12      0.11      0.12         9
         133       0.88      0.78      0.82         9
         134       1.00      0.89      0.94         9
         135       0.80      0.89      0.84         9
         136       0.50      0.57      0.53         7
         137       0.67      1.00      0.80         4
         138       0.25      0.50      0.33         4
         139       1.00      1.00      1.00         4
         140       0.60      0.75      0.67         4
         141       0.67      0.50      0.57         4
         142       0.75      0.60      0.67         5
         143       1.00      1.00      1.00         5
         144       1.00      0.75      0.86         4
         145       1.00      1.00      1.00         4
         146       0.00      0.00      0.00         4
         147       1.00      1.00      1.00         4
         148       1.00      0.25      0.40         4
         149       0.00      0.00      0.00         4
         150       1.00      0.75      0.86         4
         151       0.88      0.78      0.82         9
         152       1.00      1.00      1.00         9
         153       0.18      0.50      0.27         4
         154       1.00      1.00      1.00         4
         155       0.30      0.75      0.43         4
         156       0.73      0.89      0.80         9
         157       0.40      0.50      0.44         4
         158       0.00      0.00      0.00         4
         159       0.80      1.00      0.89         4
         160       0.80      1.00      0.89         4
         161       0.83      1.00      0.91         5
         162       1.00      1.00      1.00         5
         163       0.80      0.89      0.84         9
         164       0.00      0.00      0.00         4
         165       0.75      0.75      0.75         4
         166       0.80      1.00      0.89         4
         167       1.00      1.00      1.00         5
         168       0.75      1.00      0.86         9
         169       1.00      1.00      1.00         9
         170       0.60      0.75      0.67         4
         171       1.00      0.40      0.57         5
         172       1.00      1.00      1.00         4
         173       0.00      0.00      0.00         4
         174       0.75      0.60      0.67         5
         175       1.00      1.00      1.00         4
         176       1.00      0.80      0.89         5
         177       0.73      0.89      0.80         9
         178       0.69      1.00      0.82         9
         179       0.60      0.60      0.60         5
         180       0.67      0.50      0.57         4
         181       0.67      0.67      0.67         9
         182       0.80      1.00      0.89         4
         183       0.67      0.29      0.40         7
         184       0.75      0.75      0.75         4
         185       1.00      0.89      0.94         9
         186       0.00      0.00      0.00         4
         187       0.75      0.75      0.75         4
         188       0.33      0.33      0.33         9
         189       0.57      1.00      0.73         4
         190       0.21      0.33      0.26         9
         191       0.67      1.00      0.80         4
         192       1.00      1.00      1.00         9
         193       0.60      0.75      0.67         4
         194       0.67      0.40      0.50         5
         195       1.00      0.75      0.86         4
         196       0.67      1.00      0.80         4
         197       1.00      1.00      1.00         4
         198       1.00      1.00      1.00         4
         199       0.38      0.56      0.45         9
         200       0.00      0.00      0.00         4
         201       0.50      0.25      0.33         4
         202       0.27      0.33      0.30         9
         203       0.38      0.33      0.35         9
         204       1.00      0.75      0.86         4
         205       0.80      1.00      0.89         4
         206       1.00      0.75      0.86         4
         207       0.50      0.50      0.50         4
         208       1.00      0.75      0.86         4
         209       0.83      0.56      0.67         9
         210       0.43      0.75      0.55         4
         211       0.80      1.00      0.89         4
         212       1.00      1.00      1.00         5
         213       0.75      0.75      0.75         4
         214       1.00      0.60      0.75         5
         215       1.00      1.00      1.00         5
         216       0.00      0.00      0.00         9
         217       0.50      1.00      0.67         4
         218       0.83      1.00      0.91         5
         219       0.80      1.00      0.89         4
         220       0.44      1.00      0.62         4
         221       0.50      0.50      0.50         4
         222       0.75      0.75      0.75         4
         223       1.00      1.00      1.00         4
         224       0.82      1.00      0.90         9
         225       1.00      1.00      1.00         5
         226       1.00      1.00      1.00         4
         227       1.00      1.00      1.00         4
         228       1.00      1.00      1.00         9
         229       1.00      1.00      1.00         4
         230       0.80      1.00      0.89         4
         231       0.50      0.25      0.33         4
         232       0.86      0.67      0.75         9
         233       0.90      1.00      0.95         9
         234       1.00      0.75      0.86         4
         235       1.00      0.75      0.86         4
         236       0.89      0.89      0.89         9
         237       1.00      1.00      1.00         5
         238       0.00      0.00      0.00         4
         239       0.67      0.50      0.57         4
         240       0.50      0.75      0.60         4
         241       0.00      0.00      0.00         4
         242       0.50      0.50      0.50         4
         243       1.00      0.75      0.86         4
         244       1.00      1.00      1.00         4
         245       0.33      0.20      0.25         5
         246       1.00      1.00      1.00         4
         247       0.80      0.89      0.84         9
         248       0.67      1.00      0.80         4
         249       0.00      0.00      0.00         4
         250       1.00      1.00      1.00         4
         251       0.67      1.00      0.80         4
         252       0.73      0.89      0.80         9
         253       1.00      1.00      1.00         5
         254       0.64      0.78      0.70         9
         255       0.00      0.00      0.00         4
         256       1.00      0.67      0.80         9
         257       0.75      1.00      0.86         9
         258       0.36      0.44      0.40         9
         259       0.00      0.00      0.00         4
         260       0.73      0.89      0.80         9
         261       0.64      0.78      0.70         9
         262       0.50      0.44      0.47         9
         263       0.50      0.25      0.33         4
         264       0.60      0.75      0.67         4
         265       1.00      1.00      1.00         9
         266       0.00      0.00      0.00         4
         267       0.57      1.00      0.73         4
         268       0.67      1.00      0.80         4
         269       0.78      0.78      0.78         9
         270       0.50      1.00      0.67         4
         271       1.00      0.75      0.86         4
         272       0.00      0.00      0.00         4
         273       0.00      0.00      0.00         9
         274       0.25      0.50      0.33         4
         275       1.00      1.00      1.00         9
         276       1.00      0.78      0.88         9
         277       0.80      1.00      0.89         4
         278       0.83      1.00      0.91         5
         279       1.00      0.89      0.94         9
         280       0.00      0.00      0.00         9
         281       1.00      0.40      0.57         5
         282       0.00      0.00      0.00         4
         283       1.00      1.00      1.00         5
         284       0.80      0.89      0.84         9
         285       0.80      1.00      0.89         4
         286       0.00      0.00      0.00         4
         287       0.75      0.75      0.75         4
         288       1.00      0.56      0.71         9
         289       0.17      0.20      0.18         5
         290       0.83      0.56      0.67         9
         291       0.80      1.00      0.89         4
         292       0.73      0.89      0.80         9
         293       1.00      0.80      0.89         5
         294       0.67      0.67      0.67         9
         295       0.25      0.25      0.25         4
         296       1.00      1.00      1.00         4
         297       0.80      1.00      0.89         4
         298       0.67      1.00      0.80         4
         299       0.50      0.20      0.29         5
         300       0.83      1.00      0.91         5
         301       0.00      0.00      0.00         9
         302       0.50      0.25      0.33         4
         303       1.00      1.00      1.00         9
         304       1.00      0.60      0.75         5
         305       0.75      0.75      0.75         4
         306       0.67      1.00      0.80         4
         307       0.71      1.00      0.83         5
         308       1.00      1.00      1.00         4
         309       0.67      0.50      0.57         4
         310       1.00      0.88      0.93         8
         311       0.25      0.20      0.22         5
         312       1.00      1.00      1.00         4
         313       1.00      1.00      1.00         5
         314       1.00      0.78      0.88         9
         315       0.82      1.00      0.90         9
         316       0.60      0.75      0.67         4
         317       0.00      0.00      0.00         4
         318       1.00      0.11      0.20         9
         319       0.00      0.00      0.00         4
         320       0.80      1.00      0.89         4
         321       0.88      0.78      0.82         9
         322       0.88      0.78      0.82         9
         323       1.00      0.75      0.86         4
         324       1.00      0.75      0.86         4
         325       0.70      0.78      0.74         9
         326       1.00      0.56      0.71         9
         327       0.90      1.00      0.95         9
         328       0.57      1.00      0.73         4
         329       1.00      0.78      0.88         9
         330       0.86      0.67      0.75         9
         331       1.00      1.00      1.00         5
         332       0.67      1.00      0.80         4
         333       1.00      0.60      0.75         5
         334       0.22      0.22      0.22         9
         335       0.00      0.00      0.00         4
         336       0.43      0.75      0.55         4
         337       0.67      0.50      0.57         4
         338       1.00      0.40      0.57         5
         339       0.43      0.60      0.50         5
         340       1.00      0.75      0.86         4
         341       1.00      1.00      1.00         9
         342       0.57      1.00      0.73         4
         343       1.00      1.00      1.00         5
         344       1.00      1.00      1.00         4
         345       0.80      1.00      0.89         4
         346       0.50      0.25      0.33         4
         347       1.00      1.00      1.00         4
         348       0.58      0.78      0.67         9
         349       1.00      1.00      1.00         4
         350       1.00      0.80      0.89         5
         351       0.80      0.80      0.80         5
         352       0.80      0.80      0.80         5
         353       0.38      0.60      0.46         5
         354       1.00      0.75      0.86         4
         355       0.00      0.00      0.00         4
         356       1.00      0.50      0.67         4
         357       0.00      0.00      0.00         4
         358       0.40      1.00      0.57         4
         359       1.00      1.00      1.00         5
         360       1.00      1.00      1.00         4
         361       0.78      0.78      0.78         9
         362       1.00      0.89      0.94         9
         363       0.80      1.00      0.89         4
         364       1.00      1.00      1.00         4
         365       1.00      1.00      1.00         4
         366       0.67      1.00      0.80         4
         367       1.00      1.00      1.00         5
         368       0.00      0.00      0.00         4
         369       1.00      1.00      1.00         4
         370       0.89      1.00      0.94         8
         371       0.14      0.25      0.18         4
         372       0.75      0.75      0.75         4
         373       0.75      0.60      0.67         5
         374       0.60      0.75      0.67         4
         375       1.00      1.00      1.00         9
         376       0.57      1.00      0.73         4
         377       0.73      0.89      0.80         9
         378       1.00      0.78      0.88         9
         379       0.00      0.00      0.00         9
         380       1.00      0.78      0.88         9
         381       0.40      0.80      0.53         5
         382       0.90      1.00      0.95         9
         383       0.57      0.89      0.70         9
         384       0.75      0.75      0.75         4
         385       1.00      0.44      0.62         9
         386       0.12      0.67      0.20         9
         387       0.62      0.89      0.73         9
         388       1.00      0.40      0.57         5
         389       1.00      0.60      0.75         5
         390       0.17      0.50      0.25         4
         391       0.00      0.00      0.00         4
         392       1.00      1.00      1.00         4
         393       1.00      1.00      1.00         5
         394       0.80      0.89      0.84         9
         395       1.00      0.80      0.89         5
         396       0.80      1.00      0.89         4
         397       0.00      0.00      0.00         4
         398       1.00      0.75      0.86         4
         399       0.60      0.75      0.67         4
         400       0.62      0.56      0.59         9
         401       1.00      0.80      0.89         5
         402       0.50      1.00      0.67         4
         403       0.12      0.25      0.17         4
         404       0.80      1.00      0.89         4
         405       0.50      0.75      0.60         4
         406       0.25      0.50      0.33         4
         407       0.38      0.60      0.46         5
         408       1.00      1.00      1.00         4
         409       0.00      0.00      0.00         9
         410       0.80      1.00      0.89         4
         411       0.83      0.56      0.67         9
         412       0.80      1.00      0.89         4
         413       0.00      0.00      0.00         4
         414       1.00      0.75      0.86         4
         415       0.00      0.00      0.00         4
         416       0.57      0.44      0.50         9
         417       0.86      0.67      0.75         9
         418       0.00      0.00      0.00         4
         419       1.00      0.75      0.86         4
         420       0.75      0.75      0.75         4
         421       0.80      0.89      0.84         9
         422       0.80      0.80      0.80         5
         423       0.50      0.50      0.50         4
         424       0.75      0.33      0.46         9
         425       0.89      0.89      0.89         9
         426       1.00      0.75      0.86         4
         427       0.50      0.25      0.33         4
         428       0.00      0.00      0.00         4
         429       0.86      0.67      0.75         9
         430       1.00      0.75      0.86         4
         431       1.00      1.00      1.00         9
         432       0.80      1.00      0.89         4
         433       0.33      0.25      0.29         4
         434       1.00      0.50      0.67         4
         435       0.50      0.50      0.50         4
         436       0.67      0.80      0.73         5
         437       0.50      0.25      0.33         4
         438       1.00      0.75      0.86         4
         439       0.67      0.44      0.53         9
         440       0.70      0.78      0.74         9
         441       0.00      0.00      0.00         4
         442       0.00      0.00      0.00         4
         443       0.89      0.89      0.89         9
         444       0.82      1.00      0.90         9
         445       0.64      0.78      0.70         9
         446       1.00      0.75      0.86         4
         447       0.31      1.00      0.47         4
         448       0.00      0.00      0.00         4
         449       0.75      0.75      0.75         4
         450       0.67      1.00      0.80         4
         451       1.00      0.89      0.94         9
         452       0.00      0.00      0.00         4
         453       0.33      0.33      0.33         9
         454       0.80      1.00      0.89         4
         455       0.20      0.11      0.14         9
         456       0.60      0.75      0.67         4
         457       1.00      1.00      1.00         5
         458       1.00      1.00      1.00         4
         459       0.80      0.80      0.80         5
         460       0.80      1.00      0.89         4
         461       0.07      0.22      0.11         9
         462       0.57      0.80      0.67         5
         463       1.00      0.75      0.86         4
         464       0.50      0.25      0.33         4
         465       0.83      1.00      0.91         5
         466       1.00      1.00      1.00         4
         467       0.80      1.00      0.89         4
         468       1.00      0.78      0.88         9
         469       0.50      0.75      0.60         4
         470       0.00      0.00      0.00         4
         471       0.00      0.00      0.00         4
         472       0.00      0.00      0.00         4
         473       0.27      0.33      0.30         9
         474       0.00      0.00      0.00         4
         475       0.80      1.00      0.89         4
         476       0.27      0.60      0.37         5
         477       1.00      0.75      0.86         4
         478       0.50      0.25      0.33         4
         479       1.00      0.75      0.86         4
         480       0.50      0.75      0.60         4
         481       1.00      0.75      0.86         4
         482       0.50      0.80      0.62         5
         483       0.80      1.00      0.89         4
         484       1.00      1.00      1.00         9
         485       1.00      0.25      0.40         4
         486       0.60      0.75      0.67         4
         487       1.00      1.00      1.00         4
         488       0.00      0.00      0.00         4
         489       1.00      1.00      1.00         9
         490       0.80      0.89      0.84         9
         491       0.67      0.80      0.73         5
         492       0.00      0.00      0.00         5
         493       1.00      1.00      1.00         5
         494       0.33      0.25      0.29         4
         495       1.00      0.50      0.67         4
         496       1.00      1.00      1.00         4
         497       0.75      0.75      0.75         4
         498       0.57      1.00      0.73         4
         499       0.00      0.00      0.00         4
         500       0.80      1.00      0.89         4
         501       0.80      1.00      0.89         4
         502       0.75      0.67      0.71         9
         503       1.00      0.75      0.86         4
         504       1.00      1.00      1.00         4
         505       1.00      0.60      0.75         5
         506       0.83      1.00      0.91         5
         507       1.00      1.00      1.00         4
         508       0.50      0.33      0.40         9
         509       1.00      1.00      1.00         4
         510       0.78      0.78      0.78         9
         511       1.00      0.75      0.86         4
         512       0.50      0.75      0.60         4
         513       0.67      0.80      0.73         5
         514       1.00      1.00      1.00         5
         515       1.00      0.78      0.88         9
         516       1.00      1.00      1.00         4
         517       1.00      0.56      0.71         9
         518       1.00      1.00      1.00         4
         519       0.00      0.00      0.00         4
         520       1.00      0.89      0.94         9
         521       0.00      0.00      0.00         9
         522       1.00      0.75      0.86         4
         523       0.71      1.00      0.83         5
         524       0.60      0.60      0.60         5
         525       0.33      0.50      0.40         4
         526       1.00      0.75      0.86         4
         527       0.00      0.00      0.00         4
         528       1.00      1.00      1.00         9
         529       0.67      0.50      0.57         4
         530       0.80      0.89      0.84         9
         531       0.46      0.67      0.55         9
         532       0.67      0.80      0.73         5
         533       0.33      0.25      0.29         4
         534       1.00      1.00      1.00         4
         535       1.00      1.00      1.00         4
         536       0.80      1.00      0.89         4
         537       1.00      0.80      0.89         5
         538       0.75      0.75      0.75         4
         539       0.90      1.00      0.95         9
         540       0.00      0.00      0.00         4
         541       1.00      1.00      1.00         4
         542       0.00      0.00      0.00         4
         543       0.57      0.80      0.67         5
         544       0.50      0.22      0.31         9
         545       0.50      0.50      0.50         4
         546       0.00      0.00      0.00         4
         547       1.00      0.75      0.86         4
         548       1.00      0.25      0.40         4
         549       0.80      1.00      0.89         4
         550       0.00      0.00      0.00         4
         551       0.33      0.50      0.40         4
         552       0.80      1.00      0.89         4
         553       0.60      0.67      0.63         9
         554       0.86      0.67      0.75         9
         555       1.00      0.60      0.75         5
         556       1.00      1.00      1.00         4
         557       0.75      0.60      0.67         5
         558       0.80      1.00      0.89         4
         559       0.00      0.00      0.00         4
         560       1.00      0.78      0.88         9
         561       1.00      0.75      0.86         4
         562       1.00      1.00      1.00         4
         563       1.00      1.00      1.00         4
         564       0.00      0.00      0.00         4
         565       0.40      0.50      0.44         4
         566       1.00      0.50      0.67         4
         567       1.00      1.00      1.00         4
         568       0.00      0.00      0.00         4
         569       0.67      1.00      0.80         4
         570       0.22      0.50      0.31         4
         571       1.00      1.00      1.00         4
         572       0.00      0.00      0.00         9
         573       0.75      0.75      0.75         4
         574       0.80      1.00      0.89         4
         575       1.00      1.00      1.00         4
         576       0.75      0.75      0.75         4
         577       1.00      1.00      1.00         9
         578       0.00      0.00      0.00         4
         579       0.00      0.00      0.00         4
         580       0.00      0.00      0.00         9
         581       0.67      0.50      0.57         4
         582       0.75      0.75      0.75         4
         583       0.80      1.00      0.89         4
         584       1.00      1.00      1.00         9
         585       0.00      0.00      0.00         4
         586       0.80      1.00      0.89         4
         587       1.00      0.78      0.88         9
         588       0.67      1.00      0.80         4
         589       1.00      0.89      0.94         9
         590       0.00      0.00      0.00         4
         591       0.00      0.00      0.00         4
         592       0.80      1.00      0.89         4
         593       0.00      0.00      0.00         4
         594       0.75      0.75      0.75         4
         595       1.00      1.00      1.00         5
         596       0.78      0.78      0.78         9
         597       0.57      1.00      0.73         4
         598       1.00      1.00      1.00         4
         599       1.00      0.75      0.86         4
         600       0.71      1.00      0.83         5
         601       0.83      1.00      0.91         5
         602       0.67      0.80      0.73         5
         603       0.80      0.89      0.84         9
         604       1.00      0.75      0.86         4
         605       1.00      1.00      1.00         9
         606       0.78      0.78      0.78         9
         607       1.00      1.00      1.00         4
         608       0.50      0.75      0.60         4
         609       0.60      0.60      0.60         5
         610       0.67      0.50      0.57         4
         611       1.00      1.00      1.00         4
         612       0.43      0.50      0.46         6
         613       0.00      0.00      0.00         4
         614       0.80      1.00      0.89         4
         615       0.50      0.25      0.33         4
         616       0.00      0.00      0.00         4
         617       1.00      1.00      1.00         4
         618       1.00      0.78      0.88         9
         619       0.00      0.00      0.00         4
         620       0.67      1.00      0.80         4
         621       1.00      0.25      0.40         4
         622       1.00      1.00      1.00         4
         623       0.80      1.00      0.89         4
         624       0.57      0.80      0.67         5
         625       0.54      0.78      0.64         9
         626       0.60      0.60      0.60         5
         627       0.00      0.00      0.00         4
         628       1.00      0.75      0.86         4
         629       0.88      0.78      0.82         9
         630       0.88      0.78      0.82         9
         631       0.40      0.44      0.42         9
         632       1.00      0.78      0.88         9
         633       0.67      0.22      0.33         9
         634       0.67      0.50      0.57         4
         635       1.00      0.89      0.94         9
         636       0.50      0.60      0.55         5
         637       0.00      0.00      0.00         4
         638       1.00      1.00      1.00         4
         639       0.60      0.75      0.67         4
         640       1.00      1.00      1.00         9
         641       0.75      0.60      0.67         5
         642       0.00      0.00      0.00         4
         643       0.75      0.75      0.75         4
         644       1.00      0.75      0.86         4
         645       0.80      1.00      0.89         4
         646       0.67      0.50      0.57         4
         647       0.00      0.00      0.00         4
         648       0.67      0.50      0.57         4
         649       1.00      0.75      0.86         4
         650       1.00      0.50      0.67         4
         651       1.00      1.00      1.00         4
         652       1.00      1.00      1.00         4
         653       0.78      0.78      0.78         9
         654       0.00      0.00      0.00         4
         655       0.00      0.00      0.00         4
         656       0.89      0.89      0.89         9
         657       0.50      0.40      0.44         5
         658       0.83      1.00      0.91         5
         659       0.62      0.56      0.59         9
         660       0.80      1.00      0.89         4
         661       1.00      1.00      1.00         9
         662       1.00      0.75      0.86         4
         663       0.75      0.75      0.75         4
         664       0.62      0.56      0.59         9
         665       0.50      0.25      0.33         4
         666       0.75      1.00      0.86         9
         667       0.60      0.33      0.43         9
         668       0.90      1.00      0.95         9
         669       1.00      0.25      0.40         4
         670       0.67      1.00      0.80         4
         671       0.67      0.67      0.67         9
         672       0.80      1.00      0.89         4
         673       0.00      0.00      0.00         4
         674       0.83      0.56      0.67         9
         675       0.67      1.00      0.80         4
         676       0.89      0.89      0.89         9
         677       0.60      0.75      0.67         4
         678       0.00      0.00      0.00         4
         679       1.00      1.00      1.00         4
         680       0.00      0.00      0.00         4
         681       0.25      0.25      0.25         4
         682       0.56      0.56      0.56         9
         683       0.67      0.50      0.57         4
         684       0.60      0.75      0.67         4
         685       0.00      0.00      0.00         9
         686       0.67      1.00      0.80         4
         687       0.33      0.25      0.29         4
         688       0.47      0.78      0.58         9
         689       1.00      0.50      0.67         4
         690       1.00      1.00      1.00         4
         691       1.00      0.75      0.86         4
         692       0.00      0.00      0.00         4
         693       0.71      1.00      0.83         5
         694       0.88      0.78      0.82         9
         695       0.50      0.60      0.55         5
         696       0.40      0.50      0.44         4
         697       0.00      0.00      0.00         4
         698       0.78      0.78      0.78         9
         699       0.57      1.00      0.73         4
         700       0.62      0.56      0.59         9
         701       0.75      0.67      0.71         9
         702       0.00      0.00      0.00         4
         703       0.50      0.40      0.44         5
         704       0.25      0.25      0.25         4
         705       0.50      0.60      0.55         5
         706       0.75      0.75      0.75         4
         707       0.50      0.50      0.50         4
         708       1.00      0.78      0.88         9
         709       1.00      0.50      0.67         4
         710       0.80      1.00      0.89         4
         711       0.50      0.40      0.44         5
         712       1.00      1.00      1.00         4
         713       0.50      0.67      0.57         9
         714       1.00      0.60      0.75         5
         715       0.67      1.00      0.80         4
         716       1.00      1.00      1.00         4
         717       0.50      0.50      0.50         4
         718       0.80      0.80      0.80         5
         719       0.71      1.00      0.83         5
         720       0.62      0.56      0.59         9
         721       1.00      1.00      1.00         4
         722       0.89      0.89      0.89         9
         723       0.90      1.00      0.95         9
         724       1.00      1.00      1.00         4
         725       1.00      0.75      0.86         4
         726       0.50      0.20      0.29         5
         727       0.14      0.75      0.24         4
         728       0.33      0.50      0.40         4
         729       0.80      1.00      0.89         4
         730       0.67      0.67      0.67         9
         731       0.88      0.78      0.82         9
         732       0.80      1.00      0.89         4
         733       1.00      1.00      1.00         5
         734       0.83      0.56      0.67         9
         735       0.00      0.00      0.00         4
         736       0.00      0.00      0.00         4
         737       0.50      0.22      0.31         9
         738       0.50      0.40      0.44         5
         739       0.60      0.67      0.63         9
         740       0.50      0.20      0.29         5
         741       0.25      0.20      0.22         5
         742       1.00      0.60      0.75         5
         743       0.00      0.00      0.00         4
         744       1.00      1.00      1.00         9
         745       0.00      0.00      0.00         4
         746       1.00      0.25      0.40         4
         747       0.80      1.00      0.89         4
         748       1.00      1.00      1.00         4
         749       0.00      0.00      0.00         4
         750       1.00      1.00      1.00         4
         751       0.67      0.50      0.57         4
         752       1.00      0.75      0.86         4
         753       1.00      1.00      1.00         5
         754       0.00      0.00      0.00         9
         755       0.50      1.00      0.67         4
         756       0.75      0.75      0.75         4
         757       0.89      0.89      0.89         9
         758       0.57      0.89      0.70         9
         759       0.80      1.00      0.89         4
         760       1.00      1.00      1.00         4
         761       0.75      0.75      0.75         4
         762       0.90      1.00      0.95         9
         763       1.00      0.75      0.86         4
         764       0.00      0.00      0.00         4
         765       1.00      0.50      0.67         4
         766       1.00      0.89      0.94         9
         767       0.90      1.00      0.95         9
         768       0.75      0.60      0.67         5
         769       0.80      0.89      0.84         9
         770       1.00      0.67      0.80         9
         771       0.60      0.75      0.67         4
         772       0.60      0.38      0.46         8
         773       0.75      0.75      0.75         4
         774       1.00      0.11      0.20         9
         775       0.80      1.00      0.89         4
         776       0.00      0.00      0.00         9
         777       0.83      1.00      0.91         5
         778       0.67      1.00      0.80         4
         779       0.57      1.00      0.73         4
         780       0.78      0.78      0.78         9
         781       1.00      0.80      0.89         5
         782       0.11      0.25      0.15         4
         783       0.80      1.00      0.89         4
         784       0.38      0.67      0.48         9
         785       1.00      0.75      0.86         4
         786       0.71      0.56      0.63         9
         787       1.00      0.33      0.50         9
         788       0.67      1.00      0.80         4
         789       1.00      1.00      1.00         9
         790       1.00      1.00      1.00         4
         791       0.89      0.89      0.89         9
         792       1.00      1.00      1.00         4
         793       1.00      0.67      0.80         9
         794       0.80      1.00      0.89         4
         795       0.88      0.78      0.82         9
         796       0.67      0.40      0.50         5
         797       1.00      1.00      1.00         4
         798       0.00      0.00      0.00         4
         799       0.75      0.75      0.75         4
         800       0.62      1.00      0.77         5
         801       0.62      1.00      0.77         5
         802       0.00      0.00      0.00         4
         803       0.00      0.00      0.00         4
         804       1.00      0.78      0.88         9
         805       0.80      1.00      0.89         4
         806       1.00      0.67      0.80         9
         807       0.00      0.00      0.00         4
         808       0.50      1.00      0.67         4
         809       0.75      0.60      0.67         5
         810       0.67      1.00      0.80         4
         811       0.50      0.50      0.50         4
         812       0.00      0.00      0.00         4
         813       1.00      1.00      1.00         4
         814       0.67      0.67      0.67         9
         815       0.00      0.00      0.00         4
         816       1.00      0.80      0.89         5
         817       0.00      0.00      0.00         4
         818       0.00      0.00      0.00         4
         819       0.44      0.44      0.44         9
         820       1.00      1.00      1.00         5
         821       0.00      0.00      0.00         5
         822       1.00      1.00      1.00         5
         823       1.00      0.25      0.40         4
         824       0.13      0.22      0.17         9
         825       1.00      0.60      0.75         5
         826       1.00      1.00      1.00         4
         827       1.00      1.00      1.00         4
         828       0.75      0.75      0.75         4
         829       1.00      1.00      1.00         4
         830       1.00      0.60      0.75         5
         831       0.80      1.00      0.89         4
         832       0.89      0.89      0.89         9
         833       1.00      1.00      1.00         4
         834       0.75      0.75      0.75         4
         835       0.67      0.44      0.53         9
         836       1.00      0.25      0.40         4
         837       0.00      0.00      0.00         4
         838       0.50      0.60      0.55         5
         839       1.00      0.50      0.67         4
         840       0.00      0.00      0.00         4
         841       1.00      0.25      0.40         4
         842       1.00      0.25      0.40         4
         843       0.33      0.25      0.29         4
         844       0.00      0.00      0.00         4
         845       1.00      1.00      1.00         5
         846       0.60      0.75      0.67         4
         847       0.33      0.20      0.25         5
         848       0.00      0.00      0.00         5
         849       0.00      0.00      0.00         4
         850       1.00      1.00      1.00         4
         851       0.80      0.80      0.80         5
         852       1.00      0.75      0.86         4
         853       0.50      0.11      0.18         9
         854       0.75      1.00      0.86         9
         855       1.00      0.75      0.86         4
         856       1.00      0.71      0.83         7
         857       0.75      0.75      0.75         4
         858       0.00      0.00      0.00         4
         859       1.00      0.50      0.67         4
         860       0.00      0.00      0.00         9
         861       0.00      0.00      0.00         4
         862       1.00      0.89      0.94         9
         863       0.57      0.80      0.67         5
         864       0.00      0.00      0.00         4
         865       0.00      0.00      0.00         5
         866       1.00      0.33      0.50         9
         867       1.00      1.00      1.00         5
         868       0.89      0.89      0.89         9
         869       0.00      0.00      0.00         4
         870       0.00      0.00      0.00         4
         871       0.67      0.50      0.57         4
         872       1.00      0.80      0.89         5
         873       0.00      0.00      0.00         4
         874       0.00      0.00      0.00         4
         875       0.15      0.50      0.24         4
         876       1.00      1.00      1.00         4
         877       1.00      0.50      0.67         4
         878       0.40      0.50      0.44         4
         879       0.50      0.75      0.60         4
         880       0.20      0.25      0.22         4
         881       0.38      0.75      0.50         4
         882       0.83      1.00      0.91         5
         883       0.33      0.50      0.40         4
         884       0.82      1.00      0.90         9
         885       1.00      0.50      0.67         4
         886       1.00      1.00      1.00         4
         887       0.25      0.25      0.25         4
         888       0.14      0.50      0.22         4
         889       0.00      0.00      0.00         4
         890       1.00      0.50      0.67         4
         891       1.00      1.00      1.00         4
         892       0.75      1.00      0.86         9
         893       0.38      0.67      0.48         9

    accuracy                           0.67      4917
   macro avg       0.66      0.66      0.64      4917
weighted avg       0.68      0.67      0.66      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
