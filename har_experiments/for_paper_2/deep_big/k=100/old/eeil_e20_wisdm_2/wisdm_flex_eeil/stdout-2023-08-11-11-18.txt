	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [623, 215, 635, 811, 580, 172, 618, 243, 348, 60, 403, 543, 96, 305, 167, 616, 626, 463, 341, 779, 267, 291, 672, 637, 527, 700, 691, 584, 123, 805, 740, 861, 245, 476, 552, 4, 379, 332, 373, 398, 759, 92, 200, 694, 47, 577, 819, 507, 272, 263, 597, 818, 801, 355, 430, 633, 301, 677, 621, 846, 63, 122, 300, 89, 774, 169, 796, 467, 674, 350, 45, 458, 323, 680, 428, 741, 822, 509, 127, 653, 484, 813, 733, 617, 786, 216, 596, 174, 346, 585, 614, 744, 447, 91, 534, 574, 619, 288, 46, 821, 242, 394, 692, 155, 688, 693, 77, 17, 489, 755, 283, 562, 186, 622, 66, 790, 828, 289, 756, 592, 383, 319, 465, 57, 206, 189, 252, 531, 569, 533, 441, 256, 157, 431, 872, 690, 845, 232, 510, 97, 336, 13, 48, 842, 293, 865, 183, 508, 748, 342, 281, 282, 703, 494, 149, 724, 727, 848, 784, 217, 257, 634, 607, 137, 139, 706, 269, 681, 752, 830, 326, 473, 729, 498, 335, 70, 129, 9, 388, 340, 573, 98, 101, 638, 433, 823, 144, 553, 102, 612, 8, 266, 180, 78, 6, 701, 311, 165, 120, 485, 663, 812, 3, 479, 542, 630, 276, 781, 33, 760, 655, 685, 470, 451, 356, 236, 198, 673, 538, 399, 840, 492, 462, 864, 67, 370, 735, 525, 474, 105, 749, 162, 712, 832, 495, 514, 316, 767, 330, 625, 746, 642, 870, 307, 667, 540, 776, 602, 114, 487, 147, 347, 303, 572, 770, 142, 219, 869, 24, 107, 866, 725, 791, 595, 829, 223, 225, 555, 154, 475, 493, 143, 254, 376, 889, 81, 415, 30, 39, 816, 839, 539, 687, 146, 576, 610, 115, 535, 702, 519, 891, 188, 545, 196, 698, 711, 517, 765, 371, 0, 271, 547, 194, 321, 362, 825, 699, 490, 251, 42, 100, 651, 658, 275, 170, 184, 628, 434, 615, 883, 890, 113, 544, 389, 345, 629, 429, 804, 117, 349, 12, 557, 438, 158, 454, 61, 598, 62, 783, 393, 882, 302, 675, 166, 731, 125, 287, 847, 19, 421, 286, 213, 422, 50, 413, 478, 520, 646, 504, 145, 794, 833, 5, 108, 878, 359, 523, 518, 151, 407, 363, 351, 36, 856, 11, 893, 104, 18, 294, 502, 220, 564, 513, 453, 55, 721, 423, 558, 76, 190, 354, 226, 652, 575, 442, 274, 797, 381, 419, 471, 560, 670, 497, 334, 719, 159, 778, 168, 800, 171, 280, 93, 164, 808, 483, 21, 594, 880, 38, 191, 705, 320, 56, 649, 44, 277, 837, 416, 203, 578, 279, 480, 600, 807, 499, 472, 500, 25, 745, 408, 686, 228, 193, 136, 718, 817, 59, 432, 328, 233, 537, 298, 231, 404, 73, 859, 338, 74, 696, 34, 736, 369, 689, 135, 556, 308, 875, 7, 792, 884, 524, 753, 322, 773, 662, 75, 512, 581, 836, 195, 255, 589, 750, 831, 160, 208, 173, 824, 54, 857, 793, 849, 199, 515, 69, 112, 588, 526, 205, 855, 378, 138, 435, 521, 52, 126, 758, 608, 192, 838, 841, 221, 561, 620, 327, 265, 87, 234, 536, 568, 27, 769, 23, 448, 654, 26, 317, 329, 377, 222, 879, 295, 397, 310, 863, 678, 224, 583, 396, 809, 258, 570, 28, 43, 384, 440, 437, 364, 466, 789, 204, 505, 713, 676, 238, 456, 315, 247, 197, 868, 567, 715, 728, 309, 742, 496, 436, 390, 31, 240, 178, 94, 352, 871, 551, 110, 641, 720, 716, 511, 130, 86, 22, 881, 132, 420, 695, 772, 601, 427, 874, 211, 590, 400, 455, 559, 664, 140, 103, 611, 244, 747, 877, 579, 710, 417, 37, 412, 532, 237, 528, 14, 548, 361, 762, 764, 738, 411, 820, 853, 201, 785, 737, 503, 297, 141, 304, 207, 683, 79, 331, 391, 133, 2, 175, 273, 182, 565, 16, 58, 409, 410, 645, 72, 679, 587, 593, 65, 357, 382, 599, 795, 418, 249, 777, 843, 714, 459, 71, 605, 88, 64, 268, 554, 516, 99, 854, 668, 116, 726, 392, 339, 669, 650, 810, 851, 106, 374, 707, 95, 887, 739, 84, 360, 439, 671, 260, 325, 469, 640, 780, 529, 85, 387, 892, 10, 402, 606, 161, 358, 862, 802, 443, 253, 285, 697, 708, 460, 754, 49, 546, 631, 477, 306, 787, 624, 445, 229, 604, 660, 643, 657, 491, 814, 661, 732, 299, 109, 827, 815, 259, 722, 591, 603, 284, 586, 375, 647, 401, 582, 176, 852, 131, 782, 468, 82, 464, 156, 609, 506, 150, 296, 366, 261, 80, 235, 751, 386, 860, 152, 35, 788, 530, 734, 202, 684, 757, 227, 743, 353, 566, 457, 365, 639, 834, 425, 501, 867, 372, 613, 704, 482, 444, 118, 644, 406, 29, 763, 368, 209, 723, 405, 414, 395, 41, 119, 111, 450, 212, 761, 292, 452, 278, 886, 888, 799, 51, 40, 449, 627, 128, 656, 424, 835, 563, 153, 163, 885, 15, 717, 730, 290, 337, 666, 230, 876, 636, 850, 239, 446, 250, 550, 248, 181, 333, 270, 648, 314, 632, 803, 262, 380, 177, 68, 486, 344, 324, 1, 185, 385, 313, 187, 858, 343, 709, 83, 90, 367, 659, 488, 665, 481, 549, 541, 771, 218, 826, 246, 53, 264, 121, 426, 148, 312, 214, 768, 461, 844, 32, 522, 179, 241, 318, 806, 766, 124, 873, 682, 775, 210, 134, 571, 20, 798]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.719, TAw acc= 52.1% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.185, TAw acc= 68.5% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.844, TAw acc= 70.5% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.590, TAw acc= 75.3% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.393, TAw acc= 77.4% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.241, TAw acc= 82.2% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.107, TAw acc= 81.5% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.998, TAw acc= 83.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
| Selected 306 train exemplars, time=  0.0s
306
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.014 | TAw acc= 79.0%, forg=  0.0%| TAg acc= 79.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=3.098, TAw acc= 39.7% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.416, TAw acc= 56.4% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.997, TAw acc= 61.5% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.707, TAw acc= 65.4% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.486, TAw acc= 74.4% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.309, TAw acc= 85.9% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.177, TAw acc= 87.2% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.059, TAw acc= 89.7% | *
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.058, TAw acc= 89.7% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.058, TAw acc= 89.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 89.7% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.056, TAw acc= 89.7% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.055, TAw acc= 89.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
| Selected 486 train exemplars, time=  0.0s
486
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.105 | TAw acc= 95.9%, forg=-16.9%| TAg acc= 81.0%, forg= -2.1% <<<
>>> Test on task  1 : loss=1.021 | TAw acc= 88.8%, forg=  0.0%| TAg acc= 85.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.1s loss=3.390, TAw acc= 42.7% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.714, TAw acc= 66.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.250, TAw acc= 78.7% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.1s loss=1.932, TAw acc= 81.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.651, TAw acc= 88.0% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.446, TAw acc= 86.7% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.323, TAw acc= 90.7% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.183, TAw acc= 93.3% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.182, TAw acc= 93.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.181, TAw acc= 93.3% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.180, TAw acc= 93.3% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.179, TAw acc= 93.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.178, TAw acc= 93.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
| Selected 666 train exemplars, time=  0.0s
666
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.046 | TAw acc= 95.4%, forg=  0.5%| TAg acc= 86.2%, forg= -5.1% <<<
>>> Test on task  1 : loss=0.974 | TAw acc= 93.5%, forg= -4.7%| TAg acc= 90.7%, forg= -5.6% <<<
>>> Test on task  2 : loss=1.151 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 78.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.639, TAw acc= 47.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.559, TAw acc= 68.8% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.941, TAw acc= 77.4% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.574, TAw acc= 81.7% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.313, TAw acc= 91.4% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.154, TAw acc= 94.6% | *
| Epoch   7, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.034, TAw acc= 95.7% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.945, TAw acc= 95.7% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.945, TAw acc= 95.7% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.944, TAw acc= 95.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.943, TAw acc= 95.7% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.942, TAw acc= 96.8% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.941, TAw acc= 96.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
| Selected 846 train exemplars, time=  0.0s
846
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.056 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 83.1%, forg=  3.1% <<<
>>> Test on task  1 : loss=1.030 | TAw acc= 94.4%, forg= -0.9%| TAg acc= 83.2%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.136 | TAw acc= 95.1%, forg= -3.9%| TAg acc= 71.8%, forg=  6.8% <<<
>>> Test on task  3 : loss=1.068 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 79.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.712, TAw acc= 47.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.604, TAw acc= 56.0% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.018, TAw acc= 67.0% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.673, TAw acc= 74.7% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.462, TAw acc= 76.9% | *
| Epoch   6, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.300, TAw acc= 83.5% | *
| Epoch   7, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.170, TAw acc= 83.5% | *
| Epoch   8, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.090, TAw acc= 86.8% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.089, TAw acc= 86.8% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.088, TAw acc= 86.8% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.087, TAw acc= 86.8% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.086, TAw acc= 86.8% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.085, TAw acc= 86.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
| Selected 1026 train exemplars, time=  0.0s
1026
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.073 | TAw acc= 96.4%, forg= -0.5%| TAg acc= 77.9%, forg=  8.2% <<<
>>> Test on task  1 : loss=0.929 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 90.7%, forg=  0.0% <<<
>>> Test on task  2 : loss=1.004 | TAw acc= 97.1%, forg= -1.9%| TAg acc= 82.5%, forg= -3.9% <<<
>>> Test on task  3 : loss=1.232 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 73.2%, forg=  6.5% <<<
>>> Test on task  4 : loss=1.115 | TAw acc= 87.6%, forg=  0.0%| TAg acc= 79.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.009, TAw acc= 48.8% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.678, TAw acc= 58.3% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.130, TAw acc= 69.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.770, TAw acc= 73.8% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.534, TAw acc= 76.2% | *
| Epoch   6, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.330, TAw acc= 81.0% | *
| Epoch   7, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.193, TAw acc= 81.0% | *
| Epoch   8, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.065, TAw acc= 82.1% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.064, TAw acc= 82.1% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.063, TAw acc= 82.1% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.062, TAw acc= 82.1% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.061, TAw acc= 82.1% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.060, TAw acc= 82.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1188 train exemplars, time=  0.0s
1188
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.978 | TAw acc= 95.9%, forg=  0.5%| TAg acc= 85.6%, forg=  0.5% <<<
>>> Test on task  1 : loss=0.899 | TAw acc= 95.3%, forg= -0.9%| TAg acc= 88.8%, forg=  1.9% <<<
>>> Test on task  2 : loss=0.947 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 85.4%, forg= -2.9% <<<
>>> Test on task  3 : loss=1.231 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 62.6%, forg= 17.1% <<<
>>> Test on task  4 : loss=1.095 | TAw acc= 95.9%, forg= -8.3%| TAg acc= 81.0%, forg= -1.7% <<<
>>> Test on task  5 : loss=0.943 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 82.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.817, TAw acc= 37.0% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.422, TAw acc= 47.9% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.602, TAw acc= 78.1% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.091, TAw acc= 79.5% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.804, TAw acc= 86.3% | *
| Epoch   6, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.624, TAw acc= 87.7% | *
| Epoch   7, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.692, TAw acc= 89.0% |
| Epoch   8, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 87.7% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.489, TAw acc= 87.7% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.486, TAw acc= 87.7% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.482, TAw acc= 87.7% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.479, TAw acc= 87.7% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.476, TAw acc= 87.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1348 train exemplars, time=  0.0s
1348
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.975 | TAw acc= 95.4%, forg=  1.0%| TAg acc= 81.0%, forg=  5.1% <<<
>>> Test on task  1 : loss=0.905 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 86.0%, forg=  4.7% <<<
>>> Test on task  2 : loss=0.902 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 82.5%, forg=  2.9% <<<
>>> Test on task  3 : loss=1.199 | TAw acc= 92.7%, forg=  2.4%| TAg acc= 76.4%, forg=  3.3% <<<
>>> Test on task  4 : loss=0.979 | TAw acc= 91.7%, forg=  4.1%| TAg acc= 78.5%, forg=  2.5% <<<
>>> Test on task  5 : loss=1.059 | TAw acc= 97.3%, forg= -4.4%| TAg acc= 79.6%, forg=  2.7% <<<
>>> Test on task  6 : loss=1.340 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 66.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.639, TAw acc= 55.7% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.102, TAw acc= 62.0% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.305, TAw acc= 73.4% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.806, TAw acc= 75.9% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.517, TAw acc= 87.3% | *
| Epoch   6, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.345, TAw acc= 91.1% | *
| Epoch   7, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.243, TAw acc= 93.7% | *
| Epoch   8, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.135, TAw acc= 93.7% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.134, TAw acc= 93.7% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.133, TAw acc= 93.7% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.132, TAw acc= 93.7% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.131, TAw acc= 93.7% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 93.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1508 train exemplars, time=  0.0s
1508
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.974 | TAw acc= 95.9%, forg=  0.5%| TAg acc= 80.0%, forg=  6.2% <<<
>>> Test on task  1 : loss=0.812 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 90.7%, forg=  0.0% <<<
>>> Test on task  2 : loss=0.960 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 76.7%, forg=  8.7% <<<
>>> Test on task  3 : loss=1.214 | TAw acc= 92.7%, forg=  2.4%| TAg acc= 78.0%, forg=  1.6% <<<
>>> Test on task  4 : loss=0.892 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 81.8%, forg= -0.8% <<<
>>> Test on task  5 : loss=0.944 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 77.0%, forg=  5.3% <<<
>>> Test on task  6 : loss=1.428 | TAw acc= 92.1%, forg= -2.0%| TAg acc= 64.4%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.097 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 74.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.528, TAw acc= 40.5% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.030, TAw acc= 52.4% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.458, TAw acc= 53.6% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.014, TAw acc= 70.2% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.796, TAw acc= 78.6% | *
| Epoch   6, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.623, TAw acc= 71.4% | *
| Epoch   7, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.495, TAw acc= 83.3% | *
| Epoch   8, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.377, TAw acc= 83.3% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.375, TAw acc= 83.3% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.374, TAw acc= 84.5% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.373, TAw acc= 84.5% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.372, TAw acc= 84.5% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.371, TAw acc= 84.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1651 train exemplars, time=  0.0s
1651
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.954 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 81.5%, forg=  4.6% <<<
>>> Test on task  1 : loss=0.817 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 85.0%, forg=  5.6% <<<
>>> Test on task  2 : loss=0.901 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 87.4%, forg= -1.9% <<<
>>> Test on task  3 : loss=1.164 | TAw acc= 92.7%, forg=  2.4%| TAg acc= 85.4%, forg= -5.7% <<<
>>> Test on task  4 : loss=0.867 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 86.0%, forg= -4.1% <<<
>>> Test on task  5 : loss=0.802 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 86.7%, forg= -4.4% <<<
>>> Test on task  6 : loss=1.312 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 68.3%, forg= -2.0% <<<
>>> Test on task  7 : loss=1.151 | TAw acc= 94.4%, forg= -1.9%| TAg acc= 72.9%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.194 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 66.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=5.152, TAw acc= 38.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.739, TAw acc= 44.7% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.898, TAw acc= 60.5% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.427, TAw acc= 73.7% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.025, TAw acc= 77.6% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.774, TAw acc= 77.6% | *
| Epoch   7, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.626, TAw acc= 81.6% | *
| Epoch   8, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.509, TAw acc= 84.2% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.508, TAw acc= 84.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.508, TAw acc= 84.2% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.508, TAw acc= 84.2% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.507, TAw acc= 84.2% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.507, TAw acc= 84.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1791 train exemplars, time=  0.0s
1791
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.929 | TAw acc= 95.9%, forg=  0.5%| TAg acc= 83.6%, forg=  2.6% <<<
>>> Test on task  1 : loss=0.822 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 84.1%, forg=  6.5% <<<
>>> Test on task  2 : loss=0.861 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 87.4%, forg=  0.0% <<<
>>> Test on task  3 : loss=1.248 | TAw acc= 92.7%, forg=  2.4%| TAg acc= 69.1%, forg= 16.3% <<<
>>> Test on task  4 : loss=0.978 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 77.7%, forg=  8.3% <<<
>>> Test on task  5 : loss=0.819 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 81.4%, forg=  5.3% <<<
>>> Test on task  6 : loss=1.200 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 71.3%, forg= -3.0% <<<
>>> Test on task  7 : loss=1.094 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.9%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.299 | TAw acc= 92.9%, forg= -2.7%| TAg acc= 68.1%, forg= -1.8% <<<
>>> Test on task  9 : loss=1.219 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=5.025, TAw acc= 38.2% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=3.029, TAw acc= 47.4% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.050, TAw acc= 82.9% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.499, TAw acc= 89.5% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 90.8% | *
| Epoch   6, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.037, TAw acc= 93.4% | *
| Epoch   7, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.904, TAw acc= 96.1% | *
| Epoch   8, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.833, TAw acc= 96.1% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.831, TAw acc= 96.1% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.830, TAw acc= 96.1% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.828, TAw acc= 96.1% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.827, TAw acc= 96.1% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.826, TAw acc= 96.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1931 train exemplars, time=  0.0s
1931
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.943 | TAw acc= 95.9%, forg=  0.5%| TAg acc= 82.1%, forg=  4.1% <<<
>>> Test on task  1 : loss=0.802 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 89.7%, forg=  0.9% <<<
>>> Test on task  2 : loss=0.881 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 86.4%, forg=  1.0% <<<
>>> Test on task  3 : loss=1.196 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 78.0%, forg=  7.3% <<<
>>> Test on task  4 : loss=0.845 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 84.3%, forg=  1.7% <<<
>>> Test on task  5 : loss=0.733 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 89.4%, forg= -2.7% <<<
>>> Test on task  6 : loss=1.191 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 71.3%, forg=  0.0% <<<
>>> Test on task  7 : loss=0.954 | TAw acc= 95.3%, forg= -0.9%| TAg acc= 84.1%, forg= -9.3% <<<
>>> Test on task  8 : loss=1.138 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 81.4%, forg=-13.3% <<<
>>> Test on task  9 : loss=1.511 | TAw acc= 98.1%, forg= -1.9%| TAg acc= 53.8%, forg= 21.2% <<<
>>> Test on task 10 : loss=0.943 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 85.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.185, TAw acc= 53.9% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.627, TAw acc= 69.7% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.691, TAw acc= 87.6% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 92.1% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.127, TAw acc= 96.6% | *
| Epoch   6, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.002, TAw acc= 96.6% | *
| Epoch   7, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.957, TAw acc= 95.5% | *
| Epoch   8, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=0.885, TAw acc= 96.6% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.884, TAw acc= 96.6% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.883, TAw acc= 96.6% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.883, TAw acc= 96.6% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.882, TAw acc= 96.6% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=0.881, TAw acc= 96.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2071 train exemplars, time=  0.0s
2071
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.989 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 79.0%, forg=  7.2% <<<
>>> Test on task  1 : loss=0.802 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 88.8%, forg=  1.9% <<<
>>> Test on task  2 : loss=0.874 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 85.4%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.278 | TAw acc= 92.7%, forg=  2.4%| TAg acc= 77.2%, forg=  8.1% <<<
>>> Test on task  4 : loss=0.839 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 85.1%, forg=  0.8% <<<
>>> Test on task  5 : loss=0.817 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 85.8%, forg=  3.5% <<<
>>> Test on task  6 : loss=1.216 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 73.3%, forg= -2.0% <<<
>>> Test on task  7 : loss=0.901 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 82.2%, forg=  1.9% <<<
>>> Test on task  8 : loss=1.063 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 81.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.247 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 71.2%, forg=  3.8% <<<
>>> Test on task 10 : loss=1.122 | TAw acc= 98.1%, forg= -2.9%| TAg acc= 70.2%, forg= 15.4% <<<
>>> Test on task 11 : loss=0.960 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 80.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.480, TAw acc= 24.4% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=3.838, TAw acc= 47.4% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.704, TAw acc= 67.9% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.031, TAw acc= 82.1% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.623, TAw acc= 85.9% | *
| Epoch   6, time=  0.9s | Train: skip eval | Valid: time=  0.1s loss=1.432, TAw acc= 93.6% | *
| Epoch   7, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.288, TAw acc= 94.9% | *
| Epoch   8, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.210, TAw acc= 93.6% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.207, TAw acc= 93.6% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 93.6% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.201, TAw acc= 93.6% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.198, TAw acc= 91.0% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.196, TAw acc= 91.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2211 train exemplars, time=  0.0s
2211
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.038 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 75.9%, forg= 10.3% <<<
>>> Test on task  1 : loss=0.810 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 86.9%, forg=  3.7% <<<
>>> Test on task  2 : loss=0.823 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 87.4%, forg=  0.0% <<<
>>> Test on task  3 : loss=1.267 | TAw acc= 92.7%, forg=  2.4%| TAg acc= 77.2%, forg=  8.1% <<<
>>> Test on task  4 : loss=0.922 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 77.7%, forg=  8.3% <<<
>>> Test on task  5 : loss=0.724 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 87.6%, forg=  1.8% <<<
>>> Test on task  6 : loss=1.206 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 72.3%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.837 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 86.0%, forg= -1.9% <<<
>>> Test on task  8 : loss=1.015 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 79.6%, forg=  1.8% <<<
>>> Test on task  9 : loss=1.102 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.9%, forg= -2.9% <<<
>>> Test on task 10 : loss=0.978 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 80.8%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.280 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 71.4%, forg=  9.2% <<<
>>> Test on task 12 : loss=1.017 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 82.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=4.566, TAw acc= 41.5% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.366, TAw acc= 72.6% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.608, TAw acc= 82.1% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.441, TAw acc= 88.7% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc= 88.7% | *
| Epoch   6, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.181, TAw acc= 90.6% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.136, TAw acc= 89.6% | *
| Epoch   8, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.099, TAw acc= 91.5% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.097, TAw acc= 91.5% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.096, TAw acc= 91.5% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.094, TAw acc= 91.5% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.093, TAw acc= 91.5% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.091, TAw acc= 91.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2351 train exemplars, time=  0.0s
2351
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.041 | TAw acc= 96.9%, forg= -0.5%| TAg acc= 77.9%, forg=  8.2% <<<
>>> Test on task  1 : loss=0.788 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 90.7%, forg=  0.0% <<<
>>> Test on task  2 : loss=0.820 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 88.3%, forg= -1.0% <<<
>>> Test on task  3 : loss=1.317 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 76.4%, forg=  8.9% <<<
>>> Test on task  4 : loss=0.925 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 80.2%, forg=  5.8% <<<
>>> Test on task  5 : loss=0.845 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 80.5%, forg=  8.8% <<<
>>> Test on task  6 : loss=1.221 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 67.3%, forg=  5.9% <<<
>>> Test on task  7 : loss=0.954 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 76.6%, forg=  9.3% <<<
>>> Test on task  8 : loss=0.991 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 81.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.080 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.2%, forg=  7.7% <<<
>>> Test on task 10 : loss=0.906 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.7%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.276 | TAw acc= 96.6%, forg= -0.8%| TAg acc= 64.7%, forg= 16.0% <<<
>>> Test on task 12 : loss=1.107 | TAw acc= 99.1%, forg= -3.8%| TAg acc= 82.1%, forg=  0.0% <<<
>>> Test on task 13 : loss=0.912 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 82.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=6.156, TAw acc= 31.9% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=4.012, TAw acc= 54.2% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.689, TAw acc= 68.1% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.051, TAw acc= 75.0% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.727, TAw acc= 87.5% | *
| Epoch   6, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.518, TAw acc= 88.9% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.345, TAw acc= 88.9% | *
| Epoch   8, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.262, TAw acc= 88.9% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.261, TAw acc= 88.9% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.261, TAw acc= 88.9% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.261, TAw acc= 88.9% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.260, TAw acc= 88.9% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.260, TAw acc= 88.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2474 train exemplars, time=  0.0s
2474
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.017 | TAw acc= 96.4%, forg=  0.5%| TAg acc= 77.4%, forg=  8.7% <<<
>>> Test on task  1 : loss=0.842 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 81.3%, forg=  9.3% <<<
>>> Test on task  2 : loss=0.810 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 90.3%, forg= -1.9% <<<
>>> Test on task  3 : loss=1.295 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 79.7%, forg=  5.7% <<<
>>> Test on task  4 : loss=0.846 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 83.5%, forg=  2.5% <<<
>>> Test on task  5 : loss=0.842 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 77.9%, forg= 11.5% <<<
>>> Test on task  6 : loss=1.135 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 75.2%, forg= -2.0% <<<
>>> Test on task  7 : loss=0.866 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 82.2%, forg=  3.7% <<<
>>> Test on task  8 : loss=0.964 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 84.1%, forg= -2.7% <<<
>>> Test on task  9 : loss=1.143 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 66.3%, forg= 11.5% <<<
>>> Test on task 10 : loss=0.865 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 84.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.112 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 74.8%, forg=  5.9% <<<
>>> Test on task 12 : loss=0.988 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 86.8%, forg= -4.7% <<<
>>> Test on task 13 : loss=1.313 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 63.0%, forg= 19.6% <<<
>>> Test on task 14 : loss=1.067 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 77.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=5.726, TAw acc= 51.2% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.1s loss=3.254, TAw acc= 57.5% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.190, TAw acc= 78.8% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.700, TAw acc= 93.8% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.564, TAw acc= 97.5% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.428, TAw acc= 98.8% | *
| Epoch   7, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.357, TAw acc= 96.2% | *
| Epoch   8, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 97.5% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 97.5% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 97.5% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc= 97.5% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc= 97.5% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc= 97.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2594 train exemplars, time=  0.0s
2594
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.071 | TAw acc= 95.4%, forg=  1.5%| TAg acc= 75.9%, forg= 10.3% <<<
>>> Test on task  1 : loss=0.829 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 86.9%, forg=  3.7% <<<
>>> Test on task  2 : loss=0.784 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 91.3%, forg= -1.0% <<<
>>> Test on task  3 : loss=1.293 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 78.0%, forg=  7.3% <<<
>>> Test on task  4 : loss=0.848 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 86.0%, forg=  0.0% <<<
>>> Test on task  5 : loss=0.813 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.185 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 74.3%, forg=  1.0% <<<
>>> Test on task  7 : loss=0.812 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 86.0%, forg=  0.0% <<<
>>> Test on task  8 : loss=0.955 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 83.2%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.122 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 67.3%, forg= 10.6% <<<
>>> Test on task 10 : loss=0.908 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.8%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.158 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 69.7%, forg= 10.9% <<<
>>> Test on task 12 : loss=0.875 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 88.7%, forg= -1.9% <<<
>>> Test on task 13 : loss=1.185 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 71.0%, forg= 11.6% <<<
>>> Test on task 14 : loss=1.331 | TAw acc= 98.0%, forg= -3.0%| TAg acc= 56.0%, forg= 21.0% <<<
>>> Test on task 15 : loss=1.034 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=6.416, TAw acc= 29.2% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=4.011, TAw acc= 53.8% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.851, TAw acc= 75.4% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.188, TAw acc= 78.5% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.836, TAw acc= 87.7% | *
| Epoch   6, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.536, TAw acc= 92.3% | *
| Epoch   7, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.376, TAw acc= 90.8% | *
| Epoch   8, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.284, TAw acc= 90.8% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.283, TAw acc= 90.8% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.283, TAw acc= 90.8% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 90.8% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 90.8% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 90.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2714 train exemplars, time=  0.0s
2714
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.030 | TAw acc= 96.4%, forg=  0.5%| TAg acc= 79.0%, forg=  7.2% <<<
>>> Test on task  1 : loss=0.790 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 89.7%, forg=  0.9% <<<
>>> Test on task  2 : loss=0.864 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 88.3%, forg=  2.9% <<<
>>> Test on task  3 : loss=1.291 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 78.9%, forg=  6.5% <<<
>>> Test on task  4 : loss=0.755 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 85.1%, forg=  0.8% <<<
>>> Test on task  5 : loss=0.800 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 81.4%, forg=  8.0% <<<
>>> Test on task  6 : loss=1.174 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 73.3%, forg=  2.0% <<<
>>> Test on task  7 : loss=0.808 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 81.3%, forg=  4.7% <<<
>>> Test on task  8 : loss=0.943 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 83.2%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.067 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=0.815 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.6%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.085 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.801 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 91.5%, forg= -2.8% <<<
>>> Test on task 13 : loss=1.128 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 71.0%, forg= 11.6% <<<
>>> Test on task 14 : loss=1.193 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 58.0%, forg= 19.0% <<<
>>> Test on task 15 : loss=1.374 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 53.2%, forg= 24.8% <<<
>>> Test on task 16 : loss=1.039 | TAw acc= 85.6%, forg=  0.0%| TAg acc= 74.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=6.364, TAw acc= 31.3% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=3.896, TAw acc= 59.7% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.522, TAw acc= 83.6% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.756, TAw acc= 91.0% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.415, TAw acc= 89.6% | *
| Epoch   6, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.262, TAw acc= 89.6% | *
| Epoch   7, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.163, TAw acc= 89.6% | *
| Epoch   8, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.109, TAw acc= 89.6% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.108, TAw acc= 89.6% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.108, TAw acc= 89.6% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.107, TAw acc= 89.6% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.106, TAw acc= 89.6% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.105, TAw acc= 89.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2834 train exemplars, time=  0.0s
2834
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.992 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 78.5%, forg=  7.7% <<<
>>> Test on task  1 : loss=0.803 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 85.0%, forg=  5.6% <<<
>>> Test on task  2 : loss=0.845 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 88.3%, forg=  2.9% <<<
>>> Test on task  3 : loss=1.335 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 79.7%, forg=  5.7% <<<
>>> Test on task  4 : loss=0.827 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 77.7%, forg=  8.3% <<<
>>> Test on task  5 : loss=0.737 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 85.8%, forg=  3.5% <<<
>>> Test on task  6 : loss=1.165 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 81.2%, forg= -5.9% <<<
>>> Test on task  7 : loss=0.788 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 83.2%, forg=  2.8% <<<
>>> Test on task  8 : loss=0.934 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 83.2%, forg=  0.9% <<<
>>> Test on task  9 : loss=0.976 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.1%, forg=  5.8% <<<
>>> Test on task 10 : loss=0.825 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 80.8%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.060 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 72.3%, forg=  8.4% <<<
>>> Test on task 12 : loss=0.763 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 87.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.064 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 71.7%, forg= 10.9% <<<
>>> Test on task 14 : loss=1.179 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.0%, forg= 13.0% <<<
>>> Test on task 15 : loss=1.272 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 59.6%, forg= 18.3% <<<
>>> Test on task 16 : loss=1.264 | TAw acc= 91.1%, forg= -5.6%| TAg acc= 65.6%, forg=  8.9% <<<
>>> Test on task 17 : loss=1.100 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 77.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=5.026, TAw acc= 44.2% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=2.883, TAw acc= 75.6% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.919, TAw acc= 90.7% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 93.0% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.232, TAw acc= 94.2% | *
| Epoch   6, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.169, TAw acc= 98.8% | *
| Epoch   7, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.132, TAw acc= 98.8% | *
| Epoch   8, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.053, TAw acc= 98.8% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.052, TAw acc= 98.8% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.052, TAw acc= 98.8% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.051, TAw acc= 98.8% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.051, TAw acc= 98.8% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.050, TAw acc= 98.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2954 train exemplars, time=  0.0s
2954
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.023 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 75.9%, forg= 10.3% <<<
>>> Test on task  1 : loss=0.814 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 86.9%, forg=  3.7% <<<
>>> Test on task  2 : loss=0.883 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 89.3%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.332 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 81.3%, forg=  4.1% <<<
>>> Test on task  4 : loss=0.768 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 81.8%, forg=  4.1% <<<
>>> Test on task  5 : loss=0.788 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 85.0%, forg=  4.4% <<<
>>> Test on task  6 : loss=1.265 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 70.3%, forg= 10.9% <<<
>>> Test on task  7 : loss=0.767 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 83.2%, forg=  2.8% <<<
>>> Test on task  8 : loss=0.965 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 84.1%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.008 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.2%, forg=  6.7% <<<
>>> Test on task 10 : loss=0.783 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 86.5%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.131 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.770 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 91.5%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.062 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 73.2%, forg=  9.4% <<<
>>> Test on task 14 : loss=1.066 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.0%, forg= 14.0% <<<
>>> Test on task 15 : loss=1.337 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 57.8%, forg= 20.2% <<<
>>> Test on task 16 : loss=1.129 | TAw acc= 88.9%, forg=  2.2%| TAg acc= 70.0%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.275 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 64.5%, forg= 12.9% <<<
>>> Test on task 18 : loss=0.819 | TAw acc=100.0%, forg=  0.0%| TAg acc= 79.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=5.225, TAw acc= 55.4% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.876, TAw acc= 66.3% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.004, TAw acc= 83.1% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.460, TAw acc= 95.2% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.184, TAw acc= 95.2% | *
| Epoch   6, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.072, TAw acc= 96.4% | *
| Epoch   7, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.067, TAw acc= 94.0% | *
| Epoch   8, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 94.0% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 94.0% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 94.0% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 94.0% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 94.0% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 94.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3074 train exemplars, time=  0.0s
3074
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.064 | TAw acc= 97.4%, forg= -0.5%| TAg acc= 73.8%, forg= 12.3% <<<
>>> Test on task  1 : loss=0.827 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 84.1%, forg=  6.5% <<<
>>> Test on task  2 : loss=0.926 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 85.4%, forg=  5.8% <<<
>>> Test on task  3 : loss=1.370 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 78.0%, forg=  7.3% <<<
>>> Test on task  4 : loss=0.706 | TAw acc= 96.7%, forg= -0.8%| TAg acc= 87.6%, forg= -1.7% <<<
>>> Test on task  5 : loss=0.755 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 85.0%, forg=  4.4% <<<
>>> Test on task  6 : loss=1.229 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 76.2%, forg=  5.0% <<<
>>> Test on task  7 : loss=0.754 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 86.0%, forg=  0.0% <<<
>>> Test on task  8 : loss=0.979 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 80.5%, forg=  3.5% <<<
>>> Test on task  9 : loss=0.958 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.1%, forg=  4.8% <<<
>>> Test on task 10 : loss=0.851 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.7%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.097 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 69.7%, forg= 10.9% <<<
>>> Test on task 12 : loss=0.758 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 84.0%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.030 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 71.0%, forg= 11.6% <<<
>>> Test on task 14 : loss=1.079 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 67.0%, forg= 10.0% <<<
>>> Test on task 15 : loss=1.277 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 59.6%, forg= 18.3% <<<
>>> Test on task 16 : loss=1.054 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 70.0%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.207 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 66.7%, forg= 10.8% <<<
>>> Test on task 18 : loss=1.266 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 61.5%, forg= 17.9% <<<
>>> Test on task 19 : loss=1.050 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 78.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=5.760, TAw acc= 53.0% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.998, TAw acc= 72.3% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=2.004, TAw acc= 81.9% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.465, TAw acc= 90.4% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.370, TAw acc= 89.2% | *
| Epoch   6, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.306, TAw acc= 94.0% | *
| Epoch   7, time=  2.6s | Train: skip eval | Valid: time=  0.4s loss=1.143, TAw acc= 89.2% | *
| Epoch   8, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.050, TAw acc= 96.4% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.051, TAw acc= 96.4% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.051, TAw acc= 96.4% |
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.051, TAw acc= 96.4% |
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.052, TAw acc= 96.4% |
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.053, TAw acc= 96.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3194 train exemplars, time=  0.1s
3194
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.063 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 74.9%, forg= 11.3% <<<
>>> Test on task  1 : loss=0.896 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 82.2%, forg=  8.4% <<<
>>> Test on task  2 : loss=0.893 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 88.3%, forg=  2.9% <<<
>>> Test on task  3 : loss=1.422 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 73.2%, forg= 12.2% <<<
>>> Test on task  4 : loss=0.780 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 81.8%, forg=  5.8% <<<
>>> Test on task  5 : loss=0.745 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 83.2%, forg=  6.2% <<<
>>> Test on task  6 : loss=1.217 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 78.2%, forg=  3.0% <<<
>>> Test on task  7 : loss=0.942 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 73.8%, forg= 12.1% <<<
>>> Test on task  8 : loss=0.975 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 84.1%, forg=  0.0% <<<
>>> Test on task  9 : loss=0.980 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  2.9% <<<
>>> Test on task 10 : loss=0.884 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.9%, forg=  8.7% <<<
>>> Test on task 11 : loss=1.099 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 69.7%, forg= 10.9% <<<
>>> Test on task 12 : loss=0.750 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 87.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=0.960 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 73.9%, forg=  8.7% <<<
>>> Test on task 14 : loss=1.079 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.0%, forg= 14.0% <<<
>>> Test on task 15 : loss=1.209 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 16 : loss=1.008 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 72.2%, forg=  2.2% <<<
>>> Test on task 17 : loss=1.199 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 63.4%, forg= 14.0% <<<
>>> Test on task 18 : loss=1.129 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 65.8%, forg= 13.7% <<<
>>> Test on task 19 : loss=1.524 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 58.0%, forg= 20.5% <<<
>>> Test on task 20 : loss=1.039 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 70.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=6.613, TAw acc= 37.0% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=3.363, TAw acc= 57.5% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=2.255, TAw acc= 86.3% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.663, TAw acc= 89.0% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.449, TAw acc= 90.4% | *
| Epoch   6, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.363, TAw acc= 90.4% | *
| Epoch   7, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.211, TAw acc= 94.5% | *
| Epoch   8, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.189, TAw acc= 95.9% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.189, TAw acc= 95.9% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.189, TAw acc= 95.9% |
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.190, TAw acc= 95.9% |
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.190, TAw acc= 95.9% |
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.191, TAw acc= 95.9% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3314 train exemplars, time=  0.0s
3314
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.096 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 73.3%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.950 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 75.7%, forg= 15.0% <<<
>>> Test on task  2 : loss=0.919 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 88.3%, forg=  2.9% <<<
>>> Test on task  3 : loss=1.396 | TAw acc= 93.5%, forg=  1.6%| TAg acc= 80.5%, forg=  4.9% <<<
>>> Test on task  4 : loss=0.810 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 78.5%, forg=  9.1% <<<
>>> Test on task  5 : loss=0.751 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.4%, forg=  8.0% <<<
>>> Test on task  6 : loss=1.276 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 73.3%, forg=  7.9% <<<
>>> Test on task  7 : loss=0.800 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 84.1%, forg=  1.9% <<<
>>> Test on task  8 : loss=0.970 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 82.3%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.881 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 79.8%, forg= -1.9% <<<
>>> Test on task 10 : loss=0.777 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.141 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 70.6%, forg= 10.1% <<<
>>> Test on task 12 : loss=0.661 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 94.3%, forg= -2.8% <<<
>>> Test on task 13 : loss=0.962 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 73.2%, forg=  9.4% <<<
>>> Test on task 14 : loss=1.006 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 65.0%, forg= 12.0% <<<
>>> Test on task 15 : loss=1.269 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 57.8%, forg= 20.2% <<<
>>> Test on task 16 : loss=1.018 | TAw acc= 92.2%, forg= -1.1%| TAg acc= 76.7%, forg= -2.2% <<<
>>> Test on task 17 : loss=1.137 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 73.1%, forg=  4.3% <<<
>>> Test on task 18 : loss=1.136 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 62.4%, forg= 17.1% <<<
>>> Test on task 19 : loss=1.494 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 55.4%, forg= 23.2% <<<
>>> Test on task 20 : loss=1.473 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 46.4%, forg= 24.1% <<<
>>> Test on task 21 : loss=0.968 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 76.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=5.042, TAw acc= 60.2% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=2.537, TAw acc= 78.4% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.628, TAw acc= 85.2% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc= 92.0% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.167, TAw acc= 94.3% | *
| Epoch   6, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.103, TAw acc= 92.0% | *
| Epoch   7, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.021, TAw acc= 92.0% | *
| Epoch   8, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=0.988, TAw acc= 94.3% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.988, TAw acc= 95.5% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.988, TAw acc= 95.5% |
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=0.988, TAw acc= 95.5% |
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=0.988, TAw acc= 95.5% |
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=0.989, TAw acc= 95.5% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3434 train exemplars, time=  0.0s
3434
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.113 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 75.4%, forg= 10.8% <<<
>>> Test on task  1 : loss=0.900 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 84.1%, forg=  6.5% <<<
>>> Test on task  2 : loss=0.922 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 88.3%, forg=  2.9% <<<
>>> Test on task  3 : loss=1.429 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=0.726 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 85.1%, forg=  2.5% <<<
>>> Test on task  5 : loss=0.779 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.3%, forg=  7.1% <<<
>>> Test on task  6 : loss=1.290 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 73.3%, forg=  7.9% <<<
>>> Test on task  7 : loss=0.791 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 85.0%, forg=  0.9% <<<
>>> Test on task  8 : loss=0.942 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 85.0%, forg= -0.9% <<<
>>> Test on task  9 : loss=0.961 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 10 : loss=0.832 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 79.8%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.115 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 71.4%, forg=  9.2% <<<
>>> Test on task 12 : loss=0.731 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 90.6%, forg=  3.8% <<<
>>> Test on task 13 : loss=0.993 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 73.2%, forg=  9.4% <<<
>>> Test on task 14 : loss=0.999 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.0%, forg=  6.0% <<<
>>> Test on task 15 : loss=1.241 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 61.5%, forg= 16.5% <<<
>>> Test on task 16 : loss=0.942 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 74.4%, forg=  2.2% <<<
>>> Test on task 17 : loss=1.198 | TAw acc= 88.2%, forg=  1.1%| TAg acc= 63.4%, forg= 14.0% <<<
>>> Test on task 18 : loss=1.070 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 69.2%, forg= 10.3% <<<
>>> Test on task 19 : loss=1.491 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 57.1%, forg= 21.4% <<<
>>> Test on task 20 : loss=1.442 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 48.2%, forg= 22.3% <<<
>>> Test on task 21 : loss=1.329 | TAw acc= 95.0%, forg= -1.0%| TAg acc= 57.0%, forg= 19.0% <<<
>>> Test on task 22 : loss=0.864 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=5.752, TAw acc= 50.6% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.316, TAw acc= 79.0% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.355, TAw acc= 93.8% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.129, TAw acc= 95.1% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.028, TAw acc= 93.8% | *
| Epoch   6, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.978, TAw acc= 96.3% | *
| Epoch   7, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.916, TAw acc= 96.3% | *
| Epoch   8, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.861, TAw acc= 95.1% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.861, TAw acc= 95.1% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.1s loss=0.862, TAw acc= 95.1% |
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.862, TAw acc= 95.1% |
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=0.863, TAw acc= 95.1% |
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.863, TAw acc= 95.1% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3554 train exemplars, time=  0.0s
3554
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.084 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 71.8%, forg= 14.4% <<<
>>> Test on task  1 : loss=0.936 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 83.2%, forg=  7.5% <<<
>>> Test on task  2 : loss=0.915 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 84.5%, forg=  6.8% <<<
>>> Test on task  3 : loss=1.456 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 76.4%, forg=  8.9% <<<
>>> Test on task  4 : loss=0.741 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 86.8%, forg=  0.8% <<<
>>> Test on task  5 : loss=0.875 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 75.2%, forg= 14.2% <<<
>>> Test on task  6 : loss=1.270 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 74.3%, forg=  6.9% <<<
>>> Test on task  7 : loss=0.818 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 85.0%, forg=  0.9% <<<
>>> Test on task  8 : loss=0.985 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=0.972 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  5.8% <<<
>>> Test on task 10 : loss=0.818 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 82.7%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.101 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 69.7%, forg= 10.9% <<<
>>> Test on task 12 : loss=0.648 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 94.3%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.012 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 71.7%, forg= 10.9% <<<
>>> Test on task 14 : loss=0.940 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 73.0%, forg=  4.0% <<<
>>> Test on task 15 : loss=1.288 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 60.6%, forg= 17.4% <<<
>>> Test on task 16 : loss=1.003 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 73.3%, forg=  3.3% <<<
>>> Test on task 17 : loss=1.159 | TAw acc= 88.2%, forg=  1.1%| TAg acc= 65.6%, forg= 11.8% <<<
>>> Test on task 18 : loss=1.027 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 66.7%, forg= 12.8% <<<
>>> Test on task 19 : loss=1.411 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 60.7%, forg= 17.9% <<<
>>> Test on task 20 : loss=1.405 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 50.9%, forg= 19.6% <<<
>>> Test on task 21 : loss=1.244 | TAw acc= 97.0%, forg= -2.0%| TAg acc= 64.0%, forg= 12.0% <<<
>>> Test on task 22 : loss=1.176 | TAw acc= 97.4%, forg= -1.7%| TAg acc= 69.2%, forg= 11.1% <<<
>>> Test on task 23 : loss=0.917 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 75.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=5.564, TAw acc= 46.6% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.725, TAw acc= 71.6% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.780, TAw acc= 88.6% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.288, TAw acc= 97.7% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.245, TAw acc= 97.7% | *
| Epoch   6, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 96.6% | *
| Epoch   7, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.978, TAw acc= 97.7% | *
| Epoch   8, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=0.966, TAw acc= 96.6% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.963, TAw acc= 96.6% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=0.961, TAw acc= 96.6% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.959, TAw acc= 96.6% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=0.957, TAw acc= 96.6% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=0.955, TAw acc= 96.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3674 train exemplars, time=  0.0s
3674
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.150 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 74.4%, forg= 11.8% <<<
>>> Test on task  1 : loss=0.909 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 84.1%, forg=  6.5% <<<
>>> Test on task  2 : loss=0.880 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 86.4%, forg=  4.9% <<<
>>> Test on task  3 : loss=1.466 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 78.0%, forg=  7.3% <<<
>>> Test on task  4 : loss=0.837 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 79.3%, forg=  8.3% <<<
>>> Test on task  5 : loss=0.771 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.3%, forg=  7.1% <<<
>>> Test on task  6 : loss=1.265 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 76.2%, forg=  5.0% <<<
>>> Test on task  7 : loss=0.917 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 72.9%, forg= 13.1% <<<
>>> Test on task  8 : loss=0.981 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.936 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  4.8% <<<
>>> Test on task 10 : loss=0.793 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 85.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.233 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 59.7%, forg= 21.0% <<<
>>> Test on task 12 : loss=0.787 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 81.1%, forg= 13.2% <<<
>>> Test on task 13 : loss=1.123 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 67.4%, forg= 15.2% <<<
>>> Test on task 14 : loss=0.934 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.232 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 66.1%, forg= 11.9% <<<
>>> Test on task 16 : loss=0.992 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 67.8%, forg=  8.9% <<<
>>> Test on task 17 : loss=1.141 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 67.7%, forg=  9.7% <<<
>>> Test on task 18 : loss=0.968 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.9%, forg=  8.5% <<<
>>> Test on task 19 : loss=1.362 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 59.8%, forg= 18.8% <<<
>>> Test on task 20 : loss=1.295 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 59.8%, forg= 10.7% <<<
>>> Test on task 21 : loss=1.197 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 63.0%, forg= 13.0% <<<
>>> Test on task 22 : loss=1.131 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 70.9%, forg=  9.4% <<<
>>> Test on task 23 : loss=1.294 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 56.4%, forg= 19.1% <<<
>>> Test on task 24 : loss=1.029 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 82.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=6.145, TAw acc= 34.2% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=3.552, TAw acc= 63.3% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.075, TAw acc= 93.7% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.605, TAw acc= 94.9% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.392, TAw acc= 96.2% | *
| Epoch   6, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.301, TAw acc= 96.2% | *
| Epoch   7, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.226, TAw acc= 94.9% | *
| Epoch   8, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.197, TAw acc= 96.2% | *
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.194, TAw acc= 96.2% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.191, TAw acc= 96.2% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.189, TAw acc= 96.2% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.186, TAw acc= 96.2% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.184, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3794 train exemplars, time=  0.0s
3794
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.120 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 72.3%, forg= 13.8% <<<
>>> Test on task  1 : loss=0.853 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 86.0%, forg=  4.7% <<<
>>> Test on task  2 : loss=0.948 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 85.4%, forg=  5.8% <<<
>>> Test on task  3 : loss=1.469 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 77.2%, forg=  8.1% <<<
>>> Test on task  4 : loss=0.761 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 84.3%, forg=  3.3% <<<
>>> Test on task  5 : loss=0.776 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.3%, forg=  7.1% <<<
>>> Test on task  6 : loss=1.300 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 72.3%, forg=  8.9% <<<
>>> Test on task  7 : loss=0.882 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 81.3%, forg=  4.7% <<<
>>> Test on task  8 : loss=1.089 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 77.0%, forg=  8.0% <<<
>>> Test on task  9 : loss=0.984 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=0.855 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 83.7%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.132 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 69.7%, forg= 10.9% <<<
>>> Test on task 12 : loss=0.675 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 91.5%, forg=  2.8% <<<
>>> Test on task 13 : loss=0.996 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 71.0%, forg= 11.6% <<<
>>> Test on task 14 : loss=0.917 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.0%, forg=  7.0% <<<
>>> Test on task 15 : loss=1.342 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 56.9%, forg= 21.1% <<<
>>> Test on task 16 : loss=0.913 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 74.4%, forg=  2.2% <<<
>>> Test on task 17 : loss=1.120 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 67.7%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.000 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 69.2%, forg= 10.3% <<<
>>> Test on task 19 : loss=1.368 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 62.5%, forg= 16.1% <<<
>>> Test on task 20 : loss=1.344 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 56.2%, forg= 14.3% <<<
>>> Test on task 21 : loss=1.168 | TAw acc= 95.0%, forg=  2.0%| TAg acc= 65.0%, forg= 11.0% <<<
>>> Test on task 22 : loss=1.103 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 70.1%, forg= 10.3% <<<
>>> Test on task 23 : loss=1.217 | TAw acc= 95.5%, forg= -0.9%| TAg acc= 62.7%, forg= 12.7% <<<
>>> Test on task 24 : loss=1.489 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 54.7%, forg= 27.4% <<<
>>> Test on task 25 : loss=0.986 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=5.574, TAw acc= 58.3% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=2.734, TAw acc= 70.8% | *
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.789, TAw acc= 86.5% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.389, TAw acc= 95.8% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 97.9% | *
| Epoch   6, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.059, TAw acc= 99.0% | *
| Epoch   7, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.994, TAw acc= 99.0% | *
| Epoch   8, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=0.954, TAw acc= 99.0% | *
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.952, TAw acc= 99.0% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.950, TAw acc= 99.0% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.949, TAw acc= 99.0% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.947, TAw acc= 99.0% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.945, TAw acc= 99.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3914 train exemplars, time=  0.0s
3914
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.112 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 74.4%, forg= 11.8% <<<
>>> Test on task  1 : loss=0.923 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 83.2%, forg=  7.5% <<<
>>> Test on task  2 : loss=0.976 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 84.5%, forg=  6.8% <<<
>>> Test on task  3 : loss=1.597 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.2%, forg= 12.2% <<<
>>> Test on task  4 : loss=0.894 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 75.2%, forg= 12.4% <<<
>>> Test on task  5 : loss=0.771 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.4%, forg=  8.0% <<<
>>> Test on task  6 : loss=1.363 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 70.3%, forg= 10.9% <<<
>>> Test on task  7 : loss=0.890 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 79.4%, forg=  6.5% <<<
>>> Test on task  8 : loss=0.992 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.968 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 10 : loss=0.857 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 84.6%, forg=  1.9% <<<
>>> Test on task 11 : loss=1.150 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 70.6%, forg= 10.1% <<<
>>> Test on task 12 : loss=0.684 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 88.7%, forg=  5.7% <<<
>>> Test on task 13 : loss=1.117 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 69.6%, forg= 13.0% <<<
>>> Test on task 14 : loss=0.919 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.0%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.307 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 63.3%, forg= 14.7% <<<
>>> Test on task 16 : loss=0.975 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.129 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 69.9%, forg=  7.5% <<<
>>> Test on task 18 : loss=0.968 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 73.5%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.333 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 67.0%, forg= 11.6% <<<
>>> Test on task 20 : loss=1.332 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 60.7%, forg=  9.8% <<<
>>> Test on task 21 : loss=1.145 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 62.0%, forg= 14.0% <<<
>>> Test on task 22 : loss=1.071 | TAw acc= 94.9%, forg=  2.6%| TAg acc= 70.9%, forg=  9.4% <<<
>>> Test on task 23 : loss=1.188 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 69.1%, forg=  6.4% <<<
>>> Test on task 24 : loss=1.442 | TAw acc= 96.6%, forg= -0.9%| TAg acc= 56.4%, forg= 25.6% <<<
>>> Test on task 25 : loss=1.284 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 61.7%, forg= 15.0% <<<
>>> Test on task 26 : loss=1.058 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=4.887, TAw acc= 52.0% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.176, TAw acc= 78.6% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.482, TAw acc= 83.7% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.123, TAw acc= 95.9% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.110, TAw acc= 98.0% | *
| Epoch   6, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=0.836, TAw acc= 99.0% | *
| Epoch   7, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=0.822, TAw acc= 99.0% | *
| Epoch   8, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=0.737, TAw acc= 99.0% | *
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=0.736, TAw acc= 99.0% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.736, TAw acc= 99.0% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=0.736, TAw acc= 99.0% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=0.736, TAw acc= 99.0% |
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=0.735, TAw acc= 99.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4034 train exemplars, time=  0.0s
4034
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.127 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 74.4%, forg= 11.8% <<<
>>> Test on task  1 : loss=0.941 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 81.3%, forg=  9.3% <<<
>>> Test on task  2 : loss=0.989 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 83.5%, forg=  7.8% <<<
>>> Test on task  3 : loss=1.587 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 75.6%, forg=  9.8% <<<
>>> Test on task  4 : loss=0.895 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 79.3%, forg=  8.3% <<<
>>> Test on task  5 : loss=0.777 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 83.2%, forg=  6.2% <<<
>>> Test on task  6 : loss=1.380 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 71.3%, forg=  9.9% <<<
>>> Test on task  7 : loss=0.847 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 78.5%, forg=  7.5% <<<
>>> Test on task  8 : loss=0.997 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=0.963 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 70.2%, forg=  9.6% <<<
>>> Test on task 10 : loss=0.813 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 83.7%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.187 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.9%, forg= 11.8% <<<
>>> Test on task 12 : loss=0.668 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 90.6%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.086 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 67.4%, forg= 15.2% <<<
>>> Test on task 14 : loss=0.919 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.0%, forg=  6.0% <<<
>>> Test on task 15 : loss=1.246 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 16 : loss=0.943 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 72.2%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.076 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 71.0%, forg=  6.5% <<<
>>> Test on task 18 : loss=1.045 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 65.8%, forg= 13.7% <<<
>>> Test on task 19 : loss=1.345 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 65.2%, forg= 13.4% <<<
>>> Test on task 20 : loss=1.431 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 51.8%, forg= 18.8% <<<
>>> Test on task 21 : loss=1.099 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 69.0%, forg=  7.0% <<<
>>> Test on task 22 : loss=1.062 | TAw acc= 94.9%, forg=  2.6%| TAg acc= 70.9%, forg=  9.4% <<<
>>> Test on task 23 : loss=1.222 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 69.1%, forg=  6.4% <<<
>>> Test on task 24 : loss=1.342 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 59.0%, forg= 23.1% <<<
>>> Test on task 25 : loss=1.261 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 57.0%, forg= 19.6% <<<
>>> Test on task 26 : loss=1.458 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 52.3%, forg= 16.4% <<<
>>> Test on task 27 : loss=0.695 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 84.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=6.659, TAw acc= 40.3% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=3.200, TAw acc= 83.6% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=2.028, TAw acc= 94.0% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.957, TAw acc= 95.5% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.442, TAw acc= 95.5% | *
| Epoch   6, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.321, TAw acc= 97.0% | *
| Epoch   7, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.309, TAw acc= 97.0% | *
| Epoch   8, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.250, TAw acc= 97.0% | *
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.243, TAw acc= 97.0% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.1s loss=1.238, TAw acc= 97.0% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.230, TAw acc= 97.0% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.225, TAw acc= 97.0% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.220, TAw acc= 97.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4154 train exemplars, time=  0.0s
4154
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.143 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 75.4%, forg= 10.8% <<<
>>> Test on task  1 : loss=0.940 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 77.6%, forg= 13.1% <<<
>>> Test on task  2 : loss=0.997 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 83.5%, forg=  7.8% <<<
>>> Test on task  3 : loss=1.597 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=0.959 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 72.7%, forg= 14.9% <<<
>>> Test on task  5 : loss=0.787 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.4%, forg=  8.0% <<<
>>> Test on task  6 : loss=1.372 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 74.3%, forg=  6.9% <<<
>>> Test on task  7 : loss=0.903 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 80.4%, forg=  5.6% <<<
>>> Test on task  8 : loss=1.024 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 79.6%, forg=  5.3% <<<
>>> Test on task  9 : loss=0.852 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 76.9%, forg=  2.9% <<<
>>> Test on task 10 : loss=0.797 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 85.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.178 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.1%, forg= 12.6% <<<
>>> Test on task 12 : loss=0.711 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 86.8%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.066 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 71.0%, forg= 11.6% <<<
>>> Test on task 14 : loss=0.907 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.0%, forg=  7.0% <<<
>>> Test on task 15 : loss=1.291 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 63.3%, forg= 14.7% <<<
>>> Test on task 16 : loss=0.963 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 70.0%, forg=  6.7% <<<
>>> Test on task 17 : loss=1.093 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 73.1%, forg=  4.3% <<<
>>> Test on task 18 : loss=0.923 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 75.2%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.355 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 58.0%, forg= 20.5% <<<
>>> Test on task 20 : loss=1.361 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 59.8%, forg= 10.7% <<<
>>> Test on task 21 : loss=1.185 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 64.0%, forg= 12.0% <<<
>>> Test on task 22 : loss=1.046 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 70.1%, forg= 10.3% <<<
>>> Test on task 23 : loss=1.191 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 72.7%, forg=  2.7% <<<
>>> Test on task 24 : loss=1.360 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 58.1%, forg= 23.9% <<<
>>> Test on task 25 : loss=1.211 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.6%, forg= 13.1% <<<
>>> Test on task 26 : loss=1.445 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 56.2%, forg= 12.5% <<<
>>> Test on task 27 : loss=1.009 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 75.2%, forg=  9.3% <<<
>>> Test on task 28 : loss=1.302 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 65.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=6.295, TAw acc= 41.6% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=3.279, TAw acc= 62.3% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=2.012, TAw acc= 87.0% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.529, TAw acc= 94.8% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.398, TAw acc= 92.2% | *
| Epoch   6, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.343, TAw acc= 96.1% | *
| Epoch   7, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.283, TAw acc= 92.2% | *
| Epoch   8, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.344, TAw acc= 94.8% |
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 92.2% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.280, TAw acc= 93.5% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.279, TAw acc= 93.5% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 93.5% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 93.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4274 train exemplars, time=  0.0s
4274
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.144 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 76.4%, forg=  9.7% <<<
>>> Test on task  1 : loss=0.933 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 81.3%, forg=  9.3% <<<
>>> Test on task  2 : loss=1.016 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 82.5%, forg=  8.7% <<<
>>> Test on task  3 : loss=1.584 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.2%, forg= 12.2% <<<
>>> Test on task  4 : loss=0.867 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 80.2%, forg=  7.4% <<<
>>> Test on task  5 : loss=0.849 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 77.0%, forg= 12.4% <<<
>>> Test on task  6 : loss=1.322 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 76.2%, forg=  5.0% <<<
>>> Test on task  7 : loss=1.143 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 65.4%, forg= 20.6% <<<
>>> Test on task  8 : loss=1.027 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.985 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 70.2%, forg=  9.6% <<<
>>> Test on task 10 : loss=0.843 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 85.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.152 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.9%, forg= 11.8% <<<
>>> Test on task 12 : loss=0.692 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 88.7%, forg=  5.7% <<<
>>> Test on task 13 : loss=1.128 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 69.6%, forg= 13.0% <<<
>>> Test on task 14 : loss=0.953 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.0%, forg=  9.0% <<<
>>> Test on task 15 : loss=1.349 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 60.6%, forg= 17.4% <<<
>>> Test on task 16 : loss=0.990 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 70.0%, forg=  6.7% <<<
>>> Test on task 17 : loss=1.195 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 67.7%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.056 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 69.2%, forg= 10.3% <<<
>>> Test on task 19 : loss=1.386 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 59.8%, forg= 18.8% <<<
>>> Test on task 20 : loss=1.416 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 60.7%, forg=  9.8% <<<
>>> Test on task 21 : loss=1.081 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 72.0%, forg=  4.0% <<<
>>> Test on task 22 : loss=1.083 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.2%, forg= 11.1% <<<
>>> Test on task 23 : loss=1.218 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 70.9%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.428 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 61.5%, forg= 20.5% <<<
>>> Test on task 25 : loss=1.160 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.2%, forg=  8.4% <<<
>>> Test on task 26 : loss=1.406 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 57.8%, forg= 10.9% <<<
>>> Test on task 27 : loss=0.922 | TAw acc= 99.2%, forg= -0.8%| TAg acc= 80.6%, forg=  3.9% <<<
>>> Test on task 28 : loss=1.508 | TAw acc= 95.7%, forg= -1.1%| TAg acc= 54.8%, forg= 10.8% <<<
>>> Test on task 29 : loss=1.323 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 66.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=5.596, TAw acc= 44.9% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=2.726, TAw acc= 67.4% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.682, TAw acc= 84.3% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.269, TAw acc= 92.1% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.073, TAw acc= 93.3% | *
| Epoch   6, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.992, TAw acc= 92.1% | *
| Epoch   7, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.900, TAw acc= 93.3% | *
| Epoch   8, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.888, TAw acc= 93.3% | *
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.887, TAw acc= 93.3% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.885, TAw acc= 93.3% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.884, TAw acc= 93.3% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.882, TAw acc= 93.3% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.881, TAw acc= 93.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4394 train exemplars, time=  0.1s
4394
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.192 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 73.3%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.881 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 86.9%, forg=  3.7% <<<
>>> Test on task  2 : loss=1.062 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 82.5%, forg=  8.7% <<<
>>> Test on task  3 : loss=1.616 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.0%, forg= 11.4% <<<
>>> Test on task  4 : loss=0.890 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 77.7%, forg=  9.9% <<<
>>> Test on task  5 : loss=0.774 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.4%, forg=  8.0% <<<
>>> Test on task  6 : loss=1.350 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 73.3%, forg=  7.9% <<<
>>> Test on task  7 : loss=0.956 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 78.5%, forg=  7.5% <<<
>>> Test on task  8 : loss=1.060 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.013 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 72.1%, forg=  7.7% <<<
>>> Test on task 10 : loss=0.879 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 82.7%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.154 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.669 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 88.7%, forg=  5.7% <<<
>>> Test on task 13 : loss=1.163 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 67.4%, forg= 15.2% <<<
>>> Test on task 14 : loss=0.907 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.0%, forg=  7.0% <<<
>>> Test on task 15 : loss=1.241 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.0%, forg= 11.0% <<<
>>> Test on task 16 : loss=0.959 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 72.2%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.203 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 64.5%, forg= 12.9% <<<
>>> Test on task 18 : loss=0.987 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.8%, forg=  7.7% <<<
>>> Test on task 19 : loss=1.406 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 60.7%, forg= 17.9% <<<
>>> Test on task 20 : loss=1.389 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 60.7%, forg=  9.8% <<<
>>> Test on task 21 : loss=1.144 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 69.0%, forg=  7.0% <<<
>>> Test on task 22 : loss=1.113 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 67.5%, forg= 12.8% <<<
>>> Test on task 23 : loss=1.238 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 69.1%, forg=  6.4% <<<
>>> Test on task 24 : loss=1.466 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 57.3%, forg= 24.8% <<<
>>> Test on task 25 : loss=1.120 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 71.0%, forg=  5.6% <<<
>>> Test on task 26 : loss=1.434 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 59.4%, forg=  9.4% <<<
>>> Test on task 27 : loss=0.900 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 76.7%, forg=  7.8% <<<
>>> Test on task 28 : loss=1.403 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 59.1%, forg=  6.5% <<<
>>> Test on task 29 : loss=1.701 | TAw acc= 96.2%, forg= -1.0%| TAg acc= 36.2%, forg= 30.5% <<<
>>> Test on task 30 : loss=0.893 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 82.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=6.458, TAw acc= 56.9% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=2.862, TAw acc= 70.8% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.677, TAw acc= 84.7% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.418, TAw acc= 90.3% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 90.3% | *
| Epoch   6, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.178, TAw acc= 91.7% | *
| Epoch   7, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.024, TAw acc= 90.3% | *
| Epoch   8, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.008, TAw acc= 90.3% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.006, TAw acc= 90.3% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.004, TAw acc= 90.3% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.002, TAw acc= 90.3% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.001, TAw acc= 90.3% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.000, TAw acc= 90.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4514 train exemplars, time=  0.0s
4514
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.160 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 74.9%, forg= 11.3% <<<
>>> Test on task  1 : loss=0.946 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 83.2%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.034 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 84.5%, forg=  6.8% <<<
>>> Test on task  3 : loss=1.637 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=0.861 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 78.5%, forg=  9.1% <<<
>>> Test on task  5 : loss=0.821 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.421 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 73.3%, forg=  7.9% <<<
>>> Test on task  7 : loss=0.971 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 78.5%, forg=  7.5% <<<
>>> Test on task  8 : loss=1.043 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.011 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=0.850 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 86.5%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.185 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.1%, forg= 12.6% <<<
>>> Test on task 12 : loss=0.702 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 87.7%, forg=  6.6% <<<
>>> Test on task 13 : loss=1.172 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 69.6%, forg= 13.0% <<<
>>> Test on task 14 : loss=0.924 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 69.0%, forg=  8.0% <<<
>>> Test on task 15 : loss=1.308 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 16 : loss=1.026 | TAw acc= 88.9%, forg=  3.3%| TAg acc= 67.8%, forg=  8.9% <<<
>>> Test on task 17 : loss=1.250 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 65.6%, forg= 11.8% <<<
>>> Test on task 18 : loss=0.978 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 72.6%, forg=  6.8% <<<
>>> Test on task 19 : loss=1.310 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 67.0%, forg= 11.6% <<<
>>> Test on task 20 : loss=1.397 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 61.6%, forg=  8.9% <<<
>>> Test on task 21 : loss=1.200 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 63.0%, forg= 13.0% <<<
>>> Test on task 22 : loss=1.044 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 70.1%, forg= 10.3% <<<
>>> Test on task 23 : loss=1.308 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 70.0%, forg=  5.5% <<<
>>> Test on task 24 : loss=1.383 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 64.1%, forg= 17.9% <<<
>>> Test on task 25 : loss=1.126 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 69.2%, forg=  7.5% <<<
>>> Test on task 26 : loss=1.374 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 63.3%, forg=  5.5% <<<
>>> Test on task 27 : loss=0.863 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 78.3%, forg=  6.2% <<<
>>> Test on task 28 : loss=1.375 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 54.8%, forg= 10.8% <<<
>>> Test on task 29 : loss=1.695 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 39.0%, forg= 27.6% <<<
>>> Test on task 30 : loss=1.237 | TAw acc= 92.4%, forg= -0.8%| TAg acc= 74.8%, forg=  7.6% <<<
>>> Test on task 31 : loss=0.883 | TAw acc= 91.9%, forg=  0.0%| TAg acc= 79.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=5.529, TAw acc= 52.6% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=3.128, TAw acc= 73.7% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=2.260, TAw acc= 81.6% | *
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=2.012, TAw acc= 93.4% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.774, TAw acc= 94.7% | *
| Epoch   6, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.649, TAw acc= 94.7% | *
| Epoch   7, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.582, TAw acc= 93.4% | *
| Epoch   8, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.449, TAw acc= 94.7% | *
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.449, TAw acc= 94.7% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.449, TAw acc= 94.7% |
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.450, TAw acc= 94.7% |
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.450, TAw acc= 94.7% |
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.450, TAw acc= 94.7% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4634 train exemplars, time=  0.0s
4634
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.188 | TAw acc= 95.9%, forg=  1.5%| TAg acc= 73.3%, forg= 12.8% <<<
>>> Test on task  1 : loss=0.936 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 78.5%, forg= 12.1% <<<
>>> Test on task  2 : loss=1.085 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 83.5%, forg=  7.8% <<<
>>> Test on task  3 : loss=1.675 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 71.5%, forg= 13.8% <<<
>>> Test on task  4 : loss=0.880 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 81.0%, forg=  6.6% <<<
>>> Test on task  5 : loss=0.803 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.3%, forg=  7.1% <<<
>>> Test on task  6 : loss=1.456 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 71.3%, forg=  9.9% <<<
>>> Test on task  7 : loss=1.020 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 72.9%, forg= 13.1% <<<
>>> Test on task  8 : loss=1.075 | TAw acc= 93.8%, forg= -0.9%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.017 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 68.3%, forg= 11.5% <<<
>>> Test on task 10 : loss=0.877 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 82.7%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.183 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.9%, forg= 11.8% <<<
>>> Test on task 12 : loss=0.680 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 85.8%, forg=  8.5% <<<
>>> Test on task 13 : loss=1.162 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 69.6%, forg= 13.0% <<<
>>> Test on task 14 : loss=1.034 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 66.0%, forg= 11.0% <<<
>>> Test on task 15 : loss=1.360 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 57.8%, forg= 20.2% <<<
>>> Test on task 16 : loss=1.089 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 67.8%, forg=  8.9% <<<
>>> Test on task 17 : loss=1.180 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 67.7%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.049 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.9%, forg=  8.5% <<<
>>> Test on task 19 : loss=1.314 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 66.1%, forg= 12.5% <<<
>>> Test on task 20 : loss=1.437 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 62.5%, forg=  8.0% <<<
>>> Test on task 21 : loss=1.175 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 70.0%, forg=  6.0% <<<
>>> Test on task 22 : loss=1.079 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 72.6%, forg=  7.7% <<<
>>> Test on task 23 : loss=1.260 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 71.8%, forg=  3.6% <<<
>>> Test on task 24 : loss=1.520 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 59.0%, forg= 23.1% <<<
>>> Test on task 25 : loss=1.071 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 70.1%, forg=  6.5% <<<
>>> Test on task 26 : loss=1.343 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 64.1%, forg=  4.7% <<<
>>> Test on task 27 : loss=0.825 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.1%, forg=  5.4% <<<
>>> Test on task 28 : loss=1.297 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 61.3%, forg=  4.3% <<<
>>> Test on task 29 : loss=1.701 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 41.0%, forg= 25.7% <<<
>>> Test on task 30 : loss=1.222 | TAw acc= 93.3%, forg= -0.8%| TAg acc= 71.4%, forg= 10.9% <<<
>>> Test on task 31 : loss=1.212 | TAw acc= 94.9%, forg= -3.0%| TAg acc= 62.6%, forg= 17.2% <<<
>>> Test on task 32 : loss=1.421 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 74.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=4.922, TAw acc= 56.8% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=2.025, TAw acc= 79.5% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.105, TAw acc= 95.5% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.043, TAw acc= 95.5% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=0.878, TAw acc= 96.6% | *
| Epoch   6, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.902, TAw acc= 96.6% |
| Epoch   7, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=0.904, TAw acc= 97.7% |
| Epoch   8, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.968, TAw acc= 97.7% |
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=0.878, TAw acc= 96.6% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.879, TAw acc= 96.6% |
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.879, TAw acc= 96.6% |
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.880, TAw acc= 96.6% |
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.880, TAw acc= 96.6% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4754 train exemplars, time=  0.0s
4754
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.226 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 72.8%, forg= 13.3% <<<
>>> Test on task  1 : loss=0.930 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 83.2%, forg=  7.5% <<<
>>> Test on task  2 : loss=1.123 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 81.6%, forg=  9.7% <<<
>>> Test on task  3 : loss=1.701 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.0%, forg= 11.4% <<<
>>> Test on task  4 : loss=1.073 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 73.6%, forg= 14.0% <<<
>>> Test on task  5 : loss=0.819 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.483 | TAw acc= 93.1%, forg= -1.0%| TAg acc= 71.3%, forg=  9.9% <<<
>>> Test on task  7 : loss=1.081 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 74.8%, forg= 11.2% <<<
>>> Test on task  8 : loss=1.039 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.894 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 10 : loss=0.877 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 83.7%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.213 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.679 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 89.6%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.355 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 64.5%, forg= 18.1% <<<
>>> Test on task 14 : loss=0.974 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.0%, forg=  7.0% <<<
>>> Test on task 15 : loss=1.275 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 65.1%, forg= 12.8% <<<
>>> Test on task 16 : loss=1.047 | TAw acc= 88.9%, forg=  3.3%| TAg acc= 67.8%, forg=  8.9% <<<
>>> Test on task 17 : loss=1.234 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 63.4%, forg= 14.0% <<<
>>> Test on task 18 : loss=1.123 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 66.7%, forg= 12.8% <<<
>>> Test on task 19 : loss=1.424 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 66.1%, forg= 12.5% <<<
>>> Test on task 20 : loss=1.567 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 59.8%, forg= 10.7% <<<
>>> Test on task 21 : loss=1.238 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 67.0%, forg=  9.0% <<<
>>> Test on task 22 : loss=1.228 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 64.1%, forg= 16.2% <<<
>>> Test on task 23 : loss=1.240 | TAw acc= 96.4%, forg= -0.9%| TAg acc= 74.5%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.473 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 60.7%, forg= 21.4% <<<
>>> Test on task 25 : loss=1.188 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 67.3%, forg=  9.3% <<<
>>> Test on task 26 : loss=1.442 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 60.9%, forg=  7.8% <<<
>>> Test on task 27 : loss=0.826 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.1%, forg=  5.4% <<<
>>> Test on task 28 : loss=1.253 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 63.4%, forg=  2.2% <<<
>>> Test on task 29 : loss=1.802 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 41.9%, forg= 24.8% <<<
>>> Test on task 30 : loss=1.219 | TAw acc= 92.4%, forg=  0.8%| TAg acc= 70.6%, forg= 11.8% <<<
>>> Test on task 31 : loss=1.124 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 67.7%, forg= 12.1% <<<
>>> Test on task 32 : loss=1.910 | TAw acc= 95.1%, forg= -1.0%| TAg acc= 55.3%, forg= 19.4% <<<
>>> Test on task 33 : loss=1.068 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 76.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=6.076, TAw acc= 42.9% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=2.846, TAw acc= 74.0% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.661, TAw acc= 87.0% | *
| Epoch   4, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.551, TAw acc= 93.5% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 94.8% | *
| Epoch   6, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.364, TAw acc= 87.0% | *
| Epoch   7, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.341, TAw acc= 92.2% | *
| Epoch   8, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.172, TAw acc= 94.8% | *
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.171, TAw acc= 94.8% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.170, TAw acc= 94.8% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.166, TAw acc= 93.5% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.166, TAw acc= 93.5% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.165, TAw acc= 93.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4874 train exemplars, time=  0.0s
4874
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.181 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 74.4%, forg= 11.8% <<<
>>> Test on task  1 : loss=0.895 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 87.9%, forg=  2.8% <<<
>>> Test on task  2 : loss=1.065 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 83.5%, forg=  7.8% <<<
>>> Test on task  3 : loss=1.699 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.2%, forg= 12.2% <<<
>>> Test on task  4 : loss=0.917 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 76.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=0.802 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 80.5%, forg=  8.8% <<<
>>> Test on task  6 : loss=1.486 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 68.3%, forg= 12.9% <<<
>>> Test on task  7 : loss=1.138 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 68.2%, forg= 17.8% <<<
>>> Test on task  8 : loss=1.055 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 81.4%, forg=  3.5% <<<
>>> Test on task  9 : loss=1.051 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=0.911 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 83.7%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.247 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.9%, forg= 16.8% <<<
>>> Test on task 12 : loss=0.711 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 85.8%, forg=  8.5% <<<
>>> Test on task 13 : loss=1.369 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 63.8%, forg= 18.8% <<<
>>> Test on task 14 : loss=0.950 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.323 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 16 : loss=1.057 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 66.7%, forg= 10.0% <<<
>>> Test on task 17 : loss=1.275 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 67.7%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.060 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 66.7%, forg= 12.8% <<<
>>> Test on task 19 : loss=1.433 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 65.2%, forg= 13.4% <<<
>>> Test on task 20 : loss=1.570 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 58.0%, forg= 12.5% <<<
>>> Test on task 21 : loss=1.172 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 70.0%, forg=  6.0% <<<
>>> Test on task 22 : loss=1.179 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 68.4%, forg= 12.0% <<<
>>> Test on task 23 : loss=1.271 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 73.6%, forg=  1.8% <<<
>>> Test on task 24 : loss=1.446 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 61.5%, forg= 20.5% <<<
>>> Test on task 25 : loss=1.160 | TAw acc= 94.4%, forg=  2.8%| TAg acc= 64.5%, forg= 12.1% <<<
>>> Test on task 26 : loss=1.361 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 65.6%, forg=  3.1% <<<
>>> Test on task 27 : loss=0.866 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 79.1%, forg=  5.4% <<<
>>> Test on task 28 : loss=1.194 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 68.8%, forg= -3.2% <<<
>>> Test on task 29 : loss=1.856 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 42.9%, forg= 23.8% <<<
>>> Test on task 30 : loss=1.228 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 69.7%, forg= 12.6% <<<
>>> Test on task 31 : loss=1.189 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 62.6%, forg= 17.2% <<<
>>> Test on task 32 : loss=1.921 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 55.3%, forg= 19.4% <<<
>>> Test on task 33 : loss=1.409 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 65.3%, forg= 11.0% <<<
>>> Test on task 34 : loss=0.868 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 83.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=6.662, TAw acc= 39.5% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=3.438, TAw acc= 57.0% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=2.475, TAw acc= 79.1% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.852, TAw acc= 80.2% | *
| Epoch   5, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.738, TAw acc= 80.2% | *
| Epoch   6, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.588, TAw acc= 77.9% | *
| Epoch   7, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.505, TAw acc= 83.7% | *
| Epoch   8, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.499, TAw acc= 80.2% | *
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.491, TAw acc= 80.2% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.484, TAw acc= 80.2% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.479, TAw acc= 80.2% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.475, TAw acc= 80.2% | *
| Epoch   5, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.472, TAw acc= 80.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4994 train exemplars, time=  0.0s
4994
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.272 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 70.3%, forg= 15.9% <<<
>>> Test on task  1 : loss=1.005 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 80.4%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.126 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  9.7% <<<
>>> Test on task  3 : loss=1.749 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 71.5%, forg= 13.8% <<<
>>> Test on task  4 : loss=1.009 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 71.1%, forg= 16.5% <<<
>>> Test on task  5 : loss=0.805 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.8%, forg= 10.6% <<<
>>> Test on task  6 : loss=1.494 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 70.3%, forg= 10.9% <<<
>>> Test on task  7 : loss=1.033 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 77.6%, forg=  8.4% <<<
>>> Test on task  8 : loss=1.065 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 81.4%, forg=  3.5% <<<
>>> Test on task  9 : loss=1.071 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 72.1%, forg=  7.7% <<<
>>> Test on task 10 : loss=0.877 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 85.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.220 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.706 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 84.9%, forg=  9.4% <<<
>>> Test on task 13 : loss=1.289 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 65.9%, forg= 16.7% <<<
>>> Test on task 14 : loss=1.060 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.0%, forg=  9.0% <<<
>>> Test on task 15 : loss=1.382 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 61.5%, forg= 16.5% <<<
>>> Test on task 16 : loss=1.042 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 73.3%, forg=  3.3% <<<
>>> Test on task 17 : loss=1.225 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 64.5%, forg= 12.9% <<<
>>> Test on task 18 : loss=0.987 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 72.6%, forg=  6.8% <<<
>>> Test on task 19 : loss=1.405 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 67.0%, forg= 11.6% <<<
>>> Test on task 20 : loss=1.540 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 58.0%, forg= 12.5% <<<
>>> Test on task 21 : loss=1.115 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 68.0%, forg=  8.0% <<<
>>> Test on task 22 : loss=1.139 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.2%, forg= 11.1% <<<
>>> Test on task 23 : loss=1.314 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 69.1%, forg=  6.4% <<<
>>> Test on task 24 : loss=1.474 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 60.7%, forg= 21.4% <<<
>>> Test on task 25 : loss=1.166 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 70.1%, forg=  6.5% <<<
>>> Test on task 26 : loss=1.388 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 62.5%, forg=  6.2% <<<
>>> Test on task 27 : loss=0.783 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 82.9%, forg=  1.6% <<<
>>> Test on task 28 : loss=1.279 | TAw acc= 96.8%, forg= -1.1%| TAg acc= 62.4%, forg=  6.5% <<<
>>> Test on task 29 : loss=1.805 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 46.7%, forg= 20.0% <<<
>>> Test on task 30 : loss=1.211 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 70.6%, forg= 11.8% <<<
>>> Test on task 31 : loss=1.151 | TAw acc= 92.9%, forg=  2.0%| TAg acc= 62.6%, forg= 17.2% <<<
>>> Test on task 32 : loss=1.911 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 51.5%, forg= 23.3% <<<
>>> Test on task 33 : loss=1.462 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 61.9%, forg= 14.4% <<<
>>> Test on task 34 : loss=1.447 | TAw acc= 98.1%, forg= -1.9%| TAg acc= 54.7%, forg= 28.3% <<<
>>> Test on task 35 : loss=1.123 | TAw acc= 82.6%, forg=  0.0%| TAg acc= 66.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=7.211, TAw acc= 37.0% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=4.116, TAw acc= 74.0% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=2.646, TAw acc= 78.1% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=2.074, TAw acc= 82.2% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.029, TAw acc= 90.4% | *
| Epoch   6, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.859, TAw acc= 90.4% | *
| Epoch   7, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.648, TAw acc= 90.4% | *
| Epoch   8, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.733, TAw acc= 91.8% |
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.651, TAw acc= 90.4% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.655, TAw acc= 90.4% |
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.658, TAw acc= 90.4% |
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.661, TAw acc= 91.8% |
| Epoch   5, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.664, TAw acc= 91.8% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5114 train exemplars, time=  0.1s
5114
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.258 | TAw acc= 95.9%, forg=  1.5%| TAg acc= 73.3%, forg= 12.8% <<<
>>> Test on task  1 : loss=1.096 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 73.8%, forg= 16.8% <<<
>>> Test on task  2 : loss=1.175 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 80.6%, forg= 10.7% <<<
>>> Test on task  3 : loss=1.723 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.014 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 70.2%, forg= 17.4% <<<
>>> Test on task  5 : loss=0.802 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.536 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 69.3%, forg= 11.9% <<<
>>> Test on task  7 : loss=1.173 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 67.3%, forg= 18.7% <<<
>>> Test on task  8 : loss=1.062 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.979 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 10 : loss=0.928 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 82.7%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.277 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.5%, forg= 15.1% <<<
>>> Test on task 12 : loss=0.693 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 84.9%, forg=  9.4% <<<
>>> Test on task 13 : loss=1.282 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 66.7%, forg= 15.9% <<<
>>> Test on task 14 : loss=1.006 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.0%, forg=  7.0% <<<
>>> Test on task 15 : loss=1.292 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 65.1%, forg= 12.8% <<<
>>> Test on task 16 : loss=1.145 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 71.1%, forg=  5.6% <<<
>>> Test on task 17 : loss=1.231 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 68.8%, forg=  8.6% <<<
>>> Test on task 18 : loss=1.037 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.9%, forg=  8.5% <<<
>>> Test on task 19 : loss=1.444 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 62.5%, forg= 16.1% <<<
>>> Test on task 20 : loss=1.552 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 58.0%, forg= 12.5% <<<
>>> Test on task 21 : loss=1.106 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 68.0%, forg=  8.0% <<<
>>> Test on task 22 : loss=1.147 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 69.2%, forg= 11.1% <<<
>>> Test on task 23 : loss=1.245 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 74.5%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.565 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 59.8%, forg= 22.2% <<<
>>> Test on task 25 : loss=1.191 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 69.2%, forg=  7.5% <<<
>>> Test on task 26 : loss=1.401 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 61.7%, forg=  7.0% <<<
>>> Test on task 27 : loss=0.761 | TAw acc=100.0%, forg= -0.8%| TAg acc= 82.9%, forg=  1.6% <<<
>>> Test on task 28 : loss=1.206 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 64.5%, forg=  4.3% <<<
>>> Test on task 29 : loss=1.764 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 47.6%, forg= 19.0% <<<
>>> Test on task 30 : loss=1.245 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 68.9%, forg= 13.4% <<<
>>> Test on task 31 : loss=1.079 | TAw acc= 93.9%, forg=  1.0%| TAg acc= 65.7%, forg= 14.1% <<<
>>> Test on task 32 : loss=1.920 | TAw acc= 94.2%, forg=  1.0%| TAg acc= 56.3%, forg= 18.4% <<<
>>> Test on task 33 : loss=1.398 | TAw acc= 96.6%, forg= -0.8%| TAg acc= 62.7%, forg= 13.6% <<<
>>> Test on task 34 : loss=1.410 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 53.8%, forg= 29.2% <<<
>>> Test on task 35 : loss=1.609 | TAw acc= 81.7%, forg=  0.9%| TAg acc= 42.6%, forg= 23.5% <<<
>>> Test on task 36 : loss=1.508 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 65.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=6.424, TAw acc= 54.4% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=3.090, TAw acc= 72.2% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.940, TAw acc= 79.7% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.634, TAw acc= 86.1% | *
| Epoch   5, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.458, TAw acc= 83.5% | *
| Epoch   6, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.404, TAw acc= 86.1% | *
| Epoch   7, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.363, TAw acc= 86.1% | *
| Epoch   8, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.372, TAw acc= 82.3% |
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.361, TAw acc= 86.1% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.359, TAw acc= 86.1% | *
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.357, TAw acc= 86.1% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.356, TAw acc= 86.1% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.355, TAw acc= 86.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5234 train exemplars, time=  0.0s
5234
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.409 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 68.2%, forg= 17.9% <<<
>>> Test on task  1 : loss=1.040 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 79.4%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.171 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 78.6%, forg= 12.6% <<<
>>> Test on task  3 : loss=1.710 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 75.6%, forg=  9.8% <<<
>>> Test on task  4 : loss=1.037 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 71.9%, forg= 15.7% <<<
>>> Test on task  5 : loss=0.828 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.8%, forg= 10.6% <<<
>>> Test on task  6 : loss=1.490 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 71.3%, forg=  9.9% <<<
>>> Test on task  7 : loss=1.077 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 71.0%, forg= 15.0% <<<
>>> Test on task  8 : loss=1.048 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=1.083 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=0.929 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 82.7%, forg=  3.8% <<<
>>> Test on task 11 : loss=1.235 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.768 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 83.0%, forg= 11.3% <<<
>>> Test on task 13 : loss=1.297 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 65.2%, forg= 17.4% <<<
>>> Test on task 14 : loss=0.949 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.0%, forg=  2.0% <<<
>>> Test on task 15 : loss=1.422 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 16 : loss=1.146 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 72.2%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.294 | TAw acc= 90.3%, forg= -1.1%| TAg acc= 62.4%, forg= 15.1% <<<
>>> Test on task 18 : loss=0.995 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 72.6%, forg=  6.8% <<<
>>> Test on task 19 : loss=1.417 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 62.5%, forg= 16.1% <<<
>>> Test on task 20 : loss=1.587 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 52.7%, forg= 17.9% <<<
>>> Test on task 21 : loss=1.063 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 72.0%, forg=  4.0% <<<
>>> Test on task 22 : loss=1.207 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 68.4%, forg= 12.0% <<<
>>> Test on task 23 : loss=1.269 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 76.4%, forg= -0.9% <<<
>>> Test on task 24 : loss=1.532 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 61.5%, forg= 20.5% <<<
>>> Test on task 25 : loss=1.139 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 69.2%, forg=  7.5% <<<
>>> Test on task 26 : loss=1.435 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 62.5%, forg=  6.2% <<<
>>> Test on task 27 : loss=0.805 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 82.9%, forg=  1.6% <<<
>>> Test on task 28 : loss=1.393 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 57.0%, forg= 11.8% <<<
>>> Test on task 29 : loss=1.764 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 51.4%, forg= 15.2% <<<
>>> Test on task 30 : loss=1.276 | TAw acc= 92.4%, forg=  0.8%| TAg acc= 65.5%, forg= 16.8% <<<
>>> Test on task 31 : loss=1.135 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 65.7%, forg= 14.1% <<<
>>> Test on task 32 : loss=1.881 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 59.2%, forg= 15.5% <<<
>>> Test on task 33 : loss=1.423 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.6%, forg= 12.7% <<<
>>> Test on task 34 : loss=1.312 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 57.5%, forg= 25.5% <<<
>>> Test on task 35 : loss=1.564 | TAw acc= 82.6%, forg=  0.0%| TAg acc= 50.4%, forg= 15.7% <<<
>>> Test on task 36 : loss=1.833 | TAw acc= 94.0%, forg=  1.0%| TAg acc= 44.0%, forg= 21.0% <<<
>>> Test on task 37 : loss=1.319 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 67.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=7.121, TAw acc= 38.9% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=3.260, TAw acc= 79.2% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.1s loss=1.963, TAw acc= 93.1% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.615, TAw acc= 94.4% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.488, TAw acc= 94.4% | *
| Epoch   6, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.384, TAw acc= 93.1% | *
| Epoch   7, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.337, TAw acc= 91.7% | *
| Epoch   8, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.239, TAw acc= 94.4% | *
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 94.4% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 94.4% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 94.4% | *
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 94.4% |
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 94.4% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5354 train exemplars, time=  0.0s
5354
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.300 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 73.8%, forg= 12.3% <<<
>>> Test on task  1 : loss=1.052 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 80.4%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.168 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 79.6%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.754 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.213 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 64.5%, forg= 23.1% <<<
>>> Test on task  5 : loss=0.819 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.563 | TAw acc= 91.1%, forg=  2.0%| TAg acc= 67.3%, forg= 13.9% <<<
>>> Test on task  7 : loss=1.178 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 70.1%, forg= 15.9% <<<
>>> Test on task  8 : loss=1.042 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.976 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 75.0%, forg=  4.8% <<<
>>> Test on task 10 : loss=0.951 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 81.7%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.223 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 66.4%, forg= 14.3% <<<
>>> Test on task 12 : loss=0.696 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 85.8%, forg=  8.5% <<<
>>> Test on task 13 : loss=1.290 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 65.2%, forg= 17.4% <<<
>>> Test on task 14 : loss=0.939 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.0%, forg=  3.0% <<<
>>> Test on task 15 : loss=1.424 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 61.5%, forg= 16.5% <<<
>>> Test on task 16 : loss=1.070 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.214 | TAw acc= 89.2%, forg=  1.1%| TAg acc= 65.6%, forg= 11.8% <<<
>>> Test on task 18 : loss=1.037 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 68.4%, forg= 11.1% <<<
>>> Test on task 19 : loss=1.461 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 61.6%, forg= 17.0% <<<
>>> Test on task 20 : loss=1.606 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 55.4%, forg= 15.2% <<<
>>> Test on task 21 : loss=1.071 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 69.0%, forg=  7.0% <<<
>>> Test on task 22 : loss=1.204 | TAw acc= 94.9%, forg=  2.6%| TAg acc= 69.2%, forg= 11.1% <<<
>>> Test on task 23 : loss=1.352 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 76.4%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.481 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 63.2%, forg= 18.8% <<<
>>> Test on task 25 : loss=1.192 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 67.3%, forg=  9.3% <<<
>>> Test on task 26 : loss=1.486 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 59.4%, forg=  9.4% <<<
>>> Test on task 27 : loss=0.801 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 82.9%, forg=  1.6% <<<
>>> Test on task 28 : loss=1.239 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 65.6%, forg=  3.2% <<<
>>> Test on task 29 : loss=1.786 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 46.7%, forg= 20.0% <<<
>>> Test on task 30 : loss=1.262 | TAw acc= 92.4%, forg=  0.8%| TAg acc= 68.9%, forg= 13.4% <<<
>>> Test on task 31 : loss=1.080 | TAw acc= 93.9%, forg=  1.0%| TAg acc= 73.7%, forg=  6.1% <<<
>>> Test on task 32 : loss=1.913 | TAw acc= 96.1%, forg= -1.0%| TAg acc= 57.3%, forg= 17.5% <<<
>>> Test on task 33 : loss=1.305 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.8%, forg=  8.5% <<<
>>> Test on task 34 : loss=1.386 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 52.8%, forg= 30.2% <<<
>>> Test on task 35 : loss=1.554 | TAw acc= 81.7%, forg=  0.9%| TAg acc= 51.3%, forg= 14.8% <<<
>>> Test on task 36 : loss=1.758 | TAw acc= 94.0%, forg=  1.0%| TAg acc= 52.0%, forg= 13.0% <<<
>>> Test on task 37 : loss=1.716 | TAw acc= 95.3%, forg= -0.9%| TAg acc= 54.2%, forg= 13.1% <<<
>>> Test on task 38 : loss=0.983 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 77.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=5.160, TAw acc= 61.1% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=2.031, TAw acc= 80.0% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.290, TAw acc= 95.8% | *
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.332, TAw acc= 97.9% |
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.234, TAw acc= 97.9% | *
| Epoch   6, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.090, TAw acc= 97.9% | *
| Epoch   7, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.009, TAw acc= 97.9% | *
| Epoch   8, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.048, TAw acc= 97.9% |
| Epoch   1, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.011, TAw acc= 97.9% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.012, TAw acc= 97.9% |
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.013, TAw acc= 97.9% |
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.015, TAw acc= 97.9% |
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.016, TAw acc= 97.9% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5474 train exemplars, time=  0.0s
5474
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.339 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 71.8%, forg= 14.4% <<<
>>> Test on task  1 : loss=1.069 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 76.6%, forg= 14.0% <<<
>>> Test on task  2 : loss=1.182 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 79.6%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.768 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 71.5%, forg= 13.8% <<<
>>> Test on task  4 : loss=1.032 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 76.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=0.835 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 77.9%, forg= 11.5% <<<
>>> Test on task  6 : loss=1.553 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 70.3%, forg= 10.9% <<<
>>> Test on task  7 : loss=1.044 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 75.7%, forg= 10.3% <<<
>>> Test on task  8 : loss=1.091 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 83.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=1.056 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 66.3%, forg= 13.5% <<<
>>> Test on task 10 : loss=1.005 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 79.8%, forg=  6.7% <<<
>>> Test on task 11 : loss=1.277 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.2%, forg= 13.4% <<<
>>> Test on task 12 : loss=0.692 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 85.8%, forg=  8.5% <<<
>>> Test on task 13 : loss=1.283 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 64.5%, forg= 18.1% <<<
>>> Test on task 14 : loss=1.062 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.404 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 66.1%, forg= 11.9% <<<
>>> Test on task 16 : loss=1.093 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 71.1%, forg=  5.6% <<<
>>> Test on task 17 : loss=1.232 | TAw acc= 88.2%, forg=  2.2%| TAg acc= 67.7%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.009 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 73.5%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.415 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 66.1%, forg= 12.5% <<<
>>> Test on task 20 : loss=1.601 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 55.4%, forg= 15.2% <<<
>>> Test on task 21 : loss=1.140 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 66.0%, forg= 10.0% <<<
>>> Test on task 22 : loss=1.217 | TAw acc= 94.9%, forg=  2.6%| TAg acc= 67.5%, forg= 12.8% <<<
>>> Test on task 23 : loss=1.423 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.591 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 61.5%, forg= 20.5% <<<
>>> Test on task 25 : loss=1.242 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 67.3%, forg=  9.3% <<<
>>> Test on task 26 : loss=1.492 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 58.6%, forg= 10.2% <<<
>>> Test on task 27 : loss=0.850 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 79.8%, forg=  4.7% <<<
>>> Test on task 28 : loss=1.291 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 61.3%, forg=  7.5% <<<
>>> Test on task 29 : loss=1.827 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 47.6%, forg= 19.0% <<<
>>> Test on task 30 : loss=1.267 | TAw acc= 91.6%, forg=  1.7%| TAg acc= 72.3%, forg= 10.1% <<<
>>> Test on task 31 : loss=1.096 | TAw acc= 92.9%, forg=  2.0%| TAg acc= 70.7%, forg=  9.1% <<<
>>> Test on task 32 : loss=1.876 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 59.2%, forg= 15.5% <<<
>>> Test on task 33 : loss=1.482 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 62.7%, forg= 13.6% <<<
>>> Test on task 34 : loss=1.392 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 52.8%, forg= 30.2% <<<
>>> Test on task 35 : loss=1.633 | TAw acc= 80.0%, forg=  2.6%| TAg acc= 49.6%, forg= 16.5% <<<
>>> Test on task 36 : loss=1.783 | TAw acc= 94.0%, forg=  1.0%| TAg acc= 53.0%, forg= 12.0% <<<
>>> Test on task 37 : loss=1.719 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 55.1%, forg= 12.1% <<<
>>> Test on task 38 : loss=1.355 | TAw acc= 97.0%, forg= -1.0%| TAg acc= 59.6%, forg= 18.2% <<<
>>> Test on task 39 : loss=0.889 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 78.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=6.527, TAw acc= 50.6% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=2.870, TAw acc= 77.2% | *
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.697, TAw acc= 88.6% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.114, TAw acc= 98.7% | *
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.897, TAw acc= 98.7% | *
| Epoch   6, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.822, TAw acc= 98.7% | *
| Epoch   7, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.862, TAw acc= 98.7% |
| Epoch   8, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.796, TAw acc= 98.7% | *
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.793, TAw acc= 98.7% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.791, TAw acc= 98.7% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.788, TAw acc= 98.7% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=0.786, TAw acc= 98.7% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.784, TAw acc= 98.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
Not enough samples to store. Select all samples instead.	Needed: 6
| Selected 5579 train exemplars, time=  0.0s
5579
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.306 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 73.3%, forg= 12.8% <<<
>>> Test on task  1 : loss=1.036 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 80.4%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.191 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 79.6%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.777 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.027 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 74.4%, forg= 13.2% <<<
>>> Test on task  5 : loss=0.804 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.624 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 65.3%, forg= 15.8% <<<
>>> Test on task  7 : loss=1.077 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 72.9%, forg= 13.1% <<<
>>> Test on task  8 : loss=1.100 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.016 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 69.2%, forg= 10.6% <<<
>>> Test on task 10 : loss=1.000 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 80.8%, forg=  5.8% <<<
>>> Test on task 11 : loss=1.305 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.5%, forg= 15.1% <<<
>>> Test on task 12 : loss=0.662 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 86.8%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.327 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 65.2%, forg= 17.4% <<<
>>> Test on task 14 : loss=0.981 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.425 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 65.1%, forg= 12.8% <<<
>>> Test on task 16 : loss=1.143 | TAw acc= 88.9%, forg=  3.3%| TAg acc= 66.7%, forg= 10.0% <<<
>>> Test on task 17 : loss=1.265 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 64.5%, forg= 12.9% <<<
>>> Test on task 18 : loss=1.070 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 69.2%, forg= 10.3% <<<
>>> Test on task 19 : loss=1.539 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 59.8%, forg= 18.8% <<<
>>> Test on task 20 : loss=1.582 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 55.4%, forg= 15.2% <<<
>>> Test on task 21 : loss=1.068 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 71.0%, forg=  5.0% <<<
>>> Test on task 22 : loss=1.218 | TAw acc= 94.9%, forg=  2.6%| TAg acc= 68.4%, forg= 12.0% <<<
>>> Test on task 23 : loss=1.456 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.614 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 59.8%, forg= 22.2% <<<
>>> Test on task 25 : loss=1.246 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 64.5%, forg= 12.1% <<<
>>> Test on task 26 : loss=1.453 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 65.6%, forg=  3.1% <<<
>>> Test on task 27 : loss=0.848 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 81.4%, forg=  3.1% <<<
>>> Test on task 28 : loss=1.205 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 64.5%, forg=  4.3% <<<
>>> Test on task 29 : loss=1.796 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 48.6%, forg= 18.1% <<<
>>> Test on task 30 : loss=1.324 | TAw acc= 92.4%, forg=  0.8%| TAg acc= 65.5%, forg= 16.8% <<<
>>> Test on task 31 : loss=1.087 | TAw acc= 91.9%, forg=  3.0%| TAg acc= 70.7%, forg=  9.1% <<<
>>> Test on task 32 : loss=1.827 | TAw acc= 95.1%, forg=  1.0%| TAg acc= 63.1%, forg= 11.7% <<<
>>> Test on task 33 : loss=1.325 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 69.5%, forg=  6.8% <<<
>>> Test on task 34 : loss=1.411 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 53.8%, forg= 29.2% <<<
>>> Test on task 35 : loss=1.512 | TAw acc= 83.5%, forg= -0.9%| TAg acc= 47.8%, forg= 18.3% <<<
>>> Test on task 36 : loss=1.695 | TAw acc= 94.0%, forg=  1.0%| TAg acc= 53.0%, forg= 12.0% <<<
>>> Test on task 37 : loss=1.662 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 54.2%, forg= 13.1% <<<
>>> Test on task 38 : loss=1.340 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 62.6%, forg= 15.2% <<<
>>> Test on task 39 : loss=1.341 | TAw acc= 96.0%, forg= -0.8%| TAg acc= 56.3%, forg= 22.2% <<<
>>> Test on task 40 : loss=0.884 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=7.239, TAw acc= 38.7% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=4.196, TAw acc= 66.7% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=2.530, TAw acc= 85.3% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.861, TAw acc= 90.7% | *
| Epoch   5, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.773, TAw acc= 92.0% | *
| Epoch   6, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.575, TAw acc= 92.0% | *
| Epoch   7, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.708, TAw acc= 92.0% |
| Epoch   8, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.677, TAw acc= 92.0% |
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.578, TAw acc= 92.0% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.582, TAw acc= 92.0% |
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.1s loss=1.585, TAw acc= 92.0% |
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.587, TAw acc= 92.0% |
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.590, TAw acc= 92.0% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
Not enough samples to store. Select all samples instead.	Needed: 6
| Selected 5679 train exemplars, time=  0.0s
5679
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.370 | TAw acc= 96.4%, forg=  1.0%| TAg acc= 73.3%, forg= 12.8% <<<
>>> Test on task  1 : loss=1.086 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 79.4%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.165 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 79.6%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.827 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 72.4%, forg= 13.0% <<<
>>> Test on task  4 : loss=1.039 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 76.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=0.853 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 80.5%, forg=  8.8% <<<
>>> Test on task  6 : loss=1.556 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 69.3%, forg= 11.9% <<<
>>> Test on task  7 : loss=1.169 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 71.0%, forg= 15.0% <<<
>>> Test on task  8 : loss=1.161 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.203 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 67.3%, forg= 12.5% <<<
>>> Test on task 10 : loss=1.006 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 81.7%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.293 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.1%, forg= 12.6% <<<
>>> Test on task 12 : loss=0.694 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 84.9%, forg=  9.4% <<<
>>> Test on task 13 : loss=1.276 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 67.4%, forg= 15.2% <<<
>>> Test on task 14 : loss=0.984 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.0%, forg=  9.0% <<<
>>> Test on task 15 : loss=1.495 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 62.4%, forg= 15.6% <<<
>>> Test on task 16 : loss=1.172 | TAw acc= 88.9%, forg=  3.3%| TAg acc= 66.7%, forg= 10.0% <<<
>>> Test on task 17 : loss=1.376 | TAw acc= 89.2%, forg=  1.1%| TAg acc= 60.2%, forg= 17.2% <<<
>>> Test on task 18 : loss=1.029 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 75.2%, forg=  4.3% <<<
>>> Test on task 19 : loss=1.484 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 59.8%, forg= 18.8% <<<
>>> Test on task 20 : loss=1.618 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 55.4%, forg= 15.2% <<<
>>> Test on task 21 : loss=1.117 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 71.0%, forg=  5.0% <<<
>>> Test on task 22 : loss=1.214 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 70.1%, forg= 10.3% <<<
>>> Test on task 23 : loss=1.517 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.690 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 62.4%, forg= 19.7% <<<
>>> Test on task 25 : loss=1.185 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 70.1%, forg=  6.5% <<<
>>> Test on task 26 : loss=1.499 | TAw acc= 94.5%, forg=  0.8%| TAg acc= 62.5%, forg=  6.2% <<<
>>> Test on task 27 : loss=0.813 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.4%, forg=  3.1% <<<
>>> Test on task 28 : loss=1.226 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 66.7%, forg=  2.2% <<<
>>> Test on task 29 : loss=1.843 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 48.6%, forg= 18.1% <<<
>>> Test on task 30 : loss=1.344 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 68.1%, forg= 14.3% <<<
>>> Test on task 31 : loss=1.097 | TAw acc= 93.9%, forg=  1.0%| TAg acc= 73.7%, forg=  6.1% <<<
>>> Test on task 32 : loss=1.967 | TAw acc= 95.1%, forg=  1.0%| TAg acc= 55.3%, forg= 19.4% <<<
>>> Test on task 33 : loss=1.372 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.6%, forg= 12.7% <<<
>>> Test on task 34 : loss=1.385 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 54.7%, forg= 28.3% <<<
>>> Test on task 35 : loss=1.457 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 47.0%, forg= 19.1% <<<
>>> Test on task 36 : loss=1.681 | TAw acc= 94.0%, forg=  1.0%| TAg acc= 60.0%, forg=  5.0% <<<
>>> Test on task 37 : loss=1.675 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 59.8%, forg=  7.5% <<<
>>> Test on task 38 : loss=1.302 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 65.7%, forg= 12.1% <<<
>>> Test on task 39 : loss=1.339 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 61.1%, forg= 17.5% <<<
>>> Test on task 40 : loss=1.266 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 61.1%, forg= 13.9% <<<
>>> Test on task 41 : loss=1.030 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=6.075, TAw acc= 50.6% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=3.234, TAw acc= 76.5% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.824, TAw acc= 87.7% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.561, TAw acc= 90.1% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.445, TAw acc= 91.4% | *
| Epoch   6, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.396, TAw acc= 86.4% | *
| Epoch   7, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.212, TAw acc= 92.6% | *
| Epoch   8, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.194, TAw acc= 92.6% | *
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.199, TAw acc= 92.6% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 92.6% |
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.209, TAw acc= 92.6% |
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.214, TAw acc= 92.6% |
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.219, TAw acc= 92.6% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
Not enough samples to store. Select all samples instead.	Needed: 6
| Selected 5779 train exemplars, time=  0.1s
5779
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.320 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 72.3%, forg= 13.8% <<<
>>> Test on task  1 : loss=1.035 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 79.4%, forg= 11.2% <<<
>>> Test on task  2 : loss=1.179 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 81.6%, forg=  9.7% <<<
>>> Test on task  3 : loss=1.829 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 72.4%, forg= 13.0% <<<
>>> Test on task  4 : loss=0.996 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 77.7%, forg=  9.9% <<<
>>> Test on task  5 : loss=0.867 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.559 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 71.3%, forg=  9.9% <<<
>>> Test on task  7 : loss=1.256 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 68.2%, forg= 17.8% <<<
>>> Test on task  8 : loss=1.198 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 81.4%, forg=  3.5% <<<
>>> Test on task  9 : loss=1.244 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 62.5%, forg= 17.3% <<<
>>> Test on task 10 : loss=1.002 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 80.8%, forg=  5.8% <<<
>>> Test on task 11 : loss=1.309 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 66.4%, forg= 14.3% <<<
>>> Test on task 12 : loss=0.701 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 86.8%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.283 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 65.9%, forg= 16.7% <<<
>>> Test on task 14 : loss=1.078 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.0%, forg=  9.0% <<<
>>> Test on task 15 : loss=1.504 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 64.2%, forg= 13.8% <<<
>>> Test on task 16 : loss=1.163 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.304 | TAw acc= 89.2%, forg=  1.1%| TAg acc= 65.6%, forg= 11.8% <<<
>>> Test on task 18 : loss=1.057 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.8%, forg=  7.7% <<<
>>> Test on task 19 : loss=1.490 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 66.1%, forg= 12.5% <<<
>>> Test on task 20 : loss=1.698 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 53.6%, forg= 17.0% <<<
>>> Test on task 21 : loss=1.076 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 70.0%, forg=  6.0% <<<
>>> Test on task 22 : loss=1.175 | TAw acc= 94.9%, forg=  2.6%| TAg acc= 71.8%, forg=  8.5% <<<
>>> Test on task 23 : loss=1.522 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.625 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 61.5%, forg= 20.5% <<<
>>> Test on task 25 : loss=1.240 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 67.3%, forg=  9.3% <<<
>>> Test on task 26 : loss=1.489 | TAw acc= 94.5%, forg=  0.8%| TAg acc= 67.2%, forg=  1.6% <<<
>>> Test on task 27 : loss=0.829 | TAw acc=100.0%, forg=  0.0%| TAg acc= 80.6%, forg=  3.9% <<<
>>> Test on task 28 : loss=1.287 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 63.4%, forg=  5.4% <<<
>>> Test on task 29 : loss=1.835 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 51.4%, forg= 15.2% <<<
>>> Test on task 30 : loss=1.406 | TAw acc= 91.6%, forg=  1.7%| TAg acc= 67.2%, forg= 15.1% <<<
>>> Test on task 31 : loss=1.193 | TAw acc= 93.9%, forg=  1.0%| TAg acc= 63.6%, forg= 16.2% <<<
>>> Test on task 32 : loss=1.981 | TAw acc= 95.1%, forg=  1.0%| TAg acc= 59.2%, forg= 15.5% <<<
>>> Test on task 33 : loss=1.508 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 62.7%, forg= 13.6% <<<
>>> Test on task 34 : loss=1.486 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 52.8%, forg= 30.2% <<<
>>> Test on task 35 : loss=1.719 | TAw acc= 80.0%, forg=  3.5%| TAg acc= 47.8%, forg= 18.3% <<<
>>> Test on task 36 : loss=1.695 | TAw acc= 94.0%, forg=  1.0%| TAg acc= 61.0%, forg=  4.0% <<<
>>> Test on task 37 : loss=1.732 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 52.3%, forg= 15.0% <<<
>>> Test on task 38 : loss=1.470 | TAw acc= 94.9%, forg=  2.0%| TAg acc= 61.6%, forg= 16.2% <<<
>>> Test on task 39 : loss=1.393 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 65.1%, forg= 13.5% <<<
>>> Test on task 40 : loss=1.214 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 61.1%, forg= 13.9% <<<
>>> Test on task 41 : loss=1.537 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 52.4%, forg= 19.4% <<<
>>> Test on task 42 : loss=1.014 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 73.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=5.256, TAw acc= 57.3% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=2.227, TAw acc= 87.8% | *
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.463, TAw acc= 89.0% | *
| Epoch   4, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.148, TAw acc= 93.9% | *
| Epoch   5, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.042, TAw acc= 95.1% | *
| Epoch   6, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.038, TAw acc= 95.1% | *
| Epoch   7, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.150, TAw acc= 95.1% |
| Epoch   8, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.849, TAw acc= 98.8% | *
| Epoch   1, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.853, TAw acc= 98.8% | *
| Epoch   2, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.857, TAw acc= 98.8% |
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.860, TAw acc= 98.8% |
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.863, TAw acc= 98.8% |
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.865, TAw acc= 98.8% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
Not enough samples to store. Select all samples instead.	Needed: 6
| Selected 5879 train exemplars, time=  0.0s
5879
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.365 | TAw acc= 96.9%, forg=  0.5%| TAg acc= 72.8%, forg= 13.3% <<<
>>> Test on task  1 : loss=1.122 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.7%, forg= 15.0% <<<
>>> Test on task  2 : loss=1.250 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 78.6%, forg= 12.6% <<<
>>> Test on task  3 : loss=1.811 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.2%, forg= 12.2% <<<
>>> Test on task  4 : loss=1.025 | TAw acc= 95.9%, forg=  0.8%| TAg acc= 76.0%, forg= 11.6% <<<
>>> Test on task  5 : loss=0.838 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.6%, forg=  9.7% <<<
>>> Test on task  6 : loss=1.659 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 65.3%, forg= 15.8% <<<
>>> Test on task  7 : loss=1.186 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 70.1%, forg= 15.9% <<<
>>> Test on task  8 : loss=1.163 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 82.3%, forg=  2.7% <<<
>>> Test on task  9 : loss=1.188 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 64.4%, forg= 15.4% <<<
>>> Test on task 10 : loss=1.001 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 81.7%, forg=  4.8% <<<
>>> Test on task 11 : loss=1.346 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.5%, forg= 15.1% <<<
>>> Test on task 12 : loss=0.697 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 84.0%, forg= 10.4% <<<
>>> Test on task 13 : loss=1.291 | TAw acc= 93.5%, forg=  0.7%| TAg acc= 63.8%, forg= 18.8% <<<
>>> Test on task 14 : loss=0.973 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.0%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.468 | TAw acc= 97.2%, forg=  1.8%| TAg acc= 65.1%, forg= 12.8% <<<
>>> Test on task 16 : loss=1.109 | TAw acc= 90.0%, forg=  2.2%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.347 | TAw acc= 89.2%, forg=  1.1%| TAg acc= 64.5%, forg= 12.9% <<<
>>> Test on task 18 : loss=1.059 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 73.5%, forg=  6.0% <<<
>>> Test on task 19 : loss=1.471 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 64.3%, forg= 14.3% <<<
>>> Test on task 20 : loss=1.674 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 55.4%, forg= 15.2% <<<
>>> Test on task 21 : loss=1.195 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 60.0%, forg= 16.0% <<<
>>> Test on task 22 : loss=1.206 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 69.2%, forg= 11.1% <<<
>>> Test on task 23 : loss=1.593 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 69.1%, forg=  7.3% <<<
>>> Test on task 24 : loss=1.689 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 60.7%, forg= 21.4% <<<
>>> Test on task 25 : loss=1.284 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 64.5%, forg= 12.1% <<<
>>> Test on task 26 : loss=1.680 | TAw acc= 94.5%, forg=  0.8%| TAg acc= 59.4%, forg=  9.4% <<<
>>> Test on task 27 : loss=0.848 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.3%, forg=  6.2% <<<
>>> Test on task 28 : loss=1.256 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 63.4%, forg=  5.4% <<<
>>> Test on task 29 : loss=1.773 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 46.7%, forg= 20.0% <<<
>>> Test on task 30 : loss=1.399 | TAw acc= 91.6%, forg=  1.7%| TAg acc= 66.4%, forg= 16.0% <<<
>>> Test on task 31 : loss=1.219 | TAw acc= 93.9%, forg=  1.0%| TAg acc= 64.6%, forg= 15.2% <<<
>>> Test on task 32 : loss=2.050 | TAw acc= 95.1%, forg=  1.0%| TAg acc= 57.3%, forg= 17.5% <<<
>>> Test on task 33 : loss=1.341 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 64.4%, forg= 11.9% <<<
>>> Test on task 34 : loss=1.383 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 55.7%, forg= 27.4% <<<
>>> Test on task 35 : loss=1.515 | TAw acc= 81.7%, forg=  1.7%| TAg acc= 47.0%, forg= 19.1% <<<
>>> Test on task 36 : loss=1.717 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 59.0%, forg=  6.0% <<<
>>> Test on task 37 : loss=1.725 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 52.3%, forg= 15.0% <<<
>>> Test on task 38 : loss=1.277 | TAw acc= 94.9%, forg=  2.0%| TAg acc= 68.7%, forg=  9.1% <<<
>>> Test on task 39 : loss=1.481 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 61.1%, forg= 17.5% <<<
>>> Test on task 40 : loss=1.187 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 65.7%, forg=  9.3% <<<
>>> Test on task 41 : loss=1.559 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 46.6%, forg= 25.2% <<<
>>> Test on task 42 : loss=1.615 | TAw acc= 98.2%, forg= -2.7%| TAg acc= 49.1%, forg= 24.5% <<<
>>> Test on task 43 : loss=1.294 | TAw acc= 91.0%, forg=  0.0%| TAg acc= 69.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_2/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 79.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.0% 
	 95.9%  88.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.3% 
	 95.4%  93.5%  91.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.4% 
	 95.9%  94.4%  95.1%  95.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 96.4%  94.4%  97.1%  93.5%  87.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.8% 
	 95.9%  95.3%  97.1%  93.5%  95.9%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 95.4%  94.4%  97.1%  92.7%  91.7%  97.3%  90.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.1% 
	 95.9%  94.4%  97.1%  92.7%  95.0%  95.6%  92.1%  92.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.4% 
	 96.4%  93.5%  97.1%  92.7%  95.9%  97.3%  91.1%  94.4%  90.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 95.9%  94.4%  97.1%  92.7%  95.9%  96.5%  90.1%  94.4%  92.9%  96.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 95.9%  94.4%  97.1%  93.5%  95.9%  97.3%  90.1%  95.3%  92.0%  98.1%  95.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 96.4%  93.5%  97.1%  92.7%  95.9%  97.3%  91.1%  95.3%  92.0%  99.0%  98.1%  95.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 96.4%  94.4%  97.1%  92.7%  95.9%  97.3%  90.1%  95.3%  92.0%  99.0%  98.1%  95.8%  95.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 96.9%  94.4%  97.1%  93.5%  95.9%  96.5%  90.1%  94.4%  92.0%  99.0%  98.1%  96.6%  99.1%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.4%  94.4%  97.1%  93.5%  95.9%  96.5%  91.1%  95.3%  92.9%  99.0%  98.1%  95.8%  99.1%  94.2%  95.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 95.4%  94.4%  97.1%  93.5%  95.9%  96.5%  91.1%  95.3%  92.9%  99.0%  98.1%  95.8%  98.1%  94.2%  98.0%  98.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 96.4%  94.4%  97.1%  93.5%  95.9%  97.3%  92.1%  95.3%  92.9%  99.0%  98.1%  95.8%  99.1%  94.2%  98.0%  98.2%  85.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.9%  94.4%  97.1%  93.5%  95.9%  97.3%  91.1%  95.3%  92.9%  99.0%  98.1%  95.8%  99.1%  93.5%  98.0%  98.2%  91.1%  89.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 95.9%  94.4%  97.1%  93.5%  95.9%  97.3%  90.1%  95.3%  92.9%  99.0%  98.1%  95.8%  99.1%  93.5%  98.0%  99.1%  88.9%  89.2% 100.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 97.4%  94.4%  97.1%  93.5%  96.7%  97.3%  91.1%  95.3%  92.0%  99.0%  98.1%  95.8%  99.1%  93.5%  98.0%  98.2%  91.1%  89.2%  99.1%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 97.4%  94.4%  97.1%  93.5%  96.7%  98.2%  91.1%  95.3%  92.9%  99.0%  98.1%  95.8%  99.1%  93.5%  98.0%  98.2%  91.1%  89.2%  99.1%  92.9%  96.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.9%  94.4%  97.1%  93.5%  95.9%  98.2%  92.1%  95.3%  92.0%  99.0%  98.1%  96.6%  99.1%  94.2%  98.0%  98.2%  92.2%  89.2%  99.1%  92.9%  96.4%  94.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.4%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.0%  99.0%  97.1%  96.6%  99.1%  94.2%  98.0%  98.2%  91.1%  88.2%  99.1%  92.9%  96.4%  95.0%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.9%  94.4%  97.1%  94.3%  95.9%  97.3%  92.1%  95.3%  92.0%  99.0%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  91.1%  88.2%  99.1%  92.9%  97.3%  97.0%  97.4%  94.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.4%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.9%  99.0%  97.1%  95.8%  99.1%  93.5%  98.0%  98.2%  90.0%  89.2%  99.1%  92.9%  97.3%  96.0%  95.7%  94.5%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.9%  94.4%  97.1%  94.3%  96.7%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  90.0%  89.2%  99.1%  92.9%  97.3%  95.0%  96.6%  95.5%  95.7%  97.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 96.4%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  91.1%  89.2%  99.1%  92.9%  97.3%  97.0%  94.9%  95.5%  96.6%  97.2%  95.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 96.9%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  90.0%  89.2%  99.1%  92.9%  97.3%  97.0%  94.9%  95.5%  97.4%  97.2%  95.3%  98.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 96.9%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  91.1%  89.2%  99.1%  92.9%  96.4%  97.0%  95.7%  95.5%  96.6%  97.2%  95.3%  98.4%  94.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 96.9%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  91.1%  89.2%  99.1%  92.9%  96.4%  97.0%  96.6%  95.5%  97.4%  96.3%  95.3%  99.2%  95.7%  95.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 96.9%  94.4%  97.1%  94.3%  96.7%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  99.1%  94.2%  98.0%  98.2%  91.1%  89.2%  99.1%  92.9%  96.4%  97.0%  95.7%  95.5%  96.6%  96.3%  95.3%  99.2%  95.7%  96.2%  91.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 96.9%  95.3%  97.1%  94.3%  95.9%  98.2%  92.1%  96.3%  92.9%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  98.2%  88.9%  89.2%  99.1%  92.9%  96.4%  96.0%  96.6%  95.5%  96.6%  96.3%  95.3%  99.2%  95.7%  96.2%  92.4%  91.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 95.9%  94.4%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  93.8%  98.1%  97.1%  96.6%  98.1%  94.2%  98.0%  99.1%  91.1%  89.2%  99.1%  92.0%  96.4%  97.0%  96.6%  95.5%  97.4%  95.3%  95.3%  99.2%  95.7%  94.3%  93.3%  94.9%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.9%  96.3%  97.1%  94.3%  95.9%  98.2%  93.1%  96.3%  92.9%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  98.2%  88.9%  89.2%  99.1%  92.9%  96.4%  97.0%  95.7%  96.4%  97.4%  95.3%  95.3%  99.2%  95.7%  96.2%  92.4%  94.9%  95.1%  95.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 96.4%  96.3%  97.1%  94.3%  95.9%  98.2%  92.1%  95.3%  92.9%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  99.1%  90.0%  89.2%  99.1%  92.9%  96.4%  97.0%  96.6%  95.5%  97.4%  94.4%  95.3%  99.2%  95.7%  94.3%  93.3%  94.9%  95.1%  95.8%  96.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.4%  95.3%  96.1%  94.3%  95.9%  98.2%  92.1%  95.3%  93.8%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  99.1%  90.0%  89.2%  99.1%  92.0%  96.4%  97.0%  96.6%  95.5%  96.6%  95.3%  95.3%  99.2%  96.8%  94.3%  93.3%  92.9%  95.1%  95.8%  98.1%  82.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 95.9%  95.3%  97.1%  94.3%  95.9%  98.2%  92.1%  94.4%  93.8%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  98.2%  90.0%  89.2%  99.1%  92.0%  96.4%  97.0%  95.7%  95.5%  97.4%  95.3%  95.3% 100.0%  96.8%  95.2%  93.3%  93.9%  94.2%  96.6%  97.2%  81.7%  95.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 96.9%  95.3%  97.1%  94.3%  96.7%  98.2%  92.1%  94.4%  92.9%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  97.2%  90.0%  90.3%  99.1%  92.0%  96.4%  97.0%  96.6%  97.3%  96.6%  95.3%  95.3%  99.2%  96.8%  95.2%  92.4%  94.9%  95.1%  96.6%  98.1%  82.6%  94.0%  94.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 96.9%  94.4%  97.1%  94.3%  95.9%  98.2%  91.1%  94.4%  93.8%  98.1%  97.1%  96.6%  99.1%  93.5%  98.0%  97.2%  90.0%  89.2%  99.1%  92.0%  96.4%  97.0%  94.9%  95.5%  96.6%  95.3%  95.3%  99.2%  96.8%  95.2%  92.4%  93.9%  96.1%  96.6%  98.1%  81.7%  94.0%  95.3%  96.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 96.4%  96.3%  97.1%  94.3%  95.9%  98.2%  92.1%  94.4%  93.8%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  97.2%  90.0%  88.2%  99.1%  92.0%  96.4%  97.0%  94.9%  95.5%  96.6%  95.3%  95.3%  99.2%  96.8%  95.2%  91.6%  92.9%  96.1%  96.6%  97.2%  80.0%  94.0%  95.3%  97.0%  95.2%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 96.9%  94.4%  97.1%  94.3%  96.7%  98.2%  92.1%  94.4%  93.8%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  97.2%  88.9%  90.3%  99.1%  92.9%  96.4%  97.0%  94.9%  95.5%  96.6%  95.3%  95.3%  99.2%  96.8%  95.2%  92.4%  91.9%  95.1%  96.6%  97.2%  83.5%  94.0%  94.4%  97.0%  96.0%  99.1%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 96.4%  94.4%  97.1%  94.3%  96.7%  98.2%  92.1%  94.4%  92.9%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  97.2%  88.9%  89.2%  99.1%  92.0%  96.4%  97.0%  96.6%  95.5%  96.6%  95.3%  94.5% 100.0%  96.8%  95.2%  93.3%  93.9%  95.1%  96.6%  97.2%  83.5%  94.0%  93.5%  96.0%  96.0%  98.1%  99.0%   0.0%   0.0% 	Avg.: 95.3% 
	 96.9%  96.3%  97.1%  94.3%  95.9%  98.2%  92.1%  94.4%  92.9%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  97.2%  90.0%  89.2%  99.1%  92.0%  96.4%  97.0%  94.9%  95.5%  97.4%  95.3%  94.5% 100.0%  96.8%  95.2%  91.6%  93.9%  95.1%  96.6%  97.2%  80.0%  94.0%  93.5%  94.9%  96.0%  98.1%  98.1%  95.5%   0.0% 	Avg.: 95.2% 
	 96.9%  96.3%  97.1%  94.3%  95.9%  98.2%  92.1%  94.4%  92.9%  98.1%  97.1%  96.6%  98.1%  93.5%  98.0%  97.2%  90.0%  89.2%  99.1%  92.0%  96.4%  97.0%  95.7%  95.5%  97.4%  95.3%  94.5% 100.0%  96.8%  95.2%  91.6%  93.9%  95.1%  96.6%  97.2%  81.7%  95.0%  93.5%  94.9%  96.0%  98.1%  97.1%  98.2%  91.0% 	Avg.: 95.3% 
************************************************************************************************************
TAg Acc
	 79.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.0% 
	 81.0%  85.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 83.0% 
	 86.2%  90.7%  78.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 85.1% 
	 83.1%  83.2%  71.8%  79.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.4% 
	 77.9%  90.7%  82.5%  73.2%  79.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.7% 
	 85.6%  88.8%  85.4%  62.6%  81.0%  82.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 81.0% 
	 81.0%  86.0%  82.5%  76.4%  78.5%  79.6%  66.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 78.6% 
	 80.0%  90.7%  76.7%  78.0%  81.8%  77.0%  64.4%  74.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.9% 
	 81.5%  85.0%  87.4%  85.4%  86.0%  86.7%  68.3%  72.9%  66.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.0% 
	 83.6%  84.1%  87.4%  69.1%  77.7%  81.4%  71.3%  72.9%  68.1%  75.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.1% 
	 82.1%  89.7%  86.4%  78.0%  84.3%  89.4%  71.3%  84.1%  81.4%  53.8%  85.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.6% 
	 79.0%  88.8%  85.4%  77.2%  85.1%  85.8%  73.3%  82.2%  81.4%  71.2%  70.2%  80.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.0% 
	 75.9%  86.9%  87.4%  77.2%  77.7%  87.6%  72.3%  86.0%  79.6%  77.9%  80.8%  71.4%  82.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.2% 
	 77.9%  90.7%  88.3%  76.4%  80.2%  80.5%  67.3%  76.6%  81.4%  70.2%  83.7%  64.7%  82.1%  82.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 78.8% 
	 77.4%  81.3%  90.3%  79.7%  83.5%  77.9%  75.2%  82.2%  84.1%  66.3%  84.6%  74.8%  86.8%  63.0%  77.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 78.9% 
	 75.9%  86.9%  91.3%  78.0%  86.0%  79.6%  74.3%  86.0%  83.2%  67.3%  78.8%  69.7%  88.7%  71.0%  56.0%  78.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 78.2% 
	 79.0%  89.7%  88.3%  78.9%  85.1%  81.4%  73.3%  81.3%  83.2%  69.2%  85.6%  67.2%  91.5%  71.0%  58.0%  53.2%  74.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.1% 
	 78.5%  85.0%  88.3%  79.7%  77.7%  85.8%  81.2%  83.2%  83.2%  72.1%  80.8%  72.3%  87.7%  71.7%  64.0%  59.6%  65.6%  77.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.4% 
	 75.9%  86.9%  89.3%  81.3%  81.8%  85.0%  70.3%  83.2%  84.1%  71.2%  86.5%  67.2%  91.5%  73.2%  63.0%  57.8%  70.0%  64.5%  79.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.0% 
	 73.8%  84.1%  85.4%  78.0%  87.6%  85.0%  76.2%  86.0%  80.5%  73.1%  81.7%  69.7%  84.0%  71.0%  67.0%  59.6%  70.0%  66.7%  61.5%  78.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.0% 
	 74.9%  82.2%  88.3%  73.2%  81.8%  83.2%  78.2%  73.8%  84.1%  75.0%  77.9%  69.7%  87.7%  73.9%  63.0%  64.2%  72.2%  63.4%  65.8%  58.0%  70.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.3% 
	 73.3%  75.7%  88.3%  80.5%  78.5%  81.4%  73.3%  84.1%  82.3%  79.8%  85.6%  70.6%  94.3%  73.2%  65.0%  57.8%  76.7%  73.1%  62.4%  55.4%  46.4%  76.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.3% 
	 75.4%  84.1%  88.3%  74.8%  85.1%  82.3%  73.3%  85.0%  85.0%  73.1%  79.8%  71.4%  90.6%  73.2%  71.0%  61.5%  74.4%  63.4%  69.2%  57.1%  48.2%  57.0%  80.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.1% 
	 71.8%  83.2%  84.5%  76.4%  86.8%  75.2%  74.3%  85.0%  82.3%  74.0%  82.7%  69.7%  94.3%  71.7%  73.0%  60.6%  73.3%  65.6%  66.7%  60.7%  50.9%  64.0%  69.2%  75.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.8% 
	 74.4%  84.1%  86.4%  78.0%  79.3%  82.3%  76.2%  72.9%  83.2%  75.0%  85.6%  59.7%  81.1%  67.4%  72.0%  66.1%  67.8%  67.7%  70.9%  59.8%  59.8%  63.0%  70.9%  56.4%  82.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.9% 
	 72.3%  86.0%  85.4%  77.2%  84.3%  82.3%  72.3%  81.3%  77.0%  69.2%  83.7%  69.7%  91.5%  71.0%  70.0%  56.9%  74.4%  67.7%  69.2%  62.5%  56.2%  65.0%  70.1%  62.7%  54.7%  76.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.7% 
	 74.4%  83.2%  84.5%  73.2%  75.2%  81.4%  70.3%  79.4%  83.2%  73.1%  84.6%  70.6%  88.7%  69.6%  76.0%  63.3%  68.9%  69.9%  73.5%  67.0%  60.7%  62.0%  70.9%  69.1%  56.4%  61.7%  68.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.6% 
	 74.4%  81.3%  83.5%  75.6%  79.3%  83.2%  71.3%  78.5%  82.3%  70.2%  83.7%  68.9%  90.6%  67.4%  71.0%  64.2%  72.2%  71.0%  65.8%  65.2%  51.8%  69.0%  70.9%  69.1%  59.0%  57.0%  52.3%  84.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.9% 
	 75.4%  77.6%  83.5%  74.8%  72.7%  81.4%  74.3%  80.4%  79.6%  76.9%  85.6%  68.1%  86.8%  71.0%  70.0%  63.3%  70.0%  73.1%  75.2%  58.0%  59.8%  64.0%  70.1%  72.7%  58.1%  63.6%  56.2%  75.2%  65.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.8% 
	 76.4%  81.3%  82.5%  73.2%  80.2%  77.0%  76.2%  65.4%  83.2%  70.2%  85.6%  68.9%  88.7%  69.6%  68.0%  60.6%  70.0%  67.7%  69.2%  59.8%  60.7%  72.0%  69.2%  70.9%  61.5%  68.2%  57.8%  80.6%  54.8%  66.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.2% 
	 73.3%  86.9%  82.5%  74.0%  77.7%  81.4%  73.3%  78.5%  82.3%  72.1%  82.7%  67.2%  88.7%  67.4%  70.0%  67.0%  72.2%  64.5%  71.8%  60.7%  60.7%  69.0%  67.5%  69.1%  57.3%  71.0%  59.4%  76.7%  59.1%  36.2%  82.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.1% 
	 74.9%  83.2%  84.5%  74.8%  78.5%  79.6%  73.3%  78.5%  82.3%  69.2%  86.5%  68.1%  87.7%  69.6%  69.0%  64.2%  67.8%  65.6%  72.6%  67.0%  61.6%  63.0%  70.1%  70.0%  64.1%  69.2%  63.3%  78.3%  54.8%  39.0%  74.8%  79.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.4% 
	 73.3%  78.5%  83.5%  71.5%  81.0%  82.3%  71.3%  72.9%  82.3%  68.3%  82.7%  68.9%  85.8%  69.6%  66.0%  57.8%  67.8%  67.7%  70.9%  66.1%  62.5%  70.0%  72.6%  71.8%  59.0%  70.1%  64.1%  79.1%  61.3%  41.0%  71.4%  62.6%  74.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.6% 
	 72.8%  83.2%  81.6%  74.0%  73.6%  79.6%  71.3%  74.8%  83.2%  73.1%  83.7%  67.2%  89.6%  64.5%  70.0%  65.1%  67.8%  63.4%  66.7%  66.1%  59.8%  67.0%  64.1%  74.5%  60.7%  67.3%  60.9%  79.1%  63.4%  41.9%  70.6%  67.7%  55.3%  76.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.0% 
	 74.4%  87.9%  83.5%  73.2%  76.9%  80.5%  68.3%  68.2%  81.4%  69.2%  83.7%  63.9%  85.8%  63.8%  72.0%  64.2%  66.7%  67.7%  66.7%  65.2%  58.0%  70.0%  68.4%  73.6%  61.5%  64.5%  65.6%  79.1%  68.8%  42.9%  69.7%  62.6%  55.3%  65.3%  83.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.0% 
	 70.3%  80.4%  81.6%  71.5%  71.1%  78.8%  70.3%  77.6%  81.4%  72.1%  85.6%  67.2%  84.9%  65.9%  68.0%  61.5%  73.3%  64.5%  72.6%  67.0%  58.0%  68.0%  69.2%  69.1%  60.7%  70.1%  62.5%  82.9%  62.4%  46.7%  70.6%  62.6%  51.5%  61.9%  54.7%  66.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 73.3%  73.8%  80.6%  74.8%  70.2%  79.6%  69.3%  67.3%  83.2%  71.2%  82.7%  65.5%  84.9%  66.7%  70.0%  65.1%  71.1%  68.8%  70.9%  62.5%  58.0%  68.0%  69.2%  74.5%  59.8%  69.2%  61.7%  82.9%  64.5%  47.6%  68.9%  65.7%  56.3%  62.7%  53.8%  42.6%  65.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 68.2%  79.4%  78.6%  75.6%  71.9%  78.8%  71.3%  71.0%  83.2%  69.2%  82.7%  67.2%  83.0%  65.2%  75.0%  64.2%  72.2%  62.4%  72.6%  62.5%  52.7%  72.0%  68.4%  76.4%  61.5%  69.2%  62.5%  82.9%  57.0%  51.4%  65.5%  65.7%  59.2%  63.6%  57.5%  50.4%  44.0%  67.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.9% 
	 73.8%  80.4%  79.6%  74.8%  64.5%  79.6%  67.3%  70.1%  83.2%  75.0%  81.7%  66.4%  85.8%  65.2%  74.0%  61.5%  68.9%  65.6%  68.4%  61.6%  55.4%  69.0%  69.2%  76.4%  63.2%  67.3%  59.4%  82.9%  65.6%  46.7%  68.9%  73.7%  57.3%  67.8%  52.8%  51.3%  52.0%  54.2%  77.8%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 71.8%  76.6%  79.6%  71.5%  76.9%  77.9%  70.3%  75.7%  83.2%  66.3%  79.8%  67.2%  85.8%  64.5%  72.0%  66.1%  71.1%  67.7%  73.5%  66.1%  55.4%  66.0%  67.5%  71.8%  61.5%  67.3%  58.6%  79.8%  61.3%  47.6%  72.3%  70.7%  59.2%  62.7%  52.8%  49.6%  53.0%  55.1%  59.6%  78.6%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.9% 
	 73.3%  80.4%  79.6%  74.8%  74.4%  79.6%  65.3%  72.9%  82.3%  69.2%  80.8%  65.5%  86.8%  65.2%  72.0%  65.1%  66.7%  64.5%  69.2%  59.8%  55.4%  71.0%  68.4%  71.8%  59.8%  64.5%  65.6%  81.4%  64.5%  48.6%  65.5%  70.7%  63.1%  69.5%  53.8%  47.8%  53.0%  54.2%  62.6%  56.3%  75.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 73.3%  79.4%  79.6%  72.4%  76.9%  80.5%  69.3%  71.0%  82.3%  67.3%  81.7%  68.1%  84.9%  67.4%  68.0%  62.4%  66.7%  60.2%  75.2%  59.8%  55.4%  71.0%  70.1%  71.8%  62.4%  70.1%  62.5%  81.4%  66.7%  48.6%  68.1%  73.7%  55.3%  63.6%  54.7%  47.0%  60.0%  59.8%  65.7%  61.1%  61.1%  71.8%   0.0%   0.0% 	Avg.: 67.8% 
	 72.3%  79.4%  81.6%  72.4%  77.7%  79.6%  71.3%  68.2%  81.4%  62.5%  80.8%  66.4%  86.8%  65.9%  68.0%  64.2%  68.9%  65.6%  71.8%  66.1%  53.6%  70.0%  71.8%  71.8%  61.5%  67.3%  67.2%  80.6%  63.4%  51.4%  67.2%  63.6%  59.2%  62.7%  52.8%  47.8%  61.0%  52.3%  61.6%  65.1%  61.1%  52.4%  73.6%   0.0% 	Avg.: 67.2% 
	 72.8%  75.7%  78.6%  73.2%  76.0%  79.6%  65.3%  70.1%  82.3%  64.4%  81.7%  65.5%  84.0%  63.8%  72.0%  65.1%  68.9%  64.5%  73.5%  64.3%  55.4%  60.0%  69.2%  69.1%  60.7%  64.5%  59.4%  78.3%  63.4%  46.7%  66.4%  64.6%  57.3%  64.4%  55.7%  47.0%  59.0%  52.3%  68.7%  61.1%  65.7%  46.6%  49.1%  69.4% 	Avg.: 65.8% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	-16.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:-16.9% 
	  0.5%  -4.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.1% 
	  0.0%  -0.9%  -3.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.6% 
	 -0.5%   0.0%  -1.9%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	  0.5%  -0.9%   0.0%   1.6%  -8.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.4% 
	  1.0%   0.9%   0.0%   2.4%   4.1%  -4.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.5%   0.9%   0.0%   2.4%   0.8%   1.8%  -2.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.0%   1.9%   0.0%   2.4%   0.0%   0.0%   1.0%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   0.9%   0.0%   2.4%   0.0%   0.9%   2.0%   0.0%  -2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.5%   0.9%   0.0%   1.6%   0.0%   0.0%   2.0%  -0.9%   0.9%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.0%   1.9%   0.0%   2.4%   0.0%   0.0%   1.0%   0.0%   0.9%  -1.0%  -2.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.0%   0.9%   0.0%   2.4%   0.0%   0.0%   2.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	 -0.5%   0.9%   0.0%   1.6%   0.0%   0.9%   2.0%   0.9%   0.9%   0.0%   0.0%  -0.8%  -3.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.5%   0.9%   0.0%   1.6%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.5%   0.9%   0.0%   1.6%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.9%   0.0%  -3.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.5%   0.9%   0.0%   1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.0%   0.9%   0.0%   1.6%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.7%   0.0%   0.0%  -5.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.0% 
	  1.0%   0.9%   0.0%   1.6%   0.0%   0.0%   2.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.7%   0.0%  -0.9%   2.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	 -0.5%   0.9%   0.0%   1.6%  -0.8%   0.0%   1.0%   0.0%   0.9%   0.0%   0.0%   0.8%   0.0%   0.7%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.0%   0.9%   0.0%   1.6%   0.0%  -0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.7%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.5%   0.9%   0.0%   1.6%   0.8%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%  -1.1%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.0%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.1%   1.1%   0.9%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   0.9%   0.0%   0.8%   0.8%   0.9%   0.0%   0.0%   0.9%   0.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   1.1%   1.1%   0.9%   0.0%  -0.9%  -2.0%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.0%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.8%   0.0%   0.7%   0.0%   0.9%   2.2%   0.0%   0.9%   0.0%   0.0%   1.0%   1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.5%   0.9%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   2.2%   0.0%   0.9%   0.0%   0.0%   2.0%   0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.0%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   1.1%   0.0%   0.9%   0.0%   0.0%   0.0%   2.6%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   2.2%   0.0%   0.9%   0.0%   0.0%   0.0%   2.6%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   1.1%   0.0%   0.9%   0.0%   0.9%   0.0%   1.7%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   1.1%   0.0%   0.9%   0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%  -0.8%  -1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.5%   0.9%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.1%   0.0%   0.9%   0.0%   0.9%   0.0%   1.7%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   0.0%   0.0%   0.8%   0.8%   0.0%   0.0%  -0.9%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   0.9%   3.3%   0.0%   0.9%   0.0%   0.9%   1.0%   0.9%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.5%   0.9%   0.0%   0.8%   0.8%   0.0%   0.0%   0.9%  -0.9%   1.0%   1.0%   0.0%   0.9%   0.0%   0.0%   0.0%   1.1%   0.0%   0.9%   0.9%   0.9%   0.0%   0.9%   0.0%   0.0%   1.9%   0.0%   0.0%   0.0%   1.9%  -0.8%  -3.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%  -0.9%   0.0%   0.8%   0.8%   0.0%  -1.0%   0.0%   0.9%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   0.9%   3.3%   0.0%   0.9%   0.0%   0.9%   0.0%   1.7%  -0.9%   0.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.0%   0.0%   0.0%   0.8%   0.8%   0.0%   1.0%   0.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   0.0%   2.2%   0.0%   0.9%   0.0%   0.9%   0.0%   0.9%   0.9%   0.0%   2.8%   0.0%   0.0%   0.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.0%   0.9%   1.0%   0.8%   0.8%   0.0%   1.0%   0.9%   0.0%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   0.0%   2.2%   0.0%   0.9%   0.9%   0.9%   0.0%   0.9%   0.9%   0.9%   1.9%   0.0%   0.0%  -1.1%   1.9%   0.0%   2.0%   0.0%   0.0%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.5%   0.9%   0.0%   0.8%   0.8%   0.0%   1.0%   1.9%   0.0%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   0.9%   2.2%   0.0%   0.9%   0.9%   0.9%   0.0%   1.7%   0.9%   0.0%   1.9%   0.0%  -0.8%   0.0%   1.0%   0.0%   1.0%   1.0%  -0.8%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.5%   0.9%   0.0%   0.8%   0.0%   0.0%   1.0%   1.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   1.8%   2.2%  -1.1%   0.9%   0.9%   0.9%   0.0%   0.9%  -0.9%   0.9%   1.9%   0.0%   0.8%   0.0%   1.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.5%   1.9%   0.0%   0.8%   0.8%   0.0%   2.0%   1.9%   0.0%   1.0%   1.0%   0.0%   0.0%   0.7%   0.0%   1.8%   2.2%   1.1%   0.9%   0.9%   0.9%   0.0%   2.6%   1.8%   0.9%   1.9%   0.0%   0.8%   0.0%   1.0%   0.8%   1.0%  -1.0%   0.0%   0.0%   0.9%   1.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.0%   0.0%   0.0%   0.8%   0.8%   0.0%   1.0%   1.9%   0.0%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   1.8%   2.2%   2.2%   0.9%   0.9%   0.9%   0.0%   2.6%   1.8%   0.9%   1.9%   0.0%   0.8%   0.0%   1.0%   1.7%   2.0%   0.0%   0.0%   0.9%   2.6%   1.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.5%   1.9%   0.0%   0.8%   0.0%   0.0%   1.0%   1.9%   0.0%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   1.8%   3.3%   0.0%   0.9%   0.0%   0.9%   0.0%   2.6%   1.8%   0.9%   1.9%   0.0%   0.8%   0.0%   1.0%   0.8%   3.0%   1.0%   0.0%   0.9%  -0.9%   1.0%   0.9%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.0%   1.9%   0.0%   0.8%   0.0%   0.0%   1.0%   1.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   1.8%   3.3%   1.1%   0.9%   0.9%   0.9%   0.0%   0.9%   1.8%   0.9%   1.9%   0.8%   0.0%   0.0%   1.0%   0.0%   1.0%   1.0%   0.0%   0.9%   0.0%   1.0%   1.9%   1.0%   0.0%   0.9%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.5%   0.0%   0.0%   0.8%   0.8%   0.0%   1.0%   1.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   1.8%   2.2%   1.1%   0.9%   0.9%   0.9%   0.0%   2.6%   1.8%   0.0%   1.9%   0.8%   0.0%   0.0%   1.0%   1.7%   1.0%   1.0%   0.0%   0.9%   3.5%   1.0%   1.9%   2.0%   0.0%   0.9%   1.0%   0.0%   0.0% 	Avg.:  1.0% 
	  0.5%   0.0%   0.0%   0.8%   0.8%   0.0%   1.0%   1.9%   0.9%   1.0%   1.0%   0.0%   0.9%   0.7%   0.0%   1.8%   2.2%   1.1%   0.9%   0.9%   0.9%   0.0%   1.7%   1.8%   0.0%   1.9%   0.8%   0.0%   0.0%   1.0%   1.7%   1.0%   1.0%   0.0%   0.9%   1.7%   0.0%   1.9%   2.0%   0.0%   0.9%   1.9%  -2.7%   0.0% 	Avg.:  0.8% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -2.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.1% 
	 -5.1%  -5.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -5.4% 
	  3.1%   7.5%   6.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  8.2%   0.0%  -3.9%   6.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.7% 
	  0.5%   1.9%  -2.9%  17.1%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.0% 
	  5.1%   4.7%   2.9%   3.3%   2.5%   2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.5% 
	  6.2%   0.0%   8.7%   1.6%  -0.8%   5.3%   2.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.3% 
	  4.6%   5.6%  -1.9%  -5.7%  -4.1%  -4.4%  -2.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.8% 
	  2.6%   6.5%   0.0%  16.3%   8.3%   5.3%  -3.0%   1.9%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.0% 
	  4.1%   0.9%   1.0%   7.3%   1.7%  -2.7%   0.0%  -9.3% -13.3%  21.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  7.2%   1.9%   1.9%   8.1%   0.8%   3.5%  -2.0%   1.9%   0.0%   3.8%  15.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.9% 
	 10.3%   3.7%   0.0%   8.1%   8.3%   1.8%   1.0%  -1.9%   1.8%  -2.9%   4.8%   9.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.7% 
	  8.2%   0.0%  -1.0%   8.9%   5.8%   8.8%   5.9%   9.3%   0.0%   7.7%   1.9%  16.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.5% 
	  8.7%   9.3%  -1.9%   5.7%   2.5%  11.5%  -2.0%   3.7%  -2.7%  11.5%   1.0%   5.9%  -4.7%  19.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.9% 
	 10.3%   3.7%  -1.0%   7.3%   0.0%   9.7%   1.0%   0.0%   0.9%  10.6%   6.7%  10.9%  -1.9%  11.6%  21.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	  7.2%   0.9%   2.9%   6.5%   0.8%   8.0%   2.0%   4.7%   0.9%   8.7%   0.0%  13.4%  -2.8%  11.6%  19.0%  24.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.8% 
	  7.7%   5.6%   2.9%   5.7%   8.3%   3.5%  -5.9%   2.8%   0.9%   5.8%   4.8%   8.4%   3.8%  10.9%  13.0%  18.3%   8.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.2% 
	 10.3%   3.7%   1.9%   4.1%   4.1%   4.4%  10.9%   2.8%   0.0%   6.7%  -1.0%  13.4%   0.0%   9.4%  14.0%  20.2%   4.4%  12.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.8% 
	 12.3%   6.5%   5.8%   7.3%  -1.7%   4.4%   5.0%   0.0%   3.5%   4.8%   4.8%  10.9%   7.5%  11.6%  10.0%  18.3%   4.4%  10.8%  17.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.6% 
	 11.3%   8.4%   2.9%  12.2%   5.8%   6.2%   3.0%  12.1%   0.0%   2.9%   8.7%  10.9%   3.8%   8.7%  14.0%  13.8%   2.2%  14.0%  13.7%  20.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.8% 
	 12.8%  15.0%   2.9%   4.9%   9.1%   8.0%   7.9%   1.9%   1.8%  -1.9%   1.0%  10.1%  -2.8%   9.4%  12.0%  20.2%  -2.2%   4.3%  17.1%  23.2%  24.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 10.8%   6.5%   2.9%  10.6%   2.5%   7.1%   7.9%   0.9%  -0.9%   6.7%   6.7%   9.2%   3.8%   9.4%   6.0%  16.5%   2.2%  14.0%  10.3%  21.4%  22.3%  19.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.9% 
	 14.4%   7.5%   6.8%   8.9%   0.8%  14.2%   6.9%   0.9%   2.7%   5.8%   3.8%  10.9%   0.0%  10.9%   4.0%  17.4%   3.3%  11.8%  12.8%  17.9%  19.6%  12.0%  11.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.9% 
	 11.8%   6.5%   4.9%   7.3%   8.3%   7.1%   5.0%  13.1%   1.8%   4.8%   1.0%  21.0%  13.2%  15.2%   5.0%  11.9%   8.9%   9.7%   8.5%  18.8%  10.7%  13.0%   9.4%  19.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 13.8%   4.7%   5.8%   8.1%   3.3%   7.1%   8.9%   4.7%   8.0%  10.6%   2.9%  10.9%   2.8%  11.6%   7.0%  21.1%   2.2%   9.7%  10.3%  16.1%  14.3%  11.0%  10.3%  12.7%  27.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 11.8%   7.5%   6.8%  12.2%  12.4%   8.0%  10.9%   6.5%   1.8%   6.7%   1.9%  10.1%   5.7%  13.0%   1.0%  14.7%   7.8%   7.5%   6.0%  11.6%   9.8%  14.0%   9.4%   6.4%  25.6%  15.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 11.8%   9.3%   7.8%   9.8%   8.3%   6.2%   9.9%   7.5%   2.7%   9.6%   2.9%  11.8%   3.8%  15.2%   6.0%  13.8%   4.4%   6.5%  13.7%  13.4%  18.8%   7.0%   9.4%   6.4%  23.1%  19.6%  16.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.2% 
	 10.8%  13.1%   7.8%  10.6%  14.9%   8.0%   6.9%   5.6%   5.3%   2.9%   1.0%  12.6%   7.5%  11.6%   7.0%  14.7%   6.7%   4.3%   4.3%  20.5%  10.7%  12.0%  10.3%   2.7%  23.9%  13.1%  12.5%   9.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.7% 
	  9.7%   9.3%   8.7%  12.2%   7.4%  12.4%   5.0%  20.6%   1.8%   9.6%   1.0%  11.8%   5.7%  13.0%   9.0%  17.4%   6.7%   9.7%  10.3%  18.8%   9.8%   4.0%  11.1%   4.5%  20.5%   8.4%  10.9%   3.9%  10.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 12.8%   3.7%   8.7%  11.4%   9.9%   8.0%   7.9%   7.5%   2.7%   7.7%   3.8%  13.4%   5.7%  15.2%   7.0%  11.0%   4.4%  12.9%   7.7%  17.9%   9.8%   7.0%  12.8%   6.4%  24.8%   5.6%   9.4%   7.8%   6.5%  30.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.0% 
	 11.3%   7.5%   6.8%  10.6%   9.1%   9.7%   7.9%   7.5%   2.7%  10.6%   0.0%  12.6%   6.6%  13.0%   8.0%  13.8%   8.9%  11.8%   6.8%  11.6%   8.9%  13.0%  10.3%   5.5%  17.9%   7.5%   5.5%   6.2%  10.8%  27.6%   7.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.6% 
	 12.8%  12.1%   7.8%  13.8%   6.6%   7.1%   9.9%  13.1%   2.7%  11.5%   3.8%  11.8%   8.5%  13.0%  11.0%  20.2%   8.9%   9.7%   8.5%  12.5%   8.0%   6.0%   7.7%   3.6%  23.1%   6.5%   4.7%   5.4%   4.3%  25.7%  10.9%  17.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.3% 
	 13.3%   7.5%   9.7%  11.4%  14.0%   9.7%   9.9%  11.2%   1.8%   6.7%   2.9%  13.4%   4.7%  18.1%   7.0%  12.8%   8.9%  14.0%  12.8%  12.5%  10.7%   9.0%  16.2%   0.9%  21.4%   9.3%   7.8%   5.4%   2.2%  24.8%  11.8%  12.1%  19.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.7% 
	 11.8%   2.8%   7.8%  12.2%  10.7%   8.8%  12.9%  17.8%   3.5%  10.6%   2.9%  16.8%   8.5%  18.8%   5.0%  13.8%  10.0%   9.7%  12.8%  13.4%  12.5%   6.0%  12.0%   1.8%  20.5%  12.1%   3.1%   5.4%  -3.2%  23.8%  12.6%  17.2%  19.4%  11.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.7% 
	 15.9%  10.3%   9.7%  13.8%  16.5%  10.6%  10.9%   8.4%   3.5%   7.7%   1.0%  13.4%   9.4%  16.7%   9.0%  16.5%   3.3%  12.9%   6.8%  11.6%  12.5%   8.0%  11.1%   6.4%  21.4%   6.5%   6.2%   1.6%   6.5%  20.0%  11.8%  17.2%  23.3%  14.4%  28.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.5% 
	 12.8%  16.8%  10.7%  10.6%  17.4%   9.7%  11.9%  18.7%   1.8%   8.7%   3.8%  15.1%   9.4%  15.9%   7.0%  12.8%   5.6%   8.6%   8.5%  16.1%  12.5%   8.0%  11.1%   0.9%  22.2%   7.5%   7.0%   1.6%   4.3%  19.0%  13.4%  14.1%  18.4%  13.6%  29.2%  23.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.9% 
	 17.9%  11.2%  12.6%   9.8%  15.7%  10.6%   9.9%  15.0%   1.8%  10.6%   3.8%  13.4%  11.3%  17.4%   2.0%  13.8%   4.4%  15.1%   6.8%  16.1%  17.9%   4.0%  12.0%  -0.9%  20.5%   7.5%   6.2%   1.6%  11.8%  15.2%  16.8%  14.1%  15.5%  12.7%  25.5%  15.7%  21.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.8% 
	 12.3%  10.3%  11.7%  10.6%  23.1%   9.7%  13.9%  15.9%   1.8%   4.8%   4.8%  14.3%   8.5%  17.4%   3.0%  16.5%   7.8%  11.8%  11.1%  17.0%  15.2%   7.0%  11.1%   0.0%  18.8%   9.3%   9.4%   1.6%   3.2%  20.0%  13.4%   6.1%  17.5%   8.5%  30.2%  14.8%  13.0%  13.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.5% 
	 14.4%  14.0%  11.7%  13.8%  10.7%  11.5%  10.9%  10.3%   1.8%  13.5%   6.7%  13.4%   8.5%  18.1%   5.0%  11.9%   5.6%   9.7%   6.0%  12.5%  15.2%  10.0%  12.8%   4.5%  20.5%   9.3%  10.2%   4.7%   7.5%  19.0%  10.1%   9.1%  15.5%  13.6%  30.2%  16.5%  12.0%  12.1%  18.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 11.8% 
	 12.8%  10.3%  11.7%  10.6%  13.2%   9.7%  15.8%  13.1%   2.7%  10.6%   5.8%  15.1%   7.5%  17.4%   5.0%  12.8%  10.0%  12.9%  10.3%  18.8%  15.2%   5.0%  12.0%   4.5%  22.2%  12.1%   3.1%   3.1%   4.3%  18.1%  16.8%   9.1%  11.7%   6.8%  29.2%  18.3%  12.0%  13.1%  15.2%  22.2%   0.0%   0.0%   0.0%   0.0% 	Avg.: 12.0% 
	 12.8%  11.2%  11.7%  13.0%  10.7%   8.8%  11.9%  15.0%   2.7%  12.5%   4.8%  12.6%   9.4%  15.2%   9.0%  15.6%  10.0%  17.2%   4.3%  18.8%  15.2%   5.0%  10.3%   4.5%  19.7%   6.5%   6.2%   3.1%   2.2%  18.1%  14.3%   6.1%  19.4%  12.7%  28.3%  19.1%   5.0%   7.5%  12.1%  17.5%  13.9%   0.0%   0.0%   0.0% 	Avg.: 11.6% 
	 13.8%  11.2%   9.7%  13.0%   9.9%   9.7%   9.9%  17.8%   3.5%  17.3%   5.8%  14.3%   7.5%  16.7%   9.0%  13.8%   7.8%  11.8%   7.7%  12.5%  17.0%   6.0%   8.5%   4.5%  20.5%   9.3%   1.6%   3.9%   5.4%  15.2%  15.1%  16.2%  15.5%  13.6%  30.2%  18.3%   4.0%  15.0%  16.2%  13.5%  13.9%  19.4%   0.0%   0.0% 	Avg.: 12.0% 
	 13.3%  15.0%  12.6%  12.2%  11.6%   9.7%  15.8%  15.9%   2.7%  15.4%   4.8%  15.1%  10.4%  18.8%   5.0%  12.8%   7.8%  12.9%   6.0%  14.3%  15.2%  16.0%  11.1%   7.3%  21.4%  12.1%   9.4%   6.2%   5.4%  20.0%  16.0%  15.2%  17.5%  11.9%  27.4%  19.1%   6.0%  15.0%   9.1%  17.5%   9.3%  25.2%  24.5%   0.0% 	Avg.: 13.2% 
************************************************************************************************************
[Elapsed time = 0.5 h]
Done!

f1_score_micro: 0.6599552572706935
f1_score_macro: 0.6243750821033872
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         4
           1       0.50      0.50      0.50         4
           2       0.67      0.67      0.67         9
           3       1.00      1.00      1.00         4
           4       0.50      0.50      0.50         4
           5       1.00      0.67      0.80         9
           6       1.00      1.00      1.00         4
           7       0.89      0.89      0.89         9
           8       1.00      0.60      0.75         5
           9       0.00      0.00      0.00         4
          10       0.75      0.75      0.75         4
          11       1.00      1.00      1.00         5
          12       1.00      0.67      0.80         9
          13       1.00      1.00      1.00         4
          14       0.57      0.44      0.50         9
          15       0.50      0.75      0.60         4
          16       0.40      0.50      0.44         4
          17       0.15      0.40      0.22         5
          18       0.82      1.00      0.90         9
          19       0.57      1.00      0.73         4
          20       1.00      1.00      1.00         4
          21       0.80      0.89      0.84         9
          22       0.80      1.00      0.89         4
          23       0.62      0.89      0.73         9
          24       0.67      0.50      0.57         4
          25       0.80      1.00      0.89         4
          26       0.40      0.44      0.42         9
          27       0.20      0.25      0.22         4
          28       1.00      1.00      1.00         9
          29       0.00      0.00      0.00         4
          30       0.50      0.60      0.55         5
          31       0.38      0.75      0.50         4
          32       1.00      1.00      1.00         9
          33       0.40      0.50      0.44         4
          34       0.80      0.80      0.80         5
          35       1.00      0.75      0.86         4
          36       0.50      0.25      0.33         4
          37       1.00      0.50      0.67         4
          38       0.75      0.67      0.71         9
          39       1.00      0.80      0.89         5
          40       0.00      0.00      0.00         4
          41       1.00      0.89      0.94         9
          42       0.80      1.00      0.89         4
          43       0.67      1.00      0.80         4
          44       0.90      1.00      0.95         9
          45       0.75      0.75      0.75         4
          46       1.00      0.75      0.86         4
          47       0.12      0.75      0.21         4
          48       1.00      0.89      0.94         9
          49       0.06      0.25      0.10         4
          50       0.44      1.00      0.62         4
          51       1.00      0.88      0.93         8
          52       1.00      0.80      0.89         5
          53       1.00      0.75      0.86         4
          54       0.29      0.50      0.36         4
          55       0.80      1.00      0.89         4
          56       0.40      0.50      0.44         4
          57       0.80      1.00      0.89         4
          58       0.90      1.00      0.95         9
          59       0.30      0.60      0.40         5
          60       0.00      0.00      0.00         4
          61       1.00      0.75      0.86         4
          62       0.83      1.00      0.91         5
          63       1.00      0.89      0.94         9
          64       1.00      0.75      0.86         4
          65       1.00      0.75      0.86         4
          66       1.00      1.00      1.00         4
          67       0.43      0.75      0.55         4
          68       0.57      1.00      0.73         4
          69       1.00      0.40      0.57         5
          70       0.90      1.00      0.95         9
          71       0.15      0.50      0.24         4
          72       0.58      0.78      0.67         9
          73       0.80      1.00      0.89         4
          74       0.25      0.50      0.33         4
          75       0.45      0.56      0.50         9
          76       0.50      1.00      0.67         4
          77       0.00      0.00      0.00         4
          78       0.67      1.00      0.80         4
          79       0.56      1.00      0.72         9
          80       1.00      0.75      0.86         4
          81       0.50      0.33      0.40         9
          82       1.00      1.00      1.00         4
          83       1.00      0.75      0.86         4
          84       0.90      1.00      0.95         9
          85       1.00      0.60      0.75         5
          86       0.44      0.80      0.57         5
          87       1.00      0.67      0.80         9
          88       0.88      0.78      0.82         9
          89       0.88      0.78      0.82         9
          90       0.43      0.75      0.55         4
          91       0.60      0.75      0.67         4
          92       0.60      0.60      0.60         5
          93       1.00      0.89      0.94         9
          94       0.50      0.75      0.60         4
          95       0.75      0.75      0.75         4
          96       0.60      0.43      0.50         7
          97       0.53      0.89      0.67         9
          98       1.00      1.00      1.00         5
          99       0.82      1.00      0.90         9
         100       0.67      0.44      0.53         9
         101       0.45      1.00      0.62         9
         102       0.29      0.40      0.33         5
         103       1.00      0.80      0.89         5
         104       0.41      0.78      0.54         9
         105       0.90      1.00      0.95         9
         106       1.00      0.75      0.86         4
         107       1.00      1.00      1.00         4
         108       0.56      0.56      0.56         9
         109       0.00      0.00      0.00         4
         110       0.67      1.00      0.80         4
         111       1.00      0.50      0.67         4
         112       1.00      1.00      1.00         4
         113       0.80      1.00      0.89         4
         114       0.80      1.00      0.89         4
         115       0.83      1.00      0.91         5
         116       0.67      1.00      0.80         4
         117       1.00      0.89      0.94         9
         118       0.00      0.00      0.00         4
         119       0.80      0.89      0.84         9
         120       0.57      1.00      0.73         4
         121       1.00      0.89      0.94         9
         122       0.71      1.00      0.83         5
         123       1.00      1.00      1.00         5
         124       0.38      0.75      0.50         4
         125       0.86      0.67      0.75         9
         126       1.00      1.00      1.00         4
         127       0.67      1.00      0.80         4
         128       1.00      0.50      0.67         4
         129       1.00      0.75      0.86         4
         130       0.56      0.56      0.56         9
         131       0.80      1.00      0.89         4
         132       0.75      0.75      0.75         4
         133       0.56      0.56      0.56         9
         134       1.00      0.75      0.86         4
         135       0.80      0.80      0.80         5
         136       0.67      0.40      0.50         5
         137       0.50      0.22      0.31         9
         138       0.00      0.00      0.00         4
         139       1.00      1.00      1.00         5
         140       0.75      0.75      0.75         4
         141       0.00      0.00      0.00         4
         142       0.83      1.00      0.91         5
         143       0.80      0.80      0.80         5
         144       0.67      0.40      0.50         5
         145       1.00      0.50      0.67         4
         146       1.00      0.75      0.86         4
         147       0.00      0.00      0.00         4
         148       0.40      1.00      0.57         4
         149       0.75      1.00      0.86         9
         150       0.75      0.75      0.75         4
         151       0.80      1.00      0.89         4
         152       0.80      1.00      0.89         4
         153       0.88      0.78      0.82         9
         154       0.71      1.00      0.83         5
         155       0.50      0.75      0.60         4
         156       0.50      0.75      0.60         4
         157       0.50      0.25      0.33         4
         158       1.00      0.89      0.94         9
         159       0.80      0.44      0.57         9
         160       0.50      0.50      0.50         4
         161       0.75      0.75      0.75         4
         162       0.25      0.11      0.15         9
         163       1.00      0.75      0.86         4
         164       0.90      1.00      0.95         9
         165       0.14      0.33      0.20         9
         166       0.67      1.00      0.80         4
         167       1.00      1.00      1.00         4
         168       1.00      0.75      0.86         4
         169       1.00      1.00      1.00         4
         170       0.60      0.75      0.67         4
         171       1.00      1.00      1.00         4
         172       1.00      1.00      1.00         4
         173       0.80      0.80      0.80         5
         174       1.00      0.75      0.86         4
         175       0.67      1.00      0.80         4
         176       0.50      0.75      0.60         4
         177       1.00      1.00      1.00         4
         178       0.56      1.00      0.72         9
         179       1.00      1.00      1.00         9
         180       0.60      0.75      0.67         4
         181       1.00      1.00      1.00         9
         182       1.00      0.25      0.40         4
         183       1.00      0.89      0.94         9
         184       0.43      0.75      0.55         4
         185       0.50      0.25      0.33         4
         186       1.00      1.00      1.00         5
         187       0.89      0.89      0.89         9
         188       0.57      0.80      0.67         5
         189       1.00      1.00      1.00         4
         190       0.89      0.89      0.89         9
         191       0.00      0.00      0.00         5
         192       0.75      0.75      0.75         4
         193       1.00      1.00      1.00         4
         194       1.00      1.00      1.00         4
         195       0.50      0.40      0.44         5
         196       0.00      0.00      0.00         4
         197       0.20      0.25      0.22         4
         198       0.67      1.00      0.80         4
         199       1.00      0.89      0.94         9
         200       0.60      0.75      0.67         4
         201       0.71      1.00      0.83         5
         202       0.83      1.00      0.91         5
         203       0.67      0.50      0.57         4
         204       1.00      0.22      0.36         9
         205       0.75      0.75      0.75         4
         206       0.33      0.25      0.29         4
         207       0.50      1.00      0.67         4
         208       1.00      1.00      1.00         4
         209       0.07      0.25      0.11         4
         210       0.11      0.33      0.17         9
         211       1.00      0.89      0.94         9
         212       1.00      0.75      0.86         4
         213       1.00      0.80      0.89         5
         214       0.50      0.25      0.33         4
         215       1.00      1.00      1.00         4
         216       1.00      1.00      1.00         4
         217       1.00      0.75      0.86         4
         218       0.64      0.78      0.70         9
         219       0.80      1.00      0.89         8
         220       0.80      0.80      0.80         5
         221       0.82      1.00      0.90         9
         222       0.44      1.00      0.62         4
         223       1.00      0.60      0.75         5
         224       1.00      1.00      1.00         5
         225       0.17      0.50      0.25         4
         226       0.90      1.00      0.95         9
         227       1.00      0.75      0.86         4
         228       1.00      0.75      0.86         4
         229       0.67      1.00      0.80         4
         230       0.50      0.80      0.62         5
         231       0.00      0.00      0.00         4
         232       1.00      1.00      1.00         5
         233       1.00      0.75      0.86         4
         234       0.80      1.00      0.89         4
         235       0.75      0.33      0.46         9
         236       0.75      0.67      0.71         9
         237       0.00      0.00      0.00         4
         238       1.00      0.75      0.86         4
         239       0.67      1.00      0.80         4
         240       0.67      0.50      0.57         4
         241       0.86      0.67      0.75         9
         242       0.80      0.44      0.57         9
         243       0.00      0.00      0.00         4
         244       0.75      0.75      0.75         4
         245       0.62      0.89      0.73         9
         246       0.60      0.75      0.67         4
         247       0.83      1.00      0.91         5
         248       0.00      0.00      0.00         4
         249       0.89      0.89      0.89         9
         250       0.82      1.00      0.90         9
         251       1.00      1.00      1.00         4
         252       0.67      1.00      0.80         4
         253       1.00      0.29      0.44         7
         254       0.78      0.78      0.78         9
         255       0.75      0.67      0.71         9
         256       0.80      1.00      0.89         4
         257       0.50      0.75      0.60         4
         258       1.00      1.00      1.00         9
         259       0.67      1.00      0.80         4
         260       0.67      1.00      0.80         4
         261       1.00      0.75      0.86         4
         262       0.62      0.89      0.73         9
         263       0.75      0.75      0.75         4
         264       0.80      1.00      0.89         4
         265       0.67      1.00      0.80         4
         266       1.00      0.75      0.86         4
         267       0.00      0.00      0.00         4
         268       1.00      0.75      0.86         4
         269       0.57      1.00      0.73         4
         270       0.71      1.00      0.83         5
         271       1.00      1.00      1.00         9
         272       0.67      1.00      0.80         4
         273       0.67      0.50      0.57         4
         274       0.80      0.44      0.57         9
         275       1.00      0.78      0.88         9
         276       0.71      1.00      0.83         5
         277       1.00      0.75      0.86         4
         278       0.75      0.75      0.75         4
         279       0.67      0.89      0.76         9
         280       1.00      0.67      0.80         9
         281       0.83      0.56      0.67         9
         282       0.78      0.78      0.78         9
         283       0.83      1.00      0.91         5
         284       0.60      0.75      0.67         4
         285       0.00      0.00      0.00         9
         286       0.00      0.00      0.00         4
         287       0.25      0.22      0.24         9
         288       0.73      0.89      0.80         9
         289       0.60      0.75      0.67         4
         290       0.88      0.78      0.82         9
         291       0.50      0.44      0.47         9
         292       1.00      0.75      0.86         4
         293       1.00      1.00      1.00         5
         294       0.57      1.00      0.73         4
         295       0.60      0.75      0.67         4
         296       1.00      0.75      0.86         4
         297       1.00      1.00      1.00         4
         298       0.57      0.80      0.67         5
         299       0.67      1.00      0.80         4
         300       0.50      0.50      0.50         4
         301       0.50      0.75      0.60         4
         302       1.00      0.60      0.75         5
         303       0.80      1.00      0.89         4
         304       0.00      0.00      0.00         4
         305       0.60      0.75      0.67         4
         306       0.75      0.60      0.67         5
         307       0.67      0.89      0.76         9
         308       1.00      1.00      1.00         5
         309       0.90      1.00      0.95         9
         310       0.75      0.60      0.67         5
         311       0.80      1.00      0.89         4
         312       0.06      0.11      0.08         9
         313       0.67      0.50      0.57         4
         314       1.00      0.75      0.86         4
         315       0.00      0.00      0.00         4
         316       0.57      1.00      0.73         4
         317       0.40      0.50      0.44         4
         318       1.00      1.00      1.00         5
         319       0.67      1.00      0.80         4
         320       1.00      0.80      0.89         5
         321       0.00      0.00      0.00         4
         322       0.80      0.89      0.84         9
         323       0.88      0.78      0.82         9
         324       1.00      1.00      1.00         5
         325       0.88      0.78      0.82         9
         326       0.50      0.50      0.50         4
         327       0.00      0.00      0.00         4
         328       0.83      1.00      0.91         5
         329       1.00      0.89      0.94         9
         330       0.00      0.00      0.00         4
         331       0.00      0.00      0.00         4
         332       0.88      0.78      0.82         9
         333       0.00      0.00      0.00         4
         334       0.00      0.00      0.00         4
         335       0.17      0.25      0.20         4
         336       0.83      1.00      0.91         5
         337       0.00      0.00      0.00         4
         338       0.75      0.75      0.75         4
         339       0.50      0.40      0.44         5
         340       1.00      1.00      1.00         4
         341       1.00      1.00      1.00         5
         342       1.00      0.50      0.67         4
         343       1.00      1.00      1.00         5
         344       1.00      0.75      0.86         4
         345       1.00      1.00      1.00         4
         346       0.57      1.00      0.73         4
         347       1.00      1.00      1.00         5
         348       0.80      1.00      0.89         4
         349       0.70      0.78      0.74         9
         350       0.75      0.75      0.75         4
         351       0.00      0.00      0.00         4
         352       0.20      0.25      0.22         4
         353       1.00      1.00      1.00         4
         354       0.00      0.00      0.00         4
         355       0.80      1.00      0.89         4
         356       0.30      0.75      0.43         4
         357       0.50      0.40      0.44         5
         358       0.75      0.75      0.75         4
         359       0.73      0.89      0.80         9
         360       0.67      1.00      0.80         4
         361       1.00      1.00      1.00         4
         362       0.83      1.00      0.91         5
         363       1.00      1.00      1.00         4
         364       1.00      0.75      0.86         4
         365       0.00      0.00      0.00         4
         366       0.67      0.89      0.76         9
         367       0.25      0.50      0.33         4
         368       1.00      0.80      0.89         5
         369       0.00      0.00      0.00         4
         370       0.00      0.00      0.00         4
         371       0.33      0.50      0.40         4
         372       1.00      1.00      1.00         4
         373       0.00      0.00      0.00         4
         374       0.00      0.00      0.00         4
         375       1.00      1.00      1.00         9
         376       1.00      1.00      1.00         5
         377       1.00      1.00      1.00         5
         378       0.83      0.56      0.67         9
         379       0.50      0.40      0.44         5
         380       0.58      0.78      0.67         9
         381       1.00      1.00      1.00         4
         382       0.57      0.80      0.67         5
         383       0.80      1.00      0.89         4
         384       1.00      1.00      1.00         5
         385       1.00      0.50      0.67         4
         386       0.90      1.00      0.95         9
         387       0.00      0.00      0.00         4
         388       1.00      1.00      1.00         9
         389       1.00      1.00      1.00         9
         390       0.67      0.40      0.50         5
         391       0.50      0.25      0.33         4
         392       0.67      0.40      0.50         5
         393       0.50      0.50      0.50         4
         394       0.20      0.20      0.20         5
         395       0.55      0.67      0.60         9
         396       0.38      0.60      0.46         5
         397       0.75      1.00      0.86         9
         398       0.75      0.75      0.75         4
         399       0.67      0.67      0.67         9
         400       0.00      0.00      0.00         4
         401       0.88      0.78      0.82         9
         402       0.88      0.88      0.88         8
         403       0.67      1.00      0.80         4
         404       1.00      1.00      1.00         4
         405       0.00      0.00      0.00         4
         406       0.75      0.75      0.75         4
         407       1.00      0.60      0.75         5
         408       0.43      0.75      0.55         4
         409       0.50      0.25      0.33         4
         410       0.57      1.00      0.73         4
         411       0.89      0.89      0.89         9
         412       0.00      0.00      0.00         4
         413       0.00      0.00      0.00         4
         414       0.33      0.25      0.29         4
         415       1.00      1.00      1.00         4
         416       0.88      0.78      0.82         9
         417       0.80      1.00      0.89         4
         418       0.80      1.00      0.89         4
         419       1.00      0.22      0.36         9
         420       0.00      0.00      0.00         9
         421       0.33      0.25      0.29         4
         422       1.00      1.00      1.00         4
         423       0.00      0.00      0.00         4
         424       0.69      1.00      0.82         9
         425       0.33      0.50      0.40         4
         426       0.88      0.78      0.82         9
         427       0.62      0.56      0.59         9
         428       1.00      0.80      0.89         5
         429       0.00      0.00      0.00         4
         430       0.75      0.75      0.75         4
         431       0.75      0.75      0.75         4
         432       0.67      0.40      0.50         5
         433       0.00      0.00      0.00         4
         434       1.00      0.25      0.40         4
         435       0.00      0.00      0.00         5
         436       0.50      0.40      0.44         5
         437       1.00      1.00      1.00         4
         438       0.75      0.60      0.67         5
         439       0.00      0.00      0.00         4
         440       0.64      0.78      0.70         9
         441       0.57      1.00      0.73         4
         442       0.80      0.89      0.84         9
         443       0.75      0.75      0.75         4
         444       1.00      0.50      0.67         4
         445       0.67      0.50      0.57         4
         446       0.00      0.00      0.00         4
         447       0.75      0.75      0.75         4
         448       0.60      0.75      0.67         4
         449       1.00      1.00      1.00         4
         450       0.67      0.44      0.53         9
         451       0.71      1.00      0.83         5
         452       0.00      0.00      0.00         4
         453       1.00      1.00      1.00         5
         454       1.00      0.75      0.86         4
         455       0.00      0.00      0.00         4
         456       1.00      1.00      1.00         9
         457       1.00      0.67      0.80         9
         458       0.50      0.75      0.60         4
         459       1.00      1.00      1.00         9
         460       0.56      0.56      0.56         9
         461       1.00      1.00      1.00         4
         462       0.56      0.56      0.56         9
         463       1.00      1.00      1.00         4
         464       0.00      0.00      0.00         4
         465       0.00      0.00      0.00         4
         466       0.75      0.75      0.75         4
         467       1.00      0.80      0.89         5
         468       1.00      1.00      1.00         5
         469       1.00      0.56      0.71         9
         470       0.67      0.50      0.57         4
         471       0.75      0.75      0.75         4
         472       1.00      0.89      0.94         9
         473       0.75      0.75      0.75         4
         474       1.00      1.00      1.00         5
         475       0.75      1.00      0.86         9
         476       0.40      1.00      0.57         4
         477       0.00      0.00      0.00         4
         478       0.86      0.67      0.75         9
         479       0.64      0.78      0.70         9
         480       0.80      0.80      0.80         5
         481       0.67      0.67      0.67         9
         482       1.00      0.50      0.67         4
         483       1.00      1.00      1.00         4
         484       0.00      0.00      0.00         4
         485       0.00      0.00      0.00         4
         486       0.00      0.00      0.00         4
         487       1.00      1.00      1.00         4
         488       0.60      0.75      0.67         4
         489       0.00      0.00      0.00         4
         490       0.82      1.00      0.90         9
         491       0.67      0.80      0.73         5
         492       0.83      1.00      0.91         5
         493       0.67      0.80      0.73         5
         494       1.00      1.00      1.00         5
         495       0.00      0.00      0.00         4
         496       1.00      0.67      0.80         9
         497       0.67      0.50      0.57         4
         498       0.80      0.80      0.80         5
         499       0.00      0.00      0.00         4
         500       0.00      0.00      0.00         4
         501       1.00      1.00      1.00         4
         502       0.67      0.50      0.57         4
         503       0.89      0.89      0.89         9
         504       1.00      1.00      1.00         4
         505       1.00      0.89      0.94         9
         506       0.00      0.00      0.00         4
         507       0.12      0.22      0.16         9
         508       0.73      0.89      0.80         9
         509       0.82      1.00      0.90         9
         510       0.57      0.44      0.50         9
         511       1.00      0.50      0.67         4
         512       0.00      0.00      0.00         4
         513       1.00      0.75      0.86         4
         514       0.80      1.00      0.89         4
         515       1.00      0.25      0.40         4
         516       1.00      1.00      1.00         4
         517       0.50      0.50      0.50         4
         518       1.00      0.33      0.50         9
         519       0.00      0.00      0.00         4
         520       1.00      1.00      1.00         9
         521       0.60      0.75      0.67         4
         522       1.00      1.00      1.00         4
         523       0.80      0.89      0.84         9
         524       1.00      1.00      1.00         4
         525       1.00      0.89      0.94         9
         526       0.00      0.00      0.00         5
         527       0.50      0.25      0.33         4
         528       1.00      0.50      0.67         4
         529       0.86      0.67      0.75         9
         530       0.75      0.75      0.75         4
         531       1.00      1.00      1.00         5
         532       1.00      0.25      0.40         4
         533       0.25      0.25      0.25         4
         534       0.73      0.89      0.80         9
         535       0.82      1.00      0.90         9
         536       0.50      0.11      0.18         9
         537       0.75      0.75      0.75         4
         538       0.89      0.89      0.89         9
         539       0.00      0.00      0.00         4
         540       0.00      0.00      0.00         4
         541       0.00      0.00      0.00         4
         542       1.00      1.00      1.00         4
         543       0.88      0.78      0.82         9
         544       1.00      0.50      0.67         4
         545       0.44      0.44      0.44         9
         546       0.71      0.56      0.63         9
         547       0.00      0.00      0.00         4
         548       1.00      1.00      1.00         4
         549       1.00      0.89      0.94         9
         550       1.00      0.75      0.86         4
         551       0.00      0.00      0.00         4
         552       1.00      0.57      0.73         7
         553       0.67      0.67      0.67         9
         554       1.00      0.78      0.88         9
         555       0.00      0.00      0.00         4
         556       1.00      0.40      0.57         5
         557       0.88      0.78      0.82         9
         558       0.89      0.89      0.89         9
         559       0.64      0.78      0.70         9
         560       0.50      0.50      0.50         4
         561       1.00      1.00      1.00         4
         562       0.80      0.89      0.84         9
         563       0.12      0.25      0.17         4
         564       0.80      0.80      0.80         5
         565       1.00      0.80      0.89         5
         566       1.00      1.00      1.00         4
         567       1.00      0.67      0.80         9
         568       1.00      1.00      1.00         4
         569       1.00      0.89      0.94         9
         570       0.80      1.00      0.89         4
         571       1.00      0.78      0.88         9
         572       0.83      1.00      0.91         5
         573       1.00      1.00      1.00         9
         574       0.00      0.00      0.00         4
         575       0.00      0.00      0.00         4
         576       0.75      0.60      0.67         5
         577       0.75      1.00      0.86         9
         578       0.67      0.50      0.57         4
         579       0.00      0.00      0.00         4
         580       1.00      1.00      1.00         4
         581       0.60      0.75      0.67         4
         582       1.00      1.00      1.00         4
         583       0.67      1.00      0.80         4
         584       0.80      1.00      0.89         4
         585       0.00      0.00      0.00         4
         586       0.67      0.40      0.50         5
         587       1.00      0.50      0.67         4
         588       1.00      0.75      0.86         4
         589       0.67      0.50      0.57         4
         590       0.80      1.00      0.89         4
         591       0.00      0.00      0.00         4
         592       0.89      0.89      0.89         9
         593       1.00      1.00      1.00         5
         594       0.00      0.00      0.00         4
         595       0.17      0.50      0.25         4
         596       0.75      0.60      0.67         5
         597       1.00      0.89      0.94         9
         598       0.33      0.25      0.29         4
         599       0.00      0.00      0.00         9
         600       0.67      0.40      0.50         5
         601       1.00      0.80      0.89         5
         602       0.67      1.00      0.80         4
         603       0.80      0.44      0.57         9
         604       0.00      0.00      0.00         9
         605       0.40      0.80      0.53         5
         606       0.40      0.50      0.44         4
         607       0.09      0.25      0.13         4
         608       0.33      0.25      0.29         4
         609       1.00      1.00      1.00         4
         610       0.40      1.00      0.57         4
         611       0.00      0.00      0.00         4
         612       1.00      0.80      0.89         5
         613       0.12      0.25      0.17         4
         614       0.50      0.80      0.62         5
         615       1.00      0.60      0.75         5
         616       1.00      0.44      0.62         9
         617       0.00      0.00      0.00         4
         618       0.75      0.67      0.71         9
         619       0.50      0.75      0.60         4
         620       1.00      0.60      0.75         5
         621       1.00      0.89      0.94         9
         622       1.00      0.56      0.71         9
         623       1.00      1.00      1.00         4
         624       0.40      0.50      0.44         4
         625       0.86      0.67      0.75         9
         626       0.50      0.20      0.29         5
         627       0.80      1.00      0.89         4
         628       1.00      1.00      1.00         4
         629       0.80      1.00      0.89         4
         630       0.33      0.25      0.29         4
         631       0.83      0.56      0.67         9
         632       1.00      0.89      0.94         9
         633       1.00      1.00      1.00         4
         634       0.00      0.00      0.00         4
         635       1.00      0.25      0.40         4
         636       0.70      0.78      0.74         9
         637       1.00      0.80      0.89         5
         638       0.00      0.00      0.00         4
         639       1.00      1.00      1.00         9
         640       0.00      0.00      0.00         4
         641       0.00      0.00      0.00         4
         642       0.60      0.75      0.67         4
         643       0.80      1.00      0.89         4
         644       0.50      1.00      0.67         4
         645       1.00      0.89      0.94         9
         646       0.83      1.00      0.91         5
         647       0.00      0.00      0.00         4
         648       0.00      0.00      0.00         4
         649       1.00      1.00      1.00         4
         650       0.67      0.50      0.57         4
         651       1.00      1.00      1.00         5
         652       0.75      0.75      0.75         4
         653       1.00      1.00      1.00         5
         654       0.40      1.00      0.57         4
         655       0.75      0.67      0.71         9
         656       1.00      0.80      0.89         5
         657       0.00      0.00      0.00         4
         658       1.00      1.00      1.00         4
         659       0.00      0.00      0.00         9
         660       1.00      0.75      0.86         4
         661       0.00      0.00      0.00         4
         662       1.00      0.20      0.33         5
         663       0.40      1.00      0.57         4
         664       1.00      0.75      0.86         4
         665       1.00      0.75      0.86         4
         666       0.00      0.00      0.00         4
         667       0.11      0.25      0.15         4
         668       0.67      0.50      0.57         4
         669       0.75      0.75      0.75         4
         670       1.00      0.56      0.71         9
         671       1.00      1.00      1.00         9
         672       0.67      1.00      0.80         4
         673       0.43      0.60      0.50         5
         674       0.00      0.00      0.00         4
         675       0.67      0.80      0.73         5
         676       1.00      1.00      1.00         5
         677       1.00      0.50      0.67         4
         678       0.06      0.11      0.08         9
         679       1.00      1.00      1.00         5
         680       0.64      1.00      0.78         9
         681       0.86      0.67      0.75         9
         682       1.00      1.00      1.00         4
         683       0.00      0.00      0.00         4
         684       0.38      0.33      0.35         9
         685       1.00      0.50      0.67         4
         686       0.14      0.25      0.18         4
         687       0.50      1.00      0.67         4
         688       0.33      0.25      0.29         4
         689       0.80      0.89      0.84         9
         690       0.67      1.00      0.80         4
         691       0.89      0.89      0.89         9
         692       1.00      1.00      1.00         4
         693       0.71      0.56      0.63         9
         694       0.71      1.00      0.83         5
         695       0.00      0.00      0.00         4
         696       1.00      1.00      1.00         5
         697       0.00      0.00      0.00         9
         698       0.00      0.00      0.00         4
         699       0.00      0.00      0.00         4
         700       0.71      1.00      0.83         5
         701       1.00      1.00      1.00         4
         702       1.00      1.00      1.00         9
         703       0.75      0.60      0.67         5
         704       1.00      0.25      0.40         4
         705       0.62      1.00      0.77         5
         706       0.00      0.00      0.00         9
         707       0.00      0.00      0.00         4
         708       0.00      0.00      0.00         4
         709       0.89      0.89      0.89         9
         710       0.83      1.00      0.91         5
         711       1.00      1.00      1.00         4
         712       0.38      0.75      0.50         4
         713       1.00      0.50      0.67         4
         714       0.70      0.78      0.74         9
         715       0.67      1.00      0.80         4
         716       0.86      0.67      0.75         9
         717       0.80      1.00      0.89         4
         718       0.71      1.00      0.83         5
         719       0.00      0.00      0.00         9
         720       0.00      0.00      0.00         5
         721       0.00      0.00      0.00         9
         722       0.67      0.80      0.73         5
         723       1.00      1.00      1.00         5
         724       0.00      0.00      0.00         9
         725       1.00      1.00      1.00         4
         726       1.00      1.00      1.00         4
         727       0.00      0.00      0.00         4
         728       0.00      0.00      0.00         4
         729       0.40      0.50      0.44         4
         730       0.00      0.00      0.00         4
         731       1.00      0.75      0.86         4
         732       1.00      0.60      0.75         5
         733       1.00      0.33      0.50         9
         734       0.33      0.50      0.40         4
         735       0.82      1.00      0.90         9
         736       0.67      0.50      0.57         4
         737       1.00      0.50      0.67         4
         738       1.00      1.00      1.00         4
         739       0.33      0.25      0.29         4
         740       0.67      0.50      0.57         4
         741       1.00      0.25      0.40         4
         742       0.67      1.00      0.80         4
         743       1.00      1.00      1.00         4
         744       0.25      0.25      0.25         4
         745       1.00      1.00      1.00         4
         746       1.00      1.00      1.00         4
         747       0.50      0.25      0.33         4
         748       0.50      0.22      0.31         9
         749       0.00      0.00      0.00         4
         750       0.00      0.00      0.00         4
         751       1.00      1.00      1.00         9
         752       0.88      0.78      0.82         9
         753       0.00      0.00      0.00         4
         754       0.67      1.00      0.80         4
         755       0.60      0.75      0.67         4
         756       1.00      1.00      1.00         5
         757       1.00      0.50      0.67         4
         758       0.08      0.25      0.12         4
         759       0.00      0.00      0.00         4
         760       1.00      0.75      0.86         4
         761       0.86      0.67      0.75         9
         762       1.00      0.75      0.86         4
         763       0.78      0.78      0.78         9
         764       0.60      0.75      0.67         4
         765       0.50      0.44      0.47         9
         766       0.00      0.00      0.00         4
         767       0.40      0.50      0.44         4
         768       0.86      0.67      0.75         9
         769       0.50      0.50      0.50         4
         770       0.80      1.00      0.89         4
         771       0.00      0.00      0.00         4
         772       1.00      0.20      0.33         5
         773       0.00      0.00      0.00         9
         774       0.69      1.00      0.82         9
         775       0.80      1.00      0.89         4
         776       0.25      0.25      0.25         4
         777       0.80      1.00      0.89         4
         778       1.00      0.25      0.40         4
         779       1.00      0.80      0.89         5
         780       0.00      0.00      0.00         9
         781       0.60      0.75      0.67         4
         782       1.00      0.80      0.89         5
         783       0.75      0.75      0.75         4
         784       0.78      0.78      0.78         9
         785       0.50      0.50      0.50         4
         786       1.00      1.00      1.00         4
         787       0.62      1.00      0.77         5
         788       1.00      1.00      1.00         4
         789       0.00      0.00      0.00         4
         790       0.80      1.00      0.89         4
         791       1.00      0.25      0.40         4
         792       0.75      0.75      0.75         4
         793       1.00      1.00      1.00         5
         794       1.00      0.89      0.94         9
         795       0.80      0.80      0.80         5
         796       0.00      0.00      0.00         4
         797       1.00      0.75      0.86         4
         798       0.00      0.00      0.00         4
         799       0.00      0.00      0.00         4
         800       1.00      0.78      0.88         9
         801       0.00      0.00      0.00         4
         802       0.80      1.00      0.89         4
         803       1.00      0.22      0.36         9
         804       0.78      0.78      0.78         9
         805       0.40      0.40      0.40         5
         806       1.00      1.00      1.00         5
         807       1.00      1.00      1.00         9
         808       0.80      0.80      0.80         5
         809       0.90      1.00      0.95         9
         810       1.00      1.00      1.00         4
         811       0.00      0.00      0.00         9
         812       0.00      0.00      0.00         6
         813       1.00      1.00      1.00         9
         814       0.50      0.40      0.44         5
         815       1.00      0.40      0.57         5
         816       0.15      1.00      0.26         4
         817       1.00      1.00      1.00         9
         818       0.00      0.00      0.00         4
         819       1.00      0.75      0.86         4
         820       0.60      0.75      0.67         4
         821       0.75      0.33      0.46         9
         822       0.67      0.44      0.53         9
         823       1.00      0.25      0.40         4
         824       0.40      0.50      0.44         4
         825       0.67      0.50      0.57         4
         826       1.00      0.89      0.94         9
         827       1.00      0.75      0.86         4
         828       1.00      1.00      1.00         9
         829       0.80      1.00      0.89         4
         830       0.25      0.25      0.25         4
         831       0.83      1.00      0.91         5
         832       1.00      0.50      0.67         4
         833       0.67      1.00      0.80         4
         834       0.80      1.00      0.89         4
         835       0.50      0.11      0.18         9
         836       1.00      0.40      0.57         5
         837       0.00      0.00      0.00         4
         838       0.75      0.75      0.75         4
         839       0.50      0.25      0.33         4
         840       0.00      0.00      0.00         4
         841       0.00      0.00      0.00         4
         842       1.00      1.00      1.00         4
         843       0.75      0.75      0.75         4
         844       1.00      1.00      1.00         9
         845       0.75      0.67      0.71         9
         846       0.86      0.67      0.75         9
         847       0.75      0.60      0.67         5
         848       1.00      0.25      0.40         4
         849       0.67      0.50      0.57         4
         850       0.00      0.00      0.00         4
         851       1.00      0.25      0.40         4
         852       0.00      0.00      0.00         4
         853       0.67      0.40      0.50         5
         854       0.00      0.00      0.00         9
         855       1.00      1.00      1.00         9
         856       1.00      1.00      1.00         9
         857       0.75      0.60      0.67         5
         858       0.00      0.00      0.00         9
         859       0.58      0.78      0.67         9
         860       0.75      0.75      0.75         4
         861       0.75      0.75      0.75         4
         862       0.00      0.00      0.00         4
         863       0.00      0.00      0.00         5
         864       0.00      0.00      0.00         4
         865       1.00      0.40      0.57         5
         866       1.00      0.50      0.67         4
         867       1.00      0.80      0.89         5
         868       1.00      0.80      0.89         5
         869       0.00      0.00      0.00         4
         870       0.75      0.75      0.75         4
         871       1.00      0.50      0.67         4
         872       1.00      0.75      0.86         4
         873       0.00      0.00      0.00         4
         874       0.00      0.00      0.00         4
         875       0.64      0.78      0.70         9
         876       0.00      0.00      0.00         4
         877       0.67      1.00      0.80         4
         878       0.60      1.00      0.75         9
         879       0.27      0.89      0.41         9
         880       0.00      0.00      0.00         4
         881       0.23      0.33      0.27         9
         882       1.00      1.00      1.00         4
         883       0.00      0.00      0.00         4
         884       0.32      0.89      0.47         9
         885       0.75      0.75      0.75         4
         886       0.60      0.75      0.67         4
         887       0.50      0.75      0.60         4
         888       1.00      0.75      0.86         4
         889       0.12      1.00      0.22         4
         890       1.00      1.00      1.00         4
         891       0.41      0.78      0.54         9
         892       0.71      1.00      0.83         5
         893       0.50      0.50      0.50         4

    accuracy                           0.66      4917
   macro avg       0.65      0.64      0.62      4917
weighted avg       0.68      0.66      0.65      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
