	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [528, 24, 805, 885, 65, 304, 772, 384, 312, 752, 771, 366, 635, 12, 80, 272, 711, 590, 502, 620, 684, 815, 62, 388, 473, 101, 429, 104, 513, 705, 142, 855, 401, 295, 374, 245, 734, 527, 381, 315, 715, 137, 357, 222, 325, 5, 432, 305, 271, 735, 788, 616, 755, 585, 293, 696, 259, 424, 791, 516, 649, 883, 297, 846, 588, 192, 439, 657, 90, 397, 808, 626, 446, 790, 99, 538, 713, 314, 632, 480, 500, 203, 140, 255, 425, 849, 811, 833, 724, 225, 639, 396, 455, 483, 642, 153, 877, 478, 596, 613, 205, 123, 283, 121, 756, 580, 377, 462, 419, 27, 567, 122, 610, 852, 661, 795, 2, 495, 311, 706, 781, 431, 872, 236, 230, 280, 169, 698, 653, 566, 720, 565, 828, 515, 608, 450, 126, 146, 809, 679, 633, 6, 238, 213, 353, 59, 751, 824, 526, 645, 50, 571, 821, 95, 144, 575, 19, 70, 15, 87, 767, 699, 595, 209, 881, 157, 465, 135, 667, 88, 172, 784, 842, 387, 89, 490, 744, 313, 579, 602, 889, 233, 253, 582, 25, 489, 864, 754, 572, 789, 82, 799, 443, 503, 688, 244, 690, 834, 152, 825, 733, 85, 393, 49, 36, 184, 662, 806, 847, 100, 362, 1, 584, 887, 464, 763, 475, 719, 794, 732, 60, 338, 697, 508, 151, 621, 335, 228, 407, 229, 158, 614, 820, 832, 783, 351, 343, 586, 278, 392, 573, 110, 564, 858, 682, 349, 286, 728, 712, 839, 136, 18, 306, 198, 652, 796, 320, 350, 560, 470, 269, 46, 282, 850, 823, 292, 47, 58, 583, 418, 857, 494, 417, 303, 759, 438, 463, 227, 474, 207, 261, 569, 98, 827, 270, 214, 609, 762, 340, 125, 224, 482, 563, 217, 721, 786, 258, 625, 178, 42, 72, 395, 421, 826, 663, 838, 544, 430, 394, 10, 368, 882, 189, 33, 55, 331, 888, 562, 129, 334, 792, 323, 379, 150, 22, 166, 496, 507, 691, 289, 266, 347, 680, 30, 322, 35, 870, 592, 355, 164, 587, 818, 92, 171, 521, 775, 279, 695, 861, 477, 183, 624, 427, 778, 117, 519, 817, 654, 606, 531, 741, 702, 186, 618, 557, 619, 523, 120, 254, 517, 371, 11, 56, 492, 373, 797, 437, 493, 291, 428, 242, 651, 337, 514, 875, 31, 548, 511, 399, 197, 727, 637, 471, 273, 777, 659, 669, 422, 241, 599, 141, 641, 555, 447, 147, 380, 747, 434, 14, 840, 250, 466, 44, 660, 779, 73, 574, 863, 539, 162, 758, 383, 113, 231, 843, 52, 800, 689, 391, 591, 361, 435, 497, 243, 694, 69, 426, 541, 718, 577, 831, 202, 869, 167, 260, 201, 433, 709, 693, 177, 375, 77, 208, 761, 793, 449, 237, 61, 856, 174, 410, 268, 415, 138, 782, 441, 603, 813, 892, 288, 218, 550, 615, 68, 542, 481, 650, 570, 411, 263, 837, 745, 816, 644, 687, 369, 264, 676, 445, 672, 529, 308, 750, 773, 365, 540, 798, 504, 757, 159, 830, 262, 284, 484, 785, 486, 81, 803, 165, 479, 876, 670, 871, 607, 543, 666, 499, 13, 327, 880, 534, 509, 524, 716, 188, 63, 210, 765, 406, 681, 532, 39, 74, 416, 664, 722, 442, 605, 647, 83, 339, 658, 114, 275, 398, 546, 17, 409, 764, 787, 329, 749, 535, 512, 506, 485, 127, 265, 21, 9, 294, 692, 656, 867, 20, 378, 671, 453, 212, 359, 51, 742, 862, 630, 520, 673, 76, 505, 629, 215, 501, 148, 725, 102, 760, 601, 103, 29, 491, 589, 753, 257, 195, 743, 776, 551, 634, 211, 841, 181, 400, 119, 219, 302, 522, 239, 191, 851, 561, 700, 865, 319, 333, 43, 220, 75, 597, 16, 131, 510, 665, 845, 723, 581, 646, 822, 729, 891, 124, 96, 707, 8, 461, 132, 26, 235, 7, 91, 457, 677, 874, 328, 886, 766, 324, 675, 604, 454, 109, 554, 739, 643, 128, 358, 553, 403, 115, 488, 173, 598, 868, 420, 469, 252, 116, 360, 139, 708, 631, 249, 160, 467, 364, 770, 285, 196, 226, 890, 94, 161, 274, 176, 576, 884, 812, 134, 175, 804, 547, 549, 617, 155, 206, 342, 170, 476, 703, 133, 774, 456, 354, 67, 594, 4, 163, 256, 37, 600, 593, 276, 412, 194, 300, 578, 23, 835, 668, 402, 413, 199, 28, 678, 307, 84, 182, 64, 93, 685, 533, 854, 628, 130, 310, 440, 290, 814, 701, 451, 3, 267, 185, 341, 390, 612, 748, 367, 568, 537, 149, 558, 204, 330, 71, 298, 370, 737, 640, 251, 345, 356, 487, 623, 382, 452, 459, 317, 107, 389, 326, 559, 187, 636, 348, 545, 48, 518, 309, 240, 746, 807, 536, 57, 866, 738, 38, 40, 525, 404, 468, 740, 200, 848, 638, 710, 111, 118, 97, 45, 318, 801, 316, 168, 105, 436, 145, 346, 674, 853, 34, 726, 859, 344, 405, 143, 472, 32, 878, 299, 530, 372, 156, 221, 154, 768, 179, 0, 180, 648, 112, 234, 281, 106, 860, 655, 190, 54, 53, 893, 301, 780, 622, 802, 321, 460, 385, 836, 448, 736, 336, 78, 704, 79, 248, 879, 332, 458, 408, 246, 66, 556, 611, 731, 277, 386, 296, 193, 86, 717, 423, 247, 819, 730, 363, 844, 223, 232, 683, 444, 769, 873, 498, 810, 216, 686, 376, 714, 627, 287, 414, 41, 829, 352, 108, 552]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.770, TAw acc= 46.5% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.294, TAw acc= 57.0% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.930, TAw acc= 64.8% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.673, TAw acc= 70.4% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.453, TAw acc= 74.6% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.295, TAw acc= 76.1% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.141, TAw acc= 78.2% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.031, TAw acc= 80.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 287 train exemplars, time=  0.0s
287
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.986 | TAw acc= 80.6%, forg=  0.0%| TAg acc= 80.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=3.198, TAw acc= 56.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.319, TAw acc= 60.9% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.819, TAw acc= 66.7% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.476, TAw acc= 72.4% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.235, TAw acc= 80.5% | *
| Epoch   6, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.060, TAw acc= 95.4% | *
| Epoch   7, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.915, TAw acc= 98.9% | *
| Epoch   8, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.799, TAw acc= 98.9% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.798, TAw acc= 98.9% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.798, TAw acc= 98.9% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.797, TAw acc= 98.9% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=0.797, TAw acc= 98.9% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=0.797, TAw acc= 98.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 447 train exemplars, time=  0.0s
447
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.091 | TAw acc= 89.5%, forg= -8.9%| TAg acc= 74.9%, forg=  5.8% <<<
>>> Test on task  1 : loss=0.784 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 90.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.337, TAw acc= 39.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.537, TAw acc= 52.3% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.057, TAw acc= 55.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.733, TAw acc= 61.6% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.500, TAw acc= 65.1% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.308, TAw acc= 76.7% | *
| Epoch   7, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.180, TAw acc= 81.4% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.058, TAw acc= 81.4% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 81.4% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.056, TAw acc= 81.4% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.056, TAw acc= 81.4% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.055, TAw acc= 81.4% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.054, TAw acc= 81.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 607 train exemplars, time=  0.0s
607
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.996 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 75.9%, forg=  4.7% <<<
>>> Test on task  1 : loss=0.963 | TAw acc=100.0%, forg= -0.9%| TAg acc= 85.5%, forg=  5.1% <<<
>>> Test on task  2 : loss=1.068 | TAw acc= 85.2%, forg=  0.0%| TAg acc= 73.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=4.276, TAw acc= 30.8% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.295, TAw acc= 42.3% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.682, TAw acc= 56.4% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.273, TAw acc= 62.8% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.987, TAw acc= 74.4% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.751, TAw acc= 73.1% | *
| Epoch   7, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.598, TAw acc= 78.2% | *
| Epoch   8, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.461, TAw acc= 78.2% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.459, TAw acc= 78.2% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.458, TAw acc= 78.2% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.457, TAw acc= 79.5% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.456, TAw acc= 79.5% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 79.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 757 train exemplars, time=  0.0s
757
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.020 | TAw acc= 89.0%, forg=  0.5%| TAg acc= 70.7%, forg=  9.9% <<<
>>> Test on task  1 : loss=0.843 | TAw acc=100.0%, forg=  0.0%| TAg acc= 90.6%, forg=  0.0% <<<
>>> Test on task  2 : loss=1.196 | TAw acc= 91.3%, forg= -6.1%| TAg acc= 70.4%, forg=  3.5% <<<
>>> Test on task  3 : loss=1.312 | TAw acc= 86.0%, forg=  0.0%| TAg acc= 78.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.534, TAw acc= 34.2% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.356, TAw acc= 44.7% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.673, TAw acc= 61.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.248, TAw acc= 71.1% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.955, TAw acc= 73.7% | *
| Epoch   6, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.724, TAw acc= 77.6% | *
| Epoch   7, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.541, TAw acc= 82.9% | *
| Epoch   8, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.444, TAw acc= 89.5% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.443, TAw acc= 89.5% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.442, TAw acc= 89.5% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.440, TAw acc= 89.5% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.439, TAw acc= 89.5% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.437, TAw acc= 89.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 897 train exemplars, time=  0.0s
897
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.958 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 78.5%, forg=  2.1% <<<
>>> Test on task  1 : loss=0.763 | TAw acc=100.0%, forg=  0.0%| TAg acc= 92.3%, forg= -1.7% <<<
>>> Test on task  2 : loss=1.087 | TAw acc= 93.0%, forg= -1.7%| TAg acc= 76.5%, forg= -2.6% <<<
>>> Test on task  3 : loss=1.546 | TAw acc= 92.5%, forg= -6.5%| TAg acc= 60.7%, forg= 17.8% <<<
>>> Test on task  4 : loss=1.385 | TAw acc= 87.5%, forg=  0.0%| TAg acc= 74.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.542, TAw acc= 32.9% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.306, TAw acc= 48.1% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.618, TAw acc= 62.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.230, TAw acc= 69.6% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.986, TAw acc= 72.2% | *
| Epoch   6, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.791, TAw acc= 78.5% | *
| Epoch   7, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.649, TAw acc= 79.7% | *
| Epoch   8, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.517, TAw acc= 81.0% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.516, TAw acc= 81.0% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.515, TAw acc= 81.0% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 81.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 81.0% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.513, TAw acc= 81.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1037 train exemplars, time=  0.0s
1037
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.947 | TAw acc= 90.1%, forg= -0.5%| TAg acc= 77.5%, forg=  3.1% <<<
>>> Test on task  1 : loss=0.769 | TAw acc=100.0%, forg=  0.0%| TAg acc= 86.3%, forg=  6.0% <<<
>>> Test on task  2 : loss=1.134 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 71.3%, forg=  5.2% <<<
>>> Test on task  3 : loss=1.275 | TAw acc= 93.5%, forg= -0.9%| TAg acc= 84.1%, forg= -5.6% <<<
>>> Test on task  4 : loss=1.482 | TAw acc= 97.1%, forg= -9.6%| TAg acc= 68.3%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.272 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 78.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.501, TAw acc= 42.5% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.038, TAw acc= 63.7% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.348, TAw acc= 83.8% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.880, TAw acc= 87.5% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.598, TAw acc= 87.5% | *
| Epoch   6, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.356, TAw acc= 91.2% | *
| Epoch   7, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.260, TAw acc= 96.2% | *
| Epoch   8, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.139, TAw acc= 96.2% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.137, TAw acc= 96.2% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.135, TAw acc= 96.2% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.133, TAw acc= 96.2% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.131, TAw acc= 96.2% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1177 train exemplars, time=  0.0s
1177
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.942 | TAw acc= 89.5%, forg=  0.5%| TAg acc= 79.1%, forg=  1.6% <<<
>>> Test on task  1 : loss=0.674 | TAw acc=100.0%, forg=  0.0%| TAg acc= 92.3%, forg=  0.0% <<<
>>> Test on task  2 : loss=1.072 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 78.3%, forg= -1.7% <<<
>>> Test on task  3 : loss=1.234 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 83.2%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.459 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 63.5%, forg= 10.6% <<<
>>> Test on task  5 : loss=1.436 | TAw acc= 95.4%, forg= -5.6%| TAg acc= 65.7%, forg= 13.0% <<<
>>> Test on task  6 : loss=1.072 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 80.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.961, TAw acc= 44.4% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.336, TAw acc= 68.1% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.458, TAw acc= 76.4% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.978, TAw acc= 73.6% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.723, TAw acc= 76.4% | *
| Epoch   6, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.576, TAw acc= 84.7% | *
| Epoch   7, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.442, TAw acc= 88.9% | *
| Epoch   8, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.368, TAw acc= 88.9% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.366, TAw acc= 88.9% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.364, TAw acc= 88.9% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.362, TAw acc= 88.9% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.360, TAw acc= 88.9% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.358, TAw acc= 88.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1317 train exemplars, time=  0.0s
1317
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.914 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 81.2%, forg= -0.5% <<<
>>> Test on task  1 : loss=0.867 | TAw acc=100.0%, forg=  0.0%| TAg acc= 65.8%, forg= 26.5% <<<
>>> Test on task  2 : loss=1.091 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 73.9%, forg=  4.3% <<<
>>> Test on task  3 : loss=1.138 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 82.2%, forg=  1.9% <<<
>>> Test on task  4 : loss=1.291 | TAw acc= 98.1%, forg= -1.0%| TAg acc= 75.0%, forg= -1.0% <<<
>>> Test on task  5 : loss=1.243 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 69.4%, forg=  9.3% <<<
>>> Test on task  6 : loss=1.345 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 62.0%, forg= 18.5% <<<
>>> Test on task  7 : loss=1.303 | TAw acc= 84.8%, forg=  0.0%| TAg acc= 71.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.976, TAw acc= 25.0% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.328, TAw acc= 69.6% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.564, TAw acc= 88.0% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.200, TAw acc= 93.5% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.984, TAw acc= 94.6% | *
| Epoch   6, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.870, TAw acc= 97.8% | *
| Epoch   7, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.792, TAw acc= 93.5% | *
| Epoch   8, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.732, TAw acc= 95.7% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.731, TAw acc= 95.7% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.730, TAw acc= 95.7% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.729, TAw acc= 95.7% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.728, TAw acc= 95.7% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.727, TAw acc= 95.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1457 train exemplars, time=  0.0s
1457
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.892 | TAw acc= 89.5%, forg=  0.5%| TAg acc= 79.1%, forg=  2.1% <<<
>>> Test on task  1 : loss=0.789 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.9%, forg=  9.4% <<<
>>> Test on task  2 : loss=1.110 | TAw acc= 93.9%, forg= -0.9%| TAg acc= 73.0%, forg=  5.2% <<<
>>> Test on task  3 : loss=1.095 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 82.2%, forg=  1.9% <<<
>>> Test on task  4 : loss=1.162 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.7%, forg= -7.7% <<<
>>> Test on task  5 : loss=1.282 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 67.6%, forg= 11.1% <<<
>>> Test on task  6 : loss=1.097 | TAw acc= 98.1%, forg= -0.9%| TAg acc= 71.3%, forg=  9.3% <<<
>>> Test on task  7 : loss=1.461 | TAw acc= 92.9%, forg= -8.1%| TAg acc= 60.6%, forg= 11.1% <<<
>>> Test on task  8 : loss=0.956 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 82.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=5.098, TAw acc= 47.4% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.209, TAw acc= 62.8% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.236, TAw acc= 79.5% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.682, TAw acc= 87.2% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.365, TAw acc= 91.0% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.185, TAw acc= 92.3% | *
| Epoch   7, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.070, TAw acc= 92.3% | *
| Epoch   8, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.989, TAw acc= 92.3% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.988, TAw acc= 92.3% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=0.987, TAw acc= 92.3% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.987, TAw acc= 92.3% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.986, TAw acc= 92.3% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=0.985, TAw acc= 92.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1597 train exemplars, time=  0.0s
1597
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.889 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 79.6%, forg=  1.6% <<<
>>> Test on task  1 : loss=0.754 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.9%, forg=  9.4% <<<
>>> Test on task  2 : loss=1.075 | TAw acc= 93.0%, forg=  0.9%| TAg acc= 79.1%, forg= -0.9% <<<
>>> Test on task  3 : loss=1.125 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 74.8%, forg=  9.3% <<<
>>> Test on task  4 : loss=1.157 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.7%, forg= -1.0% <<<
>>> Test on task  5 : loss=1.075 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 75.9%, forg=  2.8% <<<
>>> Test on task  6 : loss=1.107 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.5%, forg= 12.0% <<<
>>> Test on task  7 : loss=1.314 | TAw acc= 93.9%, forg= -1.0%| TAg acc= 66.7%, forg=  5.1% <<<
>>> Test on task  8 : loss=1.208 | TAw acc= 94.3%, forg= -2.5%| TAg acc= 70.5%, forg= 12.3% <<<
>>> Test on task  9 : loss=1.248 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 70.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=5.665, TAw acc= 29.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.446, TAw acc= 48.6% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.420, TAw acc= 72.2% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.856, TAw acc= 94.4% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.554, TAw acc= 95.8% | *
| Epoch   6, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.318, TAw acc= 95.8% | *
| Epoch   7, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.166, TAw acc= 95.8% | *
| Epoch   8, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.037, TAw acc= 95.8% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.036, TAw acc= 95.8% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.036, TAw acc= 95.8% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.035, TAw acc= 95.8% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.034, TAw acc= 95.8% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.033, TAw acc= 95.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1737 train exemplars, time=  0.0s
1737
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.867 | TAw acc= 88.0%, forg=  2.1%| TAg acc= 78.0%, forg=  3.1% <<<
>>> Test on task  1 : loss=0.739 | TAw acc=100.0%, forg=  0.0%| TAg acc= 85.5%, forg=  6.8% <<<
>>> Test on task  2 : loss=1.055 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 73.0%, forg=  6.1% <<<
>>> Test on task  3 : loss=1.086 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 81.3%, forg=  2.8% <<<
>>> Test on task  4 : loss=1.059 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 79.8%, forg=  3.8% <<<
>>> Test on task  5 : loss=1.025 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 76.9%, forg=  1.9% <<<
>>> Test on task  6 : loss=1.014 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.231 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 75.8%, forg= -4.0% <<<
>>> Test on task  8 : loss=1.152 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 70.5%, forg= 12.3% <<<
>>> Test on task  9 : loss=1.551 | TAw acc= 94.4%, forg= -1.9%| TAg acc= 58.9%, forg= 11.2% <<<
>>> Test on task 10 : loss=0.955 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 81.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=5.147, TAw acc= 46.8% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=3.282, TAw acc= 64.6% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.388, TAw acc= 70.9% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.825, TAw acc= 74.7% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 81.0% | *
| Epoch   6, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.307, TAw acc= 84.8% | *
| Epoch   7, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.176, TAw acc= 91.1% | *
| Epoch   8, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.038, TAw acc= 94.9% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.038, TAw acc= 94.9% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.037, TAw acc= 94.9% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.036, TAw acc= 94.9% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.035, TAw acc= 94.9% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.035, TAw acc= 94.9% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1877 train exemplars, time=  0.0s
1877
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.880 | TAw acc= 89.0%, forg=  1.0%| TAg acc= 75.9%, forg=  5.2% <<<
>>> Test on task  1 : loss=0.717 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.8%, forg=  8.5% <<<
>>> Test on task  2 : loss=1.053 | TAw acc= 96.5%, forg= -2.6%| TAg acc= 76.5%, forg=  2.6% <<<
>>> Test on task  3 : loss=1.033 | TAw acc= 94.4%, forg= -0.9%| TAg acc= 79.4%, forg=  4.7% <<<
>>> Test on task  4 : loss=1.104 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 74.0%, forg=  9.6% <<<
>>> Test on task  5 : loss=1.016 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 78.7%, forg=  0.0% <<<
>>> Test on task  6 : loss=0.997 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.165 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 77.8%, forg= -2.0% <<<
>>> Test on task  8 : loss=1.184 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 69.7%, forg= 13.1% <<<
>>> Test on task  9 : loss=1.489 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 57.0%, forg= 13.1% <<<
>>> Test on task 10 : loss=1.034 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.6%, forg=  3.1% <<<
>>> Test on task 11 : loss=1.126 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 81.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.263, TAw acc= 32.9% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=3.475, TAw acc= 60.5% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.470, TAw acc= 75.0% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.994, TAw acc= 82.9% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.609, TAw acc= 78.9% |
| Epoch   6, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.463, TAw acc= 86.8% | *
| Epoch   7, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.244, TAw acc= 89.5% | *
| Epoch   8, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.172, TAw acc= 93.4% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.172, TAw acc= 93.4% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.171, TAw acc= 93.4% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.171, TAw acc= 93.4% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.170, TAw acc= 93.4% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.170, TAw acc= 93.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2017 train exemplars, time=  0.0s
2017
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.891 | TAw acc= 88.5%, forg=  1.6%| TAg acc= 74.3%, forg=  6.8% <<<
>>> Test on task  1 : loss=0.688 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.8%, forg=  8.5% <<<
>>> Test on task  2 : loss=1.013 | TAw acc= 93.0%, forg=  3.5%| TAg acc= 78.3%, forg=  0.9% <<<
>>> Test on task  3 : loss=1.060 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 78.5%, forg=  5.6% <<<
>>> Test on task  4 : loss=1.022 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.8%, forg=  4.8% <<<
>>> Test on task  5 : loss=1.002 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 79.6%, forg= -0.9% <<<
>>> Test on task  6 : loss=0.923 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.9%, forg=  4.6% <<<
>>> Test on task  7 : loss=1.170 | TAw acc= 92.9%, forg=  1.0%| TAg acc= 76.8%, forg=  1.0% <<<
>>> Test on task  8 : loss=1.184 | TAw acc= 95.1%, forg= -0.8%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.401 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 64.5%, forg=  5.6% <<<
>>> Test on task 10 : loss=0.925 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 86.7%, forg= -5.1% <<<
>>> Test on task 11 : loss=1.438 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 63.0%, forg= 18.5% <<<
>>> Test on task 12 : loss=1.073 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 73.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=4.455, TAw acc= 42.7% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.503, TAw acc= 68.5% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.854, TAw acc= 77.5% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.497, TAw acc= 85.4% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.323, TAw acc= 88.8% | *
| Epoch   6, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.202, TAw acc= 89.9% | *
| Epoch   7, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.151, TAw acc= 88.8% | *
| Epoch   8, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.991, TAw acc= 91.0% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.991, TAw acc= 92.1% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.991, TAw acc= 92.1% |
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=0.991, TAw acc= 92.1% |
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.992, TAw acc= 92.1% |
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.992, TAw acc= 92.1% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2157 train exemplars, time=  0.0s
2157
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.941 | TAw acc= 88.5%, forg=  1.6%| TAg acc= 75.9%, forg=  5.2% <<<
>>> Test on task  1 : loss=0.753 | TAw acc=100.0%, forg=  0.0%| TAg acc= 83.8%, forg=  8.5% <<<
>>> Test on task  2 : loss=1.074 | TAw acc= 93.9%, forg=  2.6%| TAg acc= 74.8%, forg=  4.3% <<<
>>> Test on task  3 : loss=1.002 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 77.6%, forg=  6.5% <<<
>>> Test on task  4 : loss=1.066 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.9%, forg=  5.8% <<<
>>> Test on task  5 : loss=0.959 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 75.0%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.064 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.6%, forg= 13.0% <<<
>>> Test on task  7 : loss=1.190 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 75.8%, forg=  2.0% <<<
>>> Test on task  8 : loss=1.130 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 73.0%, forg=  9.8% <<<
>>> Test on task  9 : loss=1.402 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 61.7%, forg=  8.4% <<<
>>> Test on task 10 : loss=0.959 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 81.6%, forg=  5.1% <<<
>>> Test on task 11 : loss=1.264 | TAw acc= 94.4%, forg= -0.9%| TAg acc= 73.1%, forg=  8.3% <<<
>>> Test on task 12 : loss=1.159 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 71.2%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.045 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 77.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=4.687, TAw acc= 54.0% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.644, TAw acc= 60.9% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.911, TAw acc= 82.8% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.547, TAw acc= 81.6% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 90.8% | *
| Epoch   6, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.153, TAw acc= 92.0% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.061, TAw acc= 94.3% | *
| Epoch   8, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.971, TAw acc= 93.1% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.970, TAw acc= 93.1% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.969, TAw acc= 93.1% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.967, TAw acc= 93.1% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.966, TAw acc= 94.3% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=0.965, TAw acc= 94.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2297 train exemplars, time=  0.0s
2297
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.945 | TAw acc= 90.6%, forg= -0.5%| TAg acc= 70.7%, forg= 10.5% <<<
>>> Test on task  1 : loss=0.726 | TAw acc=100.0%, forg=  0.0%| TAg acc= 82.9%, forg=  9.4% <<<
>>> Test on task  2 : loss=1.063 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 78.3%, forg=  0.9% <<<
>>> Test on task  3 : loss=1.020 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 75.7%, forg=  8.4% <<<
>>> Test on task  4 : loss=1.017 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 79.8%, forg=  3.8% <<<
>>> Test on task  5 : loss=0.967 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 76.9%, forg=  2.8% <<<
>>> Test on task  6 : loss=0.977 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.130 | TAw acc= 94.9%, forg= -1.0%| TAg acc= 75.8%, forg=  2.0% <<<
>>> Test on task  8 : loss=1.165 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.343 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 68.2%, forg=  1.9% <<<
>>> Test on task 10 : loss=0.862 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 84.7%, forg=  2.0% <<<
>>> Test on task 11 : loss=1.181 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 78.7%, forg=  2.8% <<<
>>> Test on task 12 : loss=1.137 | TAw acc= 95.2%, forg= -1.0%| TAg acc= 71.2%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.417 | TAw acc= 94.1%, forg= -0.8%| TAg acc= 53.4%, forg= 23.7% <<<
>>> Test on task 14 : loss=0.991 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 76.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=5.711, TAw acc= 41.6% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=3.105, TAw acc= 59.7% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.129, TAw acc= 76.6% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.510, TAw acc= 84.4% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.328, TAw acc= 89.6% | *
| Epoch   6, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.197, TAw acc= 84.4% | *
| Epoch   7, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.103, TAw acc= 93.5% | *
| Epoch   8, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.999, TAw acc= 94.8% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.999, TAw acc= 94.8% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=0.999, TAw acc= 94.8% |
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.000, TAw acc= 94.8% |
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.000, TAw acc= 94.8% |
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.001, TAw acc= 94.8% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2437 train exemplars, time=  0.0s
2437
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.941 | TAw acc= 90.1%, forg=  0.5%| TAg acc= 74.9%, forg=  6.3% <<<
>>> Test on task  1 : loss=0.787 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.2%, forg= 11.1% <<<
>>> Test on task  2 : loss=1.032 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 74.8%, forg=  4.3% <<<
>>> Test on task  3 : loss=0.960 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 83.2%, forg=  0.9% <<<
>>> Test on task  4 : loss=0.990 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 80.8%, forg=  2.9% <<<
>>> Test on task  5 : loss=0.921 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 78.7%, forg=  0.9% <<<
>>> Test on task  6 : loss=0.984 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.180 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 71.7%, forg=  6.1% <<<
>>> Test on task  8 : loss=1.089 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 74.6%, forg=  8.2% <<<
>>> Test on task  9 : loss=1.340 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task 10 : loss=0.829 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.7%, forg=  4.1% <<<
>>> Test on task 11 : loss=1.138 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 77.8%, forg=  3.7% <<<
>>> Test on task 12 : loss=1.054 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 71.2%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.446 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 49.2%, forg= 28.0% <<<
>>> Test on task 14 : loss=1.315 | TAw acc= 98.3%, forg= -2.6%| TAg acc= 61.2%, forg= 15.5% <<<
>>> Test on task 15 : loss=1.051 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=5.656, TAw acc= 38.4% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=3.336, TAw acc= 61.6% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.193, TAw acc= 80.2% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.604, TAw acc= 91.9% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.237, TAw acc= 91.9% | *
| Epoch   6, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.108, TAw acc= 97.7% | *
| Epoch   7, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.034, TAw acc= 97.7% | *
| Epoch   8, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=0.966, TAw acc= 97.7% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=0.964, TAw acc= 97.7% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=0.963, TAw acc= 97.7% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=0.961, TAw acc= 97.7% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=0.960, TAw acc= 97.7% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=0.959, TAw acc= 97.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2577 train exemplars, time=  0.0s
2577
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.936 | TAw acc= 90.1%, forg=  0.5%| TAg acc= 74.9%, forg=  6.3% <<<
>>> Test on task  1 : loss=0.824 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.9%, forg= 15.4% <<<
>>> Test on task  2 : loss=1.068 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.9%, forg=  5.2% <<<
>>> Test on task  3 : loss=0.949 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 82.2%, forg=  1.9% <<<
>>> Test on task  4 : loss=0.970 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.7%, forg=  0.0% <<<
>>> Test on task  5 : loss=0.910 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 77.8%, forg=  1.9% <<<
>>> Test on task  6 : loss=0.948 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.9%, forg=  4.6% <<<
>>> Test on task  7 : loss=1.121 | TAw acc= 97.0%, forg= -2.0%| TAg acc= 76.8%, forg=  1.0% <<<
>>> Test on task  8 : loss=1.048 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 77.0%, forg=  5.7% <<<
>>> Test on task  9 : loss=1.324 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 71.0%, forg= -0.9% <<<
>>> Test on task 10 : loss=0.823 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.6%, forg=  8.2% <<<
>>> Test on task 11 : loss=1.088 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 78.7%, forg=  2.8% <<<
>>> Test on task 12 : loss=1.017 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 73.1%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.318 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 56.8%, forg= 20.3% <<<
>>> Test on task 14 : loss=1.138 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.3%, forg=  3.4% <<<
>>> Test on task 15 : loss=1.397 | TAw acc= 95.2%, forg= -1.0%| TAg acc= 62.9%, forg= 18.1% <<<
>>> Test on task 16 : loss=1.042 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 84.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=5.540, TAw acc= 44.4% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=3.356, TAw acc= 67.9% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.171, TAw acc= 80.2% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.696, TAw acc= 92.6% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.302, TAw acc= 87.7% | *
| Epoch   6, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.337, TAw acc= 92.6% |
| Epoch   7, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 91.4% | *
| Epoch   8, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.070, TAw acc= 93.8% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.070, TAw acc= 93.8% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.071, TAw acc= 93.8% |
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.072, TAw acc= 93.8% |
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.072, TAw acc= 93.8% |
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.073, TAw acc= 93.8% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2717 train exemplars, time=  0.0s
2717
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.003 | TAw acc= 89.5%, forg=  1.0%| TAg acc= 71.2%, forg=  9.9% <<<
>>> Test on task  1 : loss=0.811 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.9%, forg= 15.4% <<<
>>> Test on task  2 : loss=1.033 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 78.3%, forg=  0.9% <<<
>>> Test on task  3 : loss=0.952 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 78.5%, forg=  5.6% <<<
>>> Test on task  4 : loss=1.222 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 72.1%, forg= 11.5% <<<
>>> Test on task  5 : loss=0.904 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 75.9%, forg=  3.7% <<<
>>> Test on task  6 : loss=1.037 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.9%, forg=  4.6% <<<
>>> Test on task  7 : loss=1.215 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 67.7%, forg= 10.1% <<<
>>> Test on task  8 : loss=1.082 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.6%, forg=  8.2% <<<
>>> Test on task  9 : loss=1.361 | TAw acc= 95.3%, forg= -0.9%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 10 : loss=0.791 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 87.8%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.093 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 78.7%, forg=  2.8% <<<
>>> Test on task 12 : loss=0.970 | TAw acc= 96.2%, forg= -1.0%| TAg acc= 73.1%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.498 | TAw acc= 89.0%, forg=  5.1%| TAg acc= 51.7%, forg= 25.4% <<<
>>> Test on task 14 : loss=1.090 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.7%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.375 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 70.5%, forg= 10.5% <<<
>>> Test on task 16 : loss=1.297 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 67.8%, forg= 16.5% <<<
>>> Test on task 17 : loss=1.036 | TAw acc= 90.9%, forg=  0.0%| TAg acc= 76.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.4s loss=4.262, TAw acc= 53.0% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.220, TAw acc= 78.0% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.526, TAw acc= 93.0% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.208, TAw acc= 98.0% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.099, TAw acc= 98.0% | *
| Epoch   6, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=0.972, TAw acc= 98.0% | *
| Epoch   7, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 99.0% | *
| Epoch   8, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=0.958, TAw acc= 99.0% |
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 99.0% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 99.0% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 99.0% |
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 99.0% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=0.936, TAw acc= 99.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2857 train exemplars, time=  0.0s
2857
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.979 | TAw acc= 89.5%, forg=  1.0%| TAg acc= 74.3%, forg=  6.8% <<<
>>> Test on task  1 : loss=0.873 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.2%, forg= 17.1% <<<
>>> Test on task  2 : loss=1.114 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 73.9%, forg=  5.2% <<<
>>> Test on task  3 : loss=1.034 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 77.6%, forg=  6.5% <<<
>>> Test on task  4 : loss=1.068 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.7%, forg=  1.0% <<<
>>> Test on task  5 : loss=0.901 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 75.0%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.011 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.248 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 66.7%, forg= 11.1% <<<
>>> Test on task  8 : loss=1.068 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 78.7%, forg=  4.1% <<<
>>> Test on task  9 : loss=1.436 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 10 : loss=0.889 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.5%, forg= 11.2% <<<
>>> Test on task 11 : loss=1.075 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 79.6%, forg=  1.9% <<<
>>> Test on task 12 : loss=0.980 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.9%, forg= -3.8% <<<
>>> Test on task 13 : loss=1.348 | TAw acc= 91.5%, forg=  2.5%| TAg acc= 51.7%, forg= 25.4% <<<
>>> Test on task 14 : loss=1.062 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.1%, forg=  2.6% <<<
>>> Test on task 15 : loss=1.257 | TAw acc= 96.2%, forg= -1.0%| TAg acc= 72.4%, forg=  8.6% <<<
>>> Test on task 16 : loss=1.234 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.301 | TAw acc= 95.5%, forg= -4.5%| TAg acc= 63.6%, forg= 12.7% <<<
>>> Test on task 18 : loss=1.021 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=6.120, TAw acc= 35.7% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=3.298, TAw acc= 65.5% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.299, TAw acc= 79.8% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.785, TAw acc= 86.9% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.639, TAw acc= 86.9% | *
| Epoch   6, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.435, TAw acc= 90.5% | *
| Epoch   7, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.431, TAw acc= 90.5% | *
| Epoch   8, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 88.1% |
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.429, TAw acc= 90.5% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.428, TAw acc= 90.5% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.426, TAw acc= 90.5% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.424, TAw acc= 90.5% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.423, TAw acc= 91.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2997 train exemplars, time=  0.0s
2997
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.008 | TAw acc= 89.5%, forg=  1.0%| TAg acc= 71.2%, forg=  9.9% <<<
>>> Test on task  1 : loss=0.791 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.8%, forg= 14.5% <<<
>>> Test on task  2 : loss=1.199 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 72.2%, forg=  7.0% <<<
>>> Test on task  3 : loss=1.025 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 76.6%, forg=  7.5% <<<
>>> Test on task  4 : loss=1.102 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.7%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.041 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 70.4%, forg=  9.3% <<<
>>> Test on task  6 : loss=0.986 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  3.7% <<<
>>> Test on task  7 : loss=1.149 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 75.8%, forg=  2.0% <<<
>>> Test on task  8 : loss=1.085 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 74.6%, forg=  8.2% <<<
>>> Test on task  9 : loss=1.518 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 62.6%, forg=  8.4% <<<
>>> Test on task 10 : loss=0.862 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.5%, forg= 12.2% <<<
>>> Test on task 11 : loss=1.081 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 80.6%, forg=  0.9% <<<
>>> Test on task 12 : loss=0.960 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.0%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.296 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 55.1%, forg= 22.0% <<<
>>> Test on task 14 : loss=1.036 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.9%, forg=  0.9% <<<
>>> Test on task 15 : loss=1.237 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 79.0%, forg=  1.9% <<<
>>> Test on task 16 : loss=1.244 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 74.8%, forg=  9.6% <<<
>>> Test on task 17 : loss=1.247 | TAw acc= 92.7%, forg=  2.7%| TAg acc= 62.7%, forg= 13.6% <<<
>>> Test on task 18 : loss=1.458 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 59.1%, forg= 15.9% <<<
>>> Test on task 19 : loss=1.391 | TAw acc= 86.8%, forg=  0.0%| TAg acc= 57.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=5.574, TAw acc= 62.1% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.504, TAw acc= 67.8% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.698, TAw acc= 89.7% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 90.8% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.190, TAw acc= 89.7% | *
| Epoch   6, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.119, TAw acc= 93.1% | *
| Epoch   7, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.043, TAw acc= 92.0% | *
| Epoch   8, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.962, TAw acc= 93.1% | *
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=0.962, TAw acc= 93.1% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=0.961, TAw acc= 93.1% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=0.961, TAw acc= 93.1% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=0.961, TAw acc= 93.1% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=0.960, TAw acc= 93.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3137 train exemplars, time=  0.0s
3137
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.981 | TAw acc= 89.0%, forg=  1.6%| TAg acc= 71.7%, forg=  9.4% <<<
>>> Test on task  1 : loss=0.815 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.8%, forg= 14.5% <<<
>>> Test on task  2 : loss=1.192 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 70.4%, forg=  8.7% <<<
>>> Test on task  3 : loss=0.983 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 75.7%, forg=  8.4% <<<
>>> Test on task  4 : loss=1.152 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.8%, forg=  4.8% <<<
>>> Test on task  5 : loss=0.960 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 72.2%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.043 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.240 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 74.7%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.181 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 73.0%, forg=  9.8% <<<
>>> Test on task  9 : loss=1.405 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 67.3%, forg=  3.7% <<<
>>> Test on task 10 : loss=0.835 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.6%, forg=  9.2% <<<
>>> Test on task 11 : loss=1.097 | TAw acc= 92.6%, forg=  1.9%| TAg acc= 76.9%, forg=  4.6% <<<
>>> Test on task 12 : loss=0.958 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.270 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 55.9%, forg= 21.2% <<<
>>> Test on task 14 : loss=1.071 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task 15 : loss=1.167 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 82.9%, forg= -1.9% <<<
>>> Test on task 16 : loss=1.252 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 70.4%, forg= 13.9% <<<
>>> Test on task 17 : loss=1.248 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 64.5%, forg= 11.8% <<<
>>> Test on task 18 : loss=1.407 | TAw acc= 95.5%, forg=  0.8%| TAg acc= 65.9%, forg=  9.1% <<<
>>> Test on task 19 : loss=1.683 | TAw acc= 86.8%, forg=  0.0%| TAg acc= 37.7%, forg= 19.3% <<<
>>> Test on task 20 : loss=0.905 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=6.631, TAw acc= 49.3% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=3.172, TAw acc= 78.9% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=2.050, TAw acc= 88.7% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.667, TAw acc= 93.0% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.526, TAw acc= 93.0% | *
| Epoch   6, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.420, TAw acc= 90.1% | *
| Epoch   7, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.362, TAw acc= 94.4% | *
| Epoch   8, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.379, TAw acc= 94.4% |
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.353, TAw acc= 94.4% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.345, TAw acc= 94.4% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.338, TAw acc= 94.4% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 94.4% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.329, TAw acc= 94.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3277 train exemplars, time=  0.1s
3277
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.030 | TAw acc= 89.0%, forg=  1.6%| TAg acc= 69.6%, forg= 11.5% <<<
>>> Test on task  1 : loss=0.814 | TAw acc=100.0%, forg=  0.0%| TAg acc= 81.2%, forg= 11.1% <<<
>>> Test on task  2 : loss=1.301 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 66.1%, forg= 13.0% <<<
>>> Test on task  3 : loss=0.988 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 77.6%, forg=  6.5% <<<
>>> Test on task  4 : loss=1.135 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.056 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 71.3%, forg=  8.3% <<<
>>> Test on task  6 : loss=1.018 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  3.7% <<<
>>> Test on task  7 : loss=1.286 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 71.7%, forg=  6.1% <<<
>>> Test on task  8 : loss=1.160 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.0%, forg=  9.8% <<<
>>> Test on task  9 : loss=1.489 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.2%, forg=  2.8% <<<
>>> Test on task 10 : loss=0.868 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.6%, forg=  9.2% <<<
>>> Test on task 11 : loss=1.057 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 83.3%, forg= -1.9% <<<
>>> Test on task 12 : loss=0.959 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.0%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.374 | TAw acc= 92.4%, forg=  1.7%| TAg acc= 52.5%, forg= 24.6% <<<
>>> Test on task 14 : loss=0.975 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.7%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.122 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.9%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.182 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 76.5%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.158 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 68.2%, forg=  8.2% <<<
>>> Test on task 18 : loss=1.280 | TAw acc= 95.5%, forg=  0.8%| TAg acc= 69.7%, forg=  5.3% <<<
>>> Test on task 19 : loss=1.827 | TAw acc= 86.0%, forg=  0.9%| TAg acc= 36.8%, forg= 20.2% <<<
>>> Test on task 20 : loss=1.172 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 69.6%, forg= 10.4% <<<
>>> Test on task 21 : loss=1.087 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 71.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=5.593, TAw acc= 46.0% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=2.966, TAw acc= 70.1% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.816, TAw acc= 82.8% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.397, TAw acc= 88.5% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.234, TAw acc= 94.3% | *
| Epoch   6, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.171, TAw acc= 95.4% | *
| Epoch   7, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.193, TAw acc= 93.1% |
| Epoch   8, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.086, TAw acc= 93.1% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.081, TAw acc= 93.1% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.077, TAw acc= 93.1% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.073, TAw acc= 93.1% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.069, TAw acc= 93.1% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.065, TAw acc= 93.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3417 train exemplars, time=  0.0s
3417
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.033 | TAw acc= 90.1%, forg=  0.5%| TAg acc= 69.1%, forg= 12.0% <<<
>>> Test on task  1 : loss=0.851 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 79.5%, forg= 12.8% <<<
>>> Test on task  2 : loss=1.101 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 76.5%, forg=  2.6% <<<
>>> Test on task  3 : loss=0.982 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 78.5%, forg=  5.6% <<<
>>> Test on task  4 : loss=1.090 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.7%, forg=  1.9% <<<
>>> Test on task  5 : loss=1.012 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 70.4%, forg=  9.3% <<<
>>> Test on task  6 : loss=1.058 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.9%, forg=  4.6% <<<
>>> Test on task  7 : loss=1.260 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task  8 : loss=1.209 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 76.2%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.455 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 67.3%, forg=  3.7% <<<
>>> Test on task 10 : loss=0.894 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 83.7%, forg=  4.1% <<<
>>> Test on task 11 : loss=1.106 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 83.3%, forg=  0.0% <<<
>>> Test on task 12 : loss=1.001 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 72.1%, forg=  4.8% <<<
>>> Test on task 13 : loss=1.411 | TAw acc= 92.4%, forg=  1.7%| TAg acc= 50.0%, forg= 27.1% <<<
>>> Test on task 14 : loss=1.027 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 72.4%, forg=  4.3% <<<
>>> Test on task 15 : loss=1.091 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.9%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.234 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 73.9%, forg= 10.4% <<<
>>> Test on task 17 : loss=1.096 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 66.4%, forg= 10.0% <<<
>>> Test on task 18 : loss=1.450 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 62.9%, forg= 12.1% <<<
>>> Test on task 19 : loss=1.600 | TAw acc= 86.8%, forg=  0.0%| TAg acc= 48.2%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.104 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 66.1%, forg= 13.9% <<<
>>> Test on task 21 : loss=1.333 | TAw acc= 98.0%, forg= -2.0%| TAg acc= 60.2%, forg= 11.2% <<<
>>> Test on task 22 : loss=0.972 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 70.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=5.187, TAw acc= 42.4% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=2.715, TAw acc= 73.9% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.893, TAw acc= 78.3% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.541, TAw acc= 89.1% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.279, TAw acc= 88.0% | *
| Epoch   6, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.796, TAw acc= 89.1% |
| Epoch   7, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 93.5% | *
| Epoch   8, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.046, TAw acc= 92.4% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.047, TAw acc= 92.4% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.047, TAw acc= 92.4% |
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.047, TAw acc= 93.5% |
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.048, TAw acc= 93.5% |
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.048, TAw acc= 93.5% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3557 train exemplars, time=  0.0s
3557
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.015 | TAw acc= 90.6%, forg=  0.0%| TAg acc= 72.8%, forg=  8.4% <<<
>>> Test on task  1 : loss=0.896 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.8%, forg= 20.5% <<<
>>> Test on task  2 : loss=1.149 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 69.6%, forg=  9.6% <<<
>>> Test on task  3 : loss=1.030 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 77.6%, forg=  6.5% <<<
>>> Test on task  4 : loss=1.100 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 79.8%, forg=  3.8% <<<
>>> Test on task  5 : loss=0.970 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 70.4%, forg=  9.3% <<<
>>> Test on task  6 : loss=1.083 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.0%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.214 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 73.7%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.188 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 74.6%, forg=  8.2% <<<
>>> Test on task  9 : loss=1.492 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 68.2%, forg=  2.8% <<<
>>> Test on task 10 : loss=0.931 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 76.5%, forg= 11.2% <<<
>>> Test on task 11 : loss=1.055 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 80.6%, forg=  2.8% <<<
>>> Test on task 12 : loss=1.033 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 72.1%, forg=  4.8% <<<
>>> Test on task 13 : loss=1.448 | TAw acc= 92.4%, forg=  1.7%| TAg acc= 54.2%, forg= 22.9% <<<
>>> Test on task 14 : loss=1.182 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 64.7%, forg= 12.1% <<<
>>> Test on task 15 : loss=1.093 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.0%, forg=  1.9% <<<
>>> Test on task 16 : loss=1.246 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.9%, forg= 10.4% <<<
>>> Test on task 17 : loss=1.067 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 65.5%, forg= 10.9% <<<
>>> Test on task 18 : loss=1.320 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 71.2%, forg=  3.8% <<<
>>> Test on task 19 : loss=1.545 | TAw acc= 87.7%, forg= -0.9%| TAg acc= 50.9%, forg=  6.1% <<<
>>> Test on task 20 : loss=1.037 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 70.4%, forg=  9.6% <<<
>>> Test on task 21 : loss=1.235 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 62.2%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.298 | TAw acc= 94.9%, forg=  1.7%| TAg acc= 59.0%, forg= 11.1% <<<
>>> Test on task 23 : loss=0.966 | TAw acc= 93.4%, forg=  0.0%| TAg acc= 75.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=5.372, TAw acc= 51.7% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.793, TAw acc= 77.0% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.874, TAw acc= 89.7% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.561, TAw acc= 92.0% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 92.0% | *
| Epoch   6, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.251, TAw acc= 95.4% |
| Epoch   7, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.119, TAw acc= 96.6% | *
| Epoch   8, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.107, TAw acc= 97.7% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.105, TAw acc= 97.7% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.103, TAw acc= 97.7% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.101, TAw acc= 97.7% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.099, TAw acc= 97.7% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.097, TAw acc= 97.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3697 train exemplars, time=  0.0s
3697
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.038 | TAw acc= 90.6%, forg=  0.0%| TAg acc= 70.7%, forg= 10.5% <<<
>>> Test on task  1 : loss=0.893 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 76.9%, forg= 15.4% <<<
>>> Test on task  2 : loss=1.114 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 74.8%, forg=  4.3% <<<
>>> Test on task  3 : loss=1.032 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 75.7%, forg=  8.4% <<<
>>> Test on task  4 : loss=1.145 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.003 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 73.1%, forg=  6.5% <<<
>>> Test on task  6 : loss=1.092 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  3.7% <<<
>>> Test on task  7 : loss=1.232 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 71.7%, forg=  6.1% <<<
>>> Test on task  8 : loss=1.243 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 75.4%, forg=  7.4% <<<
>>> Test on task  9 : loss=1.477 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 10 : loss=1.011 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 76.5%, forg= 11.2% <<<
>>> Test on task 11 : loss=1.075 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 82.4%, forg=  0.9% <<<
>>> Test on task 12 : loss=1.018 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.0%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.446 | TAw acc= 92.4%, forg=  1.7%| TAg acc= 55.1%, forg= 22.0% <<<
>>> Test on task 14 : loss=1.058 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 65.5%, forg= 11.2% <<<
>>> Test on task 15 : loss=1.078 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 84.8%, forg= -1.9% <<<
>>> Test on task 16 : loss=1.257 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.112 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 66.4%, forg= 10.0% <<<
>>> Test on task 18 : loss=1.355 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 64.4%, forg= 10.6% <<<
>>> Test on task 19 : loss=1.548 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 50.9%, forg=  6.1% <<<
>>> Test on task 20 : loss=0.977 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 75.7%, forg=  4.3% <<<
>>> Test on task 21 : loss=1.184 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.3%, forg=  8.2% <<<
>>> Test on task 22 : loss=1.232 | TAw acc= 94.0%, forg=  2.6%| TAg acc= 60.7%, forg=  9.4% <<<
>>> Test on task 23 : loss=1.193 | TAw acc= 94.3%, forg= -0.8%| TAg acc= 68.9%, forg=  6.6% <<<
>>> Test on task 24 : loss=0.971 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=6.853, TAw acc= 32.9% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=4.075, TAw acc= 61.4% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.890, TAw acc= 74.3% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.435, TAw acc= 78.6% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.071, TAw acc= 84.3% | *
| Epoch   6, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.881, TAw acc= 84.3% | *
| Epoch   7, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.697, TAw acc= 85.7% | *
| Epoch   8, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.757, TAw acc= 85.7% |
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.700, TAw acc= 85.7% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.703, TAw acc= 85.7% |
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.705, TAw acc= 85.7% |
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.708, TAw acc= 85.7% |
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.710, TAw acc= 85.7% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3837 train exemplars, time=  0.0s
3837
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.088 | TAw acc= 90.6%, forg=  0.0%| TAg acc= 67.5%, forg= 13.6% <<<
>>> Test on task  1 : loss=0.873 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 79.5%, forg= 12.8% <<<
>>> Test on task  2 : loss=1.156 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 71.3%, forg=  7.8% <<<
>>> Test on task  3 : loss=1.015 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 76.6%, forg=  7.5% <<<
>>> Test on task  4 : loss=1.084 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 79.8%, forg=  3.8% <<<
>>> Test on task  5 : loss=1.021 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 72.2%, forg=  7.4% <<<
>>> Test on task  6 : loss=1.130 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 71.3%, forg=  9.3% <<<
>>> Test on task  7 : loss=1.472 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 61.6%, forg= 16.2% <<<
>>> Test on task  8 : loss=1.252 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 73.8%, forg=  9.0% <<<
>>> Test on task  9 : loss=1.485 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.2%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.001 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.068 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 82.4%, forg=  0.9% <<<
>>> Test on task 12 : loss=1.007 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 74.0%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.406 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 57.6%, forg= 19.5% <<<
>>> Test on task 14 : loss=0.985 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.4%, forg= -1.7% <<<
>>> Test on task 15 : loss=1.090 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 79.0%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.211 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 76.5%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.048 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 68.2%, forg=  8.2% <<<
>>> Test on task 18 : loss=1.382 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 64.4%, forg= 10.6% <<<
>>> Test on task 19 : loss=1.503 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 56.1%, forg=  0.9% <<<
>>> Test on task 20 : loss=0.969 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 78.3%, forg=  1.7% <<<
>>> Test on task 21 : loss=1.250 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 61.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.379 | TAw acc= 94.9%, forg=  1.7%| TAg acc= 52.1%, forg= 17.9% <<<
>>> Test on task 23 : loss=1.100 | TAw acc= 95.1%, forg= -0.8%| TAg acc= 68.9%, forg=  6.6% <<<
>>> Test on task 24 : loss=1.180 | TAw acc= 98.3%, forg= -0.9%| TAg acc= 62.1%, forg= 12.9% <<<
>>> Test on task 25 : loss=1.353 | TAw acc= 90.6%, forg=  0.0%| TAg acc= 70.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=4.948, TAw acc= 64.4% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.206, TAw acc= 86.2% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.581, TAw acc= 93.1% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.216, TAw acc= 97.7% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.037, TAw acc= 97.7% | *
| Epoch   6, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.999, TAw acc= 97.7% | *
| Epoch   7, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.954, TAw acc= 96.6% | *
| Epoch   8, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.971, TAw acc= 97.7% |
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=0.954, TAw acc= 96.6% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.953, TAw acc= 96.6% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.952, TAw acc= 96.6% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.952, TAw acc= 96.6% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=0.951, TAw acc= 96.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3977 train exemplars, time=  0.0s
3977
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.035 | TAw acc= 90.6%, forg=  0.0%| TAg acc= 70.2%, forg= 11.0% <<<
>>> Test on task  1 : loss=0.922 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 76.1%, forg= 16.2% <<<
>>> Test on task  2 : loss=1.232 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 68.7%, forg= 10.4% <<<
>>> Test on task  3 : loss=1.033 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 75.7%, forg=  8.4% <<<
>>> Test on task  4 : loss=1.159 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.137 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.6%, forg= 12.0% <<<
>>> Test on task  6 : loss=1.212 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.257 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 73.7%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.253 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 73.8%, forg=  9.0% <<<
>>> Test on task  9 : loss=1.513 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 68.2%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.008 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.051 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 84.3%, forg= -0.9% <<<
>>> Test on task 12 : loss=0.993 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 76.9%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.489 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 60.2%, forg= 16.9% <<<
>>> Test on task 14 : loss=1.081 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 63.8%, forg= 14.7% <<<
>>> Test on task 15 : loss=1.103 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 79.0%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.237 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.128 | TAw acc= 97.3%, forg= -1.8%| TAg acc= 64.5%, forg= 11.8% <<<
>>> Test on task 18 : loss=1.499 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 60.6%, forg= 14.4% <<<
>>> Test on task 19 : loss=1.642 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 53.5%, forg=  3.5% <<<
>>> Test on task 20 : loss=0.953 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 77.4%, forg=  2.6% <<<
>>> Test on task 21 : loss=1.215 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.3%, forg=  8.2% <<<
>>> Test on task 22 : loss=1.262 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 63.2%, forg=  6.8% <<<
>>> Test on task 23 : loss=1.075 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 73.8%, forg=  1.6% <<<
>>> Test on task 24 : loss=1.190 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 68.1%, forg=  6.9% <<<
>>> Test on task 25 : loss=1.690 | TAw acc= 92.7%, forg= -2.1%| TAg acc= 47.9%, forg= 22.9% <<<
>>> Test on task 26 : loss=1.077 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 73.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=6.234, TAw acc= 45.8% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=3.273, TAw acc= 61.1% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.068, TAw acc= 83.3% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.830, TAw acc= 91.7% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.638, TAw acc= 93.1% | *
| Epoch   6, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.516, TAw acc= 95.8% | *
| Epoch   7, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.422, TAw acc= 97.2% | *
| Epoch   8, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.382, TAw acc= 97.2% | *
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.378, TAw acc= 97.2% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.374, TAw acc= 97.2% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.371, TAw acc= 97.2% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.369, TAw acc= 97.2% | *
| Epoch   5, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.366, TAw acc= 97.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4117 train exemplars, time=  0.0s
4117
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.041 | TAw acc= 91.1%, forg= -0.5%| TAg acc= 69.6%, forg= 11.5% <<<
>>> Test on task  1 : loss=0.896 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 75.2%, forg= 17.1% <<<
>>> Test on task  2 : loss=1.242 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 68.7%, forg= 10.4% <<<
>>> Test on task  3 : loss=1.015 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 75.7%, forg=  8.4% <<<
>>> Test on task  4 : loss=1.106 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.9%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.139 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 66.7%, forg= 13.0% <<<
>>> Test on task  6 : loss=1.119 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 74.1%, forg=  6.5% <<<
>>> Test on task  7 : loss=1.295 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task  8 : loss=1.274 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 73.8%, forg=  9.0% <<<
>>> Test on task  9 : loss=1.615 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 62.6%, forg=  8.4% <<<
>>> Test on task 10 : loss=1.050 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.091 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 82.4%, forg=  1.9% <<<
>>> Test on task 12 : loss=0.967 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 77.9%, forg= -1.0% <<<
>>> Test on task 13 : loss=1.493 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 55.9%, forg= 21.2% <<<
>>> Test on task 14 : loss=1.139 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 66.4%, forg= 12.1% <<<
>>> Test on task 15 : loss=1.154 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 80.0%, forg=  4.8% <<<
>>> Test on task 16 : loss=1.243 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.044 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 70.0%, forg=  6.4% <<<
>>> Test on task 18 : loss=1.421 | TAw acc= 95.5%, forg=  0.8%| TAg acc= 62.1%, forg= 12.9% <<<
>>> Test on task 19 : loss=1.596 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 57.0%, forg=  0.0% <<<
>>> Test on task 20 : loss=0.959 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 74.8%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.203 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 65.3%, forg=  6.1% <<<
>>> Test on task 22 : loss=1.194 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 68.4%, forg=  1.7% <<<
>>> Test on task 23 : loss=1.018 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 74.6%, forg=  0.8% <<<
>>> Test on task 24 : loss=1.114 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 66.4%, forg=  8.6% <<<
>>> Test on task 25 : loss=1.627 | TAw acc= 92.7%, forg=  0.0%| TAg acc= 55.2%, forg= 15.6% <<<
>>> Test on task 26 : loss=1.437 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 59.5%, forg= 13.8% <<<
>>> Test on task 27 : loss=1.196 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 70.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=5.834, TAw acc= 53.8% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.822, TAw acc= 79.5% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.704, TAw acc= 92.3% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 98.7% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.125, TAw acc= 97.4% | *
| Epoch   6, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.038, TAw acc= 98.7% | *
| Epoch   7, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.009, TAw acc= 98.7% | *
| Epoch   8, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=0.941, TAw acc= 98.7% | *
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=0.941, TAw acc= 98.7% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 98.7% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 98.7% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=0.939, TAw acc= 98.7% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 98.7% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4257 train exemplars, time=  0.0s
4257
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.011 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 69.6%, forg= 11.5% <<<
>>> Test on task  1 : loss=0.949 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 76.9%, forg= 15.4% <<<
>>> Test on task  2 : loss=1.206 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 70.4%, forg=  8.7% <<<
>>> Test on task  3 : loss=1.054 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 74.8%, forg=  9.3% <<<
>>> Test on task  4 : loss=1.127 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.8%, forg=  4.8% <<<
>>> Test on task  5 : loss=1.131 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 67.6%, forg= 12.0% <<<
>>> Test on task  6 : loss=1.139 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 74.1%, forg=  6.5% <<<
>>> Test on task  7 : loss=1.294 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task  8 : loss=1.296 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 76.2%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.546 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 67.3%, forg=  3.7% <<<
>>> Test on task 10 : loss=1.018 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 75.5%, forg= 12.2% <<<
>>> Test on task 11 : loss=1.143 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 80.6%, forg=  3.7% <<<
>>> Test on task 12 : loss=1.029 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 77.9%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.515 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 58.5%, forg= 18.6% <<<
>>> Test on task 14 : loss=1.057 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 68.1%, forg= 10.3% <<<
>>> Test on task 15 : loss=1.153 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.0%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.291 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.050 | TAw acc= 95.5%, forg=  1.8%| TAg acc= 70.9%, forg=  5.5% <<<
>>> Test on task 18 : loss=1.494 | TAw acc= 94.7%, forg=  1.5%| TAg acc= 61.4%, forg= 13.6% <<<
>>> Test on task 19 : loss=1.539 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 54.4%, forg=  2.6% <<<
>>> Test on task 20 : loss=0.948 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 73.9%, forg=  6.1% <<<
>>> Test on task 21 : loss=1.152 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 67.3%, forg=  4.1% <<<
>>> Test on task 22 : loss=1.187 | TAw acc= 94.9%, forg=  1.7%| TAg acc= 66.7%, forg=  3.4% <<<
>>> Test on task 23 : loss=0.990 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 77.9%, forg= -2.5% <<<
>>> Test on task 24 : loss=1.114 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.6%, forg=  3.4% <<<
>>> Test on task 25 : loss=1.499 | TAw acc= 93.8%, forg= -1.0%| TAg acc= 57.3%, forg= 13.5% <<<
>>> Test on task 26 : loss=1.469 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 53.4%, forg= 19.8% <<<
>>> Test on task 27 : loss=1.484 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 57.0%, forg= 13.0% <<<
>>> Test on task 28 : loss=1.070 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 73.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=5.203, TAw acc= 57.6% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.328, TAw acc= 82.4% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 95.3% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.140, TAw acc= 97.6% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 96.5% |
| Epoch   6, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.089, TAw acc= 97.6% | *
| Epoch   7, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.015, TAw acc= 97.6% | *
| Epoch   8, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.062, TAw acc= 97.6% |
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.012, TAw acc= 97.6% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.009, TAw acc= 97.6% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.007, TAw acc= 97.6% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.005, TAw acc= 97.6% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.003, TAw acc= 97.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4397 train exemplars, time=  0.0s
4397
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.016 | TAw acc= 90.6%, forg=  0.5%| TAg acc= 67.5%, forg= 13.6% <<<
>>> Test on task  1 : loss=1.028 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.8%, forg= 20.5% <<<
>>> Test on task  2 : loss=1.329 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 62.6%, forg= 16.5% <<<
>>> Test on task  3 : loss=1.101 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 73.8%, forg= 10.3% <<<
>>> Test on task  4 : loss=1.120 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.148 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.8%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.172 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.287 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 70.7%, forg=  7.1% <<<
>>> Test on task  8 : loss=1.361 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 78.7%, forg=  4.1% <<<
>>> Test on task  9 : loss=1.598 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 65.4%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.001 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.5%, forg= 11.2% <<<
>>> Test on task 11 : loss=1.093 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task 12 : loss=1.006 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 75.0%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.422 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 61.9%, forg= 15.3% <<<
>>> Test on task 14 : loss=1.041 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.8%, forg=  8.6% <<<
>>> Test on task 15 : loss=1.127 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.0%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.293 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.034 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 69.1%, forg=  7.3% <<<
>>> Test on task 18 : loss=1.412 | TAw acc= 95.5%, forg=  0.8%| TAg acc= 66.7%, forg=  8.3% <<<
>>> Test on task 19 : loss=1.469 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 60.5%, forg= -3.5% <<<
>>> Test on task 20 : loss=0.924 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 75.7%, forg=  4.3% <<<
>>> Test on task 21 : loss=1.168 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.4%, forg=  3.1% <<<
>>> Test on task 22 : loss=1.189 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 68.4%, forg=  1.7% <<<
>>> Test on task 23 : loss=1.024 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 74.6%, forg=  3.3% <<<
>>> Test on task 24 : loss=1.098 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.7%, forg=  4.3% <<<
>>> Test on task 25 : loss=1.570 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 54.2%, forg= 16.7% <<<
>>> Test on task 26 : loss=1.586 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 50.9%, forg= 22.4% <<<
>>> Test on task 27 : loss=1.486 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 50.0%, forg= 20.0% <<<
>>> Test on task 28 : loss=1.360 | TAw acc= 96.3%, forg=  1.9%| TAg acc= 54.2%, forg= 19.6% <<<
>>> Test on task 29 : loss=0.827 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 72.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=4.915, TAw acc= 57.8% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=2.674, TAw acc= 79.5% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.791, TAw acc= 90.4% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.331, TAw acc= 95.2% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.247, TAw acc= 96.4% | *
| Epoch   6, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.154, TAw acc= 96.4% | *
| Epoch   7, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.021, TAw acc= 97.6% | *
| Epoch   8, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.993, TAw acc= 97.6% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.994, TAw acc= 97.6% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.996, TAw acc= 97.6% |
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.997, TAw acc= 97.6% |
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.998, TAw acc= 97.6% |
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.999, TAw acc= 97.6% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4537 train exemplars, time=  0.1s
4537
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.043 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 68.1%, forg= 13.1% <<<
>>> Test on task  1 : loss=1.000 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 72.6%, forg= 19.7% <<<
>>> Test on task  2 : loss=1.485 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 66.1%, forg= 13.0% <<<
>>> Test on task  3 : loss=1.121 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 72.9%, forg= 11.2% <<<
>>> Test on task  4 : loss=1.116 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.284 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 63.9%, forg= 15.7% <<<
>>> Test on task  6 : loss=1.131 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 75.0%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.273 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 70.7%, forg=  7.1% <<<
>>> Test on task  8 : loss=1.408 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.557 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 66.4%, forg=  4.7% <<<
>>> Test on task 10 : loss=1.108 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.5%, forg= 13.3% <<<
>>> Test on task 11 : loss=1.155 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 77.8%, forg=  6.5% <<<
>>> Test on task 12 : loss=1.031 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 74.0%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.458 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 59.3%, forg= 17.8% <<<
>>> Test on task 14 : loss=1.150 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 67.2%, forg= 11.2% <<<
>>> Test on task 15 : loss=1.163 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.2%, forg=  9.5% <<<
>>> Test on task 16 : loss=1.296 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.026 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 18 : loss=1.507 | TAw acc= 95.5%, forg=  0.8%| TAg acc= 62.9%, forg= 12.1% <<<
>>> Test on task 19 : loss=1.542 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 58.8%, forg=  1.8% <<<
>>> Test on task 20 : loss=0.950 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 74.8%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.325 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 57.1%, forg= 14.3% <<<
>>> Test on task 22 : loss=1.210 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 67.5%, forg=  2.6% <<<
>>> Test on task 23 : loss=0.961 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 75.4%, forg=  2.5% <<<
>>> Test on task 24 : loss=1.088 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 68.1%, forg=  6.9% <<<
>>> Test on task 25 : loss=1.540 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 57.3%, forg= 13.5% <<<
>>> Test on task 26 : loss=1.399 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 53.4%, forg= 19.8% <<<
>>> Test on task 27 : loss=1.512 | TAw acc= 95.0%, forg=  3.0%| TAg acc= 52.0%, forg= 18.0% <<<
>>> Test on task 28 : loss=1.324 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 60.7%, forg= 13.1% <<<
>>> Test on task 29 : loss=1.088 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 65.8%, forg=  7.0% <<<
>>> Test on task 30 : loss=1.151 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 69.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=6.067, TAw acc= 47.0% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=2.935, TAw acc= 78.3% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.869, TAw acc= 85.5% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.591, TAw acc= 90.4% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.497, TAw acc= 92.8% | *
| Epoch   6, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.369, TAw acc= 94.0% | *
| Epoch   7, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.418, TAw acc= 94.0% |
| Epoch   8, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.325, TAw acc= 92.8% | *
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.323, TAw acc= 92.8% | *
| Epoch   2, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.321, TAw acc= 92.8% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.319, TAw acc= 92.8% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.317, TAw acc= 92.8% | *
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.316, TAw acc= 92.8% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4677 train exemplars, time=  0.0s
4677
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.053 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 72.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=0.972 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 74.4%, forg= 17.9% <<<
>>> Test on task  2 : loss=1.372 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 66.1%, forg= 13.0% <<<
>>> Test on task  3 : loss=1.130 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 72.9%, forg= 11.2% <<<
>>> Test on task  4 : loss=1.084 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 80.8%, forg=  2.9% <<<
>>> Test on task  5 : loss=1.255 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.8%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.264 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.363 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task  8 : loss=1.395 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 70.5%, forg= 12.3% <<<
>>> Test on task  9 : loss=1.565 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.4%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.019 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.5%, forg= 13.3% <<<
>>> Test on task 11 : loss=1.156 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task 12 : loss=0.997 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 76.9%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.459 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 59.3%, forg= 17.8% <<<
>>> Test on task 14 : loss=1.074 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 67.2%, forg= 11.2% <<<
>>> Test on task 15 : loss=1.151 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 79.0%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.270 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.041 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 73.6%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.451 | TAw acc= 94.7%, forg=  1.5%| TAg acc= 66.7%, forg=  8.3% <<<
>>> Test on task 19 : loss=1.526 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 61.4%, forg= -0.9% <<<
>>> Test on task 20 : loss=1.040 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 73.9%, forg=  6.1% <<<
>>> Test on task 21 : loss=1.226 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.3%, forg=  7.1% <<<
>>> Test on task 22 : loss=1.199 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.4%, forg=  1.7% <<<
>>> Test on task 23 : loss=1.031 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 72.1%, forg=  5.7% <<<
>>> Test on task 24 : loss=1.086 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.8%, forg=  5.2% <<<
>>> Test on task 25 : loss=1.465 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 59.4%, forg= 11.5% <<<
>>> Test on task 26 : loss=1.463 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 55.2%, forg= 18.1% <<<
>>> Test on task 27 : loss=1.455 | TAw acc= 95.0%, forg=  3.0%| TAg acc= 63.0%, forg=  7.0% <<<
>>> Test on task 28 : loss=1.267 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 61.7%, forg= 12.1% <<<
>>> Test on task 29 : loss=1.040 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 68.4%, forg=  4.4% <<<
>>> Test on task 30 : loss=1.384 | TAw acc= 95.5%, forg= -1.8%| TAg acc= 57.1%, forg= 12.5% <<<
>>> Test on task 31 : loss=0.968 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=5.415, TAw acc= 65.1% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.607, TAw acc= 78.3% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.719, TAw acc= 90.4% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.350, TAw acc= 96.4% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.330, TAw acc= 97.6% | *
| Epoch   6, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.233, TAw acc= 97.6% | *
| Epoch   7, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 97.6% | *
| Epoch   8, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.164, TAw acc= 97.6% | *
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 97.6% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.158, TAw acc= 97.6% | *
| Epoch   3, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.155, TAw acc= 97.6% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.152, TAw acc= 97.6% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.150, TAw acc= 97.6% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4817 train exemplars, time=  0.0s
4817
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.119 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 67.5%, forg= 13.6% <<<
>>> Test on task  1 : loss=1.017 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 72.6%, forg= 19.7% <<<
>>> Test on task  2 : loss=1.449 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 61.7%, forg= 17.4% <<<
>>> Test on task  3 : loss=1.143 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 72.9%, forg= 11.2% <<<
>>> Test on task  4 : loss=1.141 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.9%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.246 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.8%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.214 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.361 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 68.7%, forg=  9.1% <<<
>>> Test on task  8 : loss=1.428 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 68.0%, forg= 14.8% <<<
>>> Test on task  9 : loss=1.565 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 66.4%, forg=  4.7% <<<
>>> Test on task 10 : loss=1.080 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.149 | TAw acc= 95.4%, forg= -0.9%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task 12 : loss=1.081 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 74.0%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.444 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 63.6%, forg= 13.6% <<<
>>> Test on task 14 : loss=1.040 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.3%, forg=  5.2% <<<
>>> Test on task 15 : loss=1.137 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 80.0%, forg=  4.8% <<<
>>> Test on task 16 : loss=1.293 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.007 | TAw acc= 96.4%, forg=  0.9%| TAg acc= 69.1%, forg=  7.3% <<<
>>> Test on task 18 : loss=1.520 | TAw acc= 94.7%, forg=  1.5%| TAg acc= 63.6%, forg= 11.4% <<<
>>> Test on task 19 : loss=1.583 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 57.9%, forg=  3.5% <<<
>>> Test on task 20 : loss=0.985 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 76.5%, forg=  3.5% <<<
>>> Test on task 21 : loss=1.279 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.3%, forg=  8.2% <<<
>>> Test on task 22 : loss=1.251 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 64.1%, forg=  6.0% <<<
>>> Test on task 23 : loss=1.047 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 69.7%, forg=  8.2% <<<
>>> Test on task 24 : loss=1.063 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.7% <<<
>>> Test on task 25 : loss=1.477 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 61.5%, forg=  9.4% <<<
>>> Test on task 26 : loss=1.464 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 53.4%, forg= 19.8% <<<
>>> Test on task 27 : loss=1.535 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 60.0%, forg= 10.0% <<<
>>> Test on task 28 : loss=1.173 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 67.3%, forg=  6.5% <<<
>>> Test on task 29 : loss=0.973 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 71.9%, forg=  0.9% <<<
>>> Test on task 30 : loss=1.411 | TAw acc= 96.4%, forg= -0.9%| TAg acc= 58.9%, forg= 10.7% <<<
>>> Test on task 31 : loss=1.225 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 65.5%, forg= 12.4% <<<
>>> Test on task 32 : loss=1.261 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=7.341, TAw acc= 47.4% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=3.431, TAw acc= 77.6% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=2.170, TAw acc= 85.5% | *
| Epoch   4, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.537, TAw acc= 89.5% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.917, TAw acc= 90.8% |
| Epoch   6, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.257, TAw acc= 88.2% | *
| Epoch   7, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.273, TAw acc= 92.1% |
| Epoch   8, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.205, TAw acc= 92.1% | *
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.203, TAw acc= 92.1% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.201, TAw acc= 92.1% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.200, TAw acc= 92.1% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.198, TAw acc= 92.1% | *
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.196, TAw acc= 92.1% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4957 train exemplars, time=  0.0s
4957
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.084 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 68.1%, forg= 13.1% <<<
>>> Test on task  1 : loss=1.086 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 69.2%, forg= 23.1% <<<
>>> Test on task  2 : loss=1.393 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 66.1%, forg= 13.0% <<<
>>> Test on task  3 : loss=1.151 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 71.0%, forg= 13.1% <<<
>>> Test on task  4 : loss=1.130 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 79.8%, forg=  3.8% <<<
>>> Test on task  5 : loss=1.235 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 66.7%, forg= 13.0% <<<
>>> Test on task  6 : loss=1.219 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 75.0%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.366 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 68.7%, forg=  9.1% <<<
>>> Test on task  8 : loss=1.423 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 73.0%, forg=  9.8% <<<
>>> Test on task  9 : loss=1.647 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 66.4%, forg=  4.7% <<<
>>> Test on task 10 : loss=1.064 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.5%, forg= 12.2% <<<
>>> Test on task 11 : loss=1.182 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task 12 : loss=1.051 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.9%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.454 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 61.9%, forg= 15.3% <<<
>>> Test on task 14 : loss=1.043 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 72.4%, forg=  6.0% <<<
>>> Test on task 15 : loss=1.188 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 77.1%, forg=  7.6% <<<
>>> Test on task 16 : loss=1.320 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.006 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 72.7%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.592 | TAw acc= 95.5%, forg=  0.8%| TAg acc= 59.1%, forg= 15.9% <<<
>>> Test on task 19 : loss=1.640 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 55.3%, forg=  6.1% <<<
>>> Test on task 20 : loss=0.944 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 78.3%, forg=  1.7% <<<
>>> Test on task 21 : loss=1.270 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 61.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.231 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.8%, forg=  4.3% <<<
>>> Test on task 23 : loss=1.046 | TAw acc= 93.4%, forg=  1.6%| TAg acc= 69.7%, forg=  8.2% <<<
>>> Test on task 24 : loss=1.115 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.6%, forg=  3.4% <<<
>>> Test on task 25 : loss=1.498 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 58.3%, forg= 12.5% <<<
>>> Test on task 26 : loss=1.446 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 56.9%, forg= 16.4% <<<
>>> Test on task 27 : loss=1.457 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 66.0%, forg=  4.0% <<<
>>> Test on task 28 : loss=1.135 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.1%, forg=  3.7% <<<
>>> Test on task 29 : loss=1.027 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 68.4%, forg=  4.4% <<<
>>> Test on task 30 : loss=1.343 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 62.5%, forg=  7.1% <<<
>>> Test on task 31 : loss=1.149 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.4%, forg= 11.5% <<<
>>> Test on task 32 : loss=1.762 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 45.5%, forg= 23.2% <<<
>>> Test on task 33 : loss=0.949 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=8.109, TAw acc= 34.8% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=3.983, TAw acc= 69.6% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=2.448, TAw acc= 91.3% | *
| Epoch   4, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.915, TAw acc= 91.3% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.841, TAw acc= 94.2% | *
| Epoch   6, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.588, TAw acc= 94.2% | *
| Epoch   7, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 94.2% | *
| Epoch   8, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.485, TAw acc= 94.2% | *
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.483, TAw acc= 94.2% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.480, TAw acc= 94.2% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.478, TAw acc= 94.2% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.476, TAw acc= 94.2% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.474, TAw acc= 94.2% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5086 train exemplars, time=  0.0s
5086
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.089 | TAw acc= 89.5%, forg=  1.6%| TAg acc= 68.1%, forg= 13.1% <<<
>>> Test on task  1 : loss=1.083 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.9%, forg= 21.4% <<<
>>> Test on task  2 : loss=1.441 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 61.7%, forg= 17.4% <<<
>>> Test on task  3 : loss=1.169 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 70.1%, forg= 14.0% <<<
>>> Test on task  4 : loss=1.175 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.9%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.243 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.7%, forg= 13.9% <<<
>>> Test on task  6 : loss=1.321 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.390 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 67.7%, forg= 10.1% <<<
>>> Test on task  8 : loss=1.366 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 75.4%, forg=  7.4% <<<
>>> Test on task  9 : loss=1.595 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 64.5%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.124 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.185 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 80.6%, forg=  3.7% <<<
>>> Test on task 12 : loss=1.074 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.0%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.514 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 61.0%, forg= 16.1% <<<
>>> Test on task 14 : loss=0.991 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.7%, forg=  1.7% <<<
>>> Test on task 15 : loss=1.215 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.2%, forg=  8.6% <<<
>>> Test on task 16 : loss=1.294 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.131 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 66.4%, forg= 10.0% <<<
>>> Test on task 18 : loss=1.523 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 64.4%, forg= 10.6% <<<
>>> Test on task 19 : loss=1.582 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 53.5%, forg=  7.9% <<<
>>> Test on task 20 : loss=0.935 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 79.1%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.259 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 61.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.238 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 64.1%, forg=  6.0% <<<
>>> Test on task 23 : loss=1.065 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 68.9%, forg=  9.0% <<<
>>> Test on task 24 : loss=1.141 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.7%, forg=  4.3% <<<
>>> Test on task 25 : loss=1.448 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 61.5%, forg=  9.4% <<<
>>> Test on task 26 : loss=1.444 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 58.6%, forg= 14.7% <<<
>>> Test on task 27 : loss=1.596 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 58.0%, forg= 12.0% <<<
>>> Test on task 28 : loss=1.164 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.4%, forg=  8.4% <<<
>>> Test on task 29 : loss=0.926 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 72.8%, forg=  0.0% <<<
>>> Test on task 30 : loss=1.290 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 61.6%, forg=  8.0% <<<
>>> Test on task 31 : loss=1.172 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.1%, forg=  9.7% <<<
>>> Test on task 32 : loss=1.668 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 51.8%, forg= 17.0% <<<
>>> Test on task 33 : loss=1.186 | TAw acc= 98.1%, forg= -1.9%| TAg acc= 58.7%, forg= 18.3% <<<
>>> Test on task 34 : loss=1.084 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 70.8%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=5.767, TAw acc= 55.4% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.611, TAw acc= 73.5% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.550, TAw acc= 86.7% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.080, TAw acc= 98.8% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.936, TAw acc= 95.2% | *
| Epoch   6, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=0.994, TAw acc= 98.8% |
| Epoch   7, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=0.894, TAw acc= 94.0% | *
| Epoch   8, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=0.854, TAw acc= 96.4% | *
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=0.850, TAw acc= 96.4% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=0.848, TAw acc= 96.4% | *
| Epoch   3, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=0.845, TAw acc= 96.4% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=0.843, TAw acc= 96.4% | *
| Epoch   5, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=0.841, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5206 train exemplars, time=  0.0s
5206
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.106 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 68.6%, forg= 12.6% <<<
>>> Test on task  1 : loss=1.093 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.1%, forg= 22.2% <<<
>>> Test on task  2 : loss=1.396 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 65.2%, forg= 13.9% <<<
>>> Test on task  3 : loss=1.156 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 74.8%, forg=  9.3% <<<
>>> Test on task  4 : loss=1.152 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.8%, forg=  4.8% <<<
>>> Test on task  5 : loss=1.181 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 69.4%, forg= 10.2% <<<
>>> Test on task  6 : loss=1.264 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 75.0%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.360 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task  8 : loss=1.540 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 65.6%, forg= 17.2% <<<
>>> Test on task  9 : loss=1.652 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.4%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.091 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 77.6%, forg= 10.2% <<<
>>> Test on task 11 : loss=1.268 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 74.1%, forg= 10.2% <<<
>>> Test on task 12 : loss=1.082 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 73.1%, forg=  4.8% <<<
>>> Test on task 13 : loss=1.448 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 60.2%, forg= 16.9% <<<
>>> Test on task 14 : loss=1.026 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 76.7%, forg=  1.7% <<<
>>> Test on task 15 : loss=1.180 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.0%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.304 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.103 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 70.0%, forg=  6.4% <<<
>>> Test on task 18 : loss=1.537 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 65.9%, forg=  9.1% <<<
>>> Test on task 19 : loss=1.573 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 55.3%, forg=  6.1% <<<
>>> Test on task 20 : loss=1.006 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 73.0%, forg=  7.0% <<<
>>> Test on task 21 : loss=1.344 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 62.2%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.210 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 64.1%, forg=  6.0% <<<
>>> Test on task 23 : loss=1.103 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 68.0%, forg=  9.8% <<<
>>> Test on task 24 : loss=1.116 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.8%, forg=  5.2% <<<
>>> Test on task 25 : loss=1.507 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 60.4%, forg= 10.4% <<<
>>> Test on task 26 : loss=1.434 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 59.5%, forg= 13.8% <<<
>>> Test on task 27 : loss=1.591 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 59.0%, forg= 11.0% <<<
>>> Test on task 28 : loss=1.119 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.1%, forg=  3.7% <<<
>>> Test on task 29 : loss=0.977 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 71.1%, forg=  1.8% <<<
>>> Test on task 30 : loss=1.348 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 65.2%, forg=  4.5% <<<
>>> Test on task 31 : loss=1.150 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.0%, forg=  8.8% <<<
>>> Test on task 32 : loss=1.589 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 57.1%, forg= 11.6% <<<
>>> Test on task 33 : loss=1.142 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 59.6%, forg= 17.3% <<<
>>> Test on task 34 : loss=1.315 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 61.5%, forg=  9.4% <<<
>>> Test on task 35 : loss=1.009 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 71.4%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=6.904, TAw acc= 48.7% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=3.124, TAw acc= 79.5% | *
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.532, TAw acc= 97.4% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.064, TAw acc=100.0% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.122, TAw acc=100.0% |
| Epoch   6, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=0.947, TAw acc=100.0% | *
| Epoch   7, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=0.824, TAw acc= 98.7% | *
| Epoch   8, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.845, TAw acc=100.0% |
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.827, TAw acc= 98.7% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.829, TAw acc= 98.7% |
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.832, TAw acc= 98.7% |
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.834, TAw acc= 98.7% |
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.836, TAw acc= 98.7% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5326 train exemplars, time=  0.0s
5326
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.143 | TAw acc= 91.6%, forg= -0.5%| TAg acc= 69.6%, forg= 11.5% <<<
>>> Test on task  1 : loss=1.121 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.8%, forg= 20.5% <<<
>>> Test on task  2 : loss=1.475 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 59.1%, forg= 20.0% <<<
>>> Test on task  3 : loss=1.186 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 72.9%, forg= 11.2% <<<
>>> Test on task  4 : loss=1.169 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.265 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.8%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.315 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.431 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 68.7%, forg=  9.1% <<<
>>> Test on task  8 : loss=1.462 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.690 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 64.5%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.134 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.279 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 76.9%, forg=  7.4% <<<
>>> Test on task 12 : loss=1.048 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.9%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.540 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 57.6%, forg= 19.5% <<<
>>> Test on task 14 : loss=1.028 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.0%, forg=  3.4% <<<
>>> Test on task 15 : loss=1.307 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.2%, forg=  9.5% <<<
>>> Test on task 16 : loss=1.395 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 70.4%, forg= 13.9% <<<
>>> Test on task 17 : loss=1.028 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 72.7%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.601 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 63.6%, forg= 11.4% <<<
>>> Test on task 19 : loss=1.599 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 55.3%, forg=  6.1% <<<
>>> Test on task 20 : loss=1.001 | TAw acc= 98.3%, forg= -0.9%| TAg acc= 75.7%, forg=  4.3% <<<
>>> Test on task 21 : loss=1.332 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 65.3%, forg=  6.1% <<<
>>> Test on task 22 : loss=1.150 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 67.5%, forg=  2.6% <<<
>>> Test on task 23 : loss=1.211 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 64.8%, forg= 13.1% <<<
>>> Test on task 24 : loss=1.199 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.0%, forg=  6.0% <<<
>>> Test on task 25 : loss=1.612 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 55.2%, forg= 15.6% <<<
>>> Test on task 26 : loss=1.456 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 52.6%, forg= 20.7% <<<
>>> Test on task 27 : loss=1.613 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 58.0%, forg= 12.0% <<<
>>> Test on task 28 : loss=1.096 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.0%, forg=  1.9% <<<
>>> Test on task 29 : loss=0.971 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 71.1%, forg=  1.8% <<<
>>> Test on task 30 : loss=1.390 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 60.7%, forg=  8.9% <<<
>>> Test on task 31 : loss=1.125 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.7%, forg=  6.2% <<<
>>> Test on task 32 : loss=1.527 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 60.7%, forg=  8.0% <<<
>>> Test on task 33 : loss=1.069 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 64.4%, forg= 12.5% <<<
>>> Test on task 34 : loss=1.188 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 66.7%, forg=  4.2% <<<
>>> Test on task 35 : loss=1.377 | TAw acc= 93.8%, forg= -0.9%| TAg acc= 58.0%, forg= 13.4% <<<
>>> Test on task 36 : loss=0.912 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 78.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=6.646, TAw acc= 46.2% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=2.683, TAw acc= 75.0% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.519, TAw acc= 95.0% | *
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.147, TAw acc= 97.5% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.076, TAw acc= 97.5% | *
| Epoch   6, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=0.975, TAw acc= 97.5% | *
| Epoch   7, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=0.930, TAw acc= 98.8% | *
| Epoch   8, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.892, TAw acc= 97.5% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.890, TAw acc= 97.5% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=0.889, TAw acc= 97.5% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=0.888, TAw acc= 97.5% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.887, TAw acc= 97.5% | *
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=0.886, TAw acc= 97.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5446 train exemplars, time=  0.1s
5446
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.115 | TAw acc= 90.6%, forg=  1.0%| TAg acc= 67.5%, forg= 13.6% <<<
>>> Test on task  1 : loss=1.102 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 74.4%, forg= 17.9% <<<
>>> Test on task  2 : loss=1.499 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 58.3%, forg= 20.9% <<<
>>> Test on task  3 : loss=1.223 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.0%, forg= 12.1% <<<
>>> Test on task  4 : loss=1.177 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.347 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 63.0%, forg= 16.7% <<<
>>> Test on task  6 : loss=1.277 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 74.1%, forg=  6.5% <<<
>>> Test on task  7 : loss=1.466 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task  8 : loss=1.454 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.8%, forg=  9.0% <<<
>>> Test on task  9 : loss=1.666 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.6%, forg=  7.5% <<<
>>> Test on task 10 : loss=1.118 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 70.4%, forg= 17.3% <<<
>>> Test on task 11 : loss=1.256 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 76.9%, forg=  7.4% <<<
>>> Test on task 12 : loss=1.153 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 73.1%, forg=  4.8% <<<
>>> Test on task 13 : loss=1.547 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 55.1%, forg= 22.0% <<<
>>> Test on task 14 : loss=1.074 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.7%, forg=  7.8% <<<
>>> Test on task 15 : loss=1.175 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.0%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.411 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 71.3%, forg= 13.0% <<<
>>> Test on task 17 : loss=1.037 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.0%, forg=  6.4% <<<
>>> Test on task 18 : loss=1.601 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 66.7%, forg=  8.3% <<<
>>> Test on task 19 : loss=1.580 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 55.3%, forg=  6.1% <<<
>>> Test on task 20 : loss=1.010 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 75.7%, forg=  4.3% <<<
>>> Test on task 21 : loss=1.365 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 61.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.264 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 66.7%, forg=  3.4% <<<
>>> Test on task 23 : loss=1.076 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 71.3%, forg=  6.6% <<<
>>> Test on task 24 : loss=1.133 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.6%, forg=  3.4% <<<
>>> Test on task 25 : loss=1.541 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 62.5%, forg=  8.3% <<<
>>> Test on task 26 : loss=1.513 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 51.7%, forg= 21.6% <<<
>>> Test on task 27 : loss=1.634 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 61.0%, forg=  9.0% <<<
>>> Test on task 28 : loss=1.133 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 66.4%, forg=  7.5% <<<
>>> Test on task 29 : loss=0.957 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 73.7%, forg= -0.9% <<<
>>> Test on task 30 : loss=1.334 | TAw acc= 94.6%, forg=  1.8%| TAg acc= 64.3%, forg=  5.4% <<<
>>> Test on task 31 : loss=1.165 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.1%, forg=  9.7% <<<
>>> Test on task 32 : loss=1.571 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 58.0%, forg= 10.7% <<<
>>> Test on task 33 : loss=0.996 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 74.0%, forg=  2.9% <<<
>>> Test on task 34 : loss=1.215 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 62.5%, forg=  8.3% <<<
>>> Test on task 35 : loss=1.316 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 59.8%, forg= 11.6% <<<
>>> Test on task 36 : loss=1.378 | TAw acc= 95.3%, forg= -0.9%| TAg acc= 59.8%, forg= 18.7% <<<
>>> Test on task 37 : loss=0.925 | TAw acc=100.0%, forg=  0.0%| TAg acc= 71.6%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=5.206, TAw acc= 63.5% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=2.424, TAw acc= 80.0% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.406, TAw acc= 94.1% | *
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.197, TAw acc= 95.3% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.170, TAw acc= 92.9% | *
| Epoch   6, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.065, TAw acc= 95.3% | *
| Epoch   7, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=0.995, TAw acc= 95.3% | *
| Epoch   8, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=0.968, TAw acc= 96.5% | *
| Epoch   1, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.968, TAw acc= 96.5% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=0.968, TAw acc= 96.5% |
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=0.969, TAw acc= 96.5% |
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.969, TAw acc= 96.5% |
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=0.970, TAw acc= 96.5% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5566 train exemplars, time=  0.0s
5566
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.190 | TAw acc= 90.6%, forg=  1.0%| TAg acc= 62.8%, forg= 18.3% <<<
>>> Test on task  1 : loss=1.170 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.9%, forg= 21.4% <<<
>>> Test on task  2 : loss=1.423 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 66.1%, forg= 13.0% <<<
>>> Test on task  3 : loss=1.201 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 73.8%, forg= 10.3% <<<
>>> Test on task  4 : loss=1.189 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.9%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.360 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 66.7%, forg= 13.0% <<<
>>> Test on task  6 : loss=1.330 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 71.3%, forg=  9.3% <<<
>>> Test on task  7 : loss=1.472 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 67.7%, forg= 10.1% <<<
>>> Test on task  8 : loss=1.469 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.734 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 64.5%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.222 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 71.4%, forg= 16.3% <<<
>>> Test on task 11 : loss=1.333 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 74.1%, forg= 10.2% <<<
>>> Test on task 12 : loss=1.105 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.9%, forg=  1.0% <<<
>>> Test on task 13 : loss=1.542 | TAw acc= 93.2%, forg=  0.8%| TAg acc= 61.0%, forg= 16.1% <<<
>>> Test on task 14 : loss=1.165 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.0%, forg=  9.5% <<<
>>> Test on task 15 : loss=1.221 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.0%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.423 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 71.3%, forg= 13.0% <<<
>>> Test on task 17 : loss=1.025 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 18 : loss=1.590 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 65.9%, forg=  9.1% <<<
>>> Test on task 19 : loss=1.654 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 57.0%, forg=  4.4% <<<
>>> Test on task 20 : loss=1.048 | TAw acc= 95.7%, forg=  2.6%| TAg acc= 71.3%, forg=  8.7% <<<
>>> Test on task 21 : loss=1.374 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 63.3%, forg=  8.2% <<<
>>> Test on task 22 : loss=1.200 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 66.7%, forg=  3.4% <<<
>>> Test on task 23 : loss=1.032 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 75.4%, forg=  2.5% <<<
>>> Test on task 24 : loss=1.216 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 67.2%, forg=  7.8% <<<
>>> Test on task 25 : loss=1.456 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 63.5%, forg=  7.3% <<<
>>> Test on task 26 : loss=1.559 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 55.2%, forg= 18.1% <<<
>>> Test on task 27 : loss=1.618 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 60.0%, forg= 10.0% <<<
>>> Test on task 28 : loss=1.147 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.0%, forg=  2.8% <<<
>>> Test on task 29 : loss=1.024 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 72.8%, forg=  0.9% <<<
>>> Test on task 30 : loss=1.432 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 62.5%, forg=  7.1% <<<
>>> Test on task 31 : loss=1.099 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.3%, forg=  3.5% <<<
>>> Test on task 32 : loss=1.588 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 61.6%, forg=  7.1% <<<
>>> Test on task 33 : loss=1.062 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 66.3%, forg= 10.6% <<<
>>> Test on task 34 : loss=1.240 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 65.6%, forg=  5.2% <<<
>>> Test on task 35 : loss=1.364 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 59.8%, forg= 11.6% <<<
>>> Test on task 36 : loss=1.295 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 63.6%, forg= 15.0% <<<
>>> Test on task 37 : loss=1.231 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 59.6%, forg= 11.9% <<<
>>> Test on task 38 : loss=0.995 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 71.9%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=5.175, TAw acc= 56.8% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=2.713, TAw acc= 76.1% | *
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.825, TAw acc= 88.6% | *
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.431, TAw acc= 94.3% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.318, TAw acc= 95.5% | *
| Epoch   6, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.225, TAw acc= 95.5% | *
| Epoch   7, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.209, TAw acc= 95.5% | *
| Epoch   8, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.5% | *
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.5% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.5% | *
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.5% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.5% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.173, TAw acc= 95.5% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5686 train exemplars, time=  0.0s
5686
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.202 | TAw acc= 89.0%, forg=  2.6%| TAg acc= 66.0%, forg= 15.2% <<<
>>> Test on task  1 : loss=1.145 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.1%, forg= 22.2% <<<
>>> Test on task  2 : loss=1.457 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 65.2%, forg= 13.9% <<<
>>> Test on task  3 : loss=1.218 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 73.8%, forg= 10.3% <<<
>>> Test on task  4 : loss=1.150 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 78.8%, forg=  4.8% <<<
>>> Test on task  5 : loss=1.360 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 67.6%, forg= 12.0% <<<
>>> Test on task  6 : loss=1.330 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.422 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 70.7%, forg=  7.1% <<<
>>> Test on task  8 : loss=1.505 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 71.3%, forg= 11.5% <<<
>>> Test on task  9 : loss=1.719 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 63.6%, forg=  7.5% <<<
>>> Test on task 10 : loss=1.153 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.5%, forg= 13.3% <<<
>>> Test on task 11 : loss=1.303 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 75.9%, forg=  8.3% <<<
>>> Test on task 12 : loss=1.113 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 76.0%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.486 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 61.0%, forg= 16.1% <<<
>>> Test on task 14 : loss=1.019 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.1%, forg=  4.3% <<<
>>> Test on task 15 : loss=1.233 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 80.0%, forg=  4.8% <<<
>>> Test on task 16 : loss=1.433 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 70.4%, forg= 13.9% <<<
>>> Test on task 17 : loss=1.116 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.7%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.564 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 68.2%, forg=  6.8% <<<
>>> Test on task 19 : loss=1.652 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 55.3%, forg=  6.1% <<<
>>> Test on task 20 : loss=1.064 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 77.4%, forg=  2.6% <<<
>>> Test on task 21 : loss=1.469 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 60.2%, forg= 11.2% <<<
>>> Test on task 22 : loss=1.274 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.8%, forg=  4.3% <<<
>>> Test on task 23 : loss=1.068 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 71.3%, forg=  6.6% <<<
>>> Test on task 24 : loss=1.147 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.8%, forg=  5.2% <<<
>>> Test on task 25 : loss=1.515 | TAw acc= 94.8%, forg= -1.0%| TAg acc= 59.4%, forg= 11.5% <<<
>>> Test on task 26 : loss=1.534 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 58.6%, forg= 14.7% <<<
>>> Test on task 27 : loss=1.567 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 67.0%, forg=  3.0% <<<
>>> Test on task 28 : loss=1.123 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.0%, forg=  2.8% <<<
>>> Test on task 29 : loss=1.020 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 72.8%, forg=  0.9% <<<
>>> Test on task 30 : loss=1.394 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 61.6%, forg=  8.0% <<<
>>> Test on task 31 : loss=1.110 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.6%, forg=  5.3% <<<
>>> Test on task 32 : loss=1.577 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 61.6%, forg=  7.1% <<<
>>> Test on task 33 : loss=1.072 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 64.4%, forg= 12.5% <<<
>>> Test on task 34 : loss=1.211 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 67.7%, forg=  3.1% <<<
>>> Test on task 35 : loss=1.342 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 61.6%, forg=  9.8% <<<
>>> Test on task 36 : loss=1.209 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 68.2%, forg= 10.3% <<<
>>> Test on task 37 : loss=1.226 | TAw acc= 98.2%, forg=  1.8%| TAg acc= 67.0%, forg=  4.6% <<<
>>> Test on task 38 : loss=1.419 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 52.6%, forg= 19.3% <<<
>>> Test on task 39 : loss=1.012 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 79.7%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=7.953, TAw acc= 37.1% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=3.229, TAw acc= 68.6% | *
| Epoch   3, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.797, TAw acc= 94.3% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.490, TAw acc= 94.3% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.378, TAw acc= 94.3% | *
| Epoch   6, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.409, TAw acc= 94.3% |
| Epoch   7, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.285, TAw acc= 94.3% | *
| Epoch   8, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.314, TAw acc= 94.3% |
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.283, TAw acc= 94.3% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.279, TAw acc= 94.3% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.277, TAw acc= 94.3% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 94.3% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.273, TAw acc= 94.3% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5806 train exemplars, time=  0.0s
5806
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.253 | TAw acc= 90.1%, forg=  1.6%| TAg acc= 67.5%, forg= 13.6% <<<
>>> Test on task  1 : loss=1.157 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 73.5%, forg= 18.8% <<<
>>> Test on task  2 : loss=1.465 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 64.3%, forg= 14.8% <<<
>>> Test on task  3 : loss=1.238 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 74.8%, forg=  9.3% <<<
>>> Test on task  4 : loss=1.213 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 73.1%, forg= 10.6% <<<
>>> Test on task  5 : loss=1.415 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 63.0%, forg= 16.7% <<<
>>> Test on task  6 : loss=1.336 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.492 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 67.7%, forg= 10.1% <<<
>>> Test on task  8 : loss=1.512 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.738 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 64.5%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.161 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.5%, forg= 13.3% <<<
>>> Test on task 11 : loss=1.295 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 76.9%, forg=  7.4% <<<
>>> Test on task 12 : loss=1.145 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.0%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.464 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 59.3%, forg= 17.8% <<<
>>> Test on task 14 : loss=1.013 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.6%, forg=  0.9% <<<
>>> Test on task 15 : loss=1.222 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 81.9%, forg=  2.9% <<<
>>> Test on task 16 : loss=1.428 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 72.2%, forg= 12.2% <<<
>>> Test on task 17 : loss=1.088 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.0%, forg=  6.4% <<<
>>> Test on task 18 : loss=1.614 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 62.9%, forg= 12.1% <<<
>>> Test on task 19 : loss=1.746 | TAw acc= 86.8%, forg=  0.9%| TAg acc= 52.6%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.072 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 74.8%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.457 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 59.2%, forg= 12.2% <<<
>>> Test on task 22 : loss=1.235 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.0%, forg=  5.1% <<<
>>> Test on task 23 : loss=1.184 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 65.6%, forg= 12.3% <<<
>>> Test on task 24 : loss=1.220 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 67.2%, forg=  7.8% <<<
>>> Test on task 25 : loss=1.523 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 59.4%, forg= 11.5% <<<
>>> Test on task 26 : loss=1.610 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 56.9%, forg= 16.4% <<<
>>> Test on task 27 : loss=1.626 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 63.0%, forg=  7.0% <<<
>>> Test on task 28 : loss=1.094 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.8%, forg=  0.0% <<<
>>> Test on task 29 : loss=1.010 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 71.9%, forg=  1.8% <<<
>>> Test on task 30 : loss=1.293 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 66.1%, forg=  3.6% <<<
>>> Test on task 31 : loss=1.142 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.9%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.551 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 61.6%, forg=  7.1% <<<
>>> Test on task 33 : loss=1.016 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 73.1%, forg=  3.8% <<<
>>> Test on task 34 : loss=1.272 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 63.5%, forg=  7.3% <<<
>>> Test on task 35 : loss=1.368 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 61.6%, forg=  9.8% <<<
>>> Test on task 36 : loss=1.220 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 70.1%, forg=  8.4% <<<
>>> Test on task 37 : loss=1.170 | TAw acc= 97.2%, forg=  2.8%| TAg acc= 67.0%, forg=  4.6% <<<
>>> Test on task 38 : loss=1.384 | TAw acc= 94.7%, forg= -0.9%| TAg acc= 51.8%, forg= 20.2% <<<
>>> Test on task 39 : loss=1.287 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 72.9%, forg=  6.8% <<<
>>> Test on task 40 : loss=1.301 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 65.3%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=6.000, TAw acc= 53.0% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=2.514, TAw acc= 83.1% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.605, TAw acc= 97.6% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.150, TAw acc= 98.8% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.010, TAw acc= 98.8% | *
| Epoch   6, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.969, TAw acc= 98.8% | *
| Epoch   7, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.812, TAw acc= 98.8% | *
| Epoch   8, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.849, TAw acc= 98.8% |
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.815, TAw acc= 98.8% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.817, TAw acc= 98.8% |
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.820, TAw acc= 98.8% |
| Epoch   4, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.822, TAw acc= 98.8% |
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=0.824, TAw acc= 98.8% |
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5926 train exemplars, time=  0.0s
5926
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.217 | TAw acc= 90.1%, forg=  1.6%| TAg acc= 66.5%, forg= 14.7% <<<
>>> Test on task  1 : loss=1.183 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 71.8%, forg= 20.5% <<<
>>> Test on task  2 : loss=1.480 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 67.0%, forg= 12.2% <<<
>>> Test on task  3 : loss=1.277 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 71.0%, forg= 13.1% <<<
>>> Test on task  4 : loss=1.223 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.9%, forg=  6.7% <<<
>>> Test on task  5 : loss=1.441 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.8%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.285 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 75.0%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.463 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 71.7%, forg=  6.1% <<<
>>> Test on task  8 : loss=1.503 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 73.0%, forg=  9.8% <<<
>>> Test on task  9 : loss=1.715 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.4%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.249 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 69.4%, forg= 18.4% <<<
>>> Test on task 11 : loss=1.421 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 72.2%, forg= 12.0% <<<
>>> Test on task 12 : loss=1.152 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 73.1%, forg=  4.8% <<<
>>> Test on task 13 : loss=1.551 | TAw acc= 94.9%, forg= -0.8%| TAg acc= 58.5%, forg= 18.6% <<<
>>> Test on task 14 : loss=1.093 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.6%, forg=  6.9% <<<
>>> Test on task 15 : loss=1.341 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 74.3%, forg= 10.5% <<<
>>> Test on task 16 : loss=1.495 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 73.0%, forg= 11.3% <<<
>>> Test on task 17 : loss=1.155 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 18 : loss=1.603 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 63.6%, forg= 11.4% <<<
>>> Test on task 19 : loss=1.734 | TAw acc= 87.7%, forg=  0.0%| TAg acc= 55.3%, forg=  6.1% <<<
>>> Test on task 20 : loss=1.083 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 73.9%, forg=  6.1% <<<
>>> Test on task 21 : loss=1.587 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.2%, forg= 13.3% <<<
>>> Test on task 22 : loss=1.264 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 65.0%, forg=  5.1% <<<
>>> Test on task 23 : loss=1.047 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 73.8%, forg=  4.1% <<<
>>> Test on task 24 : loss=1.183 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.0%, forg=  6.0% <<<
>>> Test on task 25 : loss=1.539 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 62.5%, forg=  8.3% <<<
>>> Test on task 26 : loss=1.590 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 53.4%, forg= 19.8% <<<
>>> Test on task 27 : loss=1.726 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.0%, forg=  7.0% <<<
>>> Test on task 28 : loss=1.136 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.1%, forg=  3.7% <<<
>>> Test on task 29 : loss=0.939 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 75.4%, forg= -1.8% <<<
>>> Test on task 30 : loss=1.452 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 61.6%, forg=  8.0% <<<
>>> Test on task 31 : loss=1.090 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task 32 : loss=1.685 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 57.1%, forg= 11.6% <<<
>>> Test on task 33 : loss=1.014 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 73.1%, forg=  3.8% <<<
>>> Test on task 34 : loss=1.261 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 67.7%, forg=  3.1% <<<
>>> Test on task 35 : loss=1.377 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 60.7%, forg= 10.7% <<<
>>> Test on task 36 : loss=1.210 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 74.8%, forg=  3.7% <<<
>>> Test on task 37 : loss=1.124 | TAw acc= 98.2%, forg=  1.8%| TAg acc= 72.5%, forg= -0.9% <<<
>>> Test on task 38 : loss=1.557 | TAw acc= 93.9%, forg=  0.9%| TAg acc= 48.2%, forg= 23.7% <<<
>>> Test on task 39 : loss=1.320 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 72.0%, forg=  7.6% <<<
>>> Test on task 40 : loss=1.743 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 45.9%, forg= 19.4% <<<
>>> Test on task 41 : loss=0.818 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.1%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=5.405, TAw acc= 48.1% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=2.461, TAw acc= 84.4% | *
| Epoch   3, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.294, TAw acc= 98.7% | *
| Epoch   4, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.919, TAw acc= 97.4% | *
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.947, TAw acc=100.0% |
| Epoch   6, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.915, TAw acc= 97.4% | *
| Epoch   7, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.918, TAw acc= 98.7% |
| Epoch   8, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.905, TAw acc=100.0% | *
| Epoch   1, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.899, TAw acc=100.0% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=0.894, TAw acc=100.0% | *
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.890, TAw acc=100.0% | *
| Epoch   4, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=0.886, TAw acc=100.0% | *
| Epoch   5, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=0.882, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 6046 train exemplars, time=  0.1s
6046
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.166 | TAw acc= 91.1%, forg=  0.5%| TAg acc= 66.5%, forg= 14.7% <<<
>>> Test on task  1 : loss=1.203 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 69.2%, forg= 23.1% <<<
>>> Test on task  2 : loss=1.488 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 65.2%, forg= 13.9% <<<
>>> Test on task  3 : loss=1.250 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 75.7%, forg=  8.4% <<<
>>> Test on task  4 : loss=1.292 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.0%, forg=  7.7% <<<
>>> Test on task  5 : loss=1.425 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 60.2%, forg= 19.4% <<<
>>> Test on task  6 : loss=1.491 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 72.2%, forg=  8.3% <<<
>>> Test on task  7 : loss=1.361 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 72.7%, forg=  5.1% <<<
>>> Test on task  8 : loss=1.568 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 71.3%, forg= 11.5% <<<
>>> Test on task  9 : loss=1.799 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 61.7%, forg=  9.3% <<<
>>> Test on task 10 : loss=1.194 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.4%, forg= 15.3% <<<
>>> Test on task 11 : loss=1.364 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 75.0%, forg=  9.3% <<<
>>> Test on task 12 : loss=1.146 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 75.0%, forg=  2.9% <<<
>>> Test on task 13 : loss=1.516 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 61.0%, forg= 16.1% <<<
>>> Test on task 14 : loss=1.027 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 73.3%, forg=  5.2% <<<
>>> Test on task 15 : loss=1.289 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 79.0%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.481 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 71.3%, forg= 13.0% <<<
>>> Test on task 17 : loss=1.186 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 70.0%, forg=  6.4% <<<
>>> Test on task 18 : loss=1.638 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 65.2%, forg=  9.8% <<<
>>> Test on task 19 : loss=1.702 | TAw acc= 86.0%, forg=  1.8%| TAg acc= 56.1%, forg=  5.3% <<<
>>> Test on task 20 : loss=1.083 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 73.9%, forg=  6.1% <<<
>>> Test on task 21 : loss=1.492 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 62.2%, forg=  9.2% <<<
>>> Test on task 22 : loss=1.220 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 64.1%, forg=  6.0% <<<
>>> Test on task 23 : loss=1.085 | TAw acc= 91.8%, forg=  3.3%| TAg acc= 72.1%, forg=  5.7% <<<
>>> Test on task 24 : loss=1.201 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 68.1%, forg=  6.9% <<<
>>> Test on task 25 : loss=1.526 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 61.5%, forg=  9.4% <<<
>>> Test on task 26 : loss=1.554 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 56.0%, forg= 17.2% <<<
>>> Test on task 27 : loss=1.706 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 64.0%, forg=  6.0% <<<
>>> Test on task 28 : loss=1.115 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.0%, forg=  1.9% <<<
>>> Test on task 29 : loss=0.954 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 72.8%, forg=  2.6% <<<
>>> Test on task 30 : loss=1.445 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 61.6%, forg=  8.0% <<<
>>> Test on task 31 : loss=1.125 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.7%, forg=  6.2% <<<
>>> Test on task 32 : loss=1.660 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 56.2%, forg= 12.5% <<<
>>> Test on task 33 : loss=1.045 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 71.2%, forg=  5.8% <<<
>>> Test on task 34 : loss=1.234 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 65.6%, forg=  5.2% <<<
>>> Test on task 35 : loss=1.345 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 62.5%, forg=  8.9% <<<
>>> Test on task 36 : loss=1.166 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.6%, forg=  1.9% <<<
>>> Test on task 37 : loss=1.194 | TAw acc= 97.2%, forg=  2.8%| TAg acc= 67.9%, forg=  4.6% <<<
>>> Test on task 38 : loss=1.458 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 50.0%, forg= 21.9% <<<
>>> Test on task 39 : loss=1.334 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 72.0%, forg=  7.6% <<<
>>> Test on task 40 : loss=1.599 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 52.0%, forg= 13.3% <<<
>>> Test on task 41 : loss=1.228 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 63.1%, forg= 18.0% <<<
>>> Test on task 42 : loss=1.096 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 67.0%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=7.039, TAw acc= 50.0% | *
| Epoch   2, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=2.869, TAw acc= 80.8% | *
| Epoch   3, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.319, TAw acc= 97.4% | *
| Epoch   4, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.022, TAw acc= 96.2% | *
| Epoch   5, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.873, TAw acc= 97.4% | *
| Epoch   6, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.823, TAw acc= 98.7% | *
| Epoch   7, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.820, TAw acc= 98.7% | *
| Epoch   8, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=0.687, TAw acc=100.0% | *
| Epoch   1, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=0.684, TAw acc=100.0% | *
| Epoch   2, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.682, TAw acc=100.0% | *
| Epoch   3, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=0.680, TAw acc=100.0% | *
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.679, TAw acc=100.0% | *
| Epoch   5, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=0.677, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 100
Not enough samples to store. Select all samples instead.	Needed: 100
Not enough samples to store. Select all samples instead.	Needed: 12
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 6166 train exemplars, time=  0.0s
6166
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.212 | TAw acc= 90.1%, forg=  1.6%| TAg acc= 66.5%, forg= 14.7% <<<
>>> Test on task  1 : loss=1.216 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 70.9%, forg= 21.4% <<<
>>> Test on task  2 : loss=1.551 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 63.5%, forg= 15.7% <<<
>>> Test on task  3 : loss=1.294 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 73.8%, forg= 10.3% <<<
>>> Test on task  4 : loss=1.287 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.0%, forg=  8.7% <<<
>>> Test on task  5 : loss=1.355 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 64.8%, forg= 14.8% <<<
>>> Test on task  6 : loss=1.413 | TAw acc= 98.1%, forg=  0.9%| TAg acc= 73.1%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.457 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 68.7%, forg=  9.1% <<<
>>> Test on task  8 : loss=1.574 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 72.1%, forg= 10.7% <<<
>>> Test on task  9 : loss=1.877 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 60.7%, forg= 10.3% <<<
>>> Test on task 10 : loss=1.190 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 69.4%, forg= 18.4% <<<
>>> Test on task 11 : loss=1.393 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 76.9%, forg=  7.4% <<<
>>> Test on task 12 : loss=1.164 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 73.1%, forg=  4.8% <<<
>>> Test on task 13 : loss=1.564 | TAw acc= 93.2%, forg=  1.7%| TAg acc= 55.9%, forg= 21.2% <<<
>>> Test on task 14 : loss=1.092 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.7%, forg=  7.8% <<<
>>> Test on task 15 : loss=1.298 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 75.2%, forg=  9.5% <<<
>>> Test on task 16 : loss=1.511 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 70.4%, forg= 13.9% <<<
>>> Test on task 17 : loss=1.116 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 72.7%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.649 | TAw acc= 93.9%, forg=  2.3%| TAg acc= 66.7%, forg=  8.3% <<<
>>> Test on task 19 : loss=1.709 | TAw acc= 86.0%, forg=  1.8%| TAg acc= 54.4%, forg=  7.0% <<<
>>> Test on task 20 : loss=1.025 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 77.4%, forg=  2.6% <<<
>>> Test on task 21 : loss=1.471 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 61.2%, forg= 10.2% <<<
>>> Test on task 22 : loss=1.265 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.2%, forg=  6.8% <<<
>>> Test on task 23 : loss=1.092 | TAw acc= 92.6%, forg=  2.5%| TAg acc= 72.1%, forg=  5.7% <<<
>>> Test on task 24 : loss=1.245 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 68.1%, forg=  6.9% <<<
>>> Test on task 25 : loss=1.624 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 58.3%, forg= 12.5% <<<
>>> Test on task 26 : loss=1.593 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 56.0%, forg= 17.2% <<<
>>> Test on task 27 : loss=1.709 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 64.0%, forg=  6.0% <<<
>>> Test on task 28 : loss=1.104 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.8%, forg=  0.0% <<<
>>> Test on task 29 : loss=0.973 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 76.3%, forg= -0.9% <<<
>>> Test on task 30 : loss=1.454 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 62.5%, forg=  7.1% <<<
>>> Test on task 31 : loss=1.115 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.9%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.657 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 58.9%, forg=  9.8% <<<
>>> Test on task 33 : loss=1.032 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 70.2%, forg=  6.7% <<<
>>> Test on task 34 : loss=1.256 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 67.7%, forg=  3.1% <<<
>>> Test on task 35 : loss=1.355 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 59.8%, forg= 11.6% <<<
>>> Test on task 36 : loss=1.241 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 70.1%, forg=  8.4% <<<
>>> Test on task 37 : loss=1.214 | TAw acc= 97.2%, forg=  2.8%| TAg acc= 68.8%, forg=  3.7% <<<
>>> Test on task 38 : loss=1.438 | TAw acc= 92.1%, forg=  2.6%| TAg acc= 55.3%, forg= 16.7% <<<
>>> Test on task 39 : loss=1.300 | TAw acc= 94.1%, forg=  0.0%| TAg acc= 72.0%, forg=  7.6% <<<
>>> Test on task 40 : loss=1.523 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 57.1%, forg=  8.2% <<<
>>> Test on task 41 : loss=1.227 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 62.2%, forg= 18.9% <<<
>>> Test on task 42 : loss=1.481 | TAw acc= 97.2%, forg= -1.9%| TAg acc= 51.9%, forg= 15.1% <<<
>>> Test on task 43 : loss=0.930 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 74.5%, forg=  0.0% <<<
Save at k=100/eeil_e20_wisdm_5/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 80.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.6% 
	 89.5%  99.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 89.5% 100.0%  85.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.6% 
	 89.0% 100.0%  91.3%  86.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.6% 
	 89.5% 100.0%  93.0%  92.5%  87.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.5% 
	 90.1% 100.0%  93.0%  93.5%  97.1%  89.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.9% 
	 89.5% 100.0%  93.0%  93.5%  97.1%  95.4%  97.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 90.1% 100.0%  93.0%  93.5%  98.1%  93.5%  97.2%  84.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.8% 
	 89.5% 100.0%  93.9%  93.5%  98.1%  93.5%  98.1%  92.9%  91.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 90.1% 100.0%  93.0%  93.5%  98.1%  93.5%  98.1%  93.9%  94.3%  92.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 88.0% 100.0%  93.9%  93.5%  98.1%  93.5%  98.1%  93.9%  94.3%  94.4%  98.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 89.0% 100.0%  96.5%  94.4%  98.1%  93.5%  97.2%  93.9%  94.3%  94.4%  98.0%  93.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 88.5% 100.0%  93.0%  94.4%  98.1%  93.5%  98.1%  92.9%  95.1%  94.4%  98.0%  93.5%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 88.5% 100.0%  93.9%  93.5%  98.1%  93.5%  98.1%  93.9%  95.1%  94.4%  98.0%  94.4%  94.2%  93.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 90.6% 100.0%  94.8%  94.4%  98.1%  92.6%  98.1%  94.9%  95.1%  94.4%  98.0%  94.4%  95.2%  94.1%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 90.1% 100.0%  94.8%  94.4%  98.1%  92.6%  98.1%  94.9%  95.1%  94.4%  98.0%  94.4%  95.2%  93.2%  98.3%  94.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 90.1% 100.0%  94.8%  94.4%  98.1%  92.6%  98.1%  97.0%  94.3%  94.4%  98.0%  94.4%  95.2%  93.2%  98.3%  95.2%  96.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 89.5% 100.0%  94.8%  94.4%  97.1%  92.6%  98.1%  97.0%  94.3%  95.3%  98.0%  93.5%  96.2%  89.0%  98.3%  94.3%  96.5%  90.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 89.5% 100.0%  95.7%  94.4%  98.1%  92.6%  98.1%  97.0%  94.3%  94.4%  98.0%  94.4%  96.2%  91.5%  98.3%  96.2%  96.5%  95.5%  96.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 89.5% 100.0%  96.5%  94.4%  98.1%  93.5%  98.1%  97.0%  94.3%  93.5%  98.0%  93.5%  96.2%  93.2%  98.3%  95.2%  96.5%  92.7%  96.2%  86.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 89.0% 100.0%  95.7%  94.4%  98.1%  93.5%  98.1%  96.0%  93.4%  94.4%  98.0%  92.6%  96.2%  93.2%  98.3%  95.2%  95.7%  95.5%  95.5%  86.8%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 89.0% 100.0%  95.7%  94.4%  98.1%  92.6%  98.1%  96.0%  94.3%  94.4%  98.0%  94.4%  96.2%  92.4%  98.3%  96.2%  95.7%  95.5%  95.5%  86.0%  95.7%  95.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 90.1%  99.1%  95.7%  93.5%  98.1%  92.6%  98.1%  96.0%  93.4%  94.4%  96.9%  94.4%  96.2%  92.4%  98.3%  96.2%  95.7%  94.5%  96.2%  86.8%  95.7%  98.0%  96.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 90.6%  99.1%  95.7%  93.5%  98.1%  94.4%  98.1%  96.0%  93.4%  93.5%  96.9%  93.5%  96.2%  92.4%  98.3%  96.2%  94.8%  93.6%  96.2%  87.7%  96.5%  98.0%  94.9%  93.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 90.6%  99.1%  95.7%  93.5%  98.1%  94.4%  98.1%  96.0%  92.6%  94.4%  96.9%  94.4%  96.2%  92.4%  98.3%  96.2%  94.8%  94.5%  96.2%  86.8%  96.5%  98.0%  94.0%  94.3%  97.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 90.6%  99.1%  95.7%  93.5%  98.1%  93.5%  99.1%  96.0%  92.6%  94.4%  96.9%  93.5%  95.2%  93.2%  98.3%  96.2%  94.8%  95.5%  96.2%  87.7%  97.4%  98.0%  94.9%  95.1%  98.3%  90.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 90.6%  99.1%  94.8%  93.5%  98.1%  95.4%  99.1%  96.0%  92.6%  96.3%  98.0%  94.4%  95.2%  93.2%  98.3%  96.2%  95.7%  97.3%  96.2%  86.8%  95.7%  98.0%  95.7%  95.1%  98.3%  92.7%  96.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 91.1%  99.1%  95.7%  93.5%  98.1%  93.5%  98.1%  96.0%  92.6%  95.3%  96.9%  94.4%  96.2%  93.2%  98.3%  96.2%  94.8%  96.4%  95.5%  86.8%  97.4%  98.0%  95.7%  93.4%  98.3%  92.7%  97.4%  97.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 91.1%  99.1%  95.7%  93.5%  98.1%  94.4%  98.1%  96.0%  91.8%  95.3%  96.9%  93.5%  95.2%  93.2%  98.3%  96.2%  94.8%  95.5%  94.7%  86.8%  96.5%  98.0%  94.9%  93.4%  98.3%  93.8%  97.4%  96.0%  98.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 90.6%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  96.0%  92.6%  95.3%  98.0%  93.5%  95.2%  93.2%  98.3%  96.2%  94.8%  96.4%  95.5%  86.8%  97.4%  98.0%  95.7%  93.4%  98.3%  93.8%  97.4%  98.0%  96.3%  96.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 91.1%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  96.0%  93.4%  95.3%  98.0%  94.4%  95.2%  93.2%  97.4%  96.2%  94.8%  96.4%  95.5%  87.7%  97.4%  98.0%  95.7%  94.3%  98.3%  93.8%  97.4%  95.0%  99.1%  95.6%  93.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 90.1%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  97.0%  92.6%  96.3%  98.0%  93.5%  95.2%  93.2%  98.3%  96.2%  94.8%  96.4%  94.7%  86.8%  95.7%  98.0%  96.6%  92.6%  98.3%  93.8%  97.4%  95.0%  98.1%  96.5%  95.5%  97.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 91.1%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  97.0%  92.6%  95.3%  98.0%  95.4%  95.2%  93.2%  98.3%  96.2%  94.8%  96.4%  94.7%  86.8%  97.4%  98.0%  95.7%  92.6%  98.3%  93.8%  97.4%  97.0%  99.1%  96.5%  96.4%  98.2%  96.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 90.1%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  97.0%  93.4%  96.3%  98.0%  95.4%  96.2%  93.2%  98.3%  96.2%  94.8%  97.3%  95.5%  87.7%  97.4%  96.9%  96.6%  93.4%  98.3%  93.8%  97.4%  97.0%  99.1%  97.4%  95.5%  98.2%  96.4%  96.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 89.5%  99.1%  95.7%  93.5%  98.1%  95.4%  98.1%  97.0%  94.3%  95.3%  98.0%  95.4%  96.2%  94.1%  98.3%  96.2%  94.8%  97.3%  93.9%  87.7%  97.4%  98.0%  96.6%  91.8%  98.3%  93.8%  96.6%  97.0%  99.1%  96.5%  95.5%  98.2%  96.4%  98.1%  96.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 90.1%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  97.0%  94.3%  96.3%  98.0%  94.4%  96.2%  93.2%  98.3%  96.2%  94.8%  98.2%  93.9%  87.7%  97.4%  96.9%  95.7%  92.6%  98.3%  93.8%  96.6%  97.0%  99.1%  96.5%  96.4%  98.2%  96.4%  98.1%  96.9%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 91.6%  99.1%  95.7%  93.5%  98.1%  95.4%  98.1%  96.0%  94.3%  96.3%  98.0%  93.5%  96.2%  93.2%  98.3%  96.2%  94.8%  96.4%  93.9%  86.8%  98.3%  96.9%  96.6%  91.8%  98.3%  93.8%  96.6%  97.0%  99.1%  96.5%  95.5%  98.2%  96.4%  98.1%  96.9%  93.8%  94.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 90.6%  99.1%  95.7%  94.4%  98.1%  95.4%  98.1%  96.0%  94.3%  96.3%  98.0%  93.5%  96.2%  93.2%  98.3%  96.2%  94.8%  97.3%  93.9%  87.7%  97.4%  96.9%  96.6%  91.8%  98.3%  93.8%  96.6%  97.0%  99.1%  96.5%  94.6%  98.2%  96.4%  98.1%  96.9%  92.9%  95.3% 100.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 90.6%  99.1%  96.5%  93.5%  98.1%  95.4%  98.1%  96.0%  94.3%  96.3%  98.0%  93.5%  96.2%  93.2%  97.4%  96.2%  94.8%  96.4%  93.9%  87.7%  95.7%  96.9%  96.6%  92.6%  98.3%  93.8%  96.6%  97.0%  99.1%  96.5%  95.5%  98.2%  96.4%  99.0%  96.9%  93.8%  95.3%  99.1%  93.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 89.0%  99.1%  96.5%  93.5%  98.1%  94.4%  98.1%  96.0%  94.3%  95.3%  98.0%  94.4%  96.2%  94.1%  98.3%  95.2%  94.8%  98.2%  93.9%  86.8%  96.5%  96.9%  96.6%  91.8%  98.3%  94.8%  96.6%  97.0%  99.1%  96.5%  95.5%  98.2%  96.4%  99.0%  96.9%  93.8%  95.3%  98.2%  93.9%  94.1%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 90.1%  99.1%  96.5%  94.4%  98.1%  95.4%  98.1%  96.0%  94.3%  96.3%  98.0%  94.4%  96.2%  94.1%  98.3%  96.2%  94.8%  97.3%  93.9%  86.8%  96.5%  96.9%  96.6%  91.8%  98.3%  94.8%  96.6%  96.0%  99.1%  96.5%  95.5%  98.2%  96.4%  98.1%  96.9%  92.9%  96.3%  97.2%  94.7%  94.1%  95.9%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 90.1%  99.1%  95.7%  93.5%  98.1%  95.4%  98.1%  96.0%  94.3%  96.3%  98.0%  94.4%  96.2%  94.9%  98.3%  96.2%  94.8%  96.4%  93.9%  87.7%  96.5%  96.9%  96.6%  91.8%  98.3%  94.8%  96.6%  98.0%  99.1%  96.5%  95.5%  98.2%  96.4%  98.1%  96.9%  93.8%  97.2%  98.2%  93.9%  94.1%  95.9%  98.2%   0.0%   0.0% 	Avg.: 95.9% 
	 91.1%  99.1%  97.4%  93.5%  98.1%  95.4%  98.1%  96.0%  94.3%  96.3%  98.0%  94.4%  96.2%  94.1%  98.3%  96.2%  94.8%  96.4%  93.9%  86.0%  96.5%  96.9%  96.6%  91.8%  98.3%  94.8%  96.6%  97.0%  99.1%  96.5%  95.5%  98.2%  96.4%  98.1%  96.9%  93.8%  97.2%  97.2%  94.7%  94.1%  95.9%  98.2%  95.3%   0.0% 	Avg.: 95.9% 
	 90.1%  99.1%  96.5%  93.5%  98.1%  96.3%  98.1%  97.0%  94.3%  96.3%  98.0%  94.4%  95.2%  93.2%  98.3%  95.2%  94.8%  96.4%  93.9%  86.0%  96.5%  96.9%  96.6%  92.6%  98.3%  94.8%  96.6%  97.0%  99.1%  96.5%  95.5%  98.2%  96.4%  98.1%  96.9%  93.8%  97.2%  97.2%  92.1%  94.1%  95.9%  98.2%  97.2%  97.2% 	Avg.: 95.8% 
************************************************************************************************************
TAg Acc
	 80.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 80.6% 
	 74.9%  90.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 82.7% 
	 75.9%  85.5%  73.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 78.4% 
	 70.7%  90.6%  70.4%  78.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.6% 
	 78.5%  92.3%  76.5%  60.7%  74.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.4% 
	 77.5%  86.3%  71.3%  84.1%  68.3%  78.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.7% 
	 79.1%  92.3%  78.3%  83.2%  63.5%  65.7%  80.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.5% 
	 81.2%  65.8%  73.9%  82.2%  75.0%  69.4%  62.0%  71.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.7% 
	 79.1%  82.9%  73.0%  82.2%  82.7%  67.6%  71.3%  60.6%  82.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.8% 
	 79.6%  82.9%  79.1%  74.8%  83.7%  75.9%  68.5%  66.7%  70.5%  70.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.2% 
	 78.0%  85.5%  73.0%  81.3%  79.8%  76.9%  72.2%  75.8%  70.5%  58.9%  81.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.8% 
	 75.9%  83.8%  76.5%  79.4%  74.0%  78.7%  72.2%  77.8%  69.7%  57.0%  78.6%  81.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.4% 
	 74.3%  83.8%  78.3%  78.5%  78.8%  79.6%  75.9%  76.8%  72.1%  64.5%  86.7%  63.0%  73.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.8% 
	 75.9%  83.8%  74.8%  77.6%  77.9%  75.0%  67.6%  75.8%  73.0%  61.7%  81.6%  73.1%  71.2%  77.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.7% 
	 70.7%  82.9%  78.3%  75.7%  79.8%  76.9%  73.1%  75.8%  72.1%  68.2%  84.7%  78.7%  71.2%  53.4%  76.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.5% 
	 74.9%  81.2%  74.8%  83.2%  80.8%  78.7%  72.2%  71.7%  74.6%  63.6%  82.7%  77.8%  71.2%  49.2%  61.2%  81.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.7% 
	 74.9%  76.9%  73.9%  82.2%  83.7%  77.8%  75.9%  76.8%  77.0%  71.0%  78.6%  78.7%  73.1%  56.8%  73.3%  62.9%  84.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.2% 
	 71.2%  76.9%  78.3%  78.5%  72.1%  75.9%  75.9%  67.7%  74.6%  69.2%  87.8%  78.7%  73.1%  51.7%  76.7%  70.5%  67.8%  76.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.5% 
	 74.3%  75.2%  73.9%  77.6%  82.7%  75.0%  73.1%  66.7%  78.7%  69.2%  76.5%  79.6%  76.9%  51.7%  74.1%  72.4%  73.0%  63.6%  75.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.1% 
	 71.2%  77.8%  72.2%  76.6%  83.7%  70.4%  76.9%  75.8%  74.6%  62.6%  75.5%  80.6%  76.0%  55.1%  75.9%  79.0%  74.8%  62.7%  59.1%  57.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.9% 
	 71.7%  77.8%  70.4%  75.7%  78.8%  72.2%  72.2%  74.7%  73.0%  67.3%  78.6%  76.9%  76.9%  55.9%  69.8%  82.9%  70.4%  64.5%  65.9%  37.7%  80.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.1% 
	 69.6%  81.2%  66.1%  77.6%  76.9%  71.3%  76.9%  71.7%  73.0%  68.2%  78.6%  83.3%  76.0%  52.5%  76.7%  81.9%  76.5%  68.2%  69.7%  36.8%  69.6%  71.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.5% 
	 69.1%  79.5%  76.5%  78.5%  81.7%  70.4%  75.9%  69.7%  76.2%  67.3%  83.7%  83.3%  72.1%  50.0%  72.4%  81.9%  73.9%  66.4%  62.9%  48.2%  66.1%  60.2%  70.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.1% 
	 72.8%  71.8%  69.6%  77.6%  79.8%  70.4%  75.0%  73.7%  74.6%  68.2%  76.5%  80.6%  72.1%  54.2%  64.7%  81.0%  73.9%  65.5%  71.2%  50.9%  70.4%  62.2%  59.0%  75.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.5% 
	 70.7%  76.9%  74.8%  75.7%  76.9%  73.1%  76.9%  71.7%  75.4%  69.2%  76.5%  82.4%  75.0%  55.1%  65.5%  84.8%  72.2%  66.4%  64.4%  50.9%  75.7%  63.3%  60.7%  68.9%  75.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.1% 
	 67.5%  79.5%  71.3%  76.6%  79.8%  72.2%  71.3%  61.6%  73.8%  68.2%  72.4%  82.4%  74.0%  57.6%  78.4%  79.0%  76.5%  68.2%  64.4%  56.1%  78.3%  61.2%  52.1%  68.9%  62.1%  70.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.2% 
	 70.2%  76.1%  68.7%  75.7%  76.9%  67.6%  72.2%  73.7%  73.8%  68.2%  72.4%  84.3%  76.9%  60.2%  63.8%  79.0%  73.0%  64.5%  60.6%  53.5%  77.4%  63.3%  63.2%  73.8%  68.1%  47.9%  73.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.6% 
	 69.6%  75.2%  68.7%  75.7%  77.9%  66.7%  74.1%  69.7%  73.8%  62.6%  72.4%  82.4%  77.9%  55.9%  66.4%  80.0%  73.0%  70.0%  62.1%  57.0%  74.8%  65.3%  68.4%  74.6%  66.4%  55.2%  59.5%  70.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.5% 
	 69.6%  76.9%  70.4%  74.8%  78.8%  67.6%  74.1%  69.7%  76.2%  67.3%  75.5%  80.6%  77.9%  58.5%  68.1%  81.0%  72.2%  70.9%  61.4%  54.4%  73.9%  67.3%  66.7%  77.9%  71.6%  57.3%  53.4%  57.0%  73.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.8% 
	 67.5%  71.8%  62.6%  73.8%  76.9%  64.8%  72.2%  70.7%  78.7%  65.4%  76.5%  79.6%  75.0%  61.9%  69.8%  81.0%  73.0%  69.1%  66.7%  60.5%  75.7%  68.4%  68.4%  74.6%  70.7%  54.2%  50.9%  50.0%  54.2%  72.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 68.1%  72.6%  66.1%  72.9%  76.9%  63.9%  75.0%  70.7%  72.1%  66.4%  74.5%  77.8%  74.0%  59.3%  67.2%  75.2%  72.2%  71.8%  62.9%  58.8%  74.8%  57.1%  67.5%  75.4%  68.1%  57.3%  53.4%  52.0%  60.7%  65.8%  69.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 72.3%  74.4%  66.1%  72.9%  80.8%  64.8%  72.2%  69.7%  70.5%  65.4%  74.5%  78.7%  76.9%  59.3%  67.2%  79.0%  73.0%  73.6%  66.7%  61.4%  73.9%  64.3%  68.4%  72.1%  69.8%  59.4%  55.2%  63.0%  61.7%  68.4%  57.1%  77.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.1% 
	 67.5%  72.6%  61.7%  72.9%  77.9%  64.8%  73.1%  68.7%  68.0%  66.4%  72.4%  79.6%  74.0%  63.6%  73.3%  80.0%  72.2%  69.1%  63.6%  57.9%  76.5%  63.3%  64.1%  69.7%  73.3%  61.5%  53.4%  60.0%  67.3%  71.9%  58.9%  65.5%  68.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.3% 
	 68.1%  69.2%  66.1%  71.0%  79.8%  66.7%  75.0%  68.7%  73.0%  66.4%  75.5%  79.6%  76.9%  61.9%  72.4%  77.1%  72.2%  72.7%  59.1%  55.3%  78.3%  61.2%  65.8%  69.7%  71.6%  58.3%  56.9%  66.0%  70.1%  68.4%  62.5%  66.4%  45.5%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.4% 
	 68.1%  70.9%  61.7%  70.1%  77.9%  65.7%  72.2%  67.7%  75.4%  64.5%  72.4%  80.6%  76.0%  61.0%  76.7%  76.2%  73.0%  66.4%  64.4%  53.5%  79.1%  61.2%  64.1%  68.9%  70.7%  61.5%  58.6%  58.0%  65.4%  72.8%  61.6%  68.1%  51.8%  58.7%  70.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 68.6%  70.1%  65.2%  74.8%  78.8%  69.4%  75.0%  69.7%  65.6%  65.4%  77.6%  74.1%  73.1%  60.2%  76.7%  81.0%  72.2%  70.0%  65.9%  55.3%  73.0%  62.2%  64.1%  68.0%  69.8%  60.4%  59.5%  59.0%  70.1%  71.1%  65.2%  69.0%  57.1%  59.6%  61.5%  71.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 69.6%  71.8%  59.1%  72.9%  76.9%  64.8%  73.1%  68.7%  72.1%  64.5%  72.4%  76.9%  76.9%  57.6%  75.0%  75.2%  70.4%  72.7%  63.6%  55.3%  75.7%  65.3%  67.5%  64.8%  69.0%  55.2%  52.6%  58.0%  72.0%  71.1%  60.7%  71.7%  60.7%  64.4%  66.7%  58.0%  78.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 67.5%  74.4%  58.3%  72.0%  76.9%  63.0%  74.1%  69.7%  73.8%  63.6%  70.4%  76.9%  73.1%  55.1%  70.7%  81.0%  71.3%  70.0%  66.7%  55.3%  75.7%  61.2%  66.7%  71.3%  71.6%  62.5%  51.7%  61.0%  66.4%  73.7%  64.3%  68.1%  58.0%  74.0%  62.5%  59.8%  59.8%  71.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.5% 
	 62.8%  70.9%  66.1%  73.8%  77.9%  66.7%  71.3%  67.7%  72.1%  64.5%  71.4%  74.1%  76.9%  61.0%  69.0%  81.0%  71.3%  71.8%  65.9%  57.0%  71.3%  63.3%  66.7%  75.4%  67.2%  63.5%  55.2%  60.0%  71.0%  72.8%  62.5%  74.3%  61.6%  66.3%  65.6%  59.8%  63.6%  59.6%  71.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 66.0%  70.1%  65.2%  73.8%  78.8%  67.6%  72.2%  70.7%  71.3%  63.6%  74.5%  75.9%  76.0%  61.0%  74.1%  80.0%  70.4%  72.7%  68.2%  55.3%  77.4%  60.2%  65.8%  71.3%  69.8%  59.4%  58.6%  67.0%  71.0%  72.8%  61.6%  72.6%  61.6%  64.4%  67.7%  61.6%  68.2%  67.0%  52.6%  79.7%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.4% 
	 67.5%  73.5%  64.3%  74.8%  73.1%  63.0%  73.1%  67.7%  72.1%  64.5%  74.5%  76.9%  75.0%  59.3%  77.6%  81.9%  72.2%  70.0%  62.9%  52.6%  74.8%  59.2%  65.0%  65.6%  67.2%  59.4%  56.9%  63.0%  73.8%  71.9%  66.1%  69.9%  61.6%  73.1%  63.5%  61.6%  70.1%  67.0%  51.8%  72.9%  65.3%   0.0%   0.0%   0.0% 	Avg.: 67.7% 
	 66.5%  71.8%  67.0%  71.0%  76.9%  64.8%  75.0%  71.7%  73.0%  65.4%  69.4%  72.2%  73.1%  58.5%  71.6%  74.3%  73.0%  71.8%  63.6%  55.3%  73.9%  58.2%  65.0%  73.8%  69.0%  62.5%  53.4%  63.0%  70.1%  75.4%  61.6%  76.1%  57.1%  73.1%  67.7%  60.7%  74.8%  72.5%  48.2%  72.0%  45.9%  81.1%   0.0%   0.0% 	Avg.: 67.6% 
	 66.5%  69.2%  65.2%  75.7%  76.0%  60.2%  72.2%  72.7%  71.3%  61.7%  72.4%  75.0%  75.0%  61.0%  73.3%  79.0%  71.3%  70.0%  65.2%  56.1%  73.9%  62.2%  64.1%  72.1%  68.1%  61.5%  56.0%  64.0%  72.0%  72.8%  61.6%  71.7%  56.2%  71.2%  65.6%  62.5%  76.6%  67.9%  50.0%  72.0%  52.0%  63.1%  67.0%   0.0% 	Avg.: 67.3% 
	 66.5%  70.9%  63.5%  73.8%  75.0%  64.8%  73.1%  68.7%  72.1%  60.7%  69.4%  76.9%  73.1%  55.9%  70.7%  75.2%  70.4%  72.7%  66.7%  54.4%  77.4%  61.2%  63.2%  72.1%  68.1%  58.3%  56.0%  64.0%  73.8%  76.3%  62.5%  69.9%  58.9%  70.2%  67.7%  59.8%  70.1%  68.8%  55.3%  72.0%  57.1%  62.2%  51.9%  74.5% 	Avg.: 67.0% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -8.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -8.9% 
	  0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.4% 
	  0.5%   0.0%  -6.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.9% 
	  0.0%   0.0%  -1.7%  -6.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.1% 
	 -0.5%   0.0%   0.0%  -0.9%  -9.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.2% 
	  0.5%   0.0%   0.0%   0.0%   0.0%  -5.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.8% 
	  0.0%   0.0%   0.0%   0.0%  -1.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.5%   0.0%  -0.9%   0.0%   0.0%   1.9%  -0.9%  -8.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.9% 
	  0.0%   0.0%   0.9%   0.0%   0.0%   1.9%   0.0%  -1.0%  -2.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.1% 
	  2.1%   0.0%   0.0%   0.0%   0.0%   1.9%   0.0%   0.0%   0.0%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.0%   0.0%  -2.6%  -0.9%   0.0%   1.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  1.6%   0.0%   3.5%   0.0%   0.0%   1.9%   0.0%   1.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.6%   0.0%   2.6%   0.9%   0.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	 -0.5%   0.0%   1.7%   0.0%   0.0%   2.8%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%  -1.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.5%   0.0%   1.7%   0.0%   0.0%   2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%  -2.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.5%   0.0%   1.7%   0.0%   0.0%   2.8%   0.0%  -2.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.0%   0.0%   1.7%   0.0%   1.0%   2.8%   0.0%   0.0%   0.8%  -0.9%   0.0%   0.9%  -1.0%   5.1%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.0%   0.0%   0.9%   0.0%   0.0%   2.8%   0.0%   0.0%   0.8%   0.9%   0.0%   0.0%   0.0%   2.5%   0.0%  -1.0%   0.0%  -4.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.0%   0.0%   0.0%   0.0%   0.0%   1.9%   0.0%   0.0%   0.8%   1.9%   0.0%   0.9%   0.0%   0.8%   0.0%   1.0%   0.0%   2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.6%   0.0%   0.9%   0.0%   0.0%   1.9%   0.0%   1.0%   1.6%   0.9%   0.0%   1.9%   0.0%   0.8%   0.0%   1.0%   0.9%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.6%   0.0%   0.9%   0.0%   0.0%   2.8%   0.0%   1.0%   0.8%   0.9%   0.0%   0.0%   0.0%   1.7%   0.0%   0.0%   0.9%   0.0%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.5%   0.9%   0.9%   0.9%   0.0%   2.8%   0.0%   1.0%   1.6%   0.9%   1.0%   0.0%   0.0%   1.7%   0.0%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%  -2.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.9%   0.9%   0.9%   0.0%   0.9%   0.0%   1.0%   1.6%   1.9%   1.0%   0.9%   0.0%   1.7%   0.0%   0.0%   1.7%   1.8%   0.0%  -0.9%  -0.9%   0.0%   1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.0%   0.9%   0.9%   0.9%   0.0%   0.9%   0.0%   1.0%   2.5%   0.9%   1.0%   0.0%   0.0%   1.7%   0.0%   0.0%   1.7%   0.9%   0.0%   0.9%   0.0%   0.0%   2.6%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.0%   0.9%   0.9%   0.9%   0.0%   1.9%  -0.9%   1.0%   2.5%   0.9%   1.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%  -0.9%   0.0%   1.7%  -0.8%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.9%   1.7%   0.9%   0.0%   0.0%   0.0%   1.0%   2.5%  -0.9%   0.0%   0.0%   1.0%   0.8%   0.0%   0.0%   0.9%  -1.8%   0.0%   0.9%   1.7%   0.0%   0.9%   0.0%   0.0%  -2.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	 -0.5%   0.9%   0.9%   0.9%   0.0%   1.9%   0.9%   1.0%   2.5%   0.9%   1.0%   0.0%   0.0%   0.8%   0.0%   0.0%   1.7%   0.9%   0.8%   0.9%   0.0%   0.0%   0.9%   1.6%   0.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.0%   0.9%   0.9%   0.9%   0.0%   0.9%   0.9%   1.0%   3.3%   0.9%   1.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   1.8%   1.5%   0.9%   0.9%   0.0%   1.7%   1.6%   0.0%  -1.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.5%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   1.0%   2.5%   0.9%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.9%   0.8%   0.9%   0.0%   0.0%   0.9%   1.6%   0.0%   0.0%   0.0%  -1.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   1.0%   1.6%   0.9%   0.0%   0.0%   1.0%   0.8%   0.9%   0.0%   1.7%   0.9%   0.8%   0.0%   0.0%   0.0%   0.9%   0.8%   0.0%   0.0%   0.0%   3.0%  -0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%   2.5%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.9%   1.5%   0.9%   1.7%   0.0%   0.0%   2.5%   0.0%   0.0%   0.0%   3.0%   0.9%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%   2.5%   0.9%   0.0%  -0.9%   1.0%   0.8%   0.0%   0.0%   1.7%   0.9%   1.5%   0.9%   0.0%   0.0%   0.9%   2.5%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%  -0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%   1.6%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   1.7%   0.0%   0.8%   0.0%   0.0%   1.0%   0.0%   1.6%   0.0%   0.0%   0.0%   1.0%   0.0%  -0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.6%   0.9%   0.9%   0.9%   0.0%   0.0%   0.9%   0.0%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.7%   0.0%   2.3%   0.0%   0.0%   0.0%   0.0%   3.3%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   0.9%   0.0%   0.0%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   1.7%  -0.9%   2.3%   0.0%   0.0%   1.0%   0.9%   2.5%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	 -0.5%   0.9%   0.9%   0.9%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.9%   0.0%   0.8%   0.0%   0.0%   1.7%   1.8%   2.3%   0.9%  -0.9%   1.0%   0.0%   3.3%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.9%   0.0%   0.8%   0.0%   0.0%   1.7%   0.9%   2.3%   0.0%   0.9%   1.0%   0.0%   3.3%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   1.8%   0.0%   0.0%   0.0%   0.0%   0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   1.9%   0.0%   0.8%   0.9%   0.0%   1.7%   1.8%   2.3%   0.0%   2.6%   1.0%   0.0%   2.5%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   0.9%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  2.6%   0.9%   0.0%   0.9%   0.0%   0.9%   0.9%   1.0%   0.8%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   1.0%   1.7%   0.0%   2.3%   0.9%   1.7%   1.0%   0.0%   3.3%   0.0%  -1.0%   0.9%   1.0%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.6%   0.9%   0.0%   0.0%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.7%   0.9%   2.3%   0.9%   1.7%   1.0%   0.0%   3.3%   0.0%   0.0%   0.9%   2.0%   0.0%   0.9%   0.9%   0.0%   0.0%   1.0%   0.0%   0.9%  -0.9%   2.8%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.6%   0.9%   0.9%   0.9%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   0.9%   0.0%  -0.8%   0.0%   0.0%   1.7%   1.8%   2.3%   0.0%   1.7%   1.0%   0.0%   3.3%   0.0%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   0.0%   0.0%   1.0%   0.0%   0.0%  -0.9%   1.8%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.5%   0.9%  -0.9%   0.9%   0.0%   0.0%   0.9%   1.0%   0.8%   0.0%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   1.7%   1.8%   2.3%   1.8%   1.7%   1.0%   0.0%   3.3%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   0.9%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.6%   0.9%   0.9%   0.9%   0.0%  -0.9%   0.9%   0.0%   0.8%   0.0%   0.0%   0.9%   1.0%   1.7%   0.0%   1.0%   1.7%   1.8%   2.3%   1.8%   1.7%   1.0%   0.0%   2.5%   0.0%   0.0%   0.9%   1.0%   0.0%   0.9%   0.9%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   2.8%   2.6%   0.0%   0.0%   0.0%  -1.9%   0.0% 	Avg.:  0.7% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  5.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  4.7%   5.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.9% 
	  9.9%   0.0%   3.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.5% 
	  2.1%  -1.7%  -2.6%  17.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.9% 
	  3.1%   6.0%   5.2%  -5.6%   5.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.9% 
	  1.6%   0.0%  -1.7%   0.9%  10.6%  13.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.1% 
	 -0.5%  26.5%   4.3%   1.9%  -1.0%   9.3%  18.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	  2.1%   9.4%   5.2%   1.9%  -7.7%  11.1%   9.3%  11.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.3% 
	  1.6%   9.4%  -0.9%   9.3%  -1.0%   2.8%  12.0%   5.1%  12.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	  3.1%   6.8%   6.1%   2.8%   3.8%   1.9%   8.3%  -4.0%  12.3%  11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	  5.2%   8.5%   2.6%   4.7%   9.6%   0.0%   8.3%  -2.0%  13.1%  13.1%   3.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.0% 
	  6.8%   8.5%   0.9%   5.6%   4.8%  -0.9%   4.6%   1.0%  10.7%   5.6%  -5.1%  18.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	  5.2%   8.5%   4.3%   6.5%   5.8%   4.6%  13.0%   2.0%   9.8%   8.4%   5.1%   8.3%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.4% 
	 10.5%   9.4%   0.9%   8.4%   3.8%   2.8%   7.4%   2.0%  10.7%   1.9%   2.0%   2.8%   1.9%  23.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	  6.3%  11.1%   4.3%   0.9%   2.9%   0.9%   8.3%   6.1%   8.2%   6.5%   4.1%   3.7%   1.9%  28.0%  15.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.3% 
	  6.3%  15.4%   5.2%   1.9%   0.0%   1.9%   4.6%   1.0%   5.7%  -0.9%   8.2%   2.8%   0.0%  20.3%   3.4%  18.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.9% 
	  9.9%  15.4%   0.9%   5.6%  11.5%   3.7%   4.6%  10.1%   8.2%   1.9%  -1.0%   2.8%   0.0%  25.4%   0.0%  10.5%  16.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.4% 
	  6.8%  17.1%   5.2%   6.5%   1.0%   4.6%   7.4%  11.1%   4.1%   1.9%  11.2%   1.9%  -3.8%  25.4%   2.6%   8.6%  11.3%  12.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	  9.9%  14.5%   7.0%   7.5%   0.0%   9.3%   3.7%   2.0%   8.2%   8.4%  12.2%   0.9%   1.0%  22.0%   0.9%   1.9%   9.6%  13.6%  15.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.8% 
	  9.4%  14.5%   8.7%   8.4%   4.8%   7.4%   8.3%   3.0%   9.8%   3.7%   9.2%   4.6%   0.0%  21.2%   6.9%  -1.9%  13.9%  11.8%   9.1%  19.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	 11.5%  11.1%  13.0%   6.5%   6.7%   8.3%   3.7%   6.1%   9.8%   2.8%   9.2%  -1.9%   1.0%  24.6%   0.0%   1.0%   7.8%   8.2%   5.3%  20.2%  10.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.9% 
	 12.0%  12.8%   2.6%   5.6%   1.9%   9.3%   4.6%   8.1%   6.6%   3.7%   4.1%   0.0%   4.8%  27.1%   4.3%   1.0%  10.4%  10.0%  12.1%   8.8%  13.9%  11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.0% 
	  8.4%  20.5%   9.6%   6.5%   3.8%   9.3%   5.6%   4.0%   8.2%   2.8%  11.2%   2.8%   4.8%  22.9%  12.1%   1.9%  10.4%  10.9%   3.8%   6.1%   9.6%   9.2%  11.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 10.5%  15.4%   4.3%   8.4%   6.7%   6.5%   3.7%   6.1%   7.4%   1.9%  11.2%   0.9%   1.9%  22.0%  11.2%  -1.9%  12.2%  10.0%  10.6%   6.1%   4.3%   8.2%   9.4%   6.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.7% 
	 13.6%  12.8%   7.8%   7.5%   3.8%   7.4%   9.3%  16.2%   9.0%   2.8%  15.3%   0.9%   2.9%  19.5%  -1.7%   5.7%   7.8%   8.2%  10.6%   0.9%   1.7%  10.2%  17.9%   6.6%  12.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 11.0%  16.2%  10.4%   8.4%   6.7%  12.0%   8.3%   4.0%   9.0%   2.8%  15.3%  -0.9%   0.0%  16.9%  14.7%   5.7%  11.3%  11.8%  14.4%   3.5%   2.6%   8.2%   6.8%   1.6%   6.9%  22.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.9% 
	 11.5%  17.1%  10.4%   8.4%   5.8%  13.0%   6.5%   8.1%   9.0%   8.4%  15.3%   1.9%  -1.0%  21.2%  12.1%   4.8%  11.3%   6.4%  12.9%   0.0%   5.2%   6.1%   1.7%   0.8%   8.6%  15.6%  13.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.7% 
	 11.5%  15.4%   8.7%   9.3%   4.8%  12.0%   6.5%   8.1%   6.6%   3.7%  12.2%   3.7%   0.0%  18.6%  10.3%   3.8%  12.2%   5.5%  13.6%   2.6%   6.1%   4.1%   3.4%  -2.5%   3.4%  13.5%  19.8%  13.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.2% 
	 13.6%  20.5%  16.5%  10.3%   6.7%  14.8%   8.3%   7.1%   4.1%   5.6%  11.2%   4.6%   2.9%  15.3%   8.6%   3.8%  11.3%   7.3%   8.3%  -3.5%   4.3%   3.1%   1.7%   3.3%   4.3%  16.7%  22.4%  20.0%  19.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 13.1%  19.7%  13.0%  11.2%   6.7%  15.7%   5.6%   7.1%  10.7%   4.7%  13.3%   6.5%   3.8%  17.8%  11.2%   9.5%  12.2%   4.5%  12.1%   1.8%   5.2%  14.3%   2.6%   2.5%   6.9%  13.5%  19.8%  18.0%  13.1%   7.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 10.1% 
	  8.9%  17.9%  13.0%  11.2%   2.9%  14.8%   8.3%   8.1%  12.3%   5.6%  13.3%   5.6%   1.0%  17.8%  11.2%   5.7%  11.3%   2.7%   8.3%  -0.9%   6.1%   7.1%   1.7%   5.7%   5.2%  11.5%  18.1%   7.0%  12.1%   4.4%  12.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.7% 
	 13.6%  19.7%  17.4%  11.2%   5.8%  14.8%   7.4%   9.1%  14.8%   4.7%  15.3%   4.6%   3.8%  13.6%   5.2%   4.8%  12.2%   7.3%  11.4%   3.5%   3.5%   8.2%   6.0%   8.2%   1.7%   9.4%  19.8%  10.0%   6.5%   0.9%  10.7%  12.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.3% 
	 13.1%  23.1%  13.0%  13.1%   3.8%  13.0%   5.6%   9.1%   9.8%   4.7%  12.2%   4.6%   1.0%  15.3%   6.0%   7.6%  12.2%   3.6%  15.9%   6.1%   1.7%  10.2%   4.3%   8.2%   3.4%  12.5%  16.4%   4.0%   3.7%   4.4%   7.1%  11.5%  23.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.2% 
	 13.1%  21.4%  17.4%  14.0%   5.8%  13.9%   8.3%  10.1%   7.4%   6.5%  15.3%   3.7%   1.9%  16.1%   1.7%   8.6%  11.3%  10.0%  10.6%   7.9%   0.9%  10.2%   6.0%   9.0%   4.3%   9.4%  14.7%  12.0%   8.4%   0.0%   8.0%   9.7%  17.0%  18.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 12.6%  22.2%  13.9%   9.3%   4.8%  10.2%   5.6%   8.1%  17.2%   5.6%  10.2%  10.2%   4.8%  16.9%   1.7%   3.8%  12.2%   6.4%   9.1%   6.1%   7.0%   9.2%   6.0%   9.8%   5.2%  10.4%  13.8%  11.0%   3.7%   1.8%   4.5%   8.8%  11.6%  17.3%   9.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.2% 
	 11.5%  20.5%  20.0%  11.2%   6.7%  14.8%   7.4%   9.1%  10.7%   6.5%  15.3%   7.4%   1.0%  19.5%   3.4%   9.5%  13.9%   3.6%  11.4%   6.1%   4.3%   6.1%   2.6%  13.1%   6.0%  15.6%  20.7%  12.0%   1.9%   1.8%   8.9%   6.2%   8.0%  12.5%   4.2%  13.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.6% 
	 13.6%  17.9%  20.9%  12.1%   6.7%  16.7%   6.5%   8.1%   9.0%   7.5%  17.3%   7.4%   4.8%  22.0%   7.8%   3.8%  13.0%   6.4%   8.3%   6.1%   4.3%  10.2%   3.4%   6.6%   3.4%   8.3%  21.6%   9.0%   7.5%  -0.9%   5.4%   9.7%  10.7%   2.9%   8.3%  11.6%  18.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.6% 
	 18.3%  21.4%  13.0%  10.3%   5.8%  13.0%   9.3%  10.1%  10.7%   6.5%  16.3%  10.2%   1.0%  16.1%   9.5%   3.8%  13.0%   4.5%   9.1%   4.4%   8.7%   8.2%   3.4%   2.5%   7.8%   7.3%  18.1%  10.0%   2.8%   0.9%   7.1%   3.5%   7.1%  10.6%   5.2%  11.6%  15.0%  11.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.2% 
	 15.2%  22.2%  13.9%  10.3%   4.8%  12.0%   8.3%   7.1%  11.5%   7.5%  13.3%   8.3%   1.9%  16.1%   4.3%   4.8%  13.9%   3.6%   6.8%   6.1%   2.6%  11.2%   4.3%   6.6%   5.2%  11.5%  14.7%   3.0%   2.8%   0.9%   8.0%   5.3%   7.1%  12.5%   3.1%   9.8%  10.3%   4.6%  19.3%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	 13.6%  18.8%  14.8%   9.3%  10.6%  16.7%   7.4%  10.1%  10.7%   6.5%  13.3%   7.4%   2.9%  17.8%   0.9%   2.9%  12.2%   6.4%  12.1%   8.8%   5.2%  12.2%   5.1%  12.3%   7.8%  11.5%  16.4%   7.0%   0.0%   1.8%   3.6%   8.0%   7.1%   3.8%   7.3%   9.8%   8.4%   4.6%  20.2%   6.8%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.0% 
	 14.7%  20.5%  12.2%  13.1%   6.7%  14.8%   5.6%   6.1%   9.8%   5.6%  18.4%  12.0%   4.8%  18.6%   6.9%  10.5%  11.3%   4.5%  11.4%   6.1%   6.1%  13.3%   5.1%   4.1%   6.0%   8.3%  19.8%   7.0%   3.7%  -1.8%   8.0%   1.8%  11.6%   3.8%   3.1%  10.7%   3.7%  -0.9%  23.7%   7.6%  19.4%   0.0%   0.0%   0.0% 	Avg.:  9.2% 
	 14.7%  23.1%  13.9%   8.4%   7.7%  19.4%   8.3%   5.1%  11.5%   9.3%  15.3%   9.3%   2.9%  16.1%   5.2%   5.7%  13.0%   6.4%   9.8%   5.3%   6.1%   9.2%   6.0%   5.7%   6.9%   9.4%  17.2%   6.0%   1.9%   2.6%   8.0%   6.2%  12.5%   5.8%   5.2%   8.9%   1.9%   4.6%  21.9%   7.6%  13.3%  18.0%   0.0%   0.0% 	Avg.:  9.4% 
	 14.7%  21.4%  15.7%  10.3%   8.7%  14.8%   7.4%   9.1%  10.7%  10.3%  18.4%   7.4%   4.8%  21.2%   7.8%   9.5%  13.9%   3.6%   8.3%   7.0%   2.6%  10.2%   6.8%   5.7%   6.9%  12.5%  17.2%   6.0%   0.0%  -0.9%   7.1%   8.0%   9.8%   6.7%   3.1%  11.6%   8.4%   3.7%  16.7%   7.6%   8.2%  18.9%  15.1%   0.0% 	Avg.:  9.7% 
************************************************************************************************************
[Elapsed time = 0.5 h]
Done!

f1_score_micro: 0.6697173073011999
f1_score_macro: 0.6390538375967323
              precision    recall  f1-score   support

           0       0.80      1.00      0.89         4
           1       0.90      1.00      0.95         9
           2       0.00      0.00      0.00         4
           3       1.00      0.89      0.94         9
           4       0.22      0.50      0.31         4
           5       1.00      0.40      0.57         5
           6       1.00      0.75      0.86         4
           7       0.80      1.00      0.89         4
           8       0.00      0.00      0.00         4
           9       1.00      1.00      1.00         4
          10       0.33      0.25      0.29         4
          11       0.75      0.67      0.71         9
          12       0.50      0.78      0.61         9
          13       0.00      0.00      0.00         4
          14       0.80      1.00      0.89         4
          15       0.78      0.78      0.78         9
          16       0.80      1.00      0.89         4
          17       0.70      0.78      0.74         9
          18       0.71      1.00      0.83         5
          19       0.60      0.75      0.67         4
          20       0.00      0.00      0.00         9
          21       1.00      0.50      0.67         4
          22       0.00      0.00      0.00         4
          23       0.60      1.00      0.75         9
          24       1.00      1.00      1.00         4
          25       1.00      0.50      0.67         4
          26       0.50      0.75      0.60         4
          27       0.71      1.00      0.83         5
          28       0.83      1.00      0.91         5
          29       0.04      0.11      0.06         9
          30       0.58      0.78      0.67         9
          31       0.00      0.00      0.00         4
          32       1.00      1.00      1.00         4
          33       0.71      1.00      0.83         5
          34       0.50      0.75      0.60         4
          35       0.80      0.44      0.57         9
          36       0.73      0.89      0.80         9
          37       0.00      0.00      0.00         4
          38       0.90      1.00      0.95         9
          39       0.43      0.60      0.50         5
          40       1.00      1.00      1.00         4
          41       0.75      0.75      0.75         4
          42       0.00      0.00      0.00         4
          43       1.00      0.44      0.62         9
          44       0.50      0.50      0.50         4
          45       1.00      1.00      1.00         5
          46       0.67      1.00      0.80         4
          47       0.80      1.00      0.89         4
          48       0.75      0.75      0.75         4
          49       0.67      0.89      0.76         9
          50       0.90      1.00      0.95         9
          51       1.00      0.50      0.67         4
          52       0.00      0.00      0.00         4
          53       0.80      0.89      0.84         9
          54       0.29      0.40      0.33         5
          55       0.75      0.75      0.75         4
          56       0.00      0.00      0.00         4
          57       0.57      0.67      0.62         6
          58       1.00      1.00      1.00         9
          59       1.00      0.75      0.86         4
          60       0.43      0.75      0.55         4
          61       0.67      0.50      0.57         4
          62       1.00      0.75      0.86         4
          63       0.71      1.00      0.83         5
          64       0.60      0.67      0.63         9
          65       0.88      0.78      0.82         9
          66       0.67      0.44      0.53         9
          67       0.00      0.00      0.00         9
          68       1.00      0.89      0.94         9
          69       1.00      0.75      0.86         4
          70       0.00      0.00      0.00         4
          71       0.25      0.50      0.33         4
          72       0.80      1.00      0.89         4
          73       1.00      1.00      1.00         5
          74       1.00      1.00      1.00         4
          75       1.00      0.89      0.94         9
          76       0.83      0.71      0.77         7
          77       0.50      1.00      0.67         4
          78       0.67      1.00      0.80         4
          79       0.80      1.00      0.89         4
          80       0.83      1.00      0.91         5
          81       0.20      0.20      0.20         5
          82       1.00      0.78      0.88         9
          83       1.00      0.60      0.75         5
          84       0.67      0.50      0.57         4
          85       1.00      0.60      0.75         5
          86       1.00      1.00      1.00         4
          87       1.00      1.00      1.00         4
          88       0.80      1.00      0.89         4
          89       0.75      0.75      0.75         4
          90       0.36      0.56      0.43         9
          91       0.89      0.89      0.89         9
          92       0.33      0.25      0.29         4
          93       0.00      0.00      0.00         4
          94       0.70      0.78      0.74         9
          95       1.00      0.80      0.89         5
          96       0.75      0.75      0.75         4
          97       1.00      1.00      1.00         4
          98       0.50      0.80      0.62         5
          99       0.57      0.80      0.67         5
         100       1.00      1.00      1.00         5
         101       0.90      1.00      0.95         9
         102       0.67      1.00      0.80         4
         103       1.00      1.00      1.00         4
         104       0.00      0.00      0.00         4
         105       1.00      0.25      0.40         4
         106       0.75      0.75      0.75         4
         107       0.60      0.75      0.67         4
         108       1.00      1.00      1.00         4
         109       1.00      1.00      1.00         9
         110       0.50      0.50      0.50         4
         111       0.80      1.00      0.89         4
         112       0.05      0.11      0.07         9
         113       0.75      0.75      0.75         4
         114       0.00      0.00      0.00         9
         115       1.00      1.00      1.00         5
         116       1.00      1.00      1.00         4
         117       0.60      0.75      0.67         4
         118       0.00      0.00      0.00         4
         119       0.10      0.11      0.11         9
         120       1.00      1.00      1.00         4
         121       1.00      0.56      0.71         9
         122       1.00      0.75      0.86         4
         123       0.80      1.00      0.89         4
         124       0.33      0.25      0.29         4
         125       0.44      1.00      0.62         4
         126       0.80      1.00      0.89         4
         127       0.67      1.00      0.80         4
         128       1.00      1.00      1.00         9
         129       0.80      1.00      0.89         4
         130       0.78      0.78      0.78         9
         131       0.75      0.60      0.67         5
         132       1.00      1.00      1.00         4
         133       1.00      0.20      0.33         5
         134       0.00      0.00      0.00         9
         135       0.67      0.50      0.57         4
         136       1.00      1.00      1.00         9
         137       1.00      0.80      0.89         5
         138       0.00      0.00      0.00         4
         139       0.57      1.00      0.73         4
         140       1.00      1.00      1.00         4
         141       1.00      1.00      1.00         4
         142       0.89      0.89      0.89         9
         143       0.00      0.00      0.00         4
         144       0.80      1.00      0.89         4
         145       0.00      0.00      0.00         4
         146       0.80      0.80      0.80         5
         147       1.00      1.00      1.00         4
         148       1.00      0.75      0.86         4
         149       0.80      1.00      0.89         4
         150       0.80      1.00      0.89         4
         151       0.88      0.78      0.82         9
         152       0.82      1.00      0.90         9
         153       0.83      1.00      0.91         5
         154       1.00      1.00      1.00         5
         155       1.00      0.50      0.67         4
         156       1.00      1.00      1.00         4
         157       1.00      0.75      0.86         4
         158       0.00      0.00      0.00         4
         159       1.00      0.75      0.86         4
         160       1.00      0.25      0.40         4
         161       0.80      0.80      0.80         5
         162       0.80      1.00      0.89         4
         163       0.00      0.00      0.00         4
         164       1.00      1.00      1.00         4
         165       1.00      0.75      0.86         4
         166       0.75      0.60      0.67         5
         167       1.00      0.75      0.86         4
         168       0.20      0.25      0.22         4
         169       0.75      0.75      0.75         4
         170       1.00      1.00      1.00         9
         171       1.00      0.89      0.94         9
         172       0.00      0.00      0.00         5
         173       0.73      0.89      0.80         9
         174       1.00      1.00      1.00         9
         175       0.69      1.00      0.82         9
         176       0.60      0.75      0.67         4
         177       0.11      0.50      0.17         4
         178       1.00      0.56      0.71         9
         179       0.57      0.80      0.67         5
         180       0.62      0.56      0.59         9
         181       1.00      0.50      0.67         4
         182       0.83      1.00      0.91         5
         183       0.50      0.50      0.50         4
         184       0.80      1.00      0.89         4
         185       0.58      0.78      0.67         9
         186       1.00      0.80      0.89         5
         187       0.00      0.00      0.00         4
         188       0.33      0.29      0.31         7
         189       1.00      1.00      1.00         9
         190       1.00      1.00      1.00         4
         191       1.00      0.40      0.57         5
         192       1.00      0.89      0.94         9
         193       1.00      0.50      0.67         4
         194       0.25      0.11      0.15         9
         195       0.67      0.40      0.50         5
         196       0.60      0.60      0.60         5
         197       0.89      0.89      0.89         9
         198       0.67      0.50      0.57         4
         199       1.00      0.75      0.86         4
         200       0.80      1.00      0.89         4
         201       1.00      0.50      0.67         4
         202       1.00      0.40      0.57         5
         203       0.71      0.56      0.63         9
         204       0.75      0.75      0.75         4
         205       0.80      1.00      0.89         4
         206       0.83      1.00      0.91         5
         207       0.00      0.00      0.00         4
         208       1.00      1.00      1.00         5
         209       0.67      0.80      0.73         5
         210       0.00      0.00      0.00         4
         211       1.00      1.00      1.00         5
         212       0.33      0.25      0.29         4
         213       0.75      0.67      0.71         9
         214       0.50      0.75      0.60         4
         215       0.71      1.00      0.83         5
         216       0.60      0.75      0.67         4
         217       1.00      0.75      0.86         4
         218       0.80      1.00      0.89         4
         219       1.00      1.00      1.00         4
         220       0.00      0.00      0.00         4
         221       0.88      0.78      0.82         9
         222       0.80      0.80      0.80         5
         223       0.00      0.00      0.00         4
         224       1.00      1.00      1.00         5
         225       0.89      0.89      0.89         9
         226       1.00      0.75      0.86         4
         227       0.67      1.00      0.80         4
         228       0.00      0.00      0.00         4
         229       0.75      0.75      0.75         4
         230       0.00      0.00      0.00         4
         231       1.00      0.50      0.67         4
         232       0.60      0.67      0.63         9
         233       1.00      1.00      1.00         4
         234       0.80      1.00      0.89         4
         235       0.50      0.75      0.60         4
         236       0.83      1.00      0.91         5
         237       1.00      0.78      0.88         9
         238       0.80      1.00      0.89         4
         239       0.86      0.67      0.75         9
         240       0.75      0.75      0.75         4
         241       0.00      0.00      0.00         4
         242       0.60      0.75      0.67         4
         243       0.13      0.50      0.21         4
         244       1.00      1.00      1.00         4
         245       0.67      0.89      0.76         9
         246       0.80      1.00      0.89         4
         247       1.00      0.56      0.71         9
         248       1.00      1.00      1.00         5
         249       0.86      0.67      0.75         9
         250       1.00      0.25      0.40         4
         251       1.00      1.00      1.00         5
         252       0.67      1.00      0.80         4
         253       1.00      1.00      1.00         4
         254       1.00      0.60      0.75         5
         255       0.80      1.00      0.89         4
         256       0.00      0.00      0.00         4
         257       0.80      0.80      0.80         5
         258       0.12      0.50      0.20         4
         259       1.00      1.00      1.00         4
         260       0.50      0.75      0.60         4
         261       0.80      0.80      0.80         5
         262       1.00      1.00      1.00         4
         263       0.75      0.75      0.75         4
         264       0.67      1.00      0.80         4
         265       1.00      0.78      0.88         9
         266       0.89      0.89      0.89         9
         267       1.00      0.89      0.94         9
         268       0.75      0.75      0.75         4
         269       0.20      0.25      0.22         4
         270       0.25      0.25      0.25         4
         271       1.00      0.67      0.80         9
         272       0.50      0.60      0.55         5
         273       1.00      1.00      1.00         4
         274       0.00      0.00      0.00         4
         275       0.64      0.78      0.70         9
         276       0.25      0.40      0.31         5
         277       0.60      0.75      0.67         4
         278       0.75      0.75      0.75         4
         279       1.00      1.00      1.00         4
         280       0.00      0.00      0.00         4
         281       0.50      0.50      0.50         4
         282       0.89      0.89      0.89         9
         283       0.67      1.00      0.80         4
         284       0.80      0.89      0.84         9
         285       0.00      0.00      0.00         4
         286       0.14      0.11      0.12         9
         287       0.80      0.80      0.80         5
         288       1.00      0.78      0.88         9
         289       0.67      0.50      0.57         4
         290       0.50      0.11      0.18         9
         291       0.75      0.75      0.75         4
         292       1.00      0.40      0.57         5
         293       0.56      0.56      0.56         9
         294       0.33      0.50      0.40         4
         295       1.00      0.89      0.94         9
         296       0.00      0.00      0.00         4
         297       0.60      0.75      0.67         4
         298       1.00      1.00      1.00         4
         299       0.90      1.00      0.95         9
         300       1.00      1.00      1.00         4
         301       0.71      1.00      0.83         5
         302       0.73      0.89      0.80         9
         303       1.00      0.75      0.86         4
         304       0.80      1.00      0.89         4
         305       1.00      0.78      0.88         9
         306       0.83      0.56      0.67         9
         307       0.40      0.50      0.44         4
         308       0.00      0.00      0.00         9
         309       0.12      0.25      0.17         4
         310       1.00      0.50      0.67         4
         311       0.80      1.00      0.89         4
         312       0.88      0.78      0.82         9
         313       1.00      1.00      1.00         4
         314       0.83      1.00      0.91         5
         315       0.17      0.25      0.20         4
         316       0.70      0.78      0.74         9
         317       0.80      1.00      0.89         4
         318       1.00      0.50      0.67         4
         319       1.00      0.75      0.86         4
         320       1.00      0.80      0.89         5
         321       0.75      0.67      0.71         9
         322       0.22      0.50      0.31         4
         323       0.75      0.75      0.75         4
         324       0.80      1.00      0.89         4
         325       1.00      0.80      0.89         5
         326       0.83      1.00      0.91         5
         327       0.20      0.50      0.29         4
         328       0.67      0.67      0.67         9
         329       0.88      0.78      0.82         9
         330       0.67      0.40      0.50         5
         331       0.80      1.00      0.89         4
         332       0.80      1.00      0.89         4
         333       1.00      1.00      1.00         4
         334       1.00      0.78      0.88         9
         335       0.75      0.75      0.75         4
         336       0.71      0.56      0.63         9
         337       0.67      0.44      0.53         9
         338       0.60      0.75      0.67         4
         339       0.00      0.00      0.00         4
         340       0.89      0.89      0.89         9
         341       1.00      0.62      0.77         8
         342       1.00      0.89      0.94         9
         343       0.06      0.25      0.10         4
         344       0.75      1.00      0.86         9
         345       0.60      0.75      0.67         4
         346       0.67      0.50      0.57         4
         347       1.00      0.80      0.89         5
         348       0.40      0.50      0.44         4
         349       0.67      0.50      0.57         4
         350       1.00      1.00      1.00         4
         351       1.00      1.00      1.00         4
         352       1.00      0.75      0.86         4
         353       0.50      1.00      0.67         4
         354       0.80      0.80      0.80         5
         355       0.33      0.75      0.46         4
         356       0.80      1.00      0.89         4
         357       0.57      1.00      0.73         4
         358       0.11      0.11      0.11         9
         359       1.00      0.75      0.86         4
         360       0.83      0.56      0.67         9
         361       0.73      0.89      0.80         9
         362       0.80      1.00      0.89         4
         363       0.57      1.00      0.73         4
         364       0.11      0.25      0.15         4
         365       1.00      0.71      0.83         7
         366       1.00      0.89      0.94         9
         367       1.00      0.75      0.86         4
         368       0.44      1.00      0.62         4
         369       1.00      1.00      1.00         4
         370       0.50      0.60      0.55         5
         371       0.00      0.00      0.00         4
         372       0.80      1.00      0.89         4
         373       1.00      0.89      0.94         9
         374       0.75      0.67      0.71         9
         375       1.00      0.60      0.75         5
         376       0.71      0.56      0.63         9
         377       1.00      0.80      0.89         5
         378       1.00      0.89      0.94         9
         379       1.00      0.50      0.67         4
         380       0.56      0.56      0.56         9
         381       0.80      1.00      0.89         4
         382       0.55      0.67      0.60         9
         383       0.75      0.33      0.46         9
         384       0.80      1.00      0.89         4
         385       0.80      1.00      0.89         4
         386       1.00      0.60      0.75         5
         387       0.00      0.00      0.00         4
         388       0.89      1.00      0.94         8
         389       0.67      0.89      0.76         9
         390       1.00      0.75      0.86         4
         391       0.25      0.33      0.29         9
         392       1.00      0.67      0.80         9
         393       0.60      0.75      0.67         4
         394       1.00      0.75      0.86         4
         395       0.00      0.00      0.00         9
         396       0.80      1.00      0.89         4
         397       0.20      0.25      0.22         4
         398       0.29      0.22      0.25         9
         399       0.50      0.25      0.33         4
         400       1.00      1.00      1.00         9
         401       1.00      0.60      0.75         5
         402       0.00      0.00      0.00         4
         403       0.80      0.80      0.80         5
         404       0.78      0.78      0.78         9
         405       1.00      1.00      1.00         4
         406       1.00      0.80      0.89         5
         407       0.25      0.25      0.25         4
         408       0.00      0.00      0.00         4
         409       0.83      1.00      0.91         5
         410       0.50      1.00      0.67         4
         411       0.57      1.00      0.73         4
         412       0.62      0.56      0.59         9
         413       0.08      0.11      0.09         9
         414       0.67      1.00      0.80         4
         415       1.00      1.00      1.00         4
         416       1.00      0.50      0.67         4
         417       0.67      0.89      0.76         9
         418       0.60      0.67      0.63         9
         419       0.19      0.75      0.30         4
         420       0.00      0.00      0.00         4
         421       0.80      1.00      0.89         4
         422       0.00      0.00      0.00         4
         423       0.50      0.50      0.50         4
         424       0.90      1.00      0.95         9
         425       0.80      1.00      0.89         4
         426       0.57      1.00      0.73         4
         427       1.00      0.78      0.88         9
         428       0.75      1.00      0.86         9
         429       0.80      0.80      0.80         5
         430       0.10      0.25      0.14         4
         431       0.67      0.50      0.57         4
         432       1.00      0.88      0.93         8
         433       1.00      1.00      1.00         9
         434       0.75      0.75      0.75         4
         435       0.56      1.00      0.71         5
         436       1.00      0.50      0.67         4
         437       0.67      0.40      0.50         5
         438       0.80      1.00      0.89         4
         439       1.00      0.50      0.67         4
         440       1.00      1.00      1.00         4
         441       0.50      0.75      0.60         4
         442       0.67      1.00      0.80         4
         443       0.56      0.56      0.56         9
         444       0.00      0.00      0.00         4
         445       1.00      0.80      0.89         5
         446       0.67      0.50      0.57         4
         447       0.00      0.00      0.00         9
         448       0.89      0.89      0.89         9
         449       1.00      1.00      1.00         4
         450       0.17      0.25      0.20         4
         451       0.50      0.75      0.60         4
         452       0.00      0.00      0.00         4
         453       0.80      1.00      0.89         4
         454       0.71      0.56      0.63         9
         455       0.80      0.80      0.80         5
         456       1.00      1.00      1.00         4
         457       0.00      0.00      0.00         4
         458       0.00      0.00      0.00         4
         459       1.00      0.89      0.94         9
         460       0.00      0.00      0.00         4
         461       0.40      0.40      0.40         5
         462       0.62      1.00      0.77         5
         463       0.50      0.25      0.33         4
         464       1.00      1.00      1.00         4
         465       0.71      0.56      0.63         9
         466       0.86      0.67      0.75         9
         467       0.00      0.00      0.00         9
         468       0.80      0.80      0.80         5
         469       0.89      0.89      0.89         9
         470       0.56      1.00      0.71         5
         471       0.83      1.00      0.91         5
         472       0.83      1.00      0.91         5
         473       1.00      0.75      0.86         4
         474       0.45      1.00      0.62         9
         475       0.60      0.75      0.67         4
         476       0.67      0.40      0.50         5
         477       0.38      0.75      0.50         4
         478       0.00      0.00      0.00         4
         479       0.00      0.00      0.00         4
         480       1.00      0.67      0.80         9
         481       1.00      0.80      0.89         5
         482       0.80      0.89      0.84         9
         483       0.69      1.00      0.82         9
         484       0.89      0.89      0.89         9
         485       1.00      1.00      1.00         4
         486       0.00      0.00      0.00         4
         487       0.89      0.89      0.89         9
         488       0.56      0.56      0.56         9
         489       0.80      1.00      0.89         4
         490       0.89      0.89      0.89         9
         491       0.00      0.00      0.00         4
         492       1.00      0.75      0.86         4
         493       0.80      1.00      0.89         4
         494       1.00      0.40      0.57         5
         495       0.75      1.00      0.86         9
         496       0.75      0.75      0.75         4
         497       0.57      1.00      0.73         4
         498       0.05      0.25      0.08         4
         499       0.00      0.00      0.00         4
         500       1.00      1.00      1.00         4
         501       0.00      0.00      0.00         4
         502       1.00      0.75      0.86         4
         503       0.67      1.00      0.80         4
         504       1.00      1.00      1.00         9
         505       1.00      0.78      0.88         9
         506       0.90      1.00      0.95         9
         507       1.00      0.50      0.67         4
         508       0.50      0.75      0.60         4
         509       0.00      0.00      0.00         4
         510       0.80      1.00      0.89         4
         511       0.64      0.78      0.70         9
         512       0.80      0.89      0.84         9
         513       0.00      0.00      0.00         9
         514       1.00      1.00      1.00         5
         515       0.60      0.75      0.67         4
         516       0.50      0.75      0.60         4
         517       0.00      0.00      0.00         4
         518       1.00      1.00      1.00         4
         519       0.67      1.00      0.80         4
         520       0.67      0.50      0.57         4
         521       0.00      0.00      0.00         4
         522       0.00      0.00      0.00         4
         523       1.00      0.75      0.86         4
         524       0.67      0.67      0.67         9
         525       0.00      0.00      0.00         4
         526       0.00      0.00      0.00         4
         527       1.00      1.00      1.00         4
         528       0.40      0.50      0.44         4
         529       1.00      0.75      0.86         4
         530       1.00      0.25      0.40         4
         531       0.50      0.75      0.60         4
         532       1.00      1.00      1.00         9
         533       0.40      0.44      0.42         9
         534       0.80      0.80      0.80         5
         535       0.67      0.50      0.57         4
         536       0.33      0.20      0.25         5
         537       0.00      0.00      0.00         9
         538       1.00      0.75      0.86         4
         539       1.00      0.89      0.94         9
         540       1.00      0.89      0.94         9
         541       0.00      0.00      0.00         9
         542       0.00      0.00      0.00         4
         543       0.44      1.00      0.62         4
         544       0.71      1.00      0.83         5
         545       0.83      1.00      0.91         5
         546       1.00      1.00      1.00         4
         547       0.06      0.25      0.10         4
         548       0.50      0.20      0.29         5
         549       0.71      0.56      0.63         9
         550       0.43      0.75      0.55         4
         551       0.29      0.40      0.33         5
         552       0.71      0.56      0.63         9
         553       0.80      1.00      0.89         4
         554       0.00      0.00      0.00         4
         555       0.56      0.56      0.56         9
         556       1.00      0.75      0.86         4
         557       0.67      1.00      0.80         4
         558       0.80      1.00      0.89         4
         559       1.00      1.00      1.00         4
         560       0.50      0.33      0.40         9
         561       0.50      0.60      0.55         5
         562       0.10      0.22      0.14         9
         563       1.00      0.75      0.86         4
         564       1.00      1.00      1.00         5
         565       0.80      1.00      0.89         4
         566       0.60      0.75      0.67         4
         567       0.67      1.00      0.80         4
         568       0.00      0.00      0.00         4
         569       0.00      0.00      0.00         4
         570       1.00      1.00      1.00         5
         571       0.83      1.00      0.91         5
         572       0.60      0.60      0.60         5
         573       0.67      1.00      0.80         4
         574       0.50      0.75      0.60         4
         575       0.75      0.75      0.75         4
         576       1.00      1.00      1.00         9
         577       0.22      0.50      0.31         4
         578       0.88      0.78      0.82         9
         579       0.50      0.75      0.60         4
         580       0.60      0.75      0.67         4
         581       1.00      0.75      0.86         4
         582       1.00      1.00      1.00         4
         583       0.71      1.00      0.83         5
         584       0.00      0.00      0.00         4
         585       1.00      1.00      1.00         4
         586       0.60      0.75      0.67         4
         587       0.80      1.00      0.89         4
         588       0.50      0.40      0.44         5
         589       1.00      0.78      0.88         9
         590       1.00      0.50      0.67         4
         591       1.00      0.50      0.67         4
         592       0.88      0.78      0.82         9
         593       0.86      0.67      0.75         9
         594       0.80      1.00      0.89         4
         595       0.50      0.50      0.50         4
         596       1.00      0.75      0.86         4
         597       0.13      0.50      0.21         4
         598       0.82      1.00      0.90         9
         599       0.80      1.00      0.89         4
         600       0.50      1.00      0.67         5
         601       0.29      0.40      0.33         5
         602       1.00      0.75      0.86         4
         603       1.00      1.00      1.00         5
         604       1.00      0.33      0.50         9
         605       1.00      1.00      1.00         9
         606       0.67      0.67      0.67         9
         607       0.75      0.60      0.67         5
         608       0.00      0.00      0.00         4
         609       1.00      1.00      1.00         4
         610       1.00      0.75      0.86         4
         611       1.00      1.00      1.00         9
         612       0.80      1.00      0.89         4
         613       0.88      0.78      0.82         9
         614       0.33      0.11      0.17         9
         615       0.89      0.89      0.89         9
         616       0.44      1.00      0.62         4
         617       0.00      0.00      0.00         4
         618       0.67      1.00      0.80         4
         619       0.00      0.00      0.00         4
         620       1.00      1.00      1.00         4
         621       0.75      0.60      0.67         5
         622       0.80      1.00      0.89         4
         623       0.17      0.25      0.20         4
         624       1.00      0.60      0.75         5
         625       0.75      0.75      0.75         4
         626       0.50      0.50      0.50         4
         627       1.00      0.89      0.94         9
         628       1.00      1.00      1.00         4
         629       1.00      1.00      1.00         9
         630       0.00      0.00      0.00         9
         631       1.00      1.00      1.00         9
         632       0.00      0.00      0.00         4
         633       0.75      0.75      0.75         4
         634       1.00      0.89      0.94         9
         635       0.50      0.75      0.60         4
         636       1.00      1.00      1.00         5
         637       1.00      0.78      0.88         9
         638       0.08      0.25      0.12         4
         639       1.00      0.75      0.86         4
         640       0.80      1.00      0.89         4
         641       0.80      1.00      0.89         4
         642       0.86      0.67      0.75         9
         643       0.80      0.44      0.57         9
         644       0.89      0.89      0.89         9
         645       1.00      1.00      1.00         4
         646       0.62      1.00      0.77         5
         647       1.00      0.50      0.67         4
         648       0.00      0.00      0.00         4
         649       1.00      0.50      0.67         4
         650       0.75      1.00      0.86         9
         651       0.50      0.20      0.29         5
         652       1.00      0.75      0.86         4
         653       0.00      0.00      0.00         4
         654       1.00      0.67      0.80         9
         655       1.00      0.75      0.86         4
         656       0.00      0.00      0.00         4
         657       0.62      0.56      0.59         9
         658       0.33      0.50      0.40         4
         659       0.75      0.60      0.67         5
         660       0.75      0.67      0.71         9
         661       0.22      0.50      0.31         4
         662       0.33      0.25      0.29         4
         663       1.00      0.75      0.86         4
         664       0.67      1.00      0.80         4
         665       0.25      0.25      0.25         4
         666       0.90      1.00      0.95         9
         667       0.00      0.00      0.00         9
         668       0.80      1.00      0.89         4
         669       1.00      0.80      0.89         5
         670       0.00      0.00      0.00         4
         671       1.00      1.00      1.00         4
         672       0.29      0.50      0.36         4
         673       0.78      0.78      0.78         9
         674       1.00      1.00      1.00         4
         675       1.00      0.80      0.89         5
         676       0.75      0.75      0.75         4
         677       0.80      0.80      0.80         5
         678       0.78      0.78      0.78         9
         679       0.00      0.00      0.00         4
         680       1.00      0.56      0.71         9
         681       0.80      1.00      0.89         4
         682       0.67      1.00      0.80         4
         683       0.67      0.67      0.67         9
         684       1.00      0.80      0.89         5
         685       1.00      0.50      0.67         4
         686       0.50      1.00      0.67         4
         687       0.00      0.00      0.00         4
         688       0.00      0.00      0.00         4
         689       1.00      0.25      0.40         4
         690       0.60      0.75      0.67         4
         691       0.83      1.00      0.91         5
         692       0.80      1.00      0.89         4
         693       0.47      1.00      0.64         9
         694       1.00      1.00      1.00         4
         695       1.00      1.00      1.00         4
         696       1.00      0.75      0.86         4
         697       0.82      1.00      0.90         9
         698       0.60      0.75      0.67         4
         699       0.00      0.00      0.00         4
         700       0.75      0.60      0.67         5
         701       0.83      1.00      0.91         5
         702       0.73      0.89      0.80         9
         703       1.00      1.00      1.00         4
         704       0.00      0.00      0.00         4
         705       0.75      0.75      0.75         4
         706       0.40      0.50      0.44         4
         707       0.50      0.20      0.29         5
         708       0.71      1.00      0.83         5
         709       0.67      0.50      0.57         4
         710       0.00      0.00      0.00         4
         711       0.80      0.80      0.80         5
         712       1.00      0.80      0.89         5
         713       0.33      0.25      0.29         4
         714       1.00      1.00      1.00         4
         715       1.00      1.00      1.00         9
         716       1.00      0.25      0.40         4
         717       1.00      0.80      0.89         5
         718       0.00      0.00      0.00         4
         719       0.83      1.00      0.91         5
         720       0.80      1.00      0.89         4
         721       0.64      1.00      0.78         9
         722       0.00      0.00      0.00         4
         723       0.80      1.00      0.89         4
         724       1.00      0.44      0.62         9
         725       0.00      0.00      0.00         4
         726       1.00      0.89      0.94         9
         727       1.00      0.67      0.80         9
         728       1.00      0.25      0.40         4
         729       0.00      0.00      0.00         4
         730       1.00      0.50      0.67         4
         731       1.00      1.00      1.00         4
         732       0.00      0.00      0.00         4
         733       0.40      0.22      0.29         9
         734       1.00      1.00      1.00         9
         735       0.83      1.00      0.91         5
         736       0.67      0.40      0.50         5
         737       0.75      0.60      0.67         5
         738       1.00      1.00      1.00         5
         739       1.00      0.75      0.86         4
         740       0.67      0.50      0.57         4
         741       0.88      0.78      0.82         9
         742       1.00      0.89      0.94         9
         743       0.75      0.75      0.75         4
         744       0.40      0.50      0.44         4
         745       1.00      0.60      0.75         5
         746       0.00      0.00      0.00         4
         747       0.75      0.67      0.71         9
         748       1.00      1.00      1.00         5
         749       0.00      0.00      0.00         4
         750       0.00      0.00      0.00         4
         751       1.00      1.00      1.00         4
         752       0.50      0.75      0.60         4
         753       0.62      1.00      0.77         5
         754       0.00      0.00      0.00         4
         755       0.67      0.89      0.76         9
         756       0.90      1.00      0.95         9
         757       0.62      1.00      0.77         5
         758       1.00      1.00      1.00         5
         759       0.33      0.25      0.29         4
         760       0.88      0.78      0.82         9
         761       0.80      1.00      0.89         4
         762       0.75      0.75      0.75         4
         763       0.40      0.50      0.44         4
         764       0.00      0.00      0.00         4
         765       0.67      0.40      0.50         5
         766       1.00      0.75      0.86         4
         767       0.67      0.89      0.76         9
         768       0.40      0.50      0.44         4
         769       0.00      0.00      0.00         4
         770       1.00      0.75      0.86         4
         771       1.00      1.00      1.00         9
         772       0.50      0.60      0.55         5
         773       0.50      0.25      0.33         4
         774       1.00      1.00      1.00         5
         775       1.00      0.25      0.40         4
         776       0.00      0.00      0.00         4
         777       0.90      1.00      0.95         9
         778       0.00      0.00      0.00         4
         779       0.00      0.00      0.00         4
         780       0.67      0.44      0.53         9
         781       1.00      1.00      1.00         5
         782       0.50      0.50      0.50         4
         783       1.00      0.56      0.71         9
         784       0.80      1.00      0.89         4
         785       0.89      0.89      0.89         9
         786       1.00      0.75      0.86         4
         787       1.00      1.00      1.00         5
         788       0.50      0.25      0.33         4
         789       1.00      0.20      0.33         5
         790       1.00      0.25      0.40         4
         791       1.00      0.25      0.40         4
         792       0.89      0.89      0.89         9
         793       0.00      0.00      0.00         9
         794       0.00      0.00      0.00         4
         795       1.00      1.00      1.00         4
         796       1.00      0.60      0.75         5
         797       1.00      0.78      0.88         9
         798       1.00      1.00      1.00         4
         799       0.50      0.20      0.29         5
         800       1.00      0.56      0.71         9
         801       0.67      0.40      0.50         5
         802       1.00      0.75      0.86         4
         803       0.57      1.00      0.73         4
         804       1.00      0.78      0.88         9
         805       0.89      0.89      0.89         9
         806       0.67      1.00      0.80         4
         807       0.80      1.00      0.89         4
         808       1.00      1.00      1.00         9
         809       0.80      1.00      0.89         4
         810       0.00      0.00      0.00         4
         811       1.00      0.78      0.88         9
         812       1.00      0.25      0.40         4
         813       0.89      0.89      0.89         9
         814       0.50      0.40      0.44         5
         815       0.89      0.89      0.89         9
         816       0.50      0.75      0.60         4
         817       1.00      0.75      0.86         4
         818       0.50      0.75      0.60         4
         819       1.00      0.25      0.40         4
         820       1.00      0.25      0.40         4
         821       1.00      0.50      0.67         4
         822       1.00      1.00      1.00         4
         823       0.69      1.00      0.82         9
         824       0.00      0.00      0.00         4
         825       1.00      1.00      1.00         4
         826       0.00      0.00      0.00         4
         827       0.80      0.80      0.80         5
         828       0.00      0.00      0.00         4
         829       0.33      0.25      0.29         4
         830       1.00      1.00      1.00         4
         831       1.00      1.00      1.00         5
         832       0.00      0.00      0.00         4
         833       0.50      0.22      0.31         9
         834       0.64      1.00      0.78         9
         835       1.00      0.75      0.86         4
         836       1.00      1.00      1.00         5
         837       0.89      0.89      0.89         9
         838       0.00      0.00      0.00         4
         839       0.80      1.00      0.89         4
         840       0.67      0.50      0.57         4
         841       0.80      1.00      0.89         4
         842       0.50      0.25      0.33         4
         843       0.00      0.00      0.00         4
         844       0.00      0.00      0.00         4
         845       1.00      0.78      0.88         9
         846       0.67      0.44      0.53         9
         847       1.00      0.44      0.62         9
         848       0.80      1.00      0.89         4
         849       1.00      1.00      1.00         4
         850       0.00      0.00      0.00         9
         851       0.75      0.75      0.75         4
         852       1.00      0.75      0.86         4
         853       0.80      1.00      0.89         4
         854       1.00      0.50      0.67         4
         855       0.00      0.00      0.00         4
         856       0.00      0.00      0.00         4
         857       1.00      0.80      0.89         5
         858       1.00      1.00      1.00         4
         859       0.00      0.00      0.00         4
         860       0.00      0.00      0.00         9
         861       1.00      0.75      0.86         4
         862       0.00      0.00      0.00         4
         863       0.50      0.50      0.50         4
         864       1.00      0.78      0.88         9
         865       1.00      0.89      0.94         9
         866       0.50      0.25      0.33         4
         867       0.50      0.75      0.60         4
         868       1.00      0.89      0.94         9
         869       0.71      0.56      0.63         9
         870       0.00      0.00      0.00         4
         871       0.80      1.00      0.89         4
         872       0.00      0.00      0.00         4
         873       0.67      1.00      0.80         4
         874       0.25      0.25      0.25         4
         875       0.32      0.89      0.47         9
         876       1.00      1.00      1.00         4
         877       0.75      0.60      0.67         5
         878       0.00      0.00      0.00         4
         879       0.67      1.00      0.80         4
         880       0.80      0.80      0.80         5
         881       0.09      0.50      0.15         4
         882       0.62      1.00      0.77         5
         883       0.43      0.67      0.52         9
         884       0.10      0.25      0.14         4
         885       0.50      0.80      0.62         5
         886       0.67      0.89      0.76         9
         887       0.75      0.75      0.75         4
         888       0.50      0.25      0.33         4
         889       0.50      1.00      0.67         9
         890       0.50      0.75      0.60         4
         891       0.67      0.80      0.73         5
         892       0.80      1.00      0.89         4
         893       0.56      1.00      0.71         5

    accuracy                           0.67      4917
   macro avg       0.66      0.66      0.64      4917
weighted avg       0.69      0.67      0.66      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
