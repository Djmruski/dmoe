Namespace(backbone_type=None, batch_size=20, buffer_size=100, cutmix_alpha=None, dataid=4, dataset='mod-har', debug_mode=0, disable_log=0, distributed='no', fitting_epochs=250, ignore_other_metrics=0, load_data='', lr=0.01, maxlr=0.05, minibatch_size=20, minlr=0.0005, model='gdumb_har', n_epochs=1, non_verbose=0, notes=None, nowand=0, ntask=15, optim_mom=0.0, optim_nesterov=0, optim_wd=0.0001, save_path='', seed=None, validation=0, wandb_entity='frahman', wandb_project='mammoth')
Namespace(backbone_type='incremental', batch_size=20, buffer_size=100, conf_host='elquinto', conf_jobnum='24d27856-dfe5-4d06-b30a-d9a4378f404e', conf_timestamp='2023-08-14 11:10:00.291519', cutmix_alpha=None, dataid=4, dataset='mod-har', debug_mode=0, disable_log=0, distributed='no', fitting_epochs=250, ignore_other_metrics=0, load_data='', lr=0.01, maxlr=0.05, minibatch_size=20, minlr=0.0005, model='gdumb_har', n_epochs=1, non_verbose=0, notes=None, nowand=0, ntask=15, optim_mom=0.0, optim_nesterov=0, optim_wd=0.0001, save_path='', seed=None, validation=0, wandb_entity='frahman', wandb_project='mammoth')
====TRAINING TASK: 0
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=12, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=12, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=12, bias=True)
    )
  )
)

Accuracy for 1 task(s): 	 [Class-IL]: 8.33 % 	 [Task-IL]: 8.33 %

====TRAINING TASK: 1
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=22, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=22, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=22, bias=True)
    )
  )
)

Accuracy for 2 task(s): 	 [Class-IL]: 17.08 % 	 [Task-IL]: 36.88 %

====TRAINING TASK: 2
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=32, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=32, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=32, bias=True)
    )
  )
)

Accuracy for 3 task(s): 	 [Class-IL]: 11.67 % 	 [Task-IL]: 45.97 %

====TRAINING TASK: 3
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=42, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=42, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=42, bias=True)
    )
  )
)

Accuracy for 4 task(s): 	 [Class-IL]: 8.75 % 	 [Task-IL]: 45.49 %

====TRAINING TASK: 4
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=52, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=52, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=52, bias=True)
    )
  )
)

Accuracy for 5 task(s): 	 [Class-IL]: 7.5 % 	 [Task-IL]: 52.0 %

====TRAINING TASK: 5
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=62, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=62, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=62, bias=True)
    )
  )
)

Accuracy for 6 task(s): 	 [Class-IL]: 2.92 % 	 [Task-IL]: 41.74 %

====TRAINING TASK: 6
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=72, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=72, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=72, bias=True)
    )
  )
)

Accuracy for 7 task(s): 	 [Class-IL]: 4.29 % 	 [Task-IL]: 42.54 %

====TRAINING TASK: 7
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=82, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=82, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=82, bias=True)
    )
  )
)

Accuracy for 8 task(s): 	 [Class-IL]: 6.04 % 	 [Task-IL]: 44.76 %

====TRAINING TASK: 8
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=92, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=92, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=92, bias=True)
    )
  )
)

Accuracy for 9 task(s): 	 [Class-IL]: 4.26 % 	 [Task-IL]: 44.37 %

====TRAINING TASK: 9
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=102, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=102, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=102, bias=True)
    )
  )
)

Accuracy for 10 task(s): 	 [Class-IL]: 1.0 % 	 [Task-IL]: 40.92 %

====TRAINING TASK: 10
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=112, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=112, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=112, bias=True)
    )
  )
)

Accuracy for 11 task(s): 	 [Class-IL]: 1.59 % 	 [Task-IL]: 46.09 %

====TRAINING TASK: 11
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=122, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=122, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=122, bias=True)
    )
  )
)

Accuracy for 12 task(s): 	 [Class-IL]: 1.46 % 	 [Task-IL]: 38.76 %

====TRAINING TASK: 12
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=132, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=132, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=132, bias=True)
    )
  )
)

Accuracy for 13 task(s): 	 [Class-IL]: 3.85 % 	 [Task-IL]: 43.99 %

====TRAINING TASK: 13
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=142, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=142, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=142, bias=True)
    )
  )
)

Accuracy for 14 task(s): 	 [Class-IL]: 3.1 % 	 [Task-IL]: 45.5 %

====TRAINING TASK: 14
SimpleMLP(
  (fc1): Linear(in_features=405, out_features=1000, bias=True)
  (fc2): Linear(in_features=1000, out_features=152, bias=True)
  (_features): Identity()
  (classifier): Sequential(
    (0): Linear(in_features=405, out_features=1000, bias=True)
    (1): ReLU()
    (2): Linear(in_features=1000, out_features=152, bias=True)
  )
  (net): Sequential(
    (0): Identity()
    (1): Sequential(
      (0): Linear(in_features=405, out_features=1000, bias=True)
      (1): ReLU()
      (2): Linear(in_features=1000, out_features=152, bias=True)
    )
  )
)
torch.Size([15200, 405])
	LABELS: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151]	Counter({0: 9404, 82: 45, 115: 45, 4: 43, 18: 43, 70: 43, 75: 43, 81: 43, 12: 42, 54: 42, 65: 42, 74: 42, 99: 42, 104: 42, 117: 42, 139: 42, 133: 42, 146: 42, 145: 42, 6: 41, 21: 41, 29: 41, 27: 41, 31: 41, 33: 41, 40: 41, 41: 41, 58: 41, 60: 41, 68: 41, 79: 41, 88: 41, 103: 41, 105: 41, 116: 41, 126: 41, 140: 41, 136: 41, 144: 41, 151: 41, 9: 40, 38: 40, 50: 40, 48: 40, 42: 40, 46: 40, 61: 40, 71: 40, 80: 40, 91: 40, 94: 40, 97: 40, 106: 40, 112: 40, 121: 40, 114: 40, 129: 40, 124: 40, 148: 40, 8: 39, 7: 39, 2: 39, 17: 39, 19: 39, 23: 39, 39: 39, 35: 39, 44: 39, 45: 39, 52: 39, 57: 39, 53: 39, 69: 39, 63: 39, 73: 39, 84: 39, 98: 39, 101: 39, 96: 39, 122: 39, 127: 39, 135: 39, 14: 38, 25: 38, 22: 38, 30: 38, 36: 38, 51: 38, 66: 38, 89: 38, 83: 38, 93: 38, 111: 38, 108: 38, 123: 38, 128: 38, 132: 38, 147: 38, 3: 37, 5: 37, 20: 37, 26: 37, 24: 37, 37: 37, 49: 37, 62: 37, 76: 37, 85: 37, 100: 37, 110: 37, 109: 37, 119: 37, 131: 37, 137: 37, 150: 37, 10: 36, 15: 36, 16: 36, 43: 36, 55: 36, 56: 36, 86: 36, 87: 36, 92: 36, 107: 36, 125: 36, 130: 36, 134: 36, 11: 35, 34: 35, 47: 35, 67: 35, 141: 35, 142: 35, 1: 34, 28: 34, 78: 34, 90: 34, 95: 34, 102: 34, 118: 34, 149: 34, 143: 34, 13: 33, 32: 33, 77: 33, 120: 33, 138: 33, 72: 32, 113: 32, 59: 31, 64: 30})
Total buffer: 15200
CAPPING TO BUFFER_SIZE/CLASS
	LABELS: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107
 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
 144 145 146 147 148 149 150 151]	Counter({0: 100, 82: 45, 115: 45, 4: 43, 18: 43, 70: 43, 75: 43, 81: 43, 12: 42, 54: 42, 65: 42, 74: 42, 99: 42, 104: 42, 117: 42, 133: 42, 139: 42, 145: 42, 146: 42, 6: 41, 21: 41, 27: 41, 29: 41, 31: 41, 33: 41, 40: 41, 41: 41, 58: 41, 60: 41, 68: 41, 79: 41, 88: 41, 103: 41, 105: 41, 116: 41, 126: 41, 136: 41, 140: 41, 144: 41, 151: 41, 9: 40, 38: 40, 42: 40, 46: 40, 48: 40, 50: 40, 61: 40, 71: 40, 80: 40, 91: 40, 94: 40, 97: 40, 106: 40, 112: 40, 114: 40, 121: 40, 124: 40, 129: 40, 148: 40, 2: 39, 7: 39, 8: 39, 17: 39, 19: 39, 23: 39, 35: 39, 39: 39, 44: 39, 45: 39, 52: 39, 53: 39, 57: 39, 63: 39, 69: 39, 73: 39, 84: 39, 96: 39, 98: 39, 101: 39, 122: 39, 127: 39, 135: 39, 14: 38, 22: 38, 25: 38, 30: 38, 36: 38, 51: 38, 66: 38, 83: 38, 89: 38, 93: 38, 108: 38, 111: 38, 123: 38, 128: 38, 132: 38, 147: 38, 3: 37, 5: 37, 20: 37, 24: 37, 26: 37, 37: 37, 49: 37, 62: 37, 76: 37, 85: 37, 100: 37, 109: 37, 110: 37, 119: 37, 131: 37, 137: 37, 150: 37, 10: 36, 15: 36, 16: 36, 43: 36, 55: 36, 56: 36, 86: 36, 87: 36, 92: 36, 107: 36, 125: 36, 130: 36, 134: 36, 11: 35, 34: 35, 47: 35, 67: 35, 141: 35, 142: 35, 1: 34, 28: 34, 78: 34, 90: 34, 95: 34, 102: 34, 118: 34, 143: 34, 149: 34, 13: 33, 32: 33, 77: 33, 120: 33, 138: 33, 72: 32, 113: 32, 59: 31, 64: 30})
Total buffer: 5896
fit_time: 49.223751248

Accuracy for 15 task(s): 	 [Class-IL]: 90.59 % 	 [Task-IL]: 79.07 %

CLASS_IL_ACC: 
	[93.05555555555556, 96.66666666666667, 89.16666666666667, 87.5, 85.83333333333333, 96.66666666666667, 95.0, 81.66666666666667, 96.66666666666667, 87.5, 88.33333333333333, 97.5, 98.33333333333333, 78.33333333333333, 86.66666666666667]
TASK_IL_ACC: 
	[81.94444444444444, 80.0, 80.0, 76.66666666666667, 75.83333333333333, 79.16666666666666, 79.16666666666666, 70.0, 79.16666666666666, 80.0, 75.83333333333333, 80.0, 80.0, 78.33333333333333, 90.0]
f1_micro: 90.625
f1_macro: 89.93450127455871
              precision    recall  f1-score   support

           0       0.82      0.75      0.78        12
           1       1.00      0.92      0.96        12
           2       0.75      1.00      0.86        12
           3       1.00      1.00      1.00        12
           4       1.00      0.75      0.86        12
           5       1.00      1.00      1.00        12
           6       1.00      0.92      0.96        12
           7       1.00      1.00      1.00        12
           8       0.71      1.00      0.83        12
           9       1.00      1.00      1.00        12
          10       0.91      0.83      0.87        12
          11       1.00      1.00      1.00        12
          12       0.92      1.00      0.96        12
          13       0.45      0.75      0.56        12
          14       1.00      1.00      1.00        12
          15       0.85      0.92      0.88        12
          16       1.00      1.00      1.00        12
          17       1.00      1.00      1.00        12
          18       0.92      1.00      0.96        12
          19       0.48      1.00      0.65        12
          20       1.00      1.00      1.00        12
          21       0.92      1.00      0.96        12
          22       0.89      0.67      0.76        12
          23       0.67      0.67      0.67        12
          24       0.86      1.00      0.92        12
          25       1.00      0.83      0.91        12
          26       1.00      1.00      1.00        12
          27       1.00      1.00      1.00        12
          28       0.83      0.83      0.83        12
          29       1.00      0.92      0.96        12
          30       1.00      1.00      1.00        12
          31       1.00      1.00      1.00        12
          32       1.00      1.00      1.00        12
          33       1.00      1.00      1.00        12
          34       0.89      0.67      0.76        12
          35       1.00      0.67      0.80        12
          36       0.79      0.92      0.85        12
          37       1.00      1.00      1.00        12
          38       0.85      0.92      0.88        12
          39       0.67      0.67      0.67        12
          40       1.00      1.00      1.00        12
          41       0.85      0.92      0.88        12
          42       0.86      1.00      0.92        12
          43       0.64      0.58      0.61        12
          44       0.89      0.67      0.76        12
          45       0.92      0.92      0.92        12
          46       0.92      1.00      0.96        12
          47       0.90      0.75      0.82        12
          48       1.00      1.00      1.00        12
          49       1.00      0.75      0.86        12
          50       1.00      1.00      1.00        12
          51       1.00      0.92      0.96        12
          52       0.79      0.92      0.85        12
          53       1.00      1.00      1.00        12
          54       0.92      1.00      0.96        12
          55       1.00      1.00      1.00        12
          56       1.00      0.75      0.86        12
          57       1.00      1.00      1.00        12
          58       1.00      1.00      1.00        12
          59       0.92      1.00      0.96        12
          60       1.00      1.00      1.00        12
          61       1.00      1.00      1.00        12
          62       0.85      0.92      0.88        12
          63       0.90      0.75      0.82        12
          64       0.92      1.00      0.96        12
          65       0.86      1.00      0.92        12
          66       1.00      1.00      1.00        12
          67       1.00      1.00      1.00        12
          68       0.91      0.83      0.87        12
          69       1.00      1.00      1.00        12
          70       1.00      1.00      1.00        12
          71       1.00      1.00      1.00        12
          72       1.00      0.92      0.96        12
          73       0.45      0.42      0.43        12
          74       0.92      0.92      0.92        12
          75       0.67      0.50      0.57        12
          76       1.00      1.00      1.00        12
          77       1.00      1.00      1.00        12
          78       0.75      0.50      0.60        12
          79       1.00      1.00      1.00        12
          80       1.00      1.00      1.00        12
          81       0.52      0.92      0.67        12
          82       0.86      1.00      0.92        12
          83       1.00      1.00      1.00        12
          84       0.82      0.75      0.78        12
          85       0.92      1.00      0.96        12
          86       0.80      1.00      0.89        12
          87       1.00      0.92      0.96        12
          88       0.86      1.00      0.92        12
          89       1.00      1.00      1.00        12
          90       1.00      1.00      1.00        12
          91       1.00      1.00      1.00        12
          92       0.92      1.00      0.96        12
          93       0.25      0.08      0.12        12
          94       1.00      1.00      1.00        12
          95       1.00      1.00      1.00        12
          96       0.80      1.00      0.89        12
          97       0.67      0.67      0.67        12
          98       1.00      1.00      1.00        12
          99       0.92      1.00      0.96        12
         100       1.00      1.00      1.00        12
         101       1.00      1.00      1.00        12
         102       0.92      0.92      0.92        12
         103       1.00      1.00      1.00        12
         104       1.00      1.00      1.00        12
         105       0.33      0.42      0.37        12
         106       1.00      1.00      1.00        12
         107       0.75      0.50      0.60        12
         108       0.92      1.00      0.96        12
         109       1.00      1.00      1.00        12
         110       1.00      1.00      1.00        12
         111       1.00      1.00      1.00        12
         112       0.79      0.92      0.85        12
         113       1.00      0.92      0.96        12
         114       0.71      1.00      0.83        12
         115       1.00      1.00      1.00        12
         116       1.00      1.00      1.00        12
         117       0.85      0.92      0.88        12
         118       1.00      1.00      1.00        12
         119       1.00      1.00      1.00        12
         120       1.00      1.00      1.00        12
         121       1.00      1.00      1.00        12
         122       1.00      1.00      1.00        12
         123       1.00      1.00      1.00        12
         124       1.00      1.00      1.00        12
         125       1.00      1.00      1.00        12
         126       1.00      0.83      0.91        12
         127       1.00      1.00      1.00        12
         128       1.00      1.00      1.00        12
         129       1.00      1.00      1.00        12
         130       1.00      1.00      1.00        12
         131       1.00      1.00      1.00        12
         132       0.71      0.42      0.53        12
         133       0.00      0.00      0.00        12
         134       0.75      0.75      0.75        12
         135       0.67      0.67      0.67        12
         136       1.00      1.00      1.00        12
         137       1.00      1.00      1.00        12
         138       0.86      1.00      0.92        12
         139       1.00      1.00      1.00        12
         140       1.00      1.00      1.00        12
         141       0.92      1.00      0.96        12
         142       1.00      0.92      0.96        12
         143       1.00      1.00      1.00        12
         144       1.00      1.00      1.00        12
         145       1.00      1.00      1.00        12
         146       0.62      0.83      0.71        12
         147       0.92      1.00      0.96        12
         148       1.00      1.00      1.00        12
         149       1.00      1.00      1.00        12
         150       1.00      0.92      0.96        12
         151       0.00      0.00      0.00        12

    accuracy                           0.91      1824
   macro avg       0.90      0.91      0.90      1824
weighted avg       0.90      0.91      0.90      1824

task_train_time: {0: 0.11566434100000045, 1: 0.03933440399999988, 2: 0.040353382999999354, 3: 0.03947491999999997, 4: 0.040743328000001355, 5: 0.04109726899999977, 6: 0.038690658000000155, 7: 0.03955428699999963, 8: 0.03861086199999875, 9: 0.03984857200000036, 10: 0.03994471100000041, 11: 0.04022093500000068, 12: 0.03987686700000026, 13: 0.03912402699999973, 14: 0.03916627299999931}
prediction_time: 0.0002682339999964256
Parameters: 558152
Task parameters: {0: 418012, 1: 428022, 2: 438032, 3: 448042, 4: 458052, 5: 468062, 6: 478072, 7: 488082, 8: 498092, 9: 508102, 10: 518112, 11: 528122, 12: 538132, 13: 548142, 14: 558152}
