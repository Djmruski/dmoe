	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [701, 56, 120, 197, 215, 547, 88, 160, 420, 163, 856, 736, 470, 365, 793, 885, 233, 111, 48, 330, 797, 159, 756, 262, 447, 282, 668, 688, 589, 472, 649, 289, 82, 587, 595, 622, 348, 53, 452, 853, 213, 237, 627, 520, 31, 564, 84, 709, 329, 667, 873, 529, 703, 779, 290, 249, 368, 152, 733, 44, 303, 772, 696, 872, 738, 686, 134, 830, 325, 338, 401, 493, 576, 151, 621, 818, 317, 101, 800, 645, 266, 392, 680, 646, 755, 455, 675, 654, 339, 96, 306, 132, 402, 741, 154, 115, 869, 880, 650, 669, 165, 480, 69, 721, 490, 620, 483, 673, 474, 162, 421, 41, 395, 710, 11, 108, 116, 414, 153, 441, 35, 199, 623, 350, 655, 768, 449, 866, 516, 744, 352, 369, 556, 521, 129, 214, 613, 169, 346, 271, 244, 424, 83, 337, 691, 870, 803, 28, 122, 272, 482, 775, 219, 324, 37, 879, 514, 852, 844, 558, 448, 700, 2, 422, 305, 7, 12, 468, 105, 432, 647, 245, 825, 893, 73, 355, 279, 812, 256, 293, 542, 769, 526, 193, 398, 877, 209, 867, 658, 385, 312, 528, 327, 429, 766, 104, 388, 561, 568, 54, 510, 639, 631, 30, 666, 609, 99, 784, 539, 68, 671, 848, 189, 743, 751, 858, 530, 477, 478, 546, 541, 10, 728, 404, 512, 13, 823, 834, 537, 651, 665, 676, 652, 349, 319, 55, 426, 642, 187, 694, 611, 313, 286, 284, 553, 184, 804, 563, 253, 720, 693, 72, 168, 708, 460, 788, 802, 889, 127, 626, 760, 310, 753, 341, 469, 745, 577, 446, 437, 274, 18, 663, 504, 147, 121, 40, 890, 524, 462, 240, 316, 235, 781, 821, 592, 263, 268, 735, 230, 103, 143, 501, 36, 660, 843, 574, 428, 248, 275, 326, 515, 767, 296, 183, 81, 617, 340, 95, 202, 838, 261, 464, 70, 770, 431, 267, 311, 814, 827, 234, 204, 826, 150, 77, 762, 194, 408, 789, 489, 445, 217, 748, 4, 410, 387, 141, 370, 545, 439, 375, 322, 690, 331, 705, 723, 481, 771, 604, 531, 725, 250, 229, 747, 636, 615, 301, 265, 276, 674, 280, 109, 181, 845, 550, 0, 712, 17, 277, 64, 662, 58, 571, 399, 484, 875, 57, 353, 583, 114, 849, 727, 130, 285, 119, 76, 778, 427, 637, 400, 323, 361, 659, 891, 367, 390, 222, 573, 702, 156, 444, 497, 386, 698, 456, 363, 887, 857, 614, 220, 580, 874, 71, 619, 125, 591, 677, 817, 173, 418, 624, 423, 20, 578, 133, 223, 538, 142, 517, 195, 713, 117, 454, 42, 200, 321, 618, 882, 182, 518, 697, 794, 191, 606, 487, 792, 640, 212, 216, 822, 500, 776, 332, 345, 210, 373, 864, 196, 206, 752, 61, 391, 625, 384, 581, 347, 835, 865, 689, 730, 813, 146, 664, 557, 476, 616, 774, 525, 699, 24, 535, 559, 555, 333, 91, 43, 759, 672, 264, 259, 536, 413, 86, 90, 106, 299, 809, 232, 383, 495, 416, 269, 171, 653, 729, 593, 247, 596, 270, 584, 430, 417, 97, 819, 859, 433, 746, 59, 598, 273, 231, 540, 3, 706, 80, 810, 742, 532, 692, 505, 806, 507, 25, 739, 831, 113, 8, 549, 836, 861, 798, 610, 166, 687, 110, 356, 394, 425, 560, 684, 50, 670, 287, 811, 466, 33, 453, 816, 479, 45, 372, 436, 227, 242, 657, 846, 888, 334, 842, 415, 632, 378, 601, 808, 314, 511, 612, 403, 164, 126, 506, 737, 494, 764, 681, 47, 491, 862, 886, 128, 765, 435, 719, 876, 467, 773, 9, 633, 252, 102, 354, 644, 407, 635, 257, 38, 411, 717, 787, 465, 534, 343, 801, 602, 878, 499, 783, 683, 23, 731, 570, 562, 318, 807, 26, 796, 716, 144, 335, 155, 594, 188, 295, 136, 377, 711, 409, 6, 405, 833, 678, 198, 62, 297, 51, 569, 34, 374, 824, 140, 92, 87, 607, 89, 799, 527, 438, 884, 52, 281, 228, 221, 551, 740, 112, 260, 714, 255, 149, 22, 412, 226, 49, 763, 243, 503, 722, 172, 586, 364, 14, 278, 522, 315, 93, 648, 837, 565, 328, 15, 351, 871, 180, 685, 638, 419, 628, 473, 523, 397, 603, 63, 138, 777, 406, 572, 829, 283, 840, 704, 67, 508, 457, 851, 795, 308, 608, 754, 724, 167, 75, 641, 599, 509, 758, 707, 734, 554, 749, 336, 597, 543, 98, 100, 201, 342, 485, 863, 451, 137, 761, 643, 715, 682, 780, 16, 860, 205, 185, 757, 78, 381, 498, 225, 588, 393, 1, 868, 135, 85, 605, 630, 656, 304, 396, 74, 251, 211, 207, 246, 176, 785, 513, 294, 65, 186, 29, 158, 566, 19, 881, 254, 190, 380, 145, 359, 302, 161, 239, 309, 750, 389, 118, 726, 66, 459, 786, 21, 579, 291, 548, 148, 192, 123, 533, 519, 634, 362, 170, 208, 107, 790, 486, 471, 79, 782, 258, 376, 590, 300, 27, 463, 718, 475, 371, 379, 585, 575, 820, 492, 224, 358, 841, 679, 661, 320, 218, 366, 238, 203, 174, 307, 382, 450, 832, 440, 695, 442, 46, 292, 828, 236, 892, 357, 544, 815, 732, 847, 5, 39, 175, 854, 629, 502, 157, 791, 434, 177, 883, 298, 139, 552, 60, 458, 344, 461, 567, 496, 488, 288, 179, 241, 850, 839, 805, 360, 131, 600, 124, 443, 178, 855, 94, 582, 32]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.921, TAw acc= 38.8% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.500, TAw acc= 47.0% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.165, TAw acc= 61.2% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.894, TAw acc= 64.9% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.668, TAw acc= 66.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 304 train exemplars, time=  0.0s
304
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.655 | TAw acc= 69.2%, forg=  0.0%| TAg acc= 69.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=3.310, TAw acc= 45.7% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.669, TAw acc= 51.4% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.205, TAw acc= 67.1% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.903, TAw acc= 78.6% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.666, TAw acc= 84.3% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.664, TAw acc= 84.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.662, TAw acc= 84.3% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.660, TAw acc= 84.3% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.657, TAw acc= 84.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.655, TAw acc= 84.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 444 train exemplars, time=  0.0s
444
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.692 | TAw acc= 84.6%, forg=-15.4%| TAg acc= 68.1%, forg=  1.1% <<<
>>> Test on task  1 : loss=1.595 | TAw acc= 86.6%, forg=  0.0%| TAg acc= 76.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.590, TAw acc= 47.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.773, TAw acc= 51.2% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.310, TAw acc= 68.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.914, TAw acc= 75.0% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.686, TAw acc= 76.2% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.683, TAw acc= 76.2% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.680, TAw acc= 76.2% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.3s loss=1.678, TAw acc= 76.2% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.675, TAw acc= 76.2% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.3s loss=1.673, TAw acc= 76.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 584 train exemplars, time=  0.0s
584
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.600 | TAw acc= 79.1%, forg=  5.5%| TAg acc= 72.0%, forg= -2.7% <<<
>>> Test on task  1 : loss=1.673 | TAw acc= 95.9%, forg= -9.3%| TAg acc= 72.2%, forg=  4.1% <<<
>>> Test on task  2 : loss=1.672 | TAw acc= 80.6%, forg=  0.0%| TAg acc= 70.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.520, TAw acc= 47.0% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.3s loss=2.713, TAw acc= 59.0% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.266, TAw acc= 67.5% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.985, TAw acc= 74.7% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.740, TAw acc= 74.7% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.738, TAw acc= 74.7% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.3s loss=1.737, TAw acc= 74.7% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.3s loss=1.735, TAw acc= 74.7% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.733, TAw acc= 74.7% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.732, TAw acc= 74.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 724 train exemplars, time=  0.0s
724
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.496 | TAw acc= 84.1%, forg=  0.5%| TAg acc= 76.4%, forg= -4.4% <<<
>>> Test on task  1 : loss=1.483 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 84.5%, forg= -8.2% <<<
>>> Test on task  2 : loss=1.775 | TAw acc= 93.5%, forg=-13.0%| TAg acc= 59.3%, forg= 11.1% <<<
>>> Test on task  3 : loss=1.733 | TAw acc= 80.5%, forg=  0.0%| TAg acc= 60.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.016, TAw acc= 34.2% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.060, TAw acc= 51.3% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.561, TAw acc= 71.1% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.192, TAw acc= 76.3% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.938, TAw acc= 77.6% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.936, TAw acc= 77.6% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.934, TAw acc= 77.6% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.932, TAw acc= 77.6% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.930, TAw acc= 77.6% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.928, TAw acc= 77.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 864 train exemplars, time=  0.0s
864
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.417 | TAw acc= 86.3%, forg= -1.6%| TAg acc= 83.5%, forg= -7.1% <<<
>>> Test on task  1 : loss=1.464 | TAw acc= 97.9%, forg= -2.1%| TAg acc= 81.4%, forg=  3.1% <<<
>>> Test on task  2 : loss=1.558 | TAw acc= 94.4%, forg= -0.9%| TAg acc= 70.4%, forg=  0.0% <<<
>>> Test on task  3 : loss=1.959 | TAw acc= 91.2%, forg=-10.6%| TAg acc= 48.7%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.770 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 59.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.388, TAw acc= 51.3% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.094, TAw acc= 67.1% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.310, TAw acc= 68.4% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.896, TAw acc= 69.7% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.666, TAw acc= 82.9% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.664, TAw acc= 82.9% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.661, TAw acc= 82.9% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.659, TAw acc= 82.9% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.656, TAw acc= 82.9% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.654, TAw acc= 82.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1004 train exemplars, time=  0.0s
1004
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.372 | TAw acc= 87.4%, forg= -1.1%| TAg acc= 84.1%, forg= -0.5% <<<
>>> Test on task  1 : loss=1.395 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.404 | TAw acc= 97.2%, forg= -2.8%| TAg acc= 75.9%, forg= -5.6% <<<
>>> Test on task  3 : loss=1.767 | TAw acc= 93.8%, forg= -2.7%| TAg acc= 50.4%, forg=  9.7% <<<
>>> Test on task  4 : loss=2.010 | TAw acc= 98.1%, forg=-14.6%| TAg acc= 37.9%, forg= 21.4% <<<
>>> Test on task  5 : loss=1.684 | TAw acc= 81.0%, forg=  0.0%| TAg acc= 57.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.103, TAw acc= 59.6% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.033, TAw acc= 69.7% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=2.285, TAw acc= 76.4% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.910, TAw acc= 80.9% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.643, TAw acc= 85.4% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.641, TAw acc= 86.5% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.639, TAw acc= 86.5% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.637, TAw acc= 86.5% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.635, TAw acc= 86.5% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.633, TAw acc= 86.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1144 train exemplars, time=  0.0s
1144
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.460 | TAw acc= 86.8%, forg=  0.5%| TAg acc= 75.8%, forg=  8.2% <<<
>>> Test on task  1 : loss=1.341 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 82.5%, forg=  2.1% <<<
>>> Test on task  2 : loss=1.345 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.9%, forg= -0.9% <<<
>>> Test on task  3 : loss=1.725 | TAw acc= 94.7%, forg= -0.9%| TAg acc= 59.3%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.703 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 58.3%, forg=  1.0% <<<
>>> Test on task  5 : loss=1.941 | TAw acc= 89.5%, forg= -8.6%| TAg acc= 39.0%, forg= 18.1% <<<
>>> Test on task  6 : loss=1.635 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 72.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.410, TAw acc= 52.1% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.017, TAw acc= 61.6% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.421, TAw acc= 71.2% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.015, TAw acc= 79.5% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.748, TAw acc= 87.7% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.746, TAw acc= 87.7% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.744, TAw acc= 87.7% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.741, TAw acc= 87.7% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.739, TAw acc= 87.7% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.737, TAw acc= 87.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1284 train exemplars, time=  0.0s
1284
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.377 | TAw acc= 87.4%, forg=  0.0%| TAg acc= 81.9%, forg=  2.2% <<<
>>> Test on task  1 : loss=1.287 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.189 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 82.4%, forg= -5.6% <<<
>>> Test on task  3 : loss=1.437 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 80.5%, forg=-20.4% <<<
>>> Test on task  4 : loss=1.539 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.0%, forg= -8.7% <<<
>>> Test on task  5 : loss=1.734 | TAw acc= 92.4%, forg= -2.9%| TAg acc= 61.9%, forg= -4.8% <<<
>>> Test on task  6 : loss=1.895 | TAw acc= 92.4%, forg= -0.8%| TAg acc= 53.8%, forg= 18.5% <<<
>>> Test on task  7 : loss=1.762 | TAw acc= 83.2%, forg=  0.0%| TAg acc= 52.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.666, TAw acc= 35.2% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.221, TAw acc= 53.5% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.447, TAw acc= 64.8% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.026, TAw acc= 81.7% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.710, TAw acc= 87.3% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.707, TAw acc= 87.3% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.705, TAw acc= 87.3% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.703, TAw acc= 88.7% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.701, TAw acc= 88.7% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.699, TAw acc= 88.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1424 train exemplars, time=  0.0s
1424
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.359 | TAw acc= 87.9%, forg= -0.5%| TAg acc= 84.6%, forg= -0.5% <<<
>>> Test on task  1 : loss=1.299 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 77.3%, forg=  7.2% <<<
>>> Test on task  2 : loss=1.201 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 82.4%, forg=  0.0% <<<
>>> Test on task  3 : loss=1.317 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 74.3%, forg=  6.2% <<<
>>> Test on task  4 : loss=1.399 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.8%, forg= -3.9% <<<
>>> Test on task  5 : loss=1.693 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 65.7%, forg= -3.8% <<<
>>> Test on task  6 : loss=1.739 | TAw acc= 95.0%, forg= -2.5%| TAg acc= 52.1%, forg= 20.2% <<<
>>> Test on task  7 : loss=1.910 | TAw acc= 92.1%, forg= -8.9%| TAg acc= 51.5%, forg=  1.0% <<<
>>> Test on task  8 : loss=1.725 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 61.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=4.599, TAw acc= 45.1% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.157, TAw acc= 58.2% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.516, TAw acc= 62.6% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.105, TAw acc= 72.5% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.831, TAw acc= 83.5% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.829, TAw acc= 83.5% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.827, TAw acc= 83.5% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.824, TAw acc= 83.5% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.822, TAw acc= 83.5% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.820, TAw acc= 83.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 1550 train exemplars, time=  0.0s
1550
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.412 | TAw acc= 87.4%, forg=  0.5%| TAg acc= 76.4%, forg=  8.2% <<<
>>> Test on task  1 : loss=1.160 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 83.5%, forg=  1.0% <<<
>>> Test on task  2 : loss=1.328 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.0%, forg= 19.4% <<<
>>> Test on task  3 : loss=1.236 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 82.3%, forg= -1.8% <<<
>>> Test on task  4 : loss=1.330 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.8%, forg= -2.9% <<<
>>> Test on task  5 : loss=1.502 | TAw acc= 93.3%, forg= -1.0%| TAg acc= 74.3%, forg= -8.6% <<<
>>> Test on task  6 : loss=1.648 | TAw acc= 95.8%, forg= -0.8%| TAg acc= 61.3%, forg= 10.9% <<<
>>> Test on task  7 : loss=1.765 | TAw acc= 93.1%, forg= -1.0%| TAg acc= 54.5%, forg= -2.0% <<<
>>> Test on task  8 : loss=1.753 | TAw acc= 98.0%, forg= -5.1%| TAg acc= 45.9%, forg= 15.3% <<<
>>> Test on task  9 : loss=1.569 | TAw acc= 91.7%, forg=  0.0%| TAg acc= 71.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=4.800, TAw acc= 43.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.157, TAw acc= 50.6% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.315, TAw acc= 67.9% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.899, TAw acc= 76.5% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.598, TAw acc= 77.8% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.596, TAw acc= 77.8% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.594, TAw acc= 77.8% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.593, TAw acc= 77.8% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.591, TAw acc= 79.0% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.590, TAw acc= 79.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 1670 train exemplars, time=  0.0s
1670
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.394 | TAw acc= 85.7%, forg=  2.2%| TAg acc= 76.9%, forg=  7.7% <<<
>>> Test on task  1 : loss=1.167 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 84.5%, forg=  0.0% <<<
>>> Test on task  2 : loss=1.190 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  0.9% <<<
>>> Test on task  3 : loss=1.217 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 77.9%, forg=  4.4% <<<
>>> Test on task  4 : loss=1.293 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.9%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.501 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 69.5%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.536 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 66.4%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.749 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 56.4%, forg= -2.0% <<<
>>> Test on task  8 : loss=1.609 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 51.0%, forg= 10.2% <<<
>>> Test on task  9 : loss=1.927 | TAw acc= 99.2%, forg= -7.4%| TAg acc= 39.7%, forg= 31.4% <<<
>>> Test on task 10 : loss=1.474 | TAw acc= 84.5%, forg=  0.0%| TAg acc= 64.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=4.496, TAw acc= 40.4% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.856, TAw acc= 67.4% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.220, TAw acc= 74.2% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.801, TAw acc= 77.5% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.575, TAw acc= 85.4% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.572, TAw acc= 85.4% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.570, TAw acc= 85.4% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.567, TAw acc= 85.4% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.564, TAw acc= 85.4% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.561, TAw acc= 85.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 1790 train exemplars, time=  0.0s
1790
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.390 | TAw acc= 89.0%, forg= -1.1%| TAg acc= 78.6%, forg=  6.0% <<<
>>> Test on task  1 : loss=1.198 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.208 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 76.9%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.116 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 86.7%, forg= -4.4% <<<
>>> Test on task  4 : loss=1.258 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.9%, forg=  3.9% <<<
>>> Test on task  5 : loss=1.467 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 66.7%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.446 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 68.9%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.511 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 71.3%, forg=-14.9% <<<
>>> Test on task  8 : loss=1.509 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 59.2%, forg=  2.0% <<<
>>> Test on task  9 : loss=1.776 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 44.6%, forg= 26.4% <<<
>>> Test on task 10 : loss=1.849 | TAw acc= 93.6%, forg= -9.1%| TAg acc= 34.5%, forg= 30.0% <<<
>>> Test on task 11 : loss=1.443 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=4.892, TAw acc= 31.3% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=3.099, TAw acc= 60.2% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.285, TAw acc= 71.1% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.904, TAw acc= 67.5% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.639, TAw acc= 97.6% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=1.636, TAw acc= 96.4% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.632, TAw acc= 96.4% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.629, TAw acc= 96.4% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.627, TAw acc= 96.4% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.624, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 1910 train exemplars, time=  0.0s
1910
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.446 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 68.1%, forg= 16.5% <<<
>>> Test on task  1 : loss=1.172 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 77.3%, forg=  7.2% <<<
>>> Test on task  2 : loss=1.156 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 84.3%, forg= -1.9% <<<
>>> Test on task  3 : loss=1.152 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 85.0%, forg=  1.8% <<<
>>> Test on task  4 : loss=1.150 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.7%, forg= -1.9% <<<
>>> Test on task  5 : loss=1.445 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 66.7%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.365 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 73.1%, forg= -0.8% <<<
>>> Test on task  7 : loss=1.442 | TAw acc= 98.0%, forg= -5.0%| TAg acc= 71.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.484 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 63.3%, forg= -2.0% <<<
>>> Test on task  9 : loss=1.548 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 64.5%, forg=  6.6% <<<
>>> Test on task 10 : loss=1.693 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 44.5%, forg= 20.0% <<<
>>> Test on task 11 : loss=1.782 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 44.5%, forg= 29.4% <<<
>>> Test on task 12 : loss=1.543 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 62.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=4.981, TAw acc= 44.6% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=3.036, TAw acc= 63.0% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.137, TAw acc= 80.4% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.764, TAw acc= 85.9% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.475, TAw acc= 89.1% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.473, TAw acc= 89.1% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.471, TAw acc= 89.1% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.469, TAw acc= 89.1% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.468, TAw acc= 89.1% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.466, TAw acc= 89.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2030 train exemplars, time=  0.0s
2030
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.398 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 77.5%, forg=  7.1% <<<
>>> Test on task  1 : loss=1.166 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.098 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 82.4%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.066 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 85.8%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.175 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.8%, forg=  3.9% <<<
>>> Test on task  5 : loss=1.410 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 69.5%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.429 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 69.7%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.417 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 71.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.518 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 61.2%, forg=  2.0% <<<
>>> Test on task  9 : loss=1.409 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 68.6%, forg=  2.5% <<<
>>> Test on task 10 : loss=1.522 | TAw acc= 94.5%, forg= -0.9%| TAg acc= 56.4%, forg=  8.2% <<<
>>> Test on task 11 : loss=1.665 | TAw acc= 95.0%, forg= -1.7%| TAg acc= 52.9%, forg= 21.0% <<<
>>> Test on task 12 : loss=1.912 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 29.5%, forg= 33.0% <<<
>>> Test on task 13 : loss=1.277 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 69.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=4.560, TAw acc= 36.9% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.778, TAw acc= 60.7% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.087, TAw acc= 73.8% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.574, TAw acc= 78.6% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.460, TAw acc= 94.0% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.457, TAw acc= 94.0% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.453, TAw acc= 94.0% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.450, TAw acc= 94.0% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.447, TAw acc= 94.0% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.443, TAw acc= 94.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2150 train exemplars, time=  0.0s
2150
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.356 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 78.6%, forg=  6.0% <<<
>>> Test on task  1 : loss=1.186 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.126 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.061 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 81.4%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.052 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.6%, forg= -1.9% <<<
>>> Test on task  5 : loss=1.355 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.373 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 76.5%, forg= -3.4% <<<
>>> Test on task  7 : loss=1.383 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 73.3%, forg= -2.0% <<<
>>> Test on task  8 : loss=1.385 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 68.4%, forg= -5.1% <<<
>>> Test on task  9 : loss=1.416 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 67.8%, forg=  3.3% <<<
>>> Test on task 10 : loss=1.461 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 59.1%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.518 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 56.3%, forg= 17.6% <<<
>>> Test on task 12 : loss=1.864 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 33.0%, forg= 29.5% <<<
>>> Test on task 13 : loss=1.708 | TAw acc= 91.8%, forg= -1.6%| TAg acc= 43.4%, forg= 26.2% <<<
>>> Test on task 14 : loss=1.499 | TAw acc= 87.5%, forg=  0.0%| TAg acc= 66.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=5.119, TAw acc= 53.2% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=3.069, TAw acc= 57.0% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.114, TAw acc= 82.3% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.644, TAw acc= 87.3% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.427, TAw acc= 88.6% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.425, TAw acc= 88.6% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.421, TAw acc= 88.6% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.418, TAw acc= 88.6% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 88.6% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.413, TAw acc= 88.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2270 train exemplars, time=  0.0s
2270
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.378 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 78.0%, forg=  6.6% <<<
>>> Test on task  1 : loss=1.094 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.087 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 82.4%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.073 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 85.0%, forg=  1.8% <<<
>>> Test on task  4 : loss=1.128 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.9%, forg=  7.8% <<<
>>> Test on task  5 : loss=1.307 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.406 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 70.6%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.297 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 73.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.363 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 69.4%, forg= -1.0% <<<
>>> Test on task  9 : loss=1.308 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 71.9%, forg= -0.8% <<<
>>> Test on task 10 : loss=1.329 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 69.1%, forg= -4.5% <<<
>>> Test on task 11 : loss=1.417 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 63.9%, forg= 10.1% <<<
>>> Test on task 12 : loss=1.692 | TAw acc= 92.9%, forg=  0.9%| TAg acc= 42.9%, forg= 19.6% <<<
>>> Test on task 13 : loss=1.616 | TAw acc= 94.3%, forg= -2.5%| TAg acc= 48.4%, forg= 21.3% <<<
>>> Test on task 14 : loss=1.869 | TAw acc= 94.6%, forg= -7.1%| TAg acc= 52.7%, forg= 13.4% <<<
>>> Test on task 15 : loss=1.436 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 63.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=4.396, TAw acc= 45.8% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.584, TAw acc= 78.3% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.833, TAw acc= 80.7% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.468, TAw acc= 83.1% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.245, TAw acc= 90.4% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.242, TAw acc= 90.4% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=1.240, TAw acc= 90.4% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 90.4% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.235, TAw acc= 90.4% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.233, TAw acc= 90.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2390 train exemplars, time=  0.0s
2390
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.397 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 76.9%, forg=  7.7% <<<
>>> Test on task  1 : loss=1.222 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 75.3%, forg=  9.3% <<<
>>> Test on task  2 : loss=1.052 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 82.4%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.029 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 85.0%, forg=  1.8% <<<
>>> Test on task  4 : loss=1.079 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.8%, forg=  4.9% <<<
>>> Test on task  5 : loss=1.345 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.405 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 74.8%, forg=  1.7% <<<
>>> Test on task  7 : loss=1.245 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 76.2%, forg= -3.0% <<<
>>> Test on task  8 : loss=1.340 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 65.3%, forg=  4.1% <<<
>>> Test on task  9 : loss=1.267 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 72.7%, forg= -0.8% <<<
>>> Test on task 10 : loss=1.340 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 69.1%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.524 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 56.3%, forg= 17.6% <<<
>>> Test on task 12 : loss=1.841 | TAw acc= 92.0%, forg=  1.8%| TAg acc= 36.6%, forg= 25.9% <<<
>>> Test on task 13 : loss=1.569 | TAw acc= 95.1%, forg= -0.8%| TAg acc= 50.8%, forg= 18.9% <<<
>>> Test on task 14 : loss=1.743 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 62.5%, forg=  3.6% <<<
>>> Test on task 15 : loss=1.656 | TAw acc= 95.4%, forg= -4.6%| TAg acc= 46.3%, forg= 16.7% <<<
>>> Test on task 16 : loss=1.452 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 64.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=5.382, TAw acc= 33.3% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=3.839, TAw acc= 56.9% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.753, TAw acc= 77.8% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.085, TAw acc= 86.1% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.749, TAw acc= 93.1% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.741, TAw acc= 93.1% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.732, TAw acc= 93.1% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.724, TAw acc= 93.1% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.717, TAw acc= 93.1% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.709, TAw acc= 93.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2510 train exemplars, time=  0.0s
2510
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.342 | TAw acc= 89.0%, forg=  0.0%| TAg acc= 80.2%, forg=  4.4% <<<
>>> Test on task  1 : loss=1.089 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.055 | TAw acc= 95.4%, forg=  1.9%| TAg acc= 82.4%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.010 | TAw acc= 96.5%, forg= -1.8%| TAg acc= 84.1%, forg=  2.7% <<<
>>> Test on task  4 : loss=1.093 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 79.6%, forg= -1.0% <<<
>>> Test on task  5 : loss=1.303 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.414 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 71.4%, forg=  5.0% <<<
>>> Test on task  7 : loss=1.241 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.2%, forg= -2.0% <<<
>>> Test on task  8 : loss=1.241 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.5%, forg= -7.1% <<<
>>> Test on task  9 : loss=1.226 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 77.7%, forg= -5.0% <<<
>>> Test on task 10 : loss=1.304 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 68.2%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.425 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 63.0%, forg= 10.9% <<<
>>> Test on task 12 : loss=1.469 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 55.4%, forg=  7.1% <<<
>>> Test on task 13 : loss=1.392 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 57.4%, forg= 12.3% <<<
>>> Test on task 14 : loss=1.669 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 67.0%, forg= -0.9% <<<
>>> Test on task 15 : loss=1.522 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 55.6%, forg=  7.4% <<<
>>> Test on task 16 : loss=1.742 | TAw acc= 97.3%, forg= -5.3%| TAg acc= 46.0%, forg= 18.6% <<<
>>> Test on task 17 : loss=1.546 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 60.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=5.132, TAw acc= 39.5% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=3.134, TAw acc= 60.5% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.269, TAw acc= 75.3% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.895, TAw acc= 79.0% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.628, TAw acc= 81.5% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.626, TAw acc= 81.5% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.624, TAw acc= 81.5% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.622, TAw acc= 81.5% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.620, TAw acc= 81.5% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.618, TAw acc= 81.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2630 train exemplars, time=  0.0s
2630
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.371 | TAw acc= 89.0%, forg=  0.0%| TAg acc= 77.5%, forg=  7.1% <<<
>>> Test on task  1 : loss=1.157 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 75.3%, forg=  9.3% <<<
>>> Test on task  2 : loss=1.021 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 84.3%, forg=  0.0% <<<
>>> Test on task  3 : loss=1.068 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 83.2%, forg=  3.5% <<<
>>> Test on task  4 : loss=1.076 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.8%, forg=  5.8% <<<
>>> Test on task  5 : loss=1.391 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 71.4%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.455 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.9%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.307 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 67.3%, forg= 10.9% <<<
>>> Test on task  8 : loss=1.297 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.4%, forg=  8.2% <<<
>>> Test on task  9 : loss=1.221 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 76.0%, forg=  1.7% <<<
>>> Test on task 10 : loss=1.224 | TAw acc= 95.5%, forg= -0.9%| TAg acc= 76.4%, forg= -7.3% <<<
>>> Test on task 11 : loss=1.404 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 67.2%, forg=  6.7% <<<
>>> Test on task 12 : loss=1.415 | TAw acc= 94.6%, forg= -0.9%| TAg acc= 57.1%, forg=  5.4% <<<
>>> Test on task 13 : loss=1.412 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 14 : loss=1.668 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 69.6%, forg= -2.7% <<<
>>> Test on task 15 : loss=1.541 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 56.5%, forg=  6.5% <<<
>>> Test on task 16 : loss=1.658 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 53.1%, forg= 11.5% <<<
>>> Test on task 17 : loss=1.572 | TAw acc= 94.9%, forg= -1.0%| TAg acc= 61.6%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.549 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 63.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.3s loss=4.541, TAw acc= 56.0% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.563, TAw acc= 73.8% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.883, TAw acc= 84.5% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 81.0% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.285, TAw acc= 92.9% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 92.9% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.280, TAw acc= 92.9% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.277, TAw acc= 92.9% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.275, TAw acc= 92.9% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.273, TAw acc= 92.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2750 train exemplars, time=  0.0s
2750
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.360 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 76.4%, forg=  8.2% <<<
>>> Test on task  1 : loss=1.185 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 77.3%, forg=  7.2% <<<
>>> Test on task  2 : loss=1.039 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 82.4%, forg=  1.9% <<<
>>> Test on task  3 : loss=1.023 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 84.1%, forg=  2.7% <<<
>>> Test on task  4 : loss=1.076 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.8%, forg=  4.9% <<<
>>> Test on task  5 : loss=1.347 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.345 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 75.6%, forg=  0.8% <<<
>>> Test on task  7 : loss=1.220 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.2%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.290 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 73.5%, forg=  3.1% <<<
>>> Test on task  9 : loss=1.178 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 81.0%, forg= -3.3% <<<
>>> Test on task 10 : loss=1.208 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 74.5%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.408 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 66.4%, forg=  7.6% <<<
>>> Test on task 12 : loss=1.416 | TAw acc= 95.5%, forg= -0.9%| TAg acc= 55.4%, forg=  7.1% <<<
>>> Test on task 13 : loss=1.438 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 61.5%, forg=  8.2% <<<
>>> Test on task 14 : loss=1.614 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 66.1%, forg=  3.6% <<<
>>> Test on task 15 : loss=1.392 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 60.2%, forg=  2.8% <<<
>>> Test on task 16 : loss=1.615 | TAw acc= 94.7%, forg=  2.7%| TAg acc= 53.1%, forg= 11.5% <<<
>>> Test on task 17 : loss=1.467 | TAw acc= 96.0%, forg= -1.0%| TAg acc= 60.6%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.930 | TAw acc= 98.2%, forg= -3.6%| TAg acc= 40.0%, forg= 23.6% <<<
>>> Test on task 19 : loss=1.311 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 71.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=4.504, TAw acc= 45.9% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.996, TAw acc= 69.4% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.925, TAw acc= 92.9% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.403, TAw acc= 95.3% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.260, TAw acc= 96.5% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.260, TAw acc= 96.5% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.3s loss=1.259, TAw acc= 96.5% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.259, TAw acc= 96.5% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.259, TAw acc= 96.5% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.258, TAw acc= 96.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2870 train exemplars, time=  0.0s
2870
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.371 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 76.4%, forg=  8.2% <<<
>>> Test on task  1 : loss=1.144 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 79.4%, forg=  5.2% <<<
>>> Test on task  2 : loss=1.065 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 83.3%, forg=  0.9% <<<
>>> Test on task  3 : loss=1.035 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 83.2%, forg=  3.5% <<<
>>> Test on task  4 : loss=1.058 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.8%, forg=  7.8% <<<
>>> Test on task  5 : loss=1.357 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.419 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.9%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.273 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 71.3%, forg=  6.9% <<<
>>> Test on task  8 : loss=1.307 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 71.4%, forg=  5.1% <<<
>>> Test on task  9 : loss=1.215 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 80.2%, forg=  0.8% <<<
>>> Test on task 10 : loss=1.281 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 73.6%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.324 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 66.4%, forg=  7.6% <<<
>>> Test on task 12 : loss=1.341 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 59.8%, forg=  2.7% <<<
>>> Test on task 13 : loss=1.305 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 64.8%, forg=  4.9% <<<
>>> Test on task 14 : loss=1.641 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 59.8%, forg=  9.8% <<<
>>> Test on task 15 : loss=1.272 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 67.6%, forg= -4.6% <<<
>>> Test on task 16 : loss=1.461 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 62.8%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.262 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 69.7%, forg= -8.1% <<<
>>> Test on task 18 : loss=1.904 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 43.6%, forg= 20.0% <<<
>>> Test on task 19 : loss=1.692 | TAw acc= 95.6%, forg= -3.5%| TAg acc= 54.0%, forg= 17.7% <<<
>>> Test on task 20 : loss=1.338 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 71.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.3s loss=5.582, TAw acc= 46.3% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=3.232, TAw acc= 67.1% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=2.221, TAw acc= 82.9% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.783, TAw acc= 87.8% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 90.2% | *
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.508, TAw acc= 90.2% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.503, TAw acc= 90.2% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.499, TAw acc= 90.2% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.495, TAw acc= 91.5% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.492, TAw acc= 91.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 2990 train exemplars, time=  0.1s
2990
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.349 | TAw acc= 89.0%, forg=  0.0%| TAg acc= 75.8%, forg=  8.8% <<<
>>> Test on task  1 : loss=1.212 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.017 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.024 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 82.3%, forg=  4.4% <<<
>>> Test on task  4 : loss=0.991 | TAw acc=100.0%, forg= -1.0%| TAg acc= 76.7%, forg=  2.9% <<<
>>> Test on task  5 : loss=1.317 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 70.5%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.394 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 76.5%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.227 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.316 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 68.4%, forg=  8.2% <<<
>>> Test on task  9 : loss=1.249 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 71.1%, forg=  9.9% <<<
>>> Test on task 10 : loss=1.236 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 71.8%, forg=  4.5% <<<
>>> Test on task 11 : loss=1.349 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 67.2%, forg=  6.7% <<<
>>> Test on task 12 : loss=1.354 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 60.7%, forg=  1.8% <<<
>>> Test on task 13 : loss=1.276 | TAw acc= 94.3%, forg=  0.8%| TAg acc= 67.2%, forg=  2.5% <<<
>>> Test on task 14 : loss=1.560 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 73.2%, forg= -3.6% <<<
>>> Test on task 15 : loss=1.263 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 63.9%, forg=  3.7% <<<
>>> Test on task 16 : loss=1.424 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 61.9%, forg=  2.7% <<<
>>> Test on task 17 : loss=1.276 | TAw acc= 97.0%, forg= -1.0%| TAg acc= 65.7%, forg=  4.0% <<<
>>> Test on task 18 : loss=1.719 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 49.1%, forg= 14.5% <<<
>>> Test on task 19 : loss=1.647 | TAw acc= 94.7%, forg=  0.9%| TAg acc= 56.6%, forg= 15.0% <<<
>>> Test on task 20 : loss=1.736 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 57.8%, forg= 13.8% <<<
>>> Test on task 21 : loss=1.320 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 66.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=5.405, TAw acc= 54.3% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.891, TAw acc= 66.7% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.894, TAw acc= 81.5% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.452, TAw acc= 97.5% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.197, TAw acc= 97.5% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.197, TAw acc= 97.5% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.196, TAw acc= 97.5% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.196, TAw acc= 97.5% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.195, TAw acc= 97.5% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.195, TAw acc= 97.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3110 train exemplars, time=  0.0s
3110
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.362 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 76.9%, forg=  7.7% <<<
>>> Test on task  1 : loss=1.236 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 81.4%, forg=  3.1% <<<
>>> Test on task  2 : loss=1.000 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 83.3%, forg=  0.9% <<<
>>> Test on task  3 : loss=0.988 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 83.2%, forg=  3.5% <<<
>>> Test on task  4 : loss=0.980 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task  5 : loss=1.302 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 72.4%, forg=  1.9% <<<
>>> Test on task  6 : loss=1.400 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.1%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.259 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.245 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 74.5%, forg=  2.0% <<<
>>> Test on task  9 : loss=1.198 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 74.4%, forg=  6.6% <<<
>>> Test on task 10 : loss=1.239 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 69.1%, forg=  7.3% <<<
>>> Test on task 11 : loss=1.304 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 68.9%, forg=  5.0% <<<
>>> Test on task 12 : loss=1.381 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 56.2%, forg=  6.2% <<<
>>> Test on task 13 : loss=1.357 | TAw acc= 95.9%, forg= -0.8%| TAg acc= 59.0%, forg= 10.7% <<<
>>> Test on task 14 : loss=1.560 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 72.3%, forg=  0.9% <<<
>>> Test on task 15 : loss=1.276 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 67.6%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.385 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 61.9%, forg=  2.7% <<<
>>> Test on task 17 : loss=1.232 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 66.7%, forg=  3.0% <<<
>>> Test on task 18 : loss=1.620 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 52.7%, forg= 10.9% <<<
>>> Test on task 19 : loss=1.461 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 65.5%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.505 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 66.4%, forg=  5.2% <<<
>>> Test on task 21 : loss=1.708 | TAw acc= 90.9%, forg=  2.7%| TAg acc= 44.5%, forg= 21.8% <<<
>>> Test on task 22 : loss=1.189 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=5.008, TAw acc= 51.8% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=2.703, TAw acc= 63.9% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.944, TAw acc= 77.1% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.496, TAw acc= 77.1% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.303, TAw acc= 80.7% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.300, TAw acc= 80.7% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.298, TAw acc= 80.7% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 80.7% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.294, TAw acc= 80.7% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 80.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3230 train exemplars, time=  0.0s
3230
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.368 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 74.2%, forg= 10.4% <<<
>>> Test on task  1 : loss=1.219 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 80.4%, forg=  4.1% <<<
>>> Test on task  2 : loss=1.007 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 80.6%, forg=  3.7% <<<
>>> Test on task  3 : loss=0.983 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 83.2%, forg=  3.5% <<<
>>> Test on task  4 : loss=0.979 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.7%, forg=  3.9% <<<
>>> Test on task  5 : loss=1.315 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 68.6%, forg=  5.7% <<<
>>> Test on task  6 : loss=1.376 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.9%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.220 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 70.3%, forg=  7.9% <<<
>>> Test on task  8 : loss=1.287 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 73.5%, forg=  3.1% <<<
>>> Test on task  9 : loss=1.255 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 69.4%, forg= 11.6% <<<
>>> Test on task 10 : loss=1.153 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 76.4%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.353 | TAw acc= 96.6%, forg= -1.7%| TAg acc= 63.9%, forg= 10.1% <<<
>>> Test on task 12 : loss=1.368 | TAw acc= 96.4%, forg= -0.9%| TAg acc= 58.0%, forg=  4.5% <<<
>>> Test on task 13 : loss=1.278 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 66.4%, forg=  3.3% <<<
>>> Test on task 14 : loss=1.499 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 76.8%, forg= -3.6% <<<
>>> Test on task 15 : loss=1.238 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 65.7%, forg=  1.9% <<<
>>> Test on task 16 : loss=1.331 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 65.5%, forg= -0.9% <<<
>>> Test on task 17 : loss=1.139 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 70.7%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.562 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 59.1%, forg=  4.5% <<<
>>> Test on task 19 : loss=1.578 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 53.1%, forg= 18.6% <<<
>>> Test on task 20 : loss=1.498 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 61.2%, forg= 10.3% <<<
>>> Test on task 21 : loss=1.514 | TAw acc= 94.5%, forg= -0.9%| TAg acc= 60.0%, forg=  6.4% <<<
>>> Test on task 22 : loss=1.532 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 56.0%, forg= 19.3% <<<
>>> Test on task 23 : loss=1.266 | TAw acc= 86.6%, forg=  0.0%| TAg acc= 65.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=5.533, TAw acc= 46.9% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=3.056, TAw acc= 64.2% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.059, TAw acc= 90.1% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.669, TAw acc= 91.4% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.424, TAw acc= 93.8% | *
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.422, TAw acc= 95.1% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.420, TAw acc= 95.1% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.418, TAw acc= 95.1% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.417, TAw acc= 95.1% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.415, TAw acc= 95.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3350 train exemplars, time=  0.0s
3350
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.378 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 75.3%, forg=  9.3% <<<
>>> Test on task  1 : loss=1.144 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 81.4%, forg=  3.1% <<<
>>> Test on task  2 : loss=1.002 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.044 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 77.0%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.024 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.0%, forg= 11.7% <<<
>>> Test on task  5 : loss=1.260 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.367 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 75.6%, forg=  0.8% <<<
>>> Test on task  7 : loss=1.259 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 70.3%, forg=  7.9% <<<
>>> Test on task  8 : loss=1.300 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 64.3%, forg= 12.2% <<<
>>> Test on task  9 : loss=1.213 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 76.9%, forg=  4.1% <<<
>>> Test on task 10 : loss=1.199 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 75.5%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.334 | TAw acc= 97.5%, forg= -0.8%| TAg acc= 67.2%, forg=  6.7% <<<
>>> Test on task 12 : loss=1.267 | TAw acc= 94.6%, forg=  1.8%| TAg acc= 67.9%, forg= -5.4% <<<
>>> Test on task 13 : loss=1.332 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 63.1%, forg=  6.6% <<<
>>> Test on task 14 : loss=1.528 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 70.5%, forg=  6.2% <<<
>>> Test on task 15 : loss=1.204 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 71.3%, forg= -3.7% <<<
>>> Test on task 16 : loss=1.324 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 64.6%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.182 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 66.7%, forg=  4.0% <<<
>>> Test on task 18 : loss=1.513 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 56.4%, forg=  7.3% <<<
>>> Test on task 19 : loss=1.465 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 62.8%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.441 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 65.5%, forg=  6.0% <<<
>>> Test on task 21 : loss=1.519 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 57.3%, forg=  9.1% <<<
>>> Test on task 22 : loss=1.429 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 58.7%, forg= 16.5% <<<
>>> Test on task 23 : loss=1.627 | TAw acc= 90.2%, forg= -3.6%| TAg acc= 50.9%, forg= 14.3% <<<
>>> Test on task 24 : loss=1.372 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 73.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=5.845, TAw acc= 34.7% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.795, TAw acc= 73.6% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.864, TAw acc= 86.1% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.603, TAw acc= 86.1% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.492, TAw acc= 90.3% | *
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.487, TAw acc= 90.3% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.482, TAw acc= 90.3% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.478, TAw acc= 90.3% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.473, TAw acc= 90.3% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.469, TAw acc= 90.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3470 train exemplars, time=  0.0s
3470
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.339 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 76.9%, forg=  7.7% <<<
>>> Test on task  1 : loss=1.250 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 73.2%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.001 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.012 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 79.6%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.100 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.0%, forg= 12.6% <<<
>>> Test on task  5 : loss=1.258 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 71.4%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.341 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 74.8%, forg=  1.7% <<<
>>> Test on task  7 : loss=1.209 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 74.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.264 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 73.5%, forg=  3.1% <<<
>>> Test on task  9 : loss=1.246 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 66.1%, forg= 14.9% <<<
>>> Test on task 10 : loss=1.159 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 76.4%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.356 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.7%, forg=  9.2% <<<
>>> Test on task 12 : loss=1.350 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 55.4%, forg= 12.5% <<<
>>> Test on task 13 : loss=1.275 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 65.6%, forg=  4.1% <<<
>>> Test on task 14 : loss=1.489 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 75.9%, forg=  0.9% <<<
>>> Test on task 15 : loss=1.186 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 73.1%, forg= -1.9% <<<
>>> Test on task 16 : loss=1.354 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 65.5%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.099 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 72.7%, forg= -2.0% <<<
>>> Test on task 18 : loss=1.532 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 56.4%, forg=  7.3% <<<
>>> Test on task 19 : loss=1.365 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 63.7%, forg=  8.0% <<<
>>> Test on task 20 : loss=1.308 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 74.1%, forg= -2.6% <<<
>>> Test on task 21 : loss=1.426 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 63.6%, forg=  2.7% <<<
>>> Test on task 22 : loss=1.466 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 54.1%, forg= 21.1% <<<
>>> Test on task 23 : loss=1.503 | TAw acc= 89.3%, forg=  0.9%| TAg acc= 56.2%, forg=  8.9% <<<
>>> Test on task 24 : loss=1.809 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 45.9%, forg= 27.5% <<<
>>> Test on task 25 : loss=1.567 | TAw acc= 85.9%, forg=  0.0%| TAg acc= 65.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=5.437, TAw acc= 48.3% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.757, TAw acc= 72.4% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.702, TAw acc= 83.9% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.309, TAw acc= 90.8% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.162, TAw acc= 90.8% | *
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.161, TAw acc= 90.8% | *
| Epoch   2, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.161, TAw acc= 90.8% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 90.8% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 90.8% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 90.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3590 train exemplars, time=  0.0s
3590
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.496 | TAw acc= 87.4%, forg=  1.6%| TAg acc= 64.8%, forg= 19.8% <<<
>>> Test on task  1 : loss=1.230 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 74.2%, forg= 10.3% <<<
>>> Test on task  2 : loss=1.052 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.055 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 78.8%, forg=  8.0% <<<
>>> Test on task  4 : loss=1.016 | TAw acc=100.0%, forg=  0.0%| TAg acc= 72.8%, forg=  6.8% <<<
>>> Test on task  5 : loss=1.205 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 75.2%, forg= -1.0% <<<
>>> Test on task  6 : loss=1.367 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.1%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.215 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 70.3%, forg=  7.9% <<<
>>> Test on task  8 : loss=1.439 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 63.3%, forg= 13.3% <<<
>>> Test on task  9 : loss=1.310 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 71.1%, forg=  9.9% <<<
>>> Test on task 10 : loss=1.222 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 74.5%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.500 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 62.2%, forg= 11.8% <<<
>>> Test on task 12 : loss=1.314 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 60.7%, forg=  7.1% <<<
>>> Test on task 13 : loss=1.260 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 63.9%, forg=  5.7% <<<
>>> Test on task 14 : loss=1.461 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 75.0%, forg=  1.8% <<<
>>> Test on task 15 : loss=1.201 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.5%, forg=  4.6% <<<
>>> Test on task 16 : loss=1.313 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 67.3%, forg= -1.8% <<<
>>> Test on task 17 : loss=1.073 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 77.8%, forg= -5.1% <<<
>>> Test on task 18 : loss=1.470 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 60.0%, forg=  3.6% <<<
>>> Test on task 19 : loss=1.349 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 62.8%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.323 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 74.1%, forg=  0.0% <<<
>>> Test on task 21 : loss=1.448 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 61.8%, forg=  4.5% <<<
>>> Test on task 22 : loss=1.384 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 64.2%, forg= 11.0% <<<
>>> Test on task 23 : loss=1.446 | TAw acc= 89.3%, forg=  0.9%| TAg acc= 55.4%, forg=  9.8% <<<
>>> Test on task 24 : loss=1.664 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 50.5%, forg= 22.9% <<<
>>> Test on task 25 : loss=1.982 | TAw acc= 85.9%, forg=  0.0%| TAg acc= 35.4%, forg= 30.3% <<<
>>> Test on task 26 : loss=1.206 | TAw acc= 91.4%, forg=  0.0%| TAg acc= 73.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=5.202, TAw acc= 55.7% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.900, TAw acc= 75.9% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.061, TAw acc= 88.6% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.676, TAw acc= 96.2% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.401, TAw acc= 96.2% | *
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.400, TAw acc= 96.2% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.399, TAw acc= 94.9% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.398, TAw acc= 94.9% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.397, TAw acc= 94.9% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.396, TAw acc= 94.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3710 train exemplars, time=  0.0s
3710
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.440 | TAw acc= 86.3%, forg=  2.7%| TAg acc= 73.6%, forg= 11.0% <<<
>>> Test on task  1 : loss=1.285 | TAw acc= 97.9%, forg=  0.0%| TAg acc= 68.0%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.116 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.8%, forg=  6.5% <<<
>>> Test on task  3 : loss=1.098 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.7%, forg= 15.0% <<<
>>> Test on task  4 : loss=0.952 | TAw acc=100.0%, forg=  0.0%| TAg acc= 71.8%, forg=  7.8% <<<
>>> Test on task  5 : loss=1.307 | TAw acc= 94.3%, forg= -1.0%| TAg acc= 68.6%, forg=  6.7% <<<
>>> Test on task  6 : loss=1.418 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.1%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.240 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 70.3%, forg=  7.9% <<<
>>> Test on task  8 : loss=1.331 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 70.4%, forg=  6.1% <<<
>>> Test on task  9 : loss=1.211 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 74.4%, forg=  6.6% <<<
>>> Test on task 10 : loss=1.215 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 72.7%, forg=  3.6% <<<
>>> Test on task 11 : loss=1.474 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.0%, forg= 10.9% <<<
>>> Test on task 12 : loss=1.330 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 55.4%, forg= 12.5% <<<
>>> Test on task 13 : loss=1.333 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 64.8%, forg=  4.9% <<<
>>> Test on task 14 : loss=1.461 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 76.8%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.178 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.5%, forg=  4.6% <<<
>>> Test on task 16 : loss=1.330 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 63.7%, forg=  3.5% <<<
>>> Test on task 17 : loss=1.046 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 69.7%, forg=  8.1% <<<
>>> Test on task 18 : loss=1.516 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 59.1%, forg=  4.5% <<<
>>> Test on task 19 : loss=1.322 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 61.9%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.232 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 75.9%, forg= -1.7% <<<
>>> Test on task 21 : loss=1.384 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 64.5%, forg=  1.8% <<<
>>> Test on task 22 : loss=1.371 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 63.3%, forg= 11.9% <<<
>>> Test on task 23 : loss=1.362 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 53.6%, forg= 11.6% <<<
>>> Test on task 24 : loss=1.654 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 49.5%, forg= 23.9% <<<
>>> Test on task 25 : loss=1.949 | TAw acc= 88.9%, forg= -3.0%| TAg acc= 41.4%, forg= 24.2% <<<
>>> Test on task 26 : loss=1.783 | TAw acc= 93.1%, forg= -1.7%| TAg acc= 40.5%, forg= 32.8% <<<
>>> Test on task 27 : loss=1.259 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 71.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=5.589, TAw acc= 42.5% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=3.011, TAw acc= 67.5% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.997, TAw acc= 85.0% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.473, TAw acc= 87.5% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.299, TAw acc= 91.2% | *
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.297, TAw acc= 91.2% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.295, TAw acc= 91.2% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 91.2% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 91.2% | *
| Epoch   5, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.290, TAw acc= 91.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3830 train exemplars, time=  0.0s
3830
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.474 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 73.1%, forg= 11.5% <<<
>>> Test on task  1 : loss=1.276 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 77.3%, forg=  7.2% <<<
>>> Test on task  2 : loss=1.123 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.8%, forg=  6.5% <<<
>>> Test on task  3 : loss=1.027 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 79.6%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.084 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.0%, forg= 11.7% <<<
>>> Test on task  5 : loss=1.293 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 70.5%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.323 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 75.6%, forg=  0.8% <<<
>>> Test on task  7 : loss=1.205 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.377 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 67.3%, forg=  9.2% <<<
>>> Test on task  9 : loss=1.210 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 73.6%, forg=  7.4% <<<
>>> Test on task 10 : loss=1.217 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 75.5%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.462 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.2%, forg= 11.8% <<<
>>> Test on task 12 : loss=1.261 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 66.1%, forg=  1.8% <<<
>>> Test on task 13 : loss=1.280 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 67.2%, forg=  2.5% <<<
>>> Test on task 14 : loss=1.472 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 76.8%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.197 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 69.4%, forg=  3.7% <<<
>>> Test on task 16 : loss=1.318 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 68.1%, forg= -0.9% <<<
>>> Test on task 17 : loss=0.982 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 76.8%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.414 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 63.6%, forg=  0.0% <<<
>>> Test on task 19 : loss=1.375 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 60.2%, forg= 11.5% <<<
>>> Test on task 20 : loss=1.296 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 78.4%, forg= -2.6% <<<
>>> Test on task 21 : loss=1.332 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 66.4%, forg=  0.0% <<<
>>> Test on task 22 : loss=1.292 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.3%, forg= 11.9% <<<
>>> Test on task 23 : loss=1.338 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 58.0%, forg=  7.1% <<<
>>> Test on task 24 : loss=1.581 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 50.5%, forg= 22.9% <<<
>>> Test on task 25 : loss=1.945 | TAw acc= 87.9%, forg=  1.0%| TAg acc= 43.4%, forg= 22.2% <<<
>>> Test on task 26 : loss=1.639 | TAw acc= 94.0%, forg= -0.9%| TAg acc= 50.9%, forg= 22.4% <<<
>>> Test on task 27 : loss=1.788 | TAw acc= 96.3%, forg= -1.9%| TAg acc= 41.7%, forg= 29.6% <<<
>>> Test on task 28 : loss=1.234 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 69.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=6.169, TAw acc= 34.2% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=3.379, TAw acc= 61.6% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.065, TAw acc= 83.6% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.720, TAw acc= 87.7% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.482, TAw acc= 90.4% | *
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.475, TAw acc= 90.4% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.470, TAw acc= 90.4% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.465, TAw acc= 90.4% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.460, TAw acc= 90.4% | *
| Epoch   5, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.456, TAw acc= 90.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3950 train exemplars, time=  0.0s
3950
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.454 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 73.1%, forg= 11.5% <<<
>>> Test on task  1 : loss=1.269 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 77.3%, forg=  7.2% <<<
>>> Test on task  2 : loss=1.092 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.028 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.9%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.009 | TAw acc=100.0%, forg=  0.0%| TAg acc= 71.8%, forg=  7.8% <<<
>>> Test on task  5 : loss=1.304 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 70.5%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.346 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 78.2%, forg= -1.7% <<<
>>> Test on task  7 : loss=1.259 | TAw acc= 95.0%, forg=  3.0%| TAg acc= 75.2%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.291 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.4%, forg=  4.1% <<<
>>> Test on task  9 : loss=1.238 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 72.7%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.240 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 70.9%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.487 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.0%, forg= 10.9% <<<
>>> Test on task 12 : loss=1.296 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 62.5%, forg=  5.4% <<<
>>> Test on task 13 : loss=1.264 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 63.9%, forg=  5.7% <<<
>>> Test on task 14 : loss=1.471 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 76.8%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.195 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.5%, forg=  4.6% <<<
>>> Test on task 16 : loss=1.237 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 69.0%, forg= -0.9% <<<
>>> Test on task 17 : loss=0.992 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 76.8%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.450 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 61.8%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.377 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 61.1%, forg= 10.6% <<<
>>> Test on task 20 : loss=1.271 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 77.6%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.316 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 64.5%, forg=  1.8% <<<
>>> Test on task 22 : loss=1.342 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  8.3% <<<
>>> Test on task 23 : loss=1.331 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 58.9%, forg=  6.2% <<<
>>> Test on task 24 : loss=1.615 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 57.8%, forg= 15.6% <<<
>>> Test on task 25 : loss=1.952 | TAw acc= 85.9%, forg=  3.0%| TAg acc= 45.5%, forg= 20.2% <<<
>>> Test on task 26 : loss=1.583 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 49.1%, forg= 24.1% <<<
>>> Test on task 27 : loss=1.714 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 44.4%, forg= 26.9% <<<
>>> Test on task 28 : loss=1.546 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 60.2%, forg=  9.3% <<<
>>> Test on task 29 : loss=1.410 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=5.536, TAw acc= 38.6% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=2.996, TAw acc= 68.6% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.871, TAw acc= 85.7% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.593, TAw acc= 91.4% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.298, TAw acc= 92.9% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.298, TAw acc= 92.9% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.297, TAw acc= 92.9% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 92.9% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.295, TAw acc= 92.9% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.294, TAw acc= 92.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4070 train exemplars, time=  0.0s
4070
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.445 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 73.1%, forg= 11.5% <<<
>>> Test on task  1 : loss=1.331 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.2%, forg= 12.4% <<<
>>> Test on task  2 : loss=1.076 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.038 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 79.6%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.041 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.297 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 71.4%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.331 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 75.6%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.274 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.371 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 63.3%, forg= 13.3% <<<
>>> Test on task  9 : loss=1.214 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 68.6%, forg= 12.4% <<<
>>> Test on task 10 : loss=1.224 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 75.5%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.425 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.4%, forg=  7.6% <<<
>>> Test on task 12 : loss=1.431 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 55.4%, forg= 12.5% <<<
>>> Test on task 13 : loss=1.319 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 62.3%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.515 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 73.2%, forg=  3.6% <<<
>>> Test on task 15 : loss=1.167 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 67.6%, forg=  5.6% <<<
>>> Test on task 16 : loss=1.409 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 61.9%, forg=  7.1% <<<
>>> Test on task 17 : loss=0.986 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 81.8%, forg= -4.0% <<<
>>> Test on task 18 : loss=1.375 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 64.5%, forg= -0.9% <<<
>>> Test on task 19 : loss=1.294 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 62.8%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.211 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 80.2%, forg= -1.7% <<<
>>> Test on task 21 : loss=1.232 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 72.7%, forg= -6.4% <<<
>>> Test on task 22 : loss=1.254 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 65.1%, forg= 10.1% <<<
>>> Test on task 23 : loss=1.317 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 61.6%, forg=  3.6% <<<
>>> Test on task 24 : loss=1.574 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 56.0%, forg= 17.4% <<<
>>> Test on task 25 : loss=1.931 | TAw acc= 87.9%, forg=  1.0%| TAg acc= 48.5%, forg= 17.2% <<<
>>> Test on task 26 : loss=1.456 | TAw acc= 93.1%, forg=  0.9%| TAg acc= 57.8%, forg= 15.5% <<<
>>> Test on task 27 : loss=1.670 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 53.7%, forg= 17.6% <<<
>>> Test on task 28 : loss=1.416 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 54.6%, forg= 14.8% <<<
>>> Test on task 29 : loss=1.802 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 45.5%, forg= 28.7% <<<
>>> Test on task 30 : loss=1.305 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 69.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=4.934, TAw acc= 67.9% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=2.417, TAw acc= 82.1% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.360, TAw acc= 86.9% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.110, TAw acc= 92.9% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.974, TAw acc= 94.0% | *
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.968, TAw acc= 94.0% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=0.963, TAw acc= 95.2% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=0.959, TAw acc= 95.2% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.955, TAw acc= 95.2% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.952, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4190 train exemplars, time=  0.1s
4190
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.470 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 73.1%, forg= 11.5% <<<
>>> Test on task  1 : loss=1.281 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 76.3%, forg=  8.2% <<<
>>> Test on task  2 : loss=1.088 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task  3 : loss=1.041 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.9%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.017 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.296 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 72.4%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.335 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.282 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 66.3%, forg= 11.9% <<<
>>> Test on task  8 : loss=1.395 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 63.3%, forg= 13.3% <<<
>>> Test on task  9 : loss=1.287 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 65.3%, forg= 15.7% <<<
>>> Test on task 10 : loss=1.262 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 70.9%, forg=  5.5% <<<
>>> Test on task 11 : loss=1.495 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 58.8%, forg= 15.1% <<<
>>> Test on task 12 : loss=1.341 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 60.7%, forg=  7.1% <<<
>>> Test on task 13 : loss=1.243 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 68.0%, forg=  1.6% <<<
>>> Test on task 14 : loss=1.544 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 72.3%, forg=  4.5% <<<
>>> Test on task 15 : loss=1.183 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 71.3%, forg=  1.9% <<<
>>> Test on task 16 : loss=1.398 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 62.8%, forg=  6.2% <<<
>>> Test on task 17 : loss=1.002 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 76.8%, forg=  5.1% <<<
>>> Test on task 18 : loss=1.476 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 61.8%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.273 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 63.7%, forg=  8.0% <<<
>>> Test on task 20 : loss=1.291 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 77.6%, forg=  2.6% <<<
>>> Test on task 21 : loss=1.312 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 70.9%, forg=  1.8% <<<
>>> Test on task 22 : loss=1.287 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 23 : loss=1.312 | TAw acc= 88.4%, forg=  1.8%| TAg acc= 59.8%, forg=  5.4% <<<
>>> Test on task 24 : loss=1.421 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.2%, forg=  9.2% <<<
>>> Test on task 25 : loss=1.916 | TAw acc= 84.8%, forg=  4.0%| TAg acc= 51.5%, forg= 14.1% <<<
>>> Test on task 26 : loss=1.470 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 58.6%, forg= 14.7% <<<
>>> Test on task 27 : loss=1.589 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 55.6%, forg= 15.7% <<<
>>> Test on task 28 : loss=1.339 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 66.7%, forg=  2.8% <<<
>>> Test on task 29 : loss=1.626 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 53.5%, forg= 20.8% <<<
>>> Test on task 30 : loss=1.556 | TAw acc= 96.9%, forg= -4.1%| TAg acc= 55.1%, forg= 14.3% <<<
>>> Test on task 31 : loss=1.056 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 79.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=6.267, TAw acc= 54.5% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=2.839, TAw acc= 85.7% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.695, TAw acc= 92.2% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 98.7% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.158, TAw acc= 98.7% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.158, TAw acc= 98.7% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 98.7% |
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 98.7% |
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 98.7% |
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 98.7% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4310 train exemplars, time=  0.0s
4310
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.448 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 73.6%, forg= 11.0% <<<
>>> Test on task  1 : loss=1.526 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 64.9%, forg= 19.6% <<<
>>> Test on task  2 : loss=1.107 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.046 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 77.9%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.056 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.267 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 74.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.358 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 74.8%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.326 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 68.3%, forg=  9.9% <<<
>>> Test on task  8 : loss=1.468 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 59.2%, forg= 17.3% <<<
>>> Test on task  9 : loss=1.287 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.9%, forg= 14.0% <<<
>>> Test on task 10 : loss=1.243 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 74.5%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.459 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.0%, forg= 10.9% <<<
>>> Test on task 12 : loss=1.504 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 55.4%, forg= 12.5% <<<
>>> Test on task 13 : loss=1.303 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 60.7%, forg=  9.0% <<<
>>> Test on task 14 : loss=1.596 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 67.0%, forg=  9.8% <<<
>>> Test on task 15 : loss=1.245 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.5%, forg=  4.6% <<<
>>> Test on task 16 : loss=1.296 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.0%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.026 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 73.7%, forg=  8.1% <<<
>>> Test on task 18 : loss=1.416 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.2%, forg= -3.6% <<<
>>> Test on task 19 : loss=1.406 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 57.5%, forg= 14.2% <<<
>>> Test on task 20 : loss=1.225 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 78.4%, forg=  1.7% <<<
>>> Test on task 21 : loss=1.295 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 72.7%, forg=  0.0% <<<
>>> Test on task 22 : loss=1.317 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 62.4%, forg= 12.8% <<<
>>> Test on task 23 : loss=1.313 | TAw acc= 91.1%, forg= -0.9%| TAg acc= 59.8%, forg=  5.4% <<<
>>> Test on task 24 : loss=1.524 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 58.7%, forg= 14.7% <<<
>>> Test on task 25 : loss=1.903 | TAw acc= 86.9%, forg=  2.0%| TAg acc= 52.5%, forg= 13.1% <<<
>>> Test on task 26 : loss=1.399 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 64.7%, forg=  8.6% <<<
>>> Test on task 27 : loss=1.513 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.9%, forg=  7.4% <<<
>>> Test on task 28 : loss=1.287 | TAw acc= 94.4%, forg= -1.9%| TAg acc= 70.4%, forg= -0.9% <<<
>>> Test on task 29 : loss=1.620 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 55.4%, forg= 18.8% <<<
>>> Test on task 30 : loss=1.371 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 62.2%, forg=  7.1% <<<
>>> Test on task 31 : loss=1.314 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 65.2%, forg= 14.3% <<<
>>> Test on task 32 : loss=1.018 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=5.123, TAw acc= 62.0% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=2.542, TAw acc= 73.9% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.562, TAw acc= 87.0% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.214, TAw acc= 90.2% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.070, TAw acc= 91.3% | *
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.068, TAw acc= 91.3% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.067, TAw acc= 91.3% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.065, TAw acc= 91.3% | *
| Epoch   4, time=  4.1s | Train: skip eval | Valid: time=  0.3s loss=1.064, TAw acc= 91.3% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.063, TAw acc= 91.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4430 train exemplars, time=  0.0s
4430
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.525 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 68.7%, forg= 15.9% <<<
>>> Test on task  1 : loss=1.285 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 78.4%, forg=  6.2% <<<
>>> Test on task  2 : loss=1.122 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 80.6%, forg=  3.7% <<<
>>> Test on task  3 : loss=1.039 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.3%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.060 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.0%, forg= 12.6% <<<
>>> Test on task  5 : loss=1.316 | TAw acc= 95.2%, forg= -1.0%| TAg acc= 72.4%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.479 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.297 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 67.3%, forg= 10.9% <<<
>>> Test on task  8 : loss=1.407 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 62.2%, forg= 14.3% <<<
>>> Test on task  9 : loss=1.185 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 71.9%, forg=  9.1% <<<
>>> Test on task 10 : loss=1.243 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 77.3%, forg= -0.9% <<<
>>> Test on task 11 : loss=1.495 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.9%, forg= 10.1% <<<
>>> Test on task 12 : loss=1.376 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 60.7%, forg=  7.1% <<<
>>> Test on task 13 : loss=1.417 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 56.6%, forg= 13.1% <<<
>>> Test on task 14 : loss=1.543 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 73.2%, forg=  3.6% <<<
>>> Test on task 15 : loss=1.238 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.5%, forg=  4.6% <<<
>>> Test on task 16 : loss=1.370 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 64.6%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.022 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.8%, forg=  5.1% <<<
>>> Test on task 18 : loss=1.388 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.380 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 59.3%, forg= 12.4% <<<
>>> Test on task 20 : loss=1.188 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 79.3%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.329 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 66.4%, forg=  6.4% <<<
>>> Test on task 22 : loss=1.300 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 65.1%, forg= 10.1% <<<
>>> Test on task 23 : loss=1.336 | TAw acc= 90.2%, forg=  0.9%| TAg acc= 55.4%, forg=  9.8% <<<
>>> Test on task 24 : loss=1.443 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 62.4%, forg= 11.0% <<<
>>> Test on task 25 : loss=1.922 | TAw acc= 84.8%, forg=  4.0%| TAg acc= 49.5%, forg= 16.2% <<<
>>> Test on task 26 : loss=1.360 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 68.1%, forg=  5.2% <<<
>>> Test on task 27 : loss=1.500 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 61.1%, forg= 10.2% <<<
>>> Test on task 28 : loss=1.268 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 69.4%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.505 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 61.4%, forg= 12.9% <<<
>>> Test on task 30 : loss=1.405 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 60.2%, forg=  9.2% <<<
>>> Test on task 31 : loss=1.261 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 65.2%, forg= 14.3% <<<
>>> Test on task 32 : loss=1.403 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 61.0%, forg= 15.2% <<<
>>> Test on task 33 : loss=1.217 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 72.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=6.441, TAw acc= 34.7% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=3.279, TAw acc= 69.3% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.738, TAw acc=100.0% | *
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.158, TAw acc=100.0% | *
| Epoch   5, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.257, TAw acc=100.0% |
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc=100.0% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc=100.0% |
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.161, TAw acc=100.0% |
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.162, TAw acc=100.0% |
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.163, TAw acc=100.0% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4550 train exemplars, time=  0.0s
4550
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.475 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 76.4%, forg=  8.2% <<<
>>> Test on task  1 : loss=1.481 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 73.2%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.096 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.067 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.8%, forg=  8.0% <<<
>>> Test on task  4 : loss=1.166 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.0%, forg= 12.6% <<<
>>> Test on task  5 : loss=1.338 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 74.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.383 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 74.8%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.288 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 73.3%, forg=  5.0% <<<
>>> Test on task  8 : loss=1.367 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 63.3%, forg= 13.3% <<<
>>> Test on task  9 : loss=1.231 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 68.6%, forg= 12.4% <<<
>>> Test on task 10 : loss=1.228 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 75.5%, forg=  1.8% <<<
>>> Test on task 11 : loss=1.506 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 61.3%, forg= 12.6% <<<
>>> Test on task 12 : loss=1.459 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 52.7%, forg= 15.2% <<<
>>> Test on task 13 : loss=1.371 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 61.5%, forg=  8.2% <<<
>>> Test on task 14 : loss=1.556 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 71.4%, forg=  5.4% <<<
>>> Test on task 15 : loss=1.204 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 70.4%, forg=  2.8% <<<
>>> Test on task 16 : loss=1.503 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 60.2%, forg=  8.8% <<<
>>> Test on task 17 : loss=1.069 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 74.7%, forg=  7.1% <<<
>>> Test on task 18 : loss=1.446 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 62.7%, forg=  5.5% <<<
>>> Test on task 19 : loss=1.278 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 66.4%, forg=  5.3% <<<
>>> Test on task 20 : loss=1.161 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 79.3%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.397 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 68.2%, forg=  4.5% <<<
>>> Test on task 22 : loss=1.348 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 23 : loss=1.295 | TAw acc= 90.2%, forg=  0.9%| TAg acc= 59.8%, forg=  5.4% <<<
>>> Test on task 24 : loss=1.476 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 56.9%, forg= 16.5% <<<
>>> Test on task 25 : loss=1.973 | TAw acc= 85.9%, forg=  3.0%| TAg acc= 46.5%, forg= 19.2% <<<
>>> Test on task 26 : loss=1.435 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 61.2%, forg= 12.1% <<<
>>> Test on task 27 : loss=1.536 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 59.3%, forg= 12.0% <<<
>>> Test on task 28 : loss=1.264 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 71.3%, forg= -0.9% <<<
>>> Test on task 29 : loss=1.544 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 56.4%, forg= 17.8% <<<
>>> Test on task 30 : loss=1.319 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 63.3%, forg=  6.1% <<<
>>> Test on task 31 : loss=1.210 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.342 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 58.1%, forg= 18.1% <<<
>>> Test on task 33 : loss=1.729 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 47.2%, forg= 25.2% <<<
>>> Test on task 34 : loss=1.341 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 69.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=5.174, TAw acc= 57.0% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=2.653, TAw acc= 72.1% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.829, TAw acc= 81.4% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.3s loss=1.675, TAw acc= 87.2% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.524, TAw acc= 84.9% | *
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.523, TAw acc= 84.9% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.521, TAw acc= 84.9% | *
| Epoch   3, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.519, TAw acc= 86.0% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.517, TAw acc= 86.0% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.516, TAw acc= 86.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4670 train exemplars, time=  0.0s
4670
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.510 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 72.5%, forg= 12.1% <<<
>>> Test on task  1 : loss=1.491 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 73.2%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.127 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.039 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.4%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.098 | TAw acc=100.0%, forg=  0.0%| TAg acc= 69.9%, forg=  9.7% <<<
>>> Test on task  5 : loss=1.330 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 73.3%, forg=  1.9% <<<
>>> Test on task  6 : loss=1.395 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  7 : loss=1.318 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 68.3%, forg=  9.9% <<<
>>> Test on task  8 : loss=1.293 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 69.4%, forg=  7.1% <<<
>>> Test on task  9 : loss=1.254 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 68.6%, forg= 12.4% <<<
>>> Test on task 10 : loss=1.264 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 74.5%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.620 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 60.5%, forg= 13.4% <<<
>>> Test on task 12 : loss=1.456 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 59.8%, forg=  8.0% <<<
>>> Test on task 13 : loss=1.351 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 61.5%, forg=  8.2% <<<
>>> Test on task 14 : loss=1.609 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 70.5%, forg=  6.2% <<<
>>> Test on task 15 : loss=1.197 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 70.4%, forg=  2.8% <<<
>>> Test on task 16 : loss=1.424 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 65.5%, forg=  3.5% <<<
>>> Test on task 17 : loss=0.957 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 79.8%, forg=  2.0% <<<
>>> Test on task 18 : loss=1.512 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 62.7%, forg=  5.5% <<<
>>> Test on task 19 : loss=1.312 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 61.9%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.159 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 79.3%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.410 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 69.1%, forg=  3.6% <<<
>>> Test on task 22 : loss=1.341 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.3%, forg= 11.9% <<<
>>> Test on task 23 : loss=1.348 | TAw acc= 88.4%, forg=  2.7%| TAg acc= 58.9%, forg=  6.2% <<<
>>> Test on task 24 : loss=1.507 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 58.7%, forg= 14.7% <<<
>>> Test on task 25 : loss=2.088 | TAw acc= 87.9%, forg=  1.0%| TAg acc= 43.4%, forg= 22.2% <<<
>>> Test on task 26 : loss=1.544 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 50.9%, forg= 22.4% <<<
>>> Test on task 27 : loss=1.485 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.0%, forg=  8.3% <<<
>>> Test on task 28 : loss=1.197 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 71.3%, forg=  0.0% <<<
>>> Test on task 29 : loss=1.444 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 66.3%, forg=  7.9% <<<
>>> Test on task 30 : loss=1.302 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 63.3%, forg=  6.1% <<<
>>> Test on task 31 : loss=1.214 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.266 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 66.7%, forg=  9.5% <<<
>>> Test on task 33 : loss=1.567 | TAw acc= 87.8%, forg=  2.4%| TAg acc= 63.4%, forg=  8.9% <<<
>>> Test on task 34 : loss=1.842 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 45.1%, forg= 24.5% <<<
>>> Test on task 35 : loss=1.363 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 67.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=4.794, TAw acc= 58.8% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.970, TAw acc= 93.8% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.365, TAw acc= 95.0% | *
| Epoch   4, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.230, TAw acc= 93.8% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.171, TAw acc= 95.0% | *
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.164, TAw acc= 95.0% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.157, TAw acc= 95.0% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.151, TAw acc= 95.0% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.145, TAw acc= 95.0% | *
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.140, TAw acc= 95.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4790 train exemplars, time=  0.0s
4790
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.521 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 72.5%, forg= 12.1% <<<
>>> Test on task  1 : loss=1.415 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.2%, forg= 12.4% <<<
>>> Test on task  2 : loss=1.103 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 81.5%, forg=  2.8% <<<
>>> Test on task  3 : loss=1.065 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 77.9%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.098 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.0%, forg= 12.6% <<<
>>> Test on task  5 : loss=1.309 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 75.2%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.395 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 74.8%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.328 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 69.3%, forg=  8.9% <<<
>>> Test on task  8 : loss=1.377 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 66.3%, forg= 10.2% <<<
>>> Test on task  9 : loss=1.198 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 71.1%, forg=  9.9% <<<
>>> Test on task 10 : loss=1.290 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 73.6%, forg=  3.6% <<<
>>> Test on task 11 : loss=1.544 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 60.5%, forg= 13.4% <<<
>>> Test on task 12 : loss=1.400 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 55.4%, forg= 12.5% <<<
>>> Test on task 13 : loss=1.349 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 63.1%, forg=  6.6% <<<
>>> Test on task 14 : loss=1.573 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 69.6%, forg=  7.1% <<<
>>> Test on task 15 : loss=1.213 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 68.5%, forg=  4.6% <<<
>>> Test on task 16 : loss=1.428 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  2.7% <<<
>>> Test on task 17 : loss=0.936 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.8%, forg=  3.0% <<<
>>> Test on task 18 : loss=1.405 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.346 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 62.8%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.121 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 79.3%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.308 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 70.9%, forg=  1.8% <<<
>>> Test on task 22 : loss=1.382 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.3%, forg= 11.9% <<<
>>> Test on task 23 : loss=1.310 | TAw acc= 89.3%, forg=  1.8%| TAg acc= 62.5%, forg=  2.7% <<<
>>> Test on task 24 : loss=1.506 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 56.9%, forg= 16.5% <<<
>>> Test on task 25 : loss=2.037 | TAw acc= 86.9%, forg=  2.0%| TAg acc= 47.5%, forg= 18.2% <<<
>>> Test on task 26 : loss=1.491 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 56.9%, forg= 16.4% <<<
>>> Test on task 27 : loss=1.502 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 60.2%, forg= 11.1% <<<
>>> Test on task 28 : loss=1.204 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 70.4%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.424 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 63.4%, forg= 10.9% <<<
>>> Test on task 30 : loss=1.159 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 70.4%, forg= -1.0% <<<
>>> Test on task 31 : loss=1.170 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 68.8%, forg= 10.7% <<<
>>> Test on task 32 : loss=1.274 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 63.8%, forg= 12.4% <<<
>>> Test on task 33 : loss=1.580 | TAw acc= 89.4%, forg=  0.8%| TAg acc= 61.0%, forg= 11.4% <<<
>>> Test on task 34 : loss=1.723 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 45.1%, forg= 24.5% <<<
>>> Test on task 35 : loss=1.823 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 45.2%, forg= 22.6% <<<
>>> Test on task 36 : loss=1.166 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=4.855, TAw acc= 64.8% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=2.259, TAw acc= 82.4% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.587, TAw acc= 95.6% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.236, TAw acc= 98.9% | *
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.3s loss=1.145, TAw acc= 98.9% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.141, TAw acc= 98.9% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.137, TAw acc= 98.9% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.133, TAw acc= 98.9% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.129, TAw acc= 98.9% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.126, TAw acc= 98.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4910 train exemplars, time=  0.0s
4910
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.487 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 73.6%, forg= 11.0% <<<
>>> Test on task  1 : loss=1.404 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.2%, forg= 12.4% <<<
>>> Test on task  2 : loss=1.094 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task  3 : loss=1.052 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 81.4%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.121 | TAw acc=100.0%, forg=  0.0%| TAg acc= 66.0%, forg= 13.6% <<<
>>> Test on task  5 : loss=1.396 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 70.5%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.394 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 75.6%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.204 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 75.2%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.487 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 58.2%, forg= 18.4% <<<
>>> Test on task  9 : loss=1.249 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 69.4%, forg= 11.6% <<<
>>> Test on task 10 : loss=1.317 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 70.0%, forg=  7.3% <<<
>>> Test on task 11 : loss=1.624 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 60.5%, forg= 13.4% <<<
>>> Test on task 12 : loss=1.448 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 56.2%, forg= 11.6% <<<
>>> Test on task 13 : loss=1.400 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 62.3%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.603 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 73.2%, forg=  3.6% <<<
>>> Test on task 15 : loss=1.263 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 65.7%, forg=  7.4% <<<
>>> Test on task 16 : loss=1.409 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 61.9%, forg=  7.1% <<<
>>> Test on task 17 : loss=0.962 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.8%, forg=  6.1% <<<
>>> Test on task 18 : loss=1.505 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 65.5%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.361 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 61.1%, forg= 10.6% <<<
>>> Test on task 20 : loss=1.132 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 81.0%, forg= -0.9% <<<
>>> Test on task 21 : loss=1.414 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 62.7%, forg= 10.0% <<<
>>> Test on task 22 : loss=1.372 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.3%, forg= 11.9% <<<
>>> Test on task 23 : loss=1.259 | TAw acc= 88.4%, forg=  2.7%| TAg acc= 64.3%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.478 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 60.6%, forg= 12.8% <<<
>>> Test on task 25 : loss=2.009 | TAw acc= 87.9%, forg=  1.0%| TAg acc= 50.5%, forg= 15.2% <<<
>>> Test on task 26 : loss=1.379 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 58.6%, forg= 14.7% <<<
>>> Test on task 27 : loss=1.531 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.9%, forg=  7.4% <<<
>>> Test on task 28 : loss=1.202 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 70.4%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.439 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 62.4%, forg= 11.9% <<<
>>> Test on task 30 : loss=1.119 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 72.4%, forg= -2.0% <<<
>>> Test on task 31 : loss=1.227 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.6%, forg=  9.8% <<<
>>> Test on task 32 : loss=1.267 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 61.9%, forg= 14.3% <<<
>>> Test on task 33 : loss=1.594 | TAw acc= 89.4%, forg=  0.8%| TAg acc= 61.0%, forg= 11.4% <<<
>>> Test on task 34 : loss=1.750 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 47.1%, forg= 22.5% <<<
>>> Test on task 35 : loss=1.772 | TAw acc= 90.4%, forg=  2.6%| TAg acc= 49.6%, forg= 18.3% <<<
>>> Test on task 36 : loss=1.564 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 61.5%, forg= 12.8% <<<
>>> Test on task 37 : loss=1.337 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 64.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=5.571, TAw acc= 34.2% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=2.782, TAw acc= 71.2% | *
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.917, TAw acc= 80.8% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.712, TAw acc= 80.8% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.560, TAw acc= 86.3% | *
| Epoch   1, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.554, TAw acc= 86.3% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.549, TAw acc= 86.3% | *
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.545, TAw acc= 86.3% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.541, TAw acc= 86.3% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.537, TAw acc= 86.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5030 train exemplars, time=  0.1s
5030
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.568 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 69.8%, forg= 14.8% <<<
>>> Test on task  1 : loss=1.347 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 76.3%, forg=  8.2% <<<
>>> Test on task  2 : loss=1.119 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task  3 : loss=1.147 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 73.5%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.089 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.348 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 72.4%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.436 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.9%, forg=  4.2% <<<
>>> Test on task  7 : loss=1.288 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 72.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.430 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 70.4%, forg=  6.1% <<<
>>> Test on task  9 : loss=1.176 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 73.6%, forg=  7.4% <<<
>>> Test on task 10 : loss=1.343 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 70.9%, forg=  6.4% <<<
>>> Test on task 11 : loss=1.602 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 61.3%, forg= 12.6% <<<
>>> Test on task 12 : loss=1.364 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 59.8%, forg=  8.0% <<<
>>> Test on task 13 : loss=1.401 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 60.7%, forg=  9.0% <<<
>>> Test on task 14 : loss=1.652 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 70.5%, forg=  6.2% <<<
>>> Test on task 15 : loss=1.266 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 67.6%, forg=  5.6% <<<
>>> Test on task 16 : loss=1.399 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 63.7%, forg=  5.3% <<<
>>> Test on task 17 : loss=0.994 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 77.8%, forg=  4.0% <<<
>>> Test on task 18 : loss=1.450 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 63.6%, forg=  4.5% <<<
>>> Test on task 19 : loss=1.354 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 61.9%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.212 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 77.6%, forg=  3.4% <<<
>>> Test on task 21 : loss=1.379 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 65.5%, forg=  7.3% <<<
>>> Test on task 22 : loss=1.344 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 64.2%, forg= 11.0% <<<
>>> Test on task 23 : loss=1.282 | TAw acc= 90.2%, forg=  0.9%| TAg acc= 58.9%, forg=  6.2% <<<
>>> Test on task 24 : loss=1.513 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 60.6%, forg= 12.8% <<<
>>> Test on task 25 : loss=1.991 | TAw acc= 88.9%, forg=  0.0%| TAg acc= 52.5%, forg= 13.1% <<<
>>> Test on task 26 : loss=1.428 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 57.8%, forg= 15.5% <<<
>>> Test on task 27 : loss=1.467 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.0%, forg=  8.3% <<<
>>> Test on task 28 : loss=1.226 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 71.3%, forg=  0.0% <<<
>>> Test on task 29 : loss=1.366 | TAw acc= 91.1%, forg=  1.0%| TAg acc= 61.4%, forg= 12.9% <<<
>>> Test on task 30 : loss=1.085 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 74.5%, forg= -2.0% <<<
>>> Test on task 31 : loss=1.163 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.214 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 65.7%, forg= 10.5% <<<
>>> Test on task 33 : loss=1.536 | TAw acc= 89.4%, forg=  0.8%| TAg acc= 62.6%, forg=  9.8% <<<
>>> Test on task 34 : loss=1.718 | TAw acc= 96.1%, forg=  2.0%| TAg acc= 50.0%, forg= 19.6% <<<
>>> Test on task 35 : loss=1.849 | TAw acc= 90.4%, forg=  2.6%| TAg acc= 47.0%, forg= 20.9% <<<
>>> Test on task 36 : loss=1.391 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 64.2%, forg= 10.1% <<<
>>> Test on task 37 : loss=1.685 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 53.7%, forg= 10.7% <<<
>>> Test on task 38 : loss=1.422 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 71.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=4.493, TAw acc= 53.4% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=2.230, TAw acc= 87.5% | *
| Epoch   3, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.342, TAw acc= 97.7% | *
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.203, TAw acc= 97.7% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.142, TAw acc= 97.7% | *
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.135, TAw acc= 97.7% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 97.7% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.124, TAw acc= 97.7% | *
| Epoch   4, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.120, TAw acc= 97.7% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.115, TAw acc= 97.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5150 train exemplars, time=  0.0s
5150
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.603 | TAw acc= 87.4%, forg=  1.6%| TAg acc= 67.6%, forg= 17.0% <<<
>>> Test on task  1 : loss=1.406 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.2%, forg= 12.4% <<<
>>> Test on task  2 : loss=1.135 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.182 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.9%, forg= 16.8% <<<
>>> Test on task  4 : loss=1.081 | TAw acc=100.0%, forg=  0.0%| TAg acc= 66.0%, forg= 13.6% <<<
>>> Test on task  5 : loss=1.375 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 73.3%, forg=  1.9% <<<
>>> Test on task  6 : loss=1.464 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 71.4%, forg=  6.7% <<<
>>> Test on task  7 : loss=1.253 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 74.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.528 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 61.2%, forg= 15.3% <<<
>>> Test on task  9 : loss=1.296 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.1%, forg= 14.9% <<<
>>> Test on task 10 : loss=1.294 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 76.4%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.548 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 60.5%, forg= 13.4% <<<
>>> Test on task 12 : loss=1.389 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 65.2%, forg=  2.7% <<<
>>> Test on task 13 : loss=1.335 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 65.6%, forg=  4.1% <<<
>>> Test on task 14 : loss=1.657 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 69.6%, forg=  7.1% <<<
>>> Test on task 15 : loss=1.359 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 63.0%, forg= 10.2% <<<
>>> Test on task 16 : loss=1.402 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  2.7% <<<
>>> Test on task 17 : loss=0.927 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.8%, forg=  3.0% <<<
>>> Test on task 18 : loss=1.504 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 64.5%, forg=  3.6% <<<
>>> Test on task 19 : loss=1.401 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 62.8%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.173 | TAw acc= 94.8%, forg=  2.6%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.423 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 65.5%, forg=  7.3% <<<
>>> Test on task 22 : loss=1.282 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 64.2%, forg= 11.0% <<<
>>> Test on task 23 : loss=1.399 | TAw acc= 89.3%, forg=  1.8%| TAg acc= 58.0%, forg=  7.1% <<<
>>> Test on task 24 : loss=1.501 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 61.5%, forg= 11.9% <<<
>>> Test on task 25 : loss=1.935 | TAw acc= 85.9%, forg=  3.0%| TAg acc= 58.6%, forg=  7.1% <<<
>>> Test on task 26 : loss=1.300 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 69.8%, forg=  3.4% <<<
>>> Test on task 27 : loss=1.514 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.9%, forg=  7.4% <<<
>>> Test on task 28 : loss=1.165 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.2%, forg= -0.9% <<<
>>> Test on task 29 : loss=1.446 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 62.4%, forg= 11.9% <<<
>>> Test on task 30 : loss=1.176 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 70.4%, forg=  4.1% <<<
>>> Test on task 31 : loss=1.143 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.225 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 69.5%, forg=  6.7% <<<
>>> Test on task 33 : loss=1.512 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 67.5%, forg=  4.9% <<<
>>> Test on task 34 : loss=1.706 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 47.1%, forg= 22.5% <<<
>>> Test on task 35 : loss=1.655 | TAw acc= 90.4%, forg=  2.6%| TAg acc= 53.0%, forg= 14.8% <<<
>>> Test on task 36 : loss=1.376 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.1%, forg=  9.2% <<<
>>> Test on task 37 : loss=1.761 | TAw acc= 95.9%, forg= -0.8%| TAg acc= 51.2%, forg= 13.2% <<<
>>> Test on task 38 : loss=1.714 | TAw acc= 92.1%, forg= -2.0%| TAg acc= 56.4%, forg= 14.9% <<<
>>> Test on task 39 : loss=1.326 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 72.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=4.712, TAw acc= 64.1% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=2.121, TAw acc= 81.5% | *
| Epoch   3, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.538, TAw acc= 84.8% | *
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.195, TAw acc= 95.7% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.152, TAw acc= 96.7% | *
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.142, TAw acc= 96.7% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.134, TAw acc= 96.7% | *
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.126, TAw acc= 96.7% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.120, TAw acc= 96.7% | *
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.114, TAw acc= 95.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5270 train exemplars, time=  0.0s
5270
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.593 | TAw acc= 87.9%, forg=  1.1%| TAg acc= 72.5%, forg= 12.1% <<<
>>> Test on task  1 : loss=1.466 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 73.2%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.150 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 79.6%, forg=  4.6% <<<
>>> Test on task  3 : loss=1.137 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 77.0%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.084 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.476 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 72.4%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.400 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 78.2%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.334 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 67.3%, forg= 10.9% <<<
>>> Test on task  8 : loss=1.578 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 55.1%, forg= 21.4% <<<
>>> Test on task  9 : loss=1.205 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 72.7%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.316 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 76.4%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.629 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 59.7%, forg= 14.3% <<<
>>> Test on task 12 : loss=1.482 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 58.0%, forg=  9.8% <<<
>>> Test on task 13 : loss=1.453 | TAw acc= 95.1%, forg=  0.8%| TAg acc= 62.3%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.690 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 71.4%, forg=  5.4% <<<
>>> Test on task 15 : loss=1.327 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 59.3%, forg= 13.9% <<<
>>> Test on task 16 : loss=1.455 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 65.5%, forg=  3.5% <<<
>>> Test on task 17 : loss=1.016 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 75.8%, forg=  6.1% <<<
>>> Test on task 18 : loss=1.568 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 65.5%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.472 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 59.3%, forg= 12.4% <<<
>>> Test on task 20 : loss=1.161 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task 21 : loss=1.423 | TAw acc= 92.7%, forg=  1.8%| TAg acc= 64.5%, forg=  8.2% <<<
>>> Test on task 22 : loss=1.382 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.3%, forg= 11.9% <<<
>>> Test on task 23 : loss=1.366 | TAw acc= 88.4%, forg=  2.7%| TAg acc= 58.9%, forg=  6.2% <<<
>>> Test on task 24 : loss=1.483 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 58.7%, forg= 14.7% <<<
>>> Test on task 25 : loss=1.905 | TAw acc= 89.9%, forg= -1.0%| TAg acc= 56.6%, forg=  9.1% <<<
>>> Test on task 26 : loss=1.330 | TAw acc= 93.1%, forg=  0.9%| TAg acc= 69.0%, forg=  4.3% <<<
>>> Test on task 27 : loss=1.548 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.9%, forg=  7.4% <<<
>>> Test on task 28 : loss=1.181 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 73.1%, forg= -0.9% <<<
>>> Test on task 29 : loss=1.441 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 61.4%, forg= 12.9% <<<
>>> Test on task 30 : loss=1.086 | TAw acc= 95.9%, forg=  1.0%| TAg acc= 73.5%, forg=  1.0% <<<
>>> Test on task 31 : loss=1.134 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.3%, forg=  7.1% <<<
>>> Test on task 32 : loss=1.221 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 68.6%, forg=  7.6% <<<
>>> Test on task 33 : loss=1.533 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 62.6%, forg=  9.8% <<<
>>> Test on task 34 : loss=1.618 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 56.9%, forg= 12.7% <<<
>>> Test on task 35 : loss=1.675 | TAw acc= 90.4%, forg=  2.6%| TAg acc= 53.0%, forg= 14.8% <<<
>>> Test on task 36 : loss=1.363 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.1%, forg=  9.2% <<<
>>> Test on task 37 : loss=1.741 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 56.2%, forg=  8.3% <<<
>>> Test on task 38 : loss=1.574 | TAw acc= 93.1%, forg= -1.0%| TAg acc= 62.4%, forg=  8.9% <<<
>>> Test on task 39 : loss=1.764 | TAw acc= 94.9%, forg= -0.9%| TAg acc= 55.6%, forg= 17.1% <<<
>>> Test on task 40 : loss=1.295 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 69.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=5.453, TAw acc= 57.0% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=2.101, TAw acc= 82.6% | *
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.349, TAw acc= 96.5% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 95.3% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.155, TAw acc= 96.5% | *
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.148, TAw acc= 97.7% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.143, TAw acc= 97.7% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.138, TAw acc= 97.7% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.134, TAw acc= 97.7% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.130, TAw acc= 97.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5390 train exemplars, time=  0.0s
5390
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.609 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 69.2%, forg= 15.4% <<<
>>> Test on task  1 : loss=1.444 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.2%, forg= 12.4% <<<
>>> Test on task  2 : loss=1.124 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 80.6%, forg=  3.7% <<<
>>> Test on task  3 : loss=1.225 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.2%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.117 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.0%, forg= 12.6% <<<
>>> Test on task  5 : loss=1.422 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 74.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.474 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 73.9%, forg=  4.2% <<<
>>> Test on task  7 : loss=1.274 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 70.3%, forg=  7.9% <<<
>>> Test on task  8 : loss=1.509 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 64.3%, forg= 12.2% <<<
>>> Test on task  9 : loss=1.267 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.9%, forg= 14.0% <<<
>>> Test on task 10 : loss=1.354 | TAw acc= 92.7%, forg=  2.7%| TAg acc= 70.9%, forg=  6.4% <<<
>>> Test on task 11 : loss=1.580 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 61.3%, forg= 12.6% <<<
>>> Test on task 12 : loss=1.440 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 60.7%, forg=  7.1% <<<
>>> Test on task 13 : loss=1.401 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 61.5%, forg=  8.2% <<<
>>> Test on task 14 : loss=1.694 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 71.4%, forg=  5.4% <<<
>>> Test on task 15 : loss=1.528 | TAw acc= 92.6%, forg=  2.8%| TAg acc= 50.0%, forg= 23.1% <<<
>>> Test on task 16 : loss=1.475 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 65.5%, forg=  3.5% <<<
>>> Test on task 17 : loss=0.949 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.8%, forg=  5.1% <<<
>>> Test on task 18 : loss=1.533 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 65.5%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.408 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 64.6%, forg=  7.1% <<<
>>> Test on task 20 : loss=1.151 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.344 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 67.3%, forg=  5.5% <<<
>>> Test on task 22 : loss=1.462 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 55.0%, forg= 20.2% <<<
>>> Test on task 23 : loss=1.290 | TAw acc= 90.2%, forg=  0.9%| TAg acc= 62.5%, forg=  2.7% <<<
>>> Test on task 24 : loss=1.520 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 57.8%, forg= 15.6% <<<
>>> Test on task 25 : loss=1.944 | TAw acc= 87.9%, forg=  2.0%| TAg acc= 50.5%, forg= 15.2% <<<
>>> Test on task 26 : loss=1.466 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 60.3%, forg= 12.9% <<<
>>> Test on task 27 : loss=1.452 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.0%, forg=  8.3% <<<
>>> Test on task 28 : loss=1.109 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 75.9%, forg= -2.8% <<<
>>> Test on task 29 : loss=1.411 | TAw acc= 93.1%, forg= -1.0%| TAg acc= 63.4%, forg= 10.9% <<<
>>> Test on task 30 : loss=1.126 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 68.4%, forg=  6.1% <<<
>>> Test on task 31 : loss=1.133 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.3%, forg=  7.1% <<<
>>> Test on task 32 : loss=1.281 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 62.9%, forg= 13.3% <<<
>>> Test on task 33 : loss=1.542 | TAw acc= 89.4%, forg=  0.8%| TAg acc= 65.9%, forg=  6.5% <<<
>>> Test on task 34 : loss=1.608 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 55.9%, forg= 13.7% <<<
>>> Test on task 35 : loss=1.680 | TAw acc= 87.8%, forg=  5.2%| TAg acc= 53.0%, forg= 14.8% <<<
>>> Test on task 36 : loss=1.369 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.0%, forg=  7.3% <<<
>>> Test on task 37 : loss=1.712 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 57.9%, forg=  6.6% <<<
>>> Test on task 38 : loss=1.479 | TAw acc= 92.1%, forg=  1.0%| TAg acc= 63.4%, forg=  7.9% <<<
>>> Test on task 39 : loss=1.659 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 58.1%, forg= 14.5% <<<
>>> Test on task 40 : loss=1.841 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 45.5%, forg= 23.6% <<<
>>> Test on task 41 : loss=1.120 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 75.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=5.480, TAw acc= 50.6% | *
| Epoch   2, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=2.616, TAw acc= 92.2% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.592, TAw acc= 96.1% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 96.1% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 97.4% | *
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.243, TAw acc= 97.4% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.237, TAw acc= 97.4% | *
| Epoch   3, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.232, TAw acc= 97.4% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.228, TAw acc= 97.4% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.224, TAw acc= 97.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5510 train exemplars, time=  0.0s
5510
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.596 | TAw acc= 86.8%, forg=  2.2%| TAg acc= 69.2%, forg= 15.4% <<<
>>> Test on task  1 : loss=1.485 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 71.1%, forg= 13.4% <<<
>>> Test on task  2 : loss=1.135 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.7%, forg=  5.6% <<<
>>> Test on task  3 : loss=1.218 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.2%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.077 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.392 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 74.3%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.452 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 74.8%, forg=  3.4% <<<
>>> Test on task  7 : loss=1.331 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 66.3%, forg= 11.9% <<<
>>> Test on task  8 : loss=1.483 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 71.4%, forg=  5.1% <<<
>>> Test on task  9 : loss=1.278 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 69.4%, forg= 11.6% <<<
>>> Test on task 10 : loss=1.336 | TAw acc= 92.7%, forg=  2.7%| TAg acc= 76.4%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.633 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 60.5%, forg= 13.4% <<<
>>> Test on task 12 : loss=1.483 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 55.4%, forg= 12.5% <<<
>>> Test on task 13 : loss=1.421 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 59.8%, forg=  9.8% <<<
>>> Test on task 14 : loss=1.661 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 70.5%, forg=  6.2% <<<
>>> Test on task 15 : loss=1.338 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 65.7%, forg=  7.4% <<<
>>> Test on task 16 : loss=1.499 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 61.9%, forg=  7.1% <<<
>>> Test on task 17 : loss=0.918 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.8%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.576 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 62.7%, forg=  5.5% <<<
>>> Test on task 19 : loss=1.416 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 61.9%, forg=  9.7% <<<
>>> Test on task 20 : loss=1.171 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task 21 : loss=1.360 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 66.4%, forg=  6.4% <<<
>>> Test on task 22 : loss=1.420 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 60.6%, forg= 14.7% <<<
>>> Test on task 23 : loss=1.308 | TAw acc= 88.4%, forg=  2.7%| TAg acc= 65.2%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.529 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 61.5%, forg= 11.9% <<<
>>> Test on task 25 : loss=1.952 | TAw acc= 86.9%, forg=  3.0%| TAg acc= 52.5%, forg= 13.1% <<<
>>> Test on task 26 : loss=1.368 | TAw acc= 93.1%, forg=  0.9%| TAg acc= 61.2%, forg= 12.1% <<<
>>> Test on task 27 : loss=1.536 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 62.0%, forg=  9.3% <<<
>>> Test on task 28 : loss=1.143 | TAw acc= 95.4%, forg= -0.9%| TAg acc= 75.0%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.423 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 61.4%, forg= 12.9% <<<
>>> Test on task 30 : loss=1.073 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 72.4%, forg=  2.0% <<<
>>> Test on task 31 : loss=1.109 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg=  8.0% <<<
>>> Test on task 32 : loss=1.237 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 67.6%, forg=  8.6% <<<
>>> Test on task 33 : loss=1.542 | TAw acc= 91.1%, forg= -0.8%| TAg acc= 64.2%, forg=  8.1% <<<
>>> Test on task 34 : loss=1.613 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 56.9%, forg= 12.7% <<<
>>> Test on task 35 : loss=1.753 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 52.2%, forg= 15.7% <<<
>>> Test on task 36 : loss=1.380 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.1%, forg=  9.2% <<<
>>> Test on task 37 : loss=1.672 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 56.2%, forg=  8.3% <<<
>>> Test on task 38 : loss=1.473 | TAw acc= 94.1%, forg= -1.0%| TAg acc= 60.4%, forg= 10.9% <<<
>>> Test on task 39 : loss=1.730 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 58.1%, forg= 14.5% <<<
>>> Test on task 40 : loss=1.715 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 44.7%, forg= 24.4% <<<
>>> Test on task 41 : loss=1.447 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 61.2%, forg= 14.7% <<<
>>> Test on task 42 : loss=1.121 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 62.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=4.591, TAw acc= 59.1% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=2.539, TAw acc= 81.8% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.743, TAw acc= 92.0% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.352, TAw acc= 90.9% | *
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.316, TAw acc= 92.0% | *
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.304, TAw acc= 92.0% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.295, TAw acc= 92.0% | *
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc= 92.0% | *
| Epoch   4, time=  5.9s | Train: skip eval | Valid: time=  0.3s loss=1.279, TAw acc= 92.0% | *
| Epoch   5, time=  7.6s | Train: skip eval | Valid: time=  0.2s loss=1.272, TAw acc= 92.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5630 train exemplars, time=  0.1s
5630
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.618 | TAw acc= 88.5%, forg=  0.5%| TAg acc= 71.4%, forg= 13.2% <<<
>>> Test on task  1 : loss=1.406 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 72.2%, forg= 12.4% <<<
>>> Test on task  2 : loss=1.183 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.8%, forg=  6.5% <<<
>>> Test on task  3 : loss=1.224 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 77.0%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.145 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.0%, forg= 12.6% <<<
>>> Test on task  5 : loss=1.396 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 73.3%, forg=  1.9% <<<
>>> Test on task  6 : loss=1.421 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 76.5%, forg=  1.7% <<<
>>> Test on task  7 : loss=1.303 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 69.3%, forg=  8.9% <<<
>>> Test on task  8 : loss=1.562 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 61.2%, forg= 15.3% <<<
>>> Test on task  9 : loss=1.218 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 68.6%, forg= 12.4% <<<
>>> Test on task 10 : loss=1.371 | TAw acc= 92.7%, forg=  2.7%| TAg acc= 73.6%, forg=  3.6% <<<
>>> Test on task 11 : loss=1.645 | TAw acc= 95.8%, forg=  1.7%| TAg acc= 60.5%, forg= 13.4% <<<
>>> Test on task 12 : loss=1.530 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 58.0%, forg=  9.8% <<<
>>> Test on task 13 : loss=1.392 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 62.3%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.657 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 72.3%, forg=  4.5% <<<
>>> Test on task 15 : loss=1.439 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 56.5%, forg= 16.7% <<<
>>> Test on task 16 : loss=1.531 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 62.8%, forg=  6.2% <<<
>>> Test on task 17 : loss=0.916 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 80.8%, forg=  2.0% <<<
>>> Test on task 18 : loss=1.568 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 63.6%, forg=  4.5% <<<
>>> Test on task 19 : loss=1.441 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 63.7%, forg=  8.0% <<<
>>> Test on task 20 : loss=1.159 | TAw acc= 94.0%, forg=  3.4%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task 21 : loss=1.402 | TAw acc= 93.6%, forg=  0.9%| TAg acc= 65.5%, forg=  7.3% <<<
>>> Test on task 22 : loss=1.404 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.1%, forg=  9.2% <<<
>>> Test on task 23 : loss=1.405 | TAw acc= 89.3%, forg=  1.8%| TAg acc= 58.0%, forg=  7.1% <<<
>>> Test on task 24 : loss=1.499 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 60.6%, forg= 12.8% <<<
>>> Test on task 25 : loss=2.100 | TAw acc= 87.9%, forg=  2.0%| TAg acc= 50.5%, forg= 15.2% <<<
>>> Test on task 26 : loss=1.363 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 64.7%, forg=  8.6% <<<
>>> Test on task 27 : loss=1.561 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 63.0%, forg=  8.3% <<<
>>> Test on task 28 : loss=1.165 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 74.1%, forg=  1.9% <<<
>>> Test on task 29 : loss=1.367 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 66.3%, forg=  7.9% <<<
>>> Test on task 30 : loss=1.108 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 71.4%, forg=  3.1% <<<
>>> Test on task 31 : loss=1.140 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.1%, forg=  5.4% <<<
>>> Test on task 32 : loss=1.216 | TAw acc= 96.2%, forg=  2.9%| TAg acc= 66.7%, forg=  9.5% <<<
>>> Test on task 33 : loss=1.557 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 67.5%, forg=  4.9% <<<
>>> Test on task 34 : loss=1.617 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 58.8%, forg= 10.8% <<<
>>> Test on task 35 : loss=1.744 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 53.0%, forg= 14.8% <<<
>>> Test on task 36 : loss=1.389 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 62.4%, forg= 11.9% <<<
>>> Test on task 37 : loss=1.654 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 58.7%, forg=  5.8% <<<
>>> Test on task 38 : loss=1.479 | TAw acc= 92.1%, forg=  2.0%| TAg acc= 63.4%, forg=  7.9% <<<
>>> Test on task 39 : loss=1.672 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 65.0%, forg=  7.7% <<<
>>> Test on task 40 : loss=1.674 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 47.2%, forg= 22.0% <<<
>>> Test on task 41 : loss=1.477 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 61.2%, forg= 14.7% <<<
>>> Test on task 42 : loss=1.527 | TAw acc= 94.3%, forg=  0.0%| TAg acc= 51.4%, forg= 11.4% <<<
>>> Test on task 43 : loss=1.326 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 64.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_5/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 69.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.2% 
	 84.6%  86.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 85.6% 
	 79.1%  95.9%  80.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 85.2% 
	 84.1%  95.9%  93.5%  80.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 88.5% 
	 86.3%  97.9%  94.4%  91.2%  83.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 90.7% 
	 87.4%  96.9%  97.2%  93.8%  98.1%  81.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.4% 
	 86.8%  97.9%  97.2%  94.7%  99.0%  89.5%  91.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.8% 
	 87.4%  97.9%  97.2%  94.7%  99.0%  92.4%  92.4%  83.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.0% 
	 87.9%  97.9%  97.2%  94.7%  99.0%  92.4%  95.0%  92.1%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 87.4%  97.9%  97.2%  94.7%  99.0%  93.3%  95.8%  93.1%  98.0%  91.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 85.7%  97.9%  97.2%  94.7%  99.0%  93.3%  95.0%  92.1%  98.0%  99.2%  84.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.2% 
	 89.0%  97.9%  96.3%  94.7%  99.0%  93.3%  94.1%  93.1%  98.0%  99.2%  93.6%  93.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.9%  97.9%  97.2%  94.7%  99.0%  93.3%  95.0%  98.0%  98.0%  99.2%  93.6%  93.3%  93.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 87.9%  97.9%  97.2%  94.7%  99.0%  93.3%  95.8%  97.0%  98.0%  98.3%  94.5%  95.0%  92.9%  90.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.9%  97.9%  97.2%  94.7%  99.0%  93.3%  95.8%  97.0%  96.9%  98.3%  94.5%  95.0%  93.8%  91.8%  87.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 87.9%  97.9%  96.3%  94.7%  99.0%  93.3%  95.8%  98.0%  96.9%  98.3%  94.5%  94.1%  92.9%  94.3%  94.6%  90.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 87.9%  97.9%  96.3%  94.7%  99.0%  93.3%  95.8%  96.0%  98.0%  98.3%  93.6%  95.0%  92.0%  95.1%  93.8%  95.4%  92.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 89.0%  97.9%  95.4%  96.5%  99.0%  93.3%  95.8%  98.0%  98.0%  98.3%  94.5%  95.0%  93.8%  94.3%  93.8%  94.4%  97.3%  93.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 89.0%  97.9%  97.2%  95.6%  99.0%  93.3%  95.8%  97.0%  98.0%  98.3%  95.5%  95.0%  94.6%  94.3%  93.8%  94.4%  96.5%  94.9%  94.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 88.5%  97.9%  96.3%  94.7%  99.0%  93.3%  95.8%  98.0%  98.0%  98.3%  95.5%  95.0%  95.5%  94.3%  94.6%  93.5%  94.7%  96.0%  98.2%  92.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 88.5%  97.9%  96.3%  95.6%  99.0%  93.3%  95.8%  97.0%  96.9%  98.3%  93.6%  95.0%  94.6%  94.3%  94.6%  92.6%  96.5%  96.0%  96.4%  95.6%  96.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 89.0%  97.9%  97.2%  96.5% 100.0%  93.3%  95.8%  98.0%  98.0%  98.3%  94.5%  95.0%  94.6%  94.3%  94.6%  92.6%  96.5%  97.0%  98.2%  94.7%  97.4%  93.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 88.5%  97.9%  97.2%  95.6% 100.0%  93.3%  95.8%  98.0%  96.9%  98.3%  94.5%  95.0%  94.6%  95.9%  94.6%  93.5%  95.6%  97.0%  98.2%  95.6%  97.4%  90.9%  97.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 88.5%  97.9%  97.2%  96.5% 100.0%  93.3%  95.8%  97.0%  96.9%  98.3%  95.5%  96.6%  96.4%  95.9%  94.6%  93.5%  95.6%  97.0%  98.2%  96.5%  95.7%  94.5%  96.3%  86.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 88.5%  97.9%  97.2%  97.3% 100.0%  93.3%  95.8%  97.0%  96.9%  98.3%  94.5%  97.5%  94.6%  95.9%  94.6%  92.6%  96.5%  97.0%  98.2%  95.6%  96.6%  94.5%  96.3%  90.2%  95.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 87.9%  97.9%  97.2%  97.3% 100.0%  93.3%  95.8%  97.0%  96.9%  98.3%  93.6%  97.5%  96.4%  95.1%  94.6%  93.5%  96.5%  97.0%  98.2%  97.3%  96.6%  93.6%  96.3%  89.3%  93.6%  85.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 87.4%  97.9%  97.2%  97.3% 100.0%  93.3%  95.8%  97.0%  96.9%  98.3%  93.6%  96.6%  96.4%  95.9%  94.6%  94.4%  96.5%  97.0%  98.2%  97.3%  94.8%  93.6%  96.3%  89.3%  95.4%  85.9%  91.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 86.3%  97.9%  97.2%  97.3% 100.0%  94.3%  95.8%  97.0%  96.9%  98.3%  93.6%  97.5%  95.5%  95.1%  94.6%  94.4%  96.5%  97.0%  98.2%  97.3%  94.0%  94.5%  96.3%  90.2%  95.4%  88.9%  93.1%  94.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 87.9%  96.9%  97.2%  97.3% 100.0%  94.3%  95.8%  97.0%  96.9%  98.3%  93.6%  97.5%  95.5%  95.1%  94.6%  94.4%  97.3%  97.0%  98.2%  97.3%  95.7%  94.5%  97.2%  90.2%  95.4%  87.9%  94.0%  96.3%  92.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 87.9%  96.9%  97.2%  97.3% 100.0%  93.3%  95.8%  95.0%  96.9%  98.3%  93.6%  97.5%  95.5%  95.1%  94.6%  94.4%  97.3%  97.0%  98.2%  97.3%  94.0%  93.6%  97.2%  90.2%  95.4%  85.9%  94.0%  95.4%  92.6%  92.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 87.9%  96.9%  97.2%  97.3% 100.0%  94.3%  95.8%  96.0%  96.9%  98.3%  93.6%  97.5%  95.5%  95.9%  94.6%  94.4%  98.2%  97.0%  98.2%  97.3%  94.0%  94.5%  97.2%  90.2%  95.4%  87.9%  93.1%  96.3%  92.6%  92.1%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.9%  96.9%  97.2%  97.3% 100.0%  94.3%  95.8%  96.0%  96.9%  98.3%  93.6%  97.5%  95.5%  95.9%  93.8%  94.4%  98.2%  97.0%  98.2%  97.3%  94.8%  94.5%  97.2%  88.4%  95.4%  84.8%  94.0%  96.3%  92.6%  91.1%  96.9%  97.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  93.3%  95.8%  96.0%  96.9%  99.2%  94.5%  97.5%  95.5%  95.1%  94.6%  94.4%  97.3%  98.0%  98.2%  97.3%  94.0%  94.5%  97.2%  91.1%  95.4%  86.9%  94.0%  96.3%  94.4%  90.1%  96.9%  98.2%  99.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 88.5%  96.9%  97.2%  98.2% 100.0%  95.2%  95.8%  96.0%  96.9%  98.3%  94.5%  97.5%  95.5%  95.9%  93.8%  94.4%  96.5%  98.0%  97.3%  97.3%  94.8%  93.6%  97.2%  90.2%  95.4%  84.8%  94.0%  96.3%  93.5%  92.1%  95.9%  98.2%  97.1%  90.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  94.3%  95.8%  96.0%  96.9%  99.2%  93.6%  97.5%  95.5%  95.9%  92.9%  94.4%  97.3%  98.0%  97.3%  97.3%  95.7%  93.6%  97.2%  90.2%  95.4%  85.9%  94.0%  96.3%  93.5%  91.1%  96.9%  98.2%  98.1%  90.2%  97.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  94.3%  95.8%  96.0%  96.9%  99.2%  93.6%  96.6%  95.5%  95.9%  92.9%  94.4%  97.3%  98.0%  97.3%  96.5%  94.8%  93.6%  97.2%  88.4%  95.4%  87.9%  94.0%  96.3%  93.5%  91.1%  96.9%  98.2%  97.1%  87.8%  97.1%  93.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  94.3%  95.8%  96.0%  96.9%  99.2%  93.6%  96.6%  95.5%  95.9%  92.9%  94.4%  97.3%  98.0%  97.3%  96.5%  94.0%  94.5%  97.2%  89.3%  95.4%  86.9%  94.0%  96.3%  94.4%  90.1%  95.9%  98.2%  97.1%  89.4%  98.0%  91.3%  95.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  95.2%  95.8%  97.0%  95.9%  99.2%  93.6%  96.6%  95.5%  95.9%  92.9%  93.5%  97.3%  98.0%  97.3%  96.5%  94.0%  93.6%  97.2%  88.4%  95.4%  87.9%  94.0%  96.3%  94.4%  91.1%  96.9%  98.2%  97.1%  89.4%  98.0%  90.4%  96.3%  95.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  95.2%  95.8%  96.0%  96.9%  99.2%  93.6%  96.6%  95.5%  95.9%  92.9%  93.5%  97.3%  98.0%  97.3%  96.5%  94.8%  93.6%  97.2%  90.2%  95.4%  88.9%  94.0%  96.3%  94.4%  91.1%  96.9%  98.2%  97.1%  89.4%  96.1%  90.4%  96.3%  95.0%  90.1%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 87.4%  96.9%  97.2%  98.2% 100.0%  94.3%  95.8%  97.0%  96.9%  99.2%  93.6%  96.6%  95.5%  95.9%  92.9%  93.5%  97.3%  98.0%  97.3%  96.5%  94.8%  93.6%  97.2%  89.3%  96.3%  85.9%  94.0%  96.3%  94.4%  90.1%  96.9%  98.2%  97.1%  90.2%  97.1%  90.4%  96.3%  95.9%  92.1%  94.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 87.9%  96.9%  97.2%  98.2% 100.0%  95.2%  95.8%  96.0%  95.9%  99.2%  93.6%  96.6%  95.5%  95.1%  93.8%  93.5%  97.3%  98.0%  97.3%  96.5%  94.0%  92.7%  97.2%  88.4%  96.3%  89.9%  93.1%  96.3%  94.4%  92.1%  95.9%  98.2%  97.1%  90.2%  97.1%  90.4%  96.3%  95.9%  93.1%  94.9%  95.9%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 88.5%  96.9%  97.2%  98.2% 100.0%  95.2%  95.8%  96.0%  95.9%  99.2%  92.7%  96.6%  95.5%  95.9%  93.8%  92.6%  97.3%  98.0%  97.3%  97.3%  94.0%  93.6%  97.2%  90.2%  96.3%  87.9%  94.0%  96.3%  94.4%  93.1%  96.9%  98.2%  97.1%  89.4%  97.1%  87.8%  96.3%  95.9%  92.1%  94.9%  95.9%  99.1%   0.0%   0.0% 	Avg.: 95.2% 
	 86.8%  96.9%  97.2%  98.2% 100.0%  95.2%  95.8%  96.0%  96.9%  99.2%  92.7%  96.6%  95.5%  95.9%  93.8%  93.5%  97.3%  98.0%  97.3%  97.3%  94.0%  93.6%  97.2%  88.4%  96.3%  86.9%  93.1%  96.3%  95.4%  93.1%  96.9%  98.2%  97.1%  91.1%  97.1%  91.3%  96.3%  95.9%  94.1%  94.9%  95.9%  99.1%  94.3%   0.0% 	Avg.: 95.3% 
	 88.5%  96.9%  97.2%  97.3% 100.0%  95.2%  95.8%  96.0%  96.9%  99.2%  92.7%  95.8%  95.5%  95.9%  93.8%  93.5%  98.2%  98.0%  97.3%  96.5%  94.0%  93.6%  97.2%  89.3%  96.3%  87.9%  94.0%  95.4%  93.5%  93.1%  96.9%  98.2%  96.2%  91.1%  97.1%  91.3%  96.3%  95.9%  92.1%  94.0%  95.9%  99.1%  94.3%  98.3% 	Avg.: 95.3% 
************************************************************************************************************
TAg Acc
	 69.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.2% 
	 68.1%  76.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.2% 
	 72.0%  72.2%  70.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.5% 
	 76.4%  84.5%  59.3%  60.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.1% 
	 83.5%  81.4%  70.4%  48.7%  59.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 84.1%  78.4%  75.9%  50.4%  37.9%  57.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.0% 
	 75.8%  82.5%  76.9%  59.3%  58.3%  39.0%  72.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.3% 
	 81.9%  78.4%  82.4%  80.5%  68.0%  61.9%  53.8%  52.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.9% 
	 84.6%  77.3%  82.4%  74.3%  71.8%  65.7%  52.1%  51.5%  61.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 76.4%  83.5%  63.0%  82.3%  74.8%  74.3%  61.3%  54.5%  45.9%  71.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.7% 
	 76.9%  84.5%  81.5%  77.9%  68.9%  69.5%  66.4%  56.4%  51.0%  39.7%  64.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.0% 
	 78.6%  78.4%  76.9%  86.7%  70.9%  66.7%  68.9%  71.3%  59.2%  44.6%  34.5%  73.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.5% 
	 68.1%  77.3%  84.3%  85.0%  76.7%  66.7%  73.1%  71.3%  63.3%  64.5%  44.5%  44.5%  62.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 77.5%  78.4%  82.4%  85.8%  72.8%  69.5%  69.7%  71.3%  61.2%  68.6%  56.4%  52.9%  29.5%  69.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 78.6%  78.4%  81.5%  81.4%  78.6%  70.5%  76.5%  73.3%  68.4%  67.8%  59.1%  56.3%  33.0%  43.4%  66.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.5% 
	 78.0%  78.4%  82.4%  85.0%  70.9%  73.3%  70.6%  73.3%  69.4%  71.9%  69.1%  63.9%  42.9%  48.4%  52.7%  63.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.3% 
	 76.9%  75.3%  82.4%  85.0%  73.8%  70.5%  74.8%  76.2%  65.3%  72.7%  69.1%  56.3%  36.6%  50.8%  62.5%  46.3%  64.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.0% 
	 80.2%  78.4%  82.4%  84.1%  79.6%  73.3%  71.4%  78.2%  76.5%  77.7%  68.2%  63.0%  55.4%  57.4%  67.0%  55.6%  46.0%  60.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.7% 
	 77.5%  75.3%  84.3%  83.2%  73.8%  71.4%  73.9%  67.3%  68.4%  76.0%  76.4%  67.2%  57.1%  59.0%  69.6%  56.5%  53.1%  61.6%  63.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.2% 
	 76.4%  77.3%  82.4%  84.1%  74.8%  73.3%  75.6%  75.2%  73.5%  81.0%  74.5%  66.4%  55.4%  61.5%  66.1%  60.2%  53.1%  60.6%  40.0%  71.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.2% 
	 76.4%  79.4%  83.3%  83.2%  71.8%  73.3%  73.9%  71.3%  71.4%  80.2%  73.6%  66.4%  59.8%  64.8%  59.8%  67.6%  62.8%  69.7%  43.6%  54.0%  71.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.4% 
	 75.8%  78.4%  81.5%  82.3%  76.7%  70.5%  76.5%  74.3%  68.4%  71.1%  71.8%  67.2%  60.7%  67.2%  73.2%  63.9%  61.9%  65.7%  49.1%  56.6%  57.8%  66.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.9% 
	 76.9%  81.4%  83.3%  83.2%  75.7%  72.4%  73.1%  72.3%  74.5%  74.4%  69.1%  68.9%  56.2%  59.0%  72.3%  67.6%  61.9%  66.7%  52.7%  65.5%  66.4%  44.5%  75.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.3% 
	 74.2%  80.4%  80.6%  83.2%  75.7%  68.6%  73.9%  70.3%  73.5%  69.4%  76.4%  63.9%  58.0%  66.4%  76.8%  65.7%  65.5%  70.7%  59.1%  53.1%  61.2%  60.0%  56.0%  65.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.7% 
	 75.3%  81.4%  81.5%  77.0%  68.0%  73.3%  75.6%  70.3%  64.3%  76.9%  75.5%  67.2%  67.9%  63.1%  70.5%  71.3%  64.6%  66.7%  56.4%  62.8%  65.5%  57.3%  58.7%  50.9%  73.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 76.9%  73.2%  81.5%  79.6%  67.0%  71.4%  74.8%  74.3%  73.5%  66.1%  76.4%  64.7%  55.4%  65.6%  75.9%  73.1%  65.5%  72.7%  56.4%  63.7%  74.1%  63.6%  54.1%  56.2%  45.9%  65.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 64.8%  74.2%  78.7%  78.8%  72.8%  75.2%  73.1%  70.3%  63.3%  71.1%  74.5%  62.2%  60.7%  63.9%  75.0%  68.5%  67.3%  77.8%  60.0%  62.8%  74.1%  61.8%  64.2%  55.4%  50.5%  35.4%  73.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.0% 
	 73.6%  68.0%  77.8%  71.7%  71.8%  68.6%  73.1%  70.3%  70.4%  74.4%  72.7%  63.0%  55.4%  64.8%  76.8%  68.5%  63.7%  69.7%  59.1%  61.9%  75.9%  64.5%  63.3%  53.6%  49.5%  41.4%  40.5%  71.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.6% 
	 73.1%  77.3%  77.8%  79.6%  68.0%  70.5%  75.6%  72.3%  67.3%  73.6%  75.5%  62.2%  66.1%  67.2%  76.8%  69.4%  68.1%  76.8%  63.6%  60.2%  78.4%  66.4%  63.3%  58.0%  50.5%  43.4%  50.9%  41.7%  69.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.0% 
	 73.1%  77.3%  78.7%  77.9%  71.8%  70.5%  78.2%  75.2%  72.4%  72.7%  70.9%  63.0%  62.5%  63.9%  76.8%  68.5%  69.0%  76.8%  61.8%  61.1%  77.6%  64.5%  67.0%  58.9%  57.8%  45.5%  49.1%  44.4%  60.2%  74.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.4% 
	 73.1%  72.2%  78.7%  79.6%  68.9%  71.4%  75.6%  72.3%  63.3%  68.6%  75.5%  66.4%  55.4%  62.3%  73.2%  67.6%  61.9%  81.8%  64.5%  62.8%  80.2%  72.7%  65.1%  61.6%  56.0%  48.5%  57.8%  53.7%  54.6%  45.5%  69.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.5% 
	 73.1%  76.3%  79.6%  77.9%  68.9%  72.4%  72.3%  66.3%  63.3%  65.3%  70.9%  58.8%  60.7%  68.0%  72.3%  71.3%  62.8%  76.8%  61.8%  63.7%  77.6%  70.9%  66.1%  59.8%  64.2%  51.5%  58.6%  55.6%  66.7%  53.5%  55.1%  79.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.9% 
	 73.6%  64.9%  81.5%  77.9%  68.9%  74.3%  74.8%  68.3%  59.2%  66.9%  74.5%  63.0%  55.4%  60.7%  67.0%  68.5%  69.0%  73.7%  68.2%  57.5%  78.4%  72.7%  62.4%  59.8%  58.7%  52.5%  64.7%  63.9%  70.4%  55.4%  62.2%  65.2%  76.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.0% 
	 68.7%  78.4%  80.6%  74.3%  67.0%  72.4%  72.3%  67.3%  62.2%  71.9%  77.3%  63.9%  60.7%  56.6%  73.2%  68.5%  64.6%  76.8%  66.4%  59.3%  79.3%  66.4%  65.1%  55.4%  62.4%  49.5%  68.1%  61.1%  69.4%  61.4%  60.2%  65.2%  61.0%  72.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.0% 
	 76.4%  73.2%  81.5%  78.8%  67.0%  74.3%  74.8%  73.3%  63.3%  68.6%  75.5%  61.3%  52.7%  61.5%  71.4%  70.4%  60.2%  74.7%  62.7%  66.4%  79.3%  68.2%  66.1%  59.8%  56.9%  46.5%  61.2%  59.3%  71.3%  56.4%  63.3%  71.4%  58.1%  47.2%  69.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.3% 
	 72.5%  73.2%  78.7%  81.4%  69.9%  73.3%  72.3%  68.3%  69.4%  68.6%  74.5%  60.5%  59.8%  61.5%  70.5%  70.4%  65.5%  79.8%  62.7%  61.9%  79.3%  69.1%  63.3%  58.9%  58.7%  43.4%  50.9%  63.0%  71.3%  66.3%  63.3%  71.4%  66.7%  63.4%  45.1%  67.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.6% 
	 72.5%  72.2%  81.5%  77.9%  67.0%  75.2%  74.8%  69.3%  66.3%  71.1%  73.6%  60.5%  55.4%  63.1%  69.6%  68.5%  66.4%  78.8%  66.4%  62.8%  79.3%  70.9%  63.3%  62.5%  56.9%  47.5%  56.9%  60.2%  70.4%  63.4%  70.4%  68.8%  63.8%  61.0%  45.1%  45.2%  74.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.3% 
	 73.6%  72.2%  79.6%  81.4%  66.0%  70.5%  75.6%  75.2%  58.2%  69.4%  70.0%  60.5%  56.2%  62.3%  73.2%  65.7%  61.9%  75.8%  65.5%  61.1%  81.0%  62.7%  63.3%  64.3%  60.6%  50.5%  58.6%  63.9%  70.4%  62.4%  72.4%  69.6%  61.9%  61.0%  47.1%  49.6%  61.5%  64.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.8% 
	 69.8%  76.3%  79.6%  73.5%  68.9%  72.4%  73.9%  72.3%  70.4%  73.6%  70.9%  61.3%  59.8%  60.7%  70.5%  67.6%  63.7%  77.8%  63.6%  61.9%  77.6%  65.5%  64.2%  58.9%  60.6%  52.5%  57.8%  63.0%  71.3%  61.4%  74.5%  71.4%  65.7%  62.6%  50.0%  47.0%  64.2%  53.7%  71.3%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.2% 
	 67.6%  72.2%  78.7%  69.9%  66.0%  73.3%  71.4%  74.3%  61.2%  66.1%  76.4%  60.5%  65.2%  65.6%  69.6%  63.0%  66.4%  78.8%  64.5%  62.8%  80.2%  65.5%  64.2%  58.0%  61.5%  58.6%  69.8%  63.9%  72.2%  62.4%  70.4%  71.4%  69.5%  67.5%  47.1%  53.0%  65.1%  51.2%  56.4%  72.6%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.4% 
	 72.5%  73.2%  79.6%  77.0%  68.9%  72.4%  78.2%  67.3%  55.1%  72.7%  76.4%  59.7%  58.0%  62.3%  71.4%  59.3%  65.5%  75.8%  65.5%  59.3%  81.0%  64.5%  63.3%  58.9%  58.7%  56.6%  69.0%  63.9%  73.1%  61.4%  73.5%  72.3%  68.6%  62.6%  56.9%  53.0%  65.1%  56.2%  62.4%  55.6%  69.1%   0.0%   0.0%   0.0% 	Avg.: 66.2% 
	 69.2%  72.2%  80.6%  75.2%  67.0%  74.3%  73.9%  70.3%  64.3%  66.9%  70.9%  61.3%  60.7%  61.5%  71.4%  50.0%  65.5%  76.8%  65.5%  64.6%  80.2%  67.3%  55.0%  62.5%  57.8%  50.5%  60.3%  63.0%  75.9%  63.4%  68.4%  72.3%  62.9%  65.9%  55.9%  53.0%  67.0%  57.9%  63.4%  58.1%  45.5%  75.9%   0.0%   0.0% 	Avg.: 65.3% 
	 69.2%  71.1%  78.7%  75.2%  68.9%  74.3%  74.8%  66.3%  71.4%  69.4%  76.4%  60.5%  55.4%  59.8%  70.5%  65.7%  61.9%  82.8%  62.7%  61.9%  78.4%  66.4%  60.6%  65.2%  61.5%  52.5%  61.2%  62.0%  75.0%  61.4%  72.4%  71.4%  67.6%  64.2%  56.9%  52.2%  65.1%  56.2%  60.4%  58.1%  44.7%  61.2%  62.9%   0.0% 	Avg.: 65.5% 
	 71.4%  72.2%  77.8%  77.0%  67.0%  73.3%  76.5%  69.3%  61.2%  68.6%  73.6%  60.5%  58.0%  62.3%  72.3%  56.5%  62.8%  80.8%  63.6%  63.7%  80.2%  65.5%  66.1%  58.0%  60.6%  50.5%  64.7%  63.0%  74.1%  66.3%  71.4%  74.1%  66.7%  67.5%  58.8%  53.0%  62.4%  58.7%  63.4%  65.0%  47.2%  61.2%  51.4%  64.1% 	Avg.: 65.5% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	-15.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:-15.4% 
	  5.5%  -9.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.9% 
	  0.5%   0.0% -13.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.1% 
	 -1.6%  -2.1%  -0.9% -10.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -3.8% 
	 -1.1%   1.0%  -2.8%  -2.7% -14.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.0% 
	  0.5%   0.0%   0.0%  -0.9%  -1.0%  -8.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.6% 
	  0.0%   0.0%   0.0%   0.0%   0.0%  -2.9%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.5% 
	 -0.5%   0.0%   0.0%   0.0%   0.0%   0.0%  -2.5%  -8.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.5% 
	  0.5%   0.0%   0.0%   0.0%   0.0%  -1.0%  -0.8%  -1.0%  -5.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.8% 
	  2.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   1.0%   0.0%  -7.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.3% 
	 -1.1%   0.0%   0.9%   0.0%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%  -9.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.7% 
	  1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%  -5.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.3% 
	  1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   0.8%  -0.9%  -1.7%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   0.0%   0.0%   0.0%  -1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.1%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.8%   0.0%   0.8%   0.9%  -2.5%  -7.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.3% 
	  1.1%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   2.0%   0.0%   0.8%   0.9%   0.0%   1.8%  -0.8%   0.9%  -4.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.0%   0.0%   1.9%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.8%   0.9%   0.9%  -5.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.1% 
	  0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   1.0%   0.0%   0.8%  -0.9%   0.0%  -0.9%   0.8%   0.9%   0.9%   0.9%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.5%   0.0%   0.9%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%  -0.9%   0.8%   0.0%   1.9%   2.7%  -1.0%  -3.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.5%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   1.8%   0.0%   0.9%   0.8%   0.0%   2.8%   0.9%   0.0%   1.8%  -3.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.9%   0.0%   0.9%   0.8%   0.0%   2.8%   0.9%  -1.0%   0.0%   0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.5%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.0%   0.8%   0.9%   0.0%   0.9%  -0.8%   0.0%   1.9%   1.8%   0.0%   0.0%   0.0%   0.0%   2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   0.0%  -1.7%  -0.9%   0.0%   0.0%   1.9%   1.8%   0.0%   0.0%  -0.9%   1.7%  -0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.5%   0.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   0.9%  -0.8%   1.8%   0.0%   0.0%   2.8%   0.9%   0.0%   0.0%   0.9%   0.9%   0.0%   0.9%  -3.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   1.8%   0.0%   0.0%   0.8%   0.0%   1.9%   0.9%   0.0%   0.0%  -0.9%   0.9%   0.9%   0.9%   0.9%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   1.8%   0.8%   0.0%   0.0%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   2.6%   0.9%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  2.7%   0.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   1.0%   1.0%   0.8%   1.8%   0.0%   0.9%   0.8%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   3.4%   0.0%   0.9%   0.0%   0.0%  -3.0%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.0%   0.8%   1.8%   0.0%   0.9%   0.8%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.7%   0.0%   0.0%   0.0%   0.0%   1.0%  -0.9%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   1.0%   0.0%   3.0%   1.0%   0.8%   1.8%   0.0%   0.9%   0.8%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   3.4%   0.9%   0.0%   0.0%   0.0%   3.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.0%   1.0%   0.8%   1.8%   0.0%   0.9%   0.0%   0.0%   0.9%  -0.9%   0.0%   0.0%   0.0%   3.4%   0.0%   0.0%   0.0%   0.0%   1.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.0%   1.0%   0.8%   1.8%   0.0%   0.9%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   2.6%   0.0%   0.0%   1.8%   0.0%   4.0%   0.0%   0.0%   0.0%   1.0%  -4.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   1.0%   0.0%  -0.9%   0.0%   1.0%   0.0%   2.0%   1.0%   0.0%   0.9%   0.0%   0.9%   0.8%   0.0%   0.9%   0.9%  -1.0%   0.0%   0.0%   3.4%   0.0%   0.0%  -0.9%   0.0%   2.0%   0.0%   0.0%  -1.9%   2.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.5%   1.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   2.0%   1.0%   0.8%   0.9%   0.0%   0.9%   0.0%   0.9%   0.9%   1.8%   0.0%   0.9%   0.0%   2.6%   0.9%   0.0%   0.9%   0.0%   4.0%   0.0%   0.0%   0.9%   0.0%   1.0%   0.0%   1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   1.0%   0.0%   2.0%   1.0%   0.0%   1.8%   0.0%   0.9%   0.0%   1.8%   0.9%   0.9%   0.0%   0.9%   0.0%   1.7%   0.9%   0.0%   0.9%   0.0%   3.0%   0.0%   0.0%   0.9%   1.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   1.0%   0.0%   2.0%   1.0%   0.0%   1.8%   0.8%   0.9%   0.0%   1.8%   0.9%   0.9%   0.0%   0.9%   0.9%   2.6%   0.9%   0.0%   2.7%   0.0%   1.0%   0.0%   0.0%   0.9%   1.0%   0.0%   0.0%   1.9%   2.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   1.0%   0.0%   2.0%   1.0%   0.0%   1.8%   0.8%   0.9%   0.0%   1.8%   0.9%   0.9%   0.0%   0.9%   0.9%   3.4%   0.0%   0.0%   1.8%   0.0%   2.0%   0.0%   0.0%   0.0%   2.0%   1.0%   0.0%   1.9%   0.8%  -1.0%   1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   2.0%   0.0%   1.8%   0.8%   0.9%   0.0%   1.8%   1.9%   0.9%   0.0%   0.9%   0.9%   3.4%   0.9%   0.0%   2.7%   0.0%   1.0%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   1.9%   0.8%   0.0%   2.6%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.0%   1.0%   0.0%   1.8%   0.8%   0.9%   0.0%   1.8%   1.9%   0.9%   0.0%   0.9%   0.9%   2.6%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   1.9%   0.8%   2.0%   2.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.6%   1.0%   0.0%   0.0%   0.0%   1.0%   0.0%   1.0%   1.0%   0.0%   1.8%   0.8%   0.9%   0.0%   1.8%   1.9%   0.9%   0.0%   0.9%   0.9%   2.6%   0.9%   0.0%   1.8%  -0.9%   3.0%   0.0%   0.0%   0.0%   2.0%   0.0%   0.0%   1.9%   0.0%   1.0%   2.6%   0.0%  -0.8%  -2.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.1%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.0%   2.0%   0.0%   1.8%   0.8%   0.9%   0.8%   0.9%   1.9%   0.9%   0.0%   0.9%   0.9%   3.4%   1.8%   0.0%   2.7%   0.0%  -1.0%   0.9%   0.0%   0.0%   0.0%   1.0%   0.0%   1.9%   0.0%   1.0%   2.6%   0.0%   0.0%  -1.0%  -0.9%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.5%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.0%   2.0%   0.0%   2.7%   0.8%   0.9%   0.0%   0.9%   2.8%   0.9%   0.0%   0.9%   0.0%   3.4%   0.9%   0.0%   0.9%   0.0%   2.0%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   1.9%   0.8%   1.0%   5.2%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  2.2%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.0%   1.0%   0.0%   2.7%   0.8%   0.9%   0.0%   0.9%   1.9%   0.9%   0.0%   0.9%   0.0%   3.4%   0.9%   0.0%   2.7%   0.0%   3.0%   0.9%   0.0%  -0.9%   0.0%   0.0%   0.0%   1.9%  -0.8%   1.0%   1.7%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.5%   1.0%   0.0%   0.9%   0.0%   0.0%   0.0%   2.0%   1.0%   0.0%   2.7%   1.7%   0.9%   0.0%   0.9%   1.9%   0.0%   0.0%   0.9%   0.9%   3.4%   0.9%   0.0%   1.8%   0.0%   2.0%   0.0%   0.9%   1.9%   0.0%   0.0%   0.0%   2.9%   0.0%   1.0%   1.7%   0.0%   0.0%   2.0%   0.9%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	 -2.7%   4.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	 -4.4%  -8.2%  11.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.5% 
	 -7.1%   3.1%   0.0%  11.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.9% 
	 -0.5%   6.2%  -5.6%   9.7%  21.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.2% 
	  8.2%   2.1%  -0.9%   0.9%   1.0%  18.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.9% 
	  2.2%   6.2%  -5.6% -20.4%  -8.7%  -4.8%  18.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.8% 
	 -0.5%   7.2%   0.0%   6.2%  -3.9%  -3.8%  20.2%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.3% 
	  8.2%   1.0%  19.4%  -1.8%  -2.9%  -8.6%  10.9%  -2.0%  15.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.4% 
	  7.7%   0.0%   0.9%   4.4%   5.8%   4.8%   5.9%  -2.0%  10.2%  31.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	  6.0%   6.2%   5.6%  -4.4%   3.9%   7.6%   3.4% -14.9%   2.0%  26.4%  30.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.5% 
	 16.5%   7.2%  -1.9%   1.8%  -1.9%   7.6%  -0.8%   0.0%  -2.0%   6.6%  20.0%  29.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	  7.1%   6.2%   1.9%   0.9%   3.9%   4.8%   3.4%   0.0%   2.0%   2.5%   8.2%  21.0%  33.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.3% 
	  6.0%   6.2%   2.8%   5.3%  -1.9%   3.8%  -3.4%  -2.0%  -5.1%   3.3%   5.5%  17.6%  29.5%  26.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.7% 
	  6.6%   6.2%   1.9%   1.8%   7.8%   1.0%   5.9%   0.0%  -1.0%  -0.8%  -4.5%  10.1%  19.6%  21.3%  13.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.9% 
	  7.7%   9.3%   1.9%   1.8%   4.9%   3.8%   1.7%  -3.0%   4.1%  -0.8%   0.0%  17.6%  25.9%  18.9%   3.6%  16.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.1% 
	  4.4%   6.2%   1.9%   2.7%  -1.0%   1.0%   5.0%  -2.0%  -7.1%  -5.0%   0.9%  10.9%   7.1%  12.3%  -0.9%   7.4%  18.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.7% 
	  7.1%   9.3%   0.0%   3.5%   5.8%   2.9%   2.5%  10.9%   8.2%   1.7%  -7.3%   6.7%   5.4%  10.7%  -2.7%   6.5%  11.5%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.5% 
	  8.2%   7.2%   1.9%   2.7%   4.9%   1.0%   0.8%   3.0%   3.1%  -3.3%   1.8%   7.6%   7.1%   8.2%   3.6%   2.8%  11.5%   1.0%  23.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	  8.2%   5.2%   0.9%   3.5%   7.8%   1.0%   2.5%   6.9%   5.1%   0.8%   2.7%   7.6%   2.7%   4.9%   9.8%  -4.6%   1.8%  -8.1%  20.0%  17.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.8% 
	  8.8%   6.2%   2.8%   4.4%   2.9%   3.8%   0.0%   4.0%   8.2%   9.9%   4.5%   6.7%   1.8%   2.5%  -3.6%   3.7%   2.7%   4.0%  14.5%  15.0%  13.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	  7.7%   3.1%   0.9%   3.5%   3.9%   1.9%   3.4%   5.9%   2.0%   6.6%   7.3%   5.0%   6.2%  10.7%   0.9%   0.0%   2.7%   3.0%  10.9%   6.2%   5.2%  21.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.4% 
	 10.4%   4.1%   3.7%   3.5%   3.9%   5.7%   2.5%   7.9%   3.1%  11.6%   0.0%  10.1%   4.5%   3.3%  -3.6%   1.9%  -0.9%  -1.0%   4.5%  18.6%  10.3%   6.4%  19.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	  9.3%   3.1%   2.8%   9.7%  11.7%   1.0%   0.8%   7.9%  12.2%   4.1%   0.9%   6.7%  -5.4%   6.6%   6.2%  -3.7%   0.9%   4.0%   7.3%   8.8%   6.0%   9.1%  16.5%  14.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.9% 
	  7.7%  11.3%   2.8%   7.1%  12.6%   2.9%   1.7%   4.0%   3.1%  14.9%   0.0%   9.2%  12.5%   4.1%   0.9%  -1.9%   0.0%  -2.0%   7.3%   8.0%  -2.6%   2.7%  21.1%   8.9%  27.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.5% 
	 19.8%  10.3%   5.6%   8.0%   6.8%  -1.0%   3.4%   7.9%  13.3%   9.9%   1.8%  11.8%   7.1%   5.7%   1.8%   4.6%  -1.8%  -5.1%   3.6%   8.8%   0.0%   4.5%  11.0%   9.8%  22.9%  30.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.7% 
	 11.0%  16.5%   6.5%  15.0%   7.8%   6.7%   3.4%   7.9%   6.1%   6.6%   3.6%  10.9%  12.5%   4.9%   0.0%   4.6%   3.5%   8.1%   4.5%   9.7%  -1.7%   1.8%  11.9%  11.6%  23.9%  24.2%  32.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 11.5%   7.2%   6.5%   7.1%  11.7%   4.8%   0.8%   5.9%   9.2%   7.4%   0.9%  11.8%   1.8%   2.5%   0.0%   3.7%  -0.9%   1.0%   0.0%  11.5%  -2.6%   0.0%  11.9%   7.1%  22.9%  22.2%  22.4%  29.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.8% 
	 11.5%   7.2%   5.6%   8.8%   7.8%   4.8%  -1.7%   3.0%   4.1%   8.3%   5.5%  10.9%   5.4%   5.7%   0.0%   4.6%  -0.9%   1.0%   1.8%  10.6%   0.9%   1.8%   8.3%   6.2%  15.6%  20.2%  24.1%  26.9%   9.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	 11.5%  12.4%   5.6%   7.1%  10.7%   3.8%   2.5%   5.9%  13.3%  12.4%   0.9%   7.6%  12.5%   7.4%   3.6%   5.6%   7.1%  -4.0%  -0.9%   8.8%  -1.7%  -6.4%  10.1%   3.6%  17.4%  17.2%  15.5%  17.6%  14.8%  28.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.3% 
	 11.5%   8.2%   4.6%   8.8%  10.7%   2.9%   5.9%  11.9%  13.3%  15.7%   5.5%  15.1%   7.1%   1.6%   4.5%   1.9%   6.2%   5.1%   2.7%   8.0%   2.6%   1.8%   9.2%   5.4%   9.2%  14.1%  14.7%  15.7%   2.8%  20.8%  14.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 11.0%  19.6%   2.8%   8.8%  10.7%   1.0%   3.4%   9.9%  17.3%  14.0%   1.8%  10.9%  12.5%   9.0%   9.8%   4.6%   0.0%   8.1%  -3.6%  14.2%   1.7%   0.0%  12.8%   5.4%  14.7%  13.1%   8.6%   7.4%  -0.9%  18.8%   7.1%  14.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 15.9%   6.2%   3.7%  12.4%  12.6%   2.9%   5.9%  10.9%  14.3%   9.1%  -0.9%  10.1%   7.1%  13.1%   3.6%   4.6%   4.4%   5.1%   1.8%  12.4%   0.9%   6.4%  10.1%   9.8%  11.0%  16.2%   5.2%  10.2%   0.9%  12.9%   9.2%  14.3%  15.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	  8.2%  11.3%   2.8%   8.0%  12.6%   1.0%   3.4%   5.0%  13.3%  12.4%   1.8%  12.6%  15.2%   8.2%   5.4%   2.8%   8.8%   7.1%   5.5%   5.3%   0.9%   4.5%   9.2%   5.4%  16.5%  19.2%  12.1%  12.0%  -0.9%  17.8%   6.1%   8.0%  18.1%  25.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.0% 
	 12.1%  11.3%   5.6%   5.3%   9.7%   1.9%   5.9%   9.9%   7.1%  12.4%   2.7%  13.4%   8.0%   8.2%   6.2%   2.8%   3.5%   2.0%   5.5%   9.7%   0.9%   3.6%  11.9%   6.2%  14.7%  22.2%  22.4%   8.3%   0.0%   7.9%   6.1%   8.0%   9.5%   8.9%  24.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 12.1%  12.4%   2.8%   8.8%  12.6%   0.0%   3.4%   8.9%  10.2%   9.9%   3.6%  13.4%  12.5%   6.6%   7.1%   4.6%   2.7%   3.0%   1.8%   8.8%   0.9%   1.8%  11.9%   2.7%  16.5%  18.2%  16.4%  11.1%   0.9%  10.9%  -1.0%  10.7%  12.4%  11.4%  24.5%  22.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.8% 
	 11.0%  12.4%   4.6%   5.3%  13.6%   4.8%   2.5%   3.0%  18.4%  11.6%   7.3%  13.4%  11.6%   7.4%   3.6%   7.4%   7.1%   6.1%   2.7%  10.6%  -0.9%  10.0%  11.9%   0.9%  12.8%  15.2%  14.7%   7.4%   0.9%  11.9%  -2.0%   9.8%  14.3%  11.4%  22.5%  18.3%  12.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.1% 
	 14.8%   8.2%   4.6%  13.3%  10.7%   2.9%   4.2%   5.9%   6.1%   7.4%   6.4%  12.6%   8.0%   9.0%   6.2%   5.6%   5.3%   4.0%   4.5%   9.7%   3.4%   7.3%  11.0%   6.2%  12.8%  13.1%  15.5%   8.3%   0.0%  12.9%  -2.0%   8.0%  10.5%   9.8%  19.6%  20.9%  10.1%  10.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	 17.0%  12.4%   5.6%  16.8%  13.6%   1.9%   6.7%   4.0%  15.3%  14.9%   0.9%  13.4%   2.7%   4.1%   7.1%  10.2%   2.7%   3.0%   3.6%   8.8%   0.9%   7.3%  11.0%   7.1%  11.9%   7.1%   3.4%   7.4%  -0.9%  11.9%   4.1%   8.0%   6.7%   4.9%  22.5%  14.8%   9.2%  13.2%  14.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 12.1%  11.3%   4.6%   9.7%  10.7%   2.9%   0.0%  10.9%  21.4%   8.3%   0.9%  14.3%   9.8%   7.4%   5.4%  13.9%   3.5%   6.1%   2.7%  12.4%   0.0%   8.2%  11.9%   6.2%  14.7%   9.1%   4.3%   7.4%  -0.9%  12.9%   1.0%   7.1%   7.6%   9.8%  12.7%  14.8%   9.2%   8.3%   8.9%  17.1%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 15.4%  12.4%   3.7%  11.5%  12.6%   1.0%   4.2%   7.9%  12.2%  14.0%   6.4%  12.6%   7.1%   8.2%   5.4%  23.1%   3.5%   5.1%   2.7%   7.1%   0.9%   5.5%  20.2%   2.7%  15.6%  15.2%  12.9%   8.3%  -2.8%  10.9%   6.1%   7.1%  13.3%   6.5%  13.7%  14.8%   7.3%   6.6%   7.9%  14.5%  23.6%   0.0%   0.0%   0.0% 	Avg.:  9.4% 
	 15.4%  13.4%   5.6%  11.5%  10.7%   1.0%   3.4%  11.9%   5.1%  11.6%   0.9%  13.4%  12.5%   9.8%   6.2%   7.4%   7.1%  -1.0%   5.5%   9.7%   2.6%   6.4%  14.7%   0.0%  11.9%  13.1%  12.1%   9.3%   0.9%  12.9%   2.0%   8.0%   8.6%   8.1%  12.7%  15.7%   9.2%   8.3%  10.9%  14.5%  24.4%  14.7%   0.0%   0.0% 	Avg.:  9.1% 
	 13.2%  12.4%   6.5%   9.7%  12.6%   1.9%   1.7%   8.9%  15.3%  12.4%   3.6%  13.4%   9.8%   7.4%   4.5%  16.7%   6.2%   2.0%   4.5%   8.0%   0.9%   7.3%   9.2%   7.1%  12.8%  15.2%   8.6%   8.3%   1.9%   7.9%   3.1%   5.4%   9.5%   4.9%  10.8%  14.8%  11.9%   5.8%   7.9%   7.7%  22.0%  14.7%  11.4%   0.0% 	Avg.:  8.8% 
************************************************************************************************************
[Elapsed time = 0.4 h]
Done!

f1_score_micro: 0.6552776082977425
f1_score_macro: 0.6226931520071303
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         5
           1       0.75      0.75      0.75         4
           2       0.67      1.00      0.80         4
           3       0.67      0.89      0.76         9
           4       1.00      0.75      0.86         4
           5       0.50      0.75      0.60         4
           6       1.00      0.75      0.86         4
           7       0.00      0.00      0.00         4
           8       0.50      0.50      0.50         4
           9       0.00      0.00      0.00         4
          10       0.06      0.25      0.10         4
          11       0.45      0.56      0.50         9
          12       0.67      1.00      0.80         4
          13       1.00      0.60      0.75         5
          14       0.90      1.00      0.95         9
          15       1.00      1.00      1.00         9
          16       0.67      0.50      0.57         4
          17       0.00      0.00      0.00         4
          18       0.62      1.00      0.77         5
          19       0.80      1.00      0.89         4
          20       0.33      0.80      0.47         5
          21       0.15      0.75      0.25         4
          22       0.00      0.00      0.00         4
          23       0.11      0.75      0.19         4
          24       1.00      1.00      1.00         5
          25       1.00      0.75      0.86         4
          26       1.00      1.00      1.00         4
          27       0.00      0.00      0.00         9
          28       0.80      0.89      0.84         9
          29       0.67      0.40      0.50         5
          30       0.57      1.00      0.73         4
          31       0.90      1.00      0.95         9
          32       1.00      0.75      0.86         4
          33       0.75      1.00      0.86         9
          34       1.00      1.00      1.00         4
          35       1.00      0.75      0.86         4
          36       0.50      0.20      0.29         5
          37       1.00      1.00      1.00         5
          38       0.40      0.50      0.44         4
          39       1.00      0.75      0.86         4
          40       0.00      0.00      0.00         4
          41       0.80      1.00      0.89         4
          42       0.58      0.78      0.67         9
          43       0.25      0.50      0.33         4
          44       0.80      1.00      0.89         4
          45       0.80      1.00      0.89         4
          46       0.80      1.00      0.89         4
          47       0.11      0.22      0.15         9
          48       0.75      0.75      0.75         4
          49       0.67      1.00      0.80         4
          50       0.80      1.00      0.89         4
          51       0.80      0.89      0.84         9
          52       1.00      1.00      1.00         4
          53       1.00      0.50      0.67         4
          54       0.89      0.89      0.89         9
          55       1.00      0.80      0.89         5
          56       1.00      1.00      1.00         4
          57       0.67      0.50      0.57         4
          58       0.80      1.00      0.89         4
          59       0.80      0.89      0.84         9
          60       0.80      1.00      0.89         4
          61       1.00      0.25      0.40         4
          62       0.40      0.50      0.44         4
          63       0.75      0.75      0.75         4
          64       0.45      0.56      0.50         9
          65       0.70      0.78      0.74         9
          66       0.57      1.00      0.73         4
          67       1.00      1.00      1.00         4
          68       0.60      0.75      0.67         4
          69       1.00      0.78      0.88         9
          70       1.00      1.00      1.00         4
          71       1.00      1.00      1.00         5
          72       0.50      0.25      0.33         4
          73       1.00      0.80      0.89         5
          74       0.82      1.00      0.90         9
          75       0.58      0.88      0.70         8
          76       0.33      0.20      0.25         5
          77       0.67      1.00      0.80         4
          78       1.00      0.75      0.86         4
          79       0.50      1.00      0.67         4
          80       0.50      0.80      0.62         5
          81       0.75      0.67      0.71         9
          82       0.80      1.00      0.89         4
          83       0.50      0.60      0.55         5
          84       0.09      0.50      0.15         4
          85       0.00      0.00      0.00         4
          86       1.00      0.75      0.86         4
          87       0.57      1.00      0.73         4
          88       0.89      0.89      0.89         9
          89       1.00      0.78      0.88         9
          90       0.75      0.75      0.75         4
          91       0.67      1.00      0.80         4
          92       1.00      0.60      0.75         5
          93       0.89      0.89      0.89         9
          94       1.00      1.00      1.00         4
          95       0.06      0.25      0.10         4
          96       0.80      1.00      0.89         4
          97       0.80      1.00      0.89         4
          98       0.33      0.40      0.36         5
          99       0.67      1.00      0.80         4
         100       0.75      0.75      0.75         4
         101       0.75      0.75      0.75         4
         102       1.00      1.00      1.00         5
         103       0.57      1.00      0.73         4
         104       0.78      0.78      0.78         9
         105       0.25      0.25      0.25         4
         106       0.20      0.50      0.29         4
         107       0.67      0.50      0.57         4
         108       0.80      1.00      0.89         4
         109       0.00      0.00      0.00         4
         110       0.67      0.67      0.67         9
         111       0.90      1.00      0.95         9
         112       1.00      0.80      0.89         5
         113       0.00      0.00      0.00         9
         114       0.00      0.00      0.00         4
         115       1.00      1.00      1.00         4
         116       0.75      0.75      0.75         4
         117       0.57      1.00      0.73         4
         118       1.00      0.80      0.89         5
         119       0.55      0.67      0.60         9
         120       0.75      0.75      0.75         4
         121       1.00      1.00      1.00         5
         122       0.80      1.00      0.89         4
         123       0.75      0.60      0.67         5
         124       0.18      0.22      0.20         9
         125       0.88      0.78      0.82         9
         126       0.71      1.00      0.83         5
         127       0.50      1.00      0.67         4
         128       1.00      1.00      1.00         4
         129       0.67      1.00      0.80         4
         130       0.80      0.80      0.80         5
         131       1.00      0.75      0.86         4
         132       0.00      0.00      0.00         4
         133       0.89      0.89      0.89         9
         134       0.60      0.75      0.67         4
         135       0.16      0.75      0.26         4
         136       1.00      0.60      0.75         5
         137       1.00      1.00      1.00         4
         138       0.90      1.00      0.95         9
         139       0.27      0.75      0.40         4
         140       0.75      0.60      0.67         5
         141       0.57      0.67      0.62         6
         142       1.00      0.89      0.94         9
         143       0.67      0.67      0.67         9
         144       0.43      0.33      0.38         9
         145       1.00      0.67      0.80         9
         146       0.67      0.50      0.57         4
         147       0.80      1.00      0.89         4
         148       1.00      1.00      1.00         4
         149       1.00      0.78      0.88         9
         150       0.75      0.75      0.75         4
         151       1.00      0.75      0.86         4
         152       1.00      1.00      1.00         4
         153       0.69      1.00      0.82         9
         154       0.75      0.75      0.75         4
         155       0.57      1.00      0.73         4
         156       0.44      0.44      0.44         9
         157       0.33      0.25      0.29         4
         158       0.75      0.75      0.75         4
         159       0.09      0.25      0.13         4
         160       0.70      0.78      0.74         9
         161       1.00      1.00      1.00         4
         162       1.00      1.00      1.00         4
         163       0.17      0.25      0.20         4
         164       0.75      0.75      0.75         4
         165       1.00      1.00      1.00         5
         166       0.00      0.00      0.00         4
         167       0.25      0.25      0.25         4
         168       1.00      1.00      1.00         4
         169       0.57      1.00      0.73         4
         170       0.75      0.75      0.75         4
         171       1.00      0.67      0.80         9
         172       0.57      1.00      0.73         4
         173       0.80      0.89      0.84         9
         174       0.80      1.00      0.89         4
         175       1.00      0.75      0.86         4
         176       1.00      0.25      0.40         4
         177       1.00      1.00      1.00         5
         178       0.80      1.00      0.89         4
         179       0.33      0.20      0.25         5
         180       0.17      0.11      0.13         9
         181       0.40      0.50      0.44         4
         182       0.20      0.25      0.22         4
         183       0.88      0.78      0.82         9
         184       0.71      1.00      0.83         5
         185       1.00      1.00      1.00         4
         186       0.00      0.00      0.00         4
         187       0.57      1.00      0.73         4
         188       0.14      0.33      0.19         9
         189       1.00      0.75      0.86         4
         190       0.08      0.25      0.12         4
         191       0.57      1.00      0.73         4
         192       0.44      1.00      0.62         4
         193       0.60      0.75      0.67         4
         194       0.89      0.89      0.89         9
         195       0.83      1.00      0.91         5
         196       0.47      0.78      0.58         9
         197       0.07      0.25      0.11         4
         198       0.12      0.25      0.17         4
         199       0.75      0.75      0.75         4
         200       0.00      0.00      0.00         4
         201       1.00      0.67      0.80         9
         202       1.00      1.00      1.00         4
         203       1.00      1.00      1.00         4
         204       0.67      0.50      0.57         4
         205       0.15      0.22      0.18         9
         206       1.00      1.00      1.00         4
         207       1.00      0.89      0.94         9
         208       0.83      0.56      0.67         9
         209       1.00      0.50      0.67         4
         210       0.75      0.75      0.75         4
         211       0.25      0.25      0.25         4
         212       0.73      0.89      0.80         9
         213       0.82      1.00      0.90         9
         214       0.80      0.80      0.80         5
         215       0.50      0.25      0.33         4
         216       0.67      1.00      0.80         4
         217       0.67      1.00      0.80         4
         218       1.00      0.50      0.67         4
         219       1.00      1.00      1.00         5
         220       0.22      0.40      0.29         5
         221       0.00      0.00      0.00         4
         222       0.89      0.89      0.89         9
         223       1.00      1.00      1.00         5
         224       1.00      0.75      0.86         4
         225       0.00      0.00      0.00         4
         226       0.57      1.00      0.73         4
         227       0.82      1.00      0.90         9
         228       0.64      0.78      0.70         9
         229       0.67      1.00      0.80         4
         230       1.00      1.00      1.00         4
         231       1.00      0.78      0.88         9
         232       0.50      0.40      0.44         5
         233       0.60      0.67      0.63         9
         234       1.00      1.00      1.00         9
         235       1.00      1.00      1.00         5
         236       0.75      0.75      0.75         4
         237       1.00      0.56      0.71         9
         238       1.00      1.00      1.00         4
         239       1.00      0.50      0.67         4
         240       0.00      0.00      0.00         9
         241       0.00      0.00      0.00         4
         242       1.00      1.00      1.00         4
         243       0.75      0.75      0.75         4
         244       0.86      0.67      0.75         9
         245       0.67      1.00      0.80         4
         246       0.00      0.00      0.00         4
         247       0.50      0.40      0.44         5
         248       1.00      0.60      0.75         5
         249       0.88      0.78      0.82         9
         250       0.86      0.67      0.75         9
         251       0.80      1.00      0.89         4
         252       0.57      0.80      0.67         5
         253       0.08      0.11      0.09         9
         254       0.00      0.00      0.00         4
         255       1.00      0.89      0.94         9
         256       1.00      1.00      1.00         4
         257       0.50      0.44      0.47         9
         258       0.60      0.75      0.67         4
         259       0.67      1.00      0.80         4
         260       0.00      0.00      0.00         4
         261       0.00      0.00      0.00         4
         262       0.57      1.00      0.73         4
         263       0.88      0.78      0.82         9
         264       0.33      0.50      0.40         4
         265       0.50      0.60      0.55         5
         266       0.00      0.00      0.00         4
         267       0.80      1.00      0.89         4
         268       0.31      0.44      0.36         9
         269       1.00      0.56      0.71         9
         270       1.00      1.00      1.00         5
         271       1.00      0.75      0.86         4
         272       0.50      0.75      0.60         4
         273       0.40      0.22      0.29         9
         274       1.00      0.75      0.86         4
         275       1.00      1.00      1.00         9
         276       0.80      0.80      0.80         5
         277       0.33      0.25      0.29         4
         278       0.67      0.50      0.57         4
         279       0.73      0.89      0.80         9
         280       0.86      0.67      0.75         9
         281       0.67      0.50      0.57         4
         282       1.00      0.50      0.67         4
         283       0.82      1.00      0.90         9
         284       0.75      0.67      0.71         9
         285       0.00      0.00      0.00         4
         286       0.12      0.20      0.15         5
         287       1.00      0.67      0.80         9
         288       0.40      0.50      0.44         4
         289       1.00      0.25      0.40         4
         290       0.82      1.00      0.90         9
         291       1.00      0.50      0.67         4
         292       0.75      0.75      0.75         4
         293       0.00      0.00      0.00         9
         294       0.70      0.78      0.74         9
         295       1.00      0.50      0.67         4
         296       0.33      0.75      0.46         4
         297       1.00      1.00      1.00         4
         298       0.80      1.00      0.89         4
         299       0.60      0.75      0.67         4
         300       1.00      0.40      0.57         5
         301       0.00      0.00      0.00         4
         302       1.00      0.89      0.94         9
         303       1.00      0.50      0.67         4
         304       1.00      0.89      0.94         9
         305       0.50      0.75      0.60         4
         306       1.00      0.67      0.80         9
         307       1.00      1.00      1.00         5
         308       1.00      0.75      0.86         4
         309       0.89      0.89      0.89         9
         310       0.00      0.00      0.00         4
         311       0.50      0.25      0.33         4
         312       1.00      1.00      1.00         4
         313       1.00      0.89      0.94         9
         314       0.83      0.56      0.67         9
         315       0.67      1.00      0.80         4
         316       0.00      0.00      0.00         4
         317       1.00      1.00      1.00         5
         318       0.50      0.25      0.33         4
         319       0.50      0.75      0.60         4
         320       0.40      0.50      0.44         4
         321       1.00      1.00      1.00         4
         322       0.50      0.50      0.50         4
         323       0.60      0.75      0.67         4
         324       0.50      0.40      0.44         5
         325       0.80      0.80      0.80         5
         326       0.00      0.00      0.00         4
         327       1.00      0.89      0.94         9
         328       0.75      0.67      0.71         9
         329       1.00      0.22      0.36         9
         330       0.29      0.44      0.35         9
         331       0.40      0.50      0.44         4
         332       1.00      1.00      1.00         4
         333       0.00      0.00      0.00         4
         334       1.00      0.33      0.50         9
         335       1.00      0.89      0.94         9
         336       0.00      0.00      0.00         4
         337       0.50      0.75      0.60         4
         338       1.00      0.44      0.62         9
         339       0.00      0.00      0.00         4
         340       1.00      0.78      0.88         9
         341       0.67      0.80      0.73         5
         342       1.00      0.25      0.40         4
         343       0.00      0.00      0.00         9
         344       1.00      0.75      0.86         4
         345       0.75      0.75      0.75         4
         346       0.67      1.00      0.80         4
         347       0.80      0.80      0.80         5
         348       0.50      0.25      0.33         4
         349       0.80      1.00      0.89         4
         350       1.00      1.00      1.00         4
         351       1.00      1.00      1.00         4
         352       0.83      1.00      0.91         5
         353       0.90      1.00      0.95         9
         354       0.56      1.00      0.71         5
         355       0.75      0.75      0.75         4
         356       0.50      0.25      0.33         4
         357       0.50      0.75      0.60         4
         358       0.80      1.00      0.89         4
         359       0.80      1.00      0.89         4
         360       0.00      0.00      0.00         4
         361       1.00      1.00      1.00         4
         362       1.00      0.60      0.75         5
         363       0.71      1.00      0.83         5
         364       0.80      1.00      0.89         4
         365       0.80      0.80      0.80         5
         366       1.00      1.00      1.00         4
         367       0.75      0.75      0.75         4
         368       0.00      0.00      0.00         4
         369       1.00      1.00      1.00         5
         370       1.00      0.89      0.94         9
         371       0.73      0.89      0.80         9
         372       1.00      1.00      1.00         8
         373       1.00      1.00      1.00         4
         374       1.00      1.00      1.00         4
         375       1.00      0.80      0.89         5
         376       0.17      0.25      0.20         4
         377       1.00      0.25      0.40         4
         378       0.00      0.00      0.00         4
         379       0.83      1.00      0.91         5
         380       0.80      1.00      0.89         4
         381       1.00      1.00      1.00         4
         382       0.80      1.00      0.89         4
         383       0.75      0.60      0.67         5
         384       0.75      1.00      0.86         9
         385       0.80      1.00      0.89         4
         386       0.67      0.50      0.57         4
         387       0.29      0.56      0.38         9
         388       0.29      0.40      0.33         5
         389       0.56      0.56      0.56         9
         390       0.00      0.00      0.00         4
         391       0.00      0.00      0.00         9
         392       0.73      0.89      0.80         9
         393       1.00      1.00      1.00         5
         394       0.88      0.78      0.82         9
         395       0.67      0.67      0.67         9
         396       0.50      0.25      0.33         4
         397       0.67      0.44      0.53         9
         398       1.00      1.00      1.00         4
         399       1.00      0.60      0.75         5
         400       1.00      0.75      0.86         8
         401       0.67      1.00      0.80         4
         402       1.00      1.00      1.00         4
         403       0.00      0.00      0.00         4
         404       0.00      0.00      0.00         4
         405       0.55      0.67      0.60         9
         406       0.00      0.00      0.00         4
         407       1.00      0.75      0.86         4
         408       1.00      0.78      0.88         9
         409       0.67      1.00      0.80         4
         410       1.00      1.00      1.00         4
         411       0.80      1.00      0.89         4
         412       0.60      0.43      0.50         7
         413       1.00      0.50      0.67         4
         414       0.62      1.00      0.77         5
         415       1.00      0.75      0.86         4
         416       1.00      0.25      0.40         4
         417       0.75      0.75      0.75         4
         418       1.00      1.00      1.00         4
         419       1.00      0.75      0.86         4
         420       0.89      0.89      0.89         9
         421       1.00      0.80      0.89         5
         422       0.50      1.00      0.67         4
         423       0.90      1.00      0.95         9
         424       0.80      1.00      0.89         4
         425       0.50      0.44      0.47         9
         426       0.67      0.67      0.67         9
         427       0.60      0.75      0.67         4
         428       1.00      1.00      1.00         9
         429       1.00      1.00      1.00         7
         430       0.57      0.80      0.67         5
         431       0.67      0.50      0.57         4
         432       1.00      0.89      0.94         9
         433       1.00      0.50      0.67         4
         434       0.44      1.00      0.62         4
         435       1.00      1.00      1.00         4
         436       1.00      1.00      1.00         4
         437       0.88      0.78      0.82         9
         438       0.00      0.00      0.00         4
         439       1.00      0.60      0.75         5
         440       0.57      1.00      0.73         4
         441       0.40      0.44      0.42         9
         442       0.00      0.00      0.00         9
         443       0.67      0.89      0.76         9
         444       1.00      1.00      1.00         5
         445       0.89      0.89      0.89         9
         446       0.00      0.00      0.00         4
         447       1.00      1.00      1.00         5
         448       0.30      0.75      0.43         4
         449       1.00      0.20      0.33         5
         450       0.80      1.00      0.89         4
         451       1.00      0.75      0.86         4
         452       1.00      1.00      1.00         5
         453       0.00      0.00      0.00         4
         454       0.46      0.67      0.55         9
         455       0.50      0.20      0.29         5
         456       1.00      0.80      0.89         5
         457       0.57      1.00      0.73         4
         458       0.38      0.75      0.50         4
         459       0.00      0.00      0.00         4
         460       0.88      0.78      0.82         9
         461       0.80      1.00      0.89         4
         462       1.00      0.75      0.86         4
         463       0.57      1.00      0.73         4
         464       0.80      1.00      0.89         4
         465       1.00      1.00      1.00         9
         466       0.67      0.50      0.57         4
         467       1.00      0.44      0.62         9
         468       1.00      1.00      1.00         4
         469       0.33      0.22      0.27         9
         470       0.71      1.00      0.83         5
         471       0.75      0.60      0.67         5
         472       0.00      0.00      0.00         4
         473       0.75      0.75      0.75         4
         474       1.00      1.00      1.00         4
         475       0.80      1.00      0.89         4
         476       1.00      0.75      0.86         4
         477       0.80      0.80      0.80         5
         478       0.90      1.00      0.95         9
         479       0.20      0.11      0.14         9
         480       0.00      0.00      0.00         4
         481       0.00      0.00      0.00         4
         482       0.80      1.00      0.89         4
         483       1.00      1.00      1.00         9
         484       0.80      0.89      0.84         9
         485       0.00      0.00      0.00         4
         486       1.00      0.75      0.86         4
         487       0.00      0.00      0.00         4
         488       0.00      0.00      0.00         4
         489       0.50      0.22      0.31         9
         490       0.00      0.00      0.00         4
         491       0.00      0.00      0.00         4
         492       1.00      1.00      1.00         9
         493       0.83      1.00      0.91         5
         494       1.00      1.00      1.00         4
         495       0.00      0.00      0.00         4
         496       0.78      0.78      0.78         9
         497       0.50      0.50      0.50         4
         498       0.33      0.25      0.29         4
         499       0.43      0.33      0.38         9
         500       0.80      1.00      0.89         4
         501       0.00      0.00      0.00         4
         502       0.73      0.89      0.80         9
         503       0.67      0.50      0.57         4
         504       0.71      1.00      0.83         5
         505       0.90      1.00      0.95         9
         506       0.75      0.60      0.67         5
         507       0.80      0.89      0.84         9
         508       0.00      0.00      0.00         4
         509       0.00      0.00      0.00         4
         510       0.17      0.20      0.18         5
         511       1.00      1.00      1.00         5
         512       1.00      1.00      1.00         4
         513       0.00      0.00      0.00         4
         514       0.50      0.50      0.50         4
         515       0.00      0.00      0.00         4
         516       0.00      0.00      0.00         4
         517       0.50      1.00      0.67         5
         518       0.75      0.75      0.75         4
         519       0.80      1.00      0.89         4
         520       1.00      0.22      0.36         9
         521       1.00      1.00      1.00         5
         522       0.20      0.11      0.14         9
         523       1.00      1.00      1.00         4
         524       0.00      0.00      0.00         4
         525       0.83      1.00      0.91         5
         526       1.00      0.25      0.40         4
         527       0.33      0.20      0.25         5
         528       0.00      0.00      0.00         4
         529       0.20      0.25      0.22         4
         530       0.00      0.00      0.00         4
         531       0.80      1.00      0.89         4
         532       0.73      0.89      0.80         9
         533       1.00      1.00      1.00         4
         534       0.00      0.00      0.00         4
         535       0.90      1.00      0.95         9
         536       0.50      0.50      0.50         4
         537       1.00      0.67      0.80         9
         538       0.80      1.00      0.89         4
         539       0.50      0.75      0.60         4
         540       0.24      0.44      0.31         9
         541       0.67      0.40      0.50         5
         542       0.78      0.78      0.78         9
         543       0.00      0.00      0.00         4
         544       0.50      0.25      0.33         4
         545       0.36      0.44      0.40         9
         546       0.57      1.00      0.73         4
         547       0.50      0.25      0.33         4
         548       0.44      0.78      0.56         9
         549       1.00      0.75      0.86         4
         550       0.80      0.89      0.84         9
         551       0.75      0.75      0.75         4
         552       1.00      1.00      1.00         4
         553       0.75      0.75      0.75         4
         554       0.80      1.00      0.89         4
         555       0.75      0.75      0.75         4
         556       1.00      0.78      0.88         9
         557       0.50      0.50      0.50         4
         558       1.00      1.00      1.00         9
         559       1.00      0.50      0.67         4
         560       1.00      0.25      0.40         4
         561       0.80      1.00      0.89         4
         562       0.80      0.44      0.57         9
         563       0.00      0.00      0.00         9
         564       1.00      0.20      0.33         5
         565       1.00      1.00      1.00         9
         566       1.00      1.00      1.00         4
         567       0.33      0.60      0.43         5
         568       0.43      0.60      0.50         5
         569       1.00      0.75      0.86         4
         570       0.67      0.50      0.57         4
         571       0.60      0.75      0.67         4
         572       0.00      0.00      0.00         4
         573       0.57      1.00      0.73         4
         574       0.13      0.75      0.22         4
         575       0.50      0.75      0.60         4
         576       0.67      0.50      0.57         4
         577       0.00      0.00      0.00         4
         578       1.00      1.00      1.00         9
         579       0.00      0.00      0.00         4
         580       0.83      0.56      0.67         9
         581       0.89      0.89      0.89         9
         582       1.00      0.60      0.75         5
         583       0.80      1.00      0.89         4
         584       0.89      0.89      0.89         9
         585       0.33      0.20      0.25         5
         586       0.60      0.60      0.60         5
         587       0.62      0.89      0.73         9
         588       1.00      1.00      1.00         4
         589       1.00      1.00      1.00         4
         590       0.57      1.00      0.73         4
         591       1.00      0.75      0.86         4
         592       1.00      1.00      1.00         4
         593       0.67      1.00      0.80         4
         594       0.67      1.00      0.80         4
         595       0.80      1.00      0.89         4
         596       1.00      1.00      1.00         4
         597       1.00      1.00      1.00         4
         598       1.00      0.80      0.89         5
         599       0.17      0.20      0.18         5
         600       0.88      0.78      0.82         9
         601       0.14      1.00      0.25         4
         602       0.50      0.22      0.31         9
         603       0.50      0.25      0.33         4
         604       1.00      1.00      1.00         4
         605       0.00      0.00      0.00         4
         606       0.43      0.75      0.55         4
         607       0.70      0.78      0.74         9
         608       1.00      0.80      0.89         5
         609       0.43      0.75      0.55         4
         610       1.00      0.40      0.57         5
         611       0.43      0.60      0.50         5
         612       1.00      0.40      0.57         5
         613       1.00      1.00      1.00         4
         614       0.40      0.50      0.44         4
         615       1.00      0.75      0.86         4
         616       1.00      1.00      1.00         4
         617       1.00      1.00      1.00         4
         618       1.00      1.00      1.00         4
         619       0.50      0.25      0.33         4
         620       0.67      0.50      0.57         4
         621       1.00      1.00      1.00         4
         622       0.11      0.25      0.15         4
         623       0.89      0.89      0.89         9
         624       0.75      0.75      0.75         4
         625       0.50      0.75      0.60         4
         626       0.62      1.00      0.77         5
         627       0.67      1.00      0.80         4
         628       0.80      0.80      0.80         5
         629       1.00      0.89      0.94         9
         630       0.29      0.22      0.25         9
         631       1.00      0.40      0.57         5
         632       0.60      0.75      0.67         4
         633       0.75      0.75      0.75         4
         634       1.00      0.75      0.86         4
         635       0.00      0.00      0.00         4
         636       0.80      1.00      0.89         4
         637       0.67      0.50      0.57         4
         638       1.00      1.00      1.00         4
         639       0.80      0.89      0.84         9
         640       1.00      0.75      0.86         4
         641       0.00      0.00      0.00         4
         642       0.50      1.00      0.67         4
         643       1.00      1.00      1.00         5
         644       0.67      0.50      0.57         4
         645       1.00      1.00      1.00         9
         646       0.38      0.75      0.50         4
         647       1.00      0.75      0.86         4
         648       0.82      1.00      0.90         9
         649       0.80      0.89      0.84         9
         650       1.00      1.00      1.00         4
         651       0.00      0.00      0.00         9
         652       0.89      0.89      0.89         9
         653       0.50      0.80      0.62         5
         654       1.00      0.25      0.40         4
         655       0.88      0.78      0.82         9
         656       0.75      0.67      0.71         9
         657       1.00      1.00      1.00         4
         658       1.00      0.75      0.86         4
         659       0.57      1.00      0.73         4
         660       1.00      0.50      0.67         4
         661       1.00      0.50      0.67         4
         662       0.60      0.60      0.60         5
         663       0.00      0.00      0.00         4
         664       0.00      0.00      0.00         4
         665       0.67      0.80      0.73         5
         666       0.75      0.60      0.67         5
         667       1.00      1.00      1.00         5
         668       0.80      1.00      0.89         4
         669       0.00      0.00      0.00         4
         670       0.33      0.75      0.46         4
         671       0.64      1.00      0.78         9
         672       0.67      0.40      0.50         5
         673       0.73      0.89      0.80         9
         674       1.00      0.75      0.86         4
         675       1.00      0.75      0.86         4
         676       0.88      0.78      0.82         9
         677       1.00      0.78      0.88         9
         678       0.50      0.50      0.50         4
         679       0.00      0.00      0.00         4
         680       1.00      1.00      1.00         4
         681       0.50      0.67      0.57         9
         682       0.40      0.40      0.40         5
         683       1.00      0.89      0.94         9
         684       1.00      1.00      1.00         5
         685       0.78      0.78      0.78         9
         686       0.75      0.60      0.67         5
         687       0.50      0.50      0.50         4
         688       0.00      0.00      0.00         4
         689       0.50      0.25      0.33         4
         690       0.90      1.00      0.95         9
         691       0.67      0.50      0.57         4
         692       0.75      0.67      0.71         9
         693       1.00      0.67      0.80         9
         694       0.67      0.50      0.57         4
         695       1.00      0.50      0.67         4
         696       0.60      0.75      0.67         4
         697       1.00      0.33      0.50         9
         698       1.00      0.50      0.67         4
         699       0.67      0.89      0.76         9
         700       0.00      0.00      0.00         4
         701       1.00      1.00      1.00         4
         702       0.36      1.00      0.53         4
         703       1.00      0.75      0.86         4
         704       0.50      0.43      0.46         7
         705       0.80      1.00      0.89         4
         706       0.75      0.75      0.75         4
         707       0.83      1.00      0.91         5
         708       0.00      0.00      0.00         9
         709       1.00      1.00      1.00         5
         710       0.00      0.00      0.00         4
         711       0.00      0.00      0.00         4
         712       0.80      0.80      0.80         5
         713       1.00      1.00      1.00         5
         714       0.00      0.00      0.00         4
         715       0.00      0.00      0.00         9
         716       0.00      0.00      0.00         4
         717       0.67      1.00      0.80         4
         718       0.75      0.33      0.46         9
         719       0.58      0.78      0.67         9
         720       0.80      0.80      0.80         5
         721       0.67      0.50      0.57         4
         722       0.00      0.00      0.00         4
         723       0.00      0.00      0.00         4
         724       0.18      0.33      0.23         9
         725       0.86      0.67      0.75         9
         726       1.00      0.50      0.67         4
         727       0.00      0.00      0.00         5
         728       1.00      1.00      1.00         4
         729       0.50      0.75      0.60         4
         730       1.00      0.80      0.89         5
         731       0.90      1.00      0.95         9
         732       0.83      1.00      0.91         5
         733       0.71      1.00      0.83         5
         734       1.00      0.67      0.80         9
         735       1.00      0.44      0.62         9
         736       0.50      0.78      0.61         9
         737       1.00      0.20      0.33         5
         738       0.75      0.75      0.75         4
         739       0.67      1.00      0.80         4
         740       0.00      0.00      0.00         5
         741       1.00      1.00      1.00         4
         742       0.50      1.00      0.67         4
         743       1.00      1.00      1.00         4
         744       0.00      0.00      0.00         4
         745       0.00      0.00      0.00         4
         746       0.80      0.80      0.80         5
         747       0.67      0.50      0.57         4
         748       0.00      0.00      0.00         4
         749       0.80      1.00      0.89         4
         750       0.69      1.00      0.82         9
         751       0.62      1.00      0.77         5
         752       0.80      1.00      0.89         4
         753       1.00      0.33      0.50         9
         754       0.00      0.00      0.00         5
         755       1.00      1.00      1.00         5
         756       0.71      0.56      0.63         9
         757       0.67      1.00      0.80         4
         758       1.00      0.75      0.86         4
         759       0.14      0.11      0.12         9
         760       0.67      1.00      0.80         4
         761       0.00      0.00      0.00         9
         762       1.00      0.40      0.57         5
         763       0.70      0.78      0.74         9
         764       1.00      0.89      0.94         9
         765       0.83      1.00      0.91         5
         766       0.00      0.00      0.00         4
         767       1.00      0.75      0.86         4
         768       0.62      1.00      0.77         5
         769       1.00      0.50      0.67         4
         770       1.00      1.00      1.00         9
         771       0.80      0.80      0.80         5
         772       0.38      0.33      0.35         9
         773       0.33      0.25      0.29         4
         774       1.00      1.00      1.00         4
         775       1.00      1.00      1.00         4
         776       0.00      0.00      0.00         4
         777       0.67      1.00      0.80         4
         778       0.80      1.00      0.89         4
         779       1.00      0.50      0.67         4
         780       0.60      0.75      0.67         4
         781       0.89      0.89      0.89         9
         782       1.00      0.50      0.67         4
         783       0.80      0.44      0.57         9
         784       0.00      0.00      0.00         4
         785       1.00      0.80      0.89         5
         786       0.00      0.00      0.00         4
         787       0.88      0.78      0.82         9
         788       0.00      0.00      0.00         4
         789       0.75      0.75      0.75         4
         790       0.62      0.56      0.59         9
         791       1.00      1.00      1.00         4
         792       1.00      0.50      0.67         4
         793       0.80      1.00      0.89         4
         794       0.00      0.00      0.00         4
         795       1.00      0.89      0.94         9
         796       1.00      1.00      1.00         4
         797       0.58      0.78      0.67         9
         798       0.88      0.78      0.82         9
         799       1.00      0.80      0.89         5
         800       0.75      0.75      0.75         4
         801       0.62      0.56      0.59         9
         802       1.00      0.78      0.88         9
         803       0.50      0.25      0.33         4
         804       0.00      0.00      0.00         4
         805       0.80      1.00      0.89         4
         806       0.00      0.00      0.00         4
         807       0.50      0.75      0.60         4
         808       0.00      0.00      0.00         4
         809       1.00      0.25      0.40         4
         810       1.00      0.60      0.75         5
         811       0.88      0.78      0.82         9
         812       0.89      0.89      0.89         9
         813       0.67      1.00      0.80         4
         814       1.00      0.75      0.86         4
         815       0.00      0.00      0.00         4
         816       0.00      0.00      0.00         4
         817       1.00      0.67      0.80         9
         818       0.67      0.40      0.50         5
         819       0.88      0.78      0.82         9
         820       0.00      0.00      0.00         5
         821       0.80      1.00      0.89         4
         822       0.25      0.25      0.25         4
         823       1.00      0.60      0.75         5
         824       0.33      0.50      0.40         4
         825       0.78      0.78      0.78         9
         826       0.00      0.00      0.00         4
         827       0.67      0.44      0.53         9
         828       0.89      0.89      0.89         9
         829       1.00      0.11      0.20         9
         830       0.00      0.00      0.00         4
         831       0.75      0.67      0.71         9
         832       1.00      1.00      1.00         4
         833       0.00      0.00      0.00         9
         834       0.00      0.00      0.00         4
         835       0.40      0.40      0.40         5
         836       0.78      0.78      0.78         9
         837       0.82      1.00      0.90         9
         838       0.00      0.00      0.00         5
         839       0.86      0.67      0.75         9
         840       0.00      0.00      0.00         4
         841       0.60      0.75      0.67         4
         842       0.67      0.50      0.57         4
         843       1.00      1.00      1.00         4
         844       0.64      0.78      0.70         9
         845       0.57      0.80      0.67         5
         846       0.00      0.00      0.00         5
         847       1.00      0.80      0.89         5
         848       0.88      0.78      0.82         9
         849       0.60      0.75      0.67         4
         850       1.00      1.00      1.00         4
         851       1.00      0.60      0.75         5
         852       0.00      0.00      0.00         4
         853       0.86      0.67      0.75         9
         854       0.60      0.75      0.67         4
         855       0.75      0.75      0.75         4
         856       1.00      0.80      0.89         5
         857       0.83      1.00      0.91         5
         858       0.75      0.75      0.75         4
         859       0.00      0.00      0.00         4
         860       0.00      0.00      0.00         4
         861       1.00      0.78      0.88         9
         862       0.00      0.00      0.00         5
         863       1.00      0.25      0.40         4
         864       0.00      0.00      0.00         9
         865       0.00      0.00      0.00         4
         866       0.67      1.00      0.80         4
         867       0.75      0.75      0.75         4
         868       0.20      0.20      0.20         5
         869       0.88      0.78      0.82         9
         870       0.71      1.00      0.83         5
         871       0.00      0.00      0.00         4
         872       0.00      0.00      0.00         4
         873       0.80      0.89      0.84         9
         874       0.14      0.75      0.24         4
         875       1.00      0.75      0.86         4
         876       1.00      1.00      1.00         5
         877       0.36      0.89      0.52         9
         878       0.43      1.00      0.60         9
         879       0.20      0.25      0.22         4
         880       0.25      0.22      0.24         9
         881       1.00      0.75      0.86         4
         882       0.44      0.78      0.56         9
         883       0.00      0.00      0.00         4
         884       0.07      0.25      0.11         4
         885       0.67      0.50      0.57         4
         886       0.67      0.40      0.50         5
         887       0.50      0.25      0.33         4
         888       0.60      1.00      0.75         9
         889       0.00      0.00      0.00         4
         890       0.33      0.25      0.29         4
         891       0.64      1.00      0.78         9
         892       0.00      0.00      0.00         4
         893       0.64      1.00      0.78         9

    accuracy                           0.66      4917
   macro avg       0.65      0.64      0.62      4917
weighted avg       0.67      0.66      0.64      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
