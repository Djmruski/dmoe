	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [486, 571, 62, 860, 524, 664, 261, 600, 119, 716, 413, 39, 740, 387, 112, 56, 251, 429, 737, 865, 125, 764, 233, 836, 187, 840, 362, 832, 534, 891, 453, 827, 780, 526, 698, 166, 171, 342, 163, 456, 510, 339, 845, 877, 380, 681, 733, 307, 363, 397, 223, 102, 280, 581, 795, 408, 624, 488, 722, 742, 874, 93, 810, 371, 783, 36, 797, 299, 283, 489, 484, 715, 483, 227, 72, 48, 754, 192, 687, 536, 164, 882, 330, 248, 143, 199, 604, 384, 297, 229, 350, 379, 432, 851, 778, 31, 257, 763, 744, 247, 814, 424, 196, 782, 73, 340, 314, 394, 513, 300, 813, 642, 761, 859, 616, 179, 768, 288, 588, 591, 746, 611, 529, 156, 368, 628, 465, 427, 822, 6, 7, 249, 127, 414, 311, 315, 843, 820, 520, 791, 720, 867, 117, 302, 194, 475, 242, 103, 18, 461, 772, 174, 26, 494, 868, 541, 159, 596, 812, 167, 421, 683, 335, 279, 636, 662, 855, 181, 100, 792, 563, 645, 561, 823, 70, 405, 638, 364, 674, 719, 560, 220, 354, 605, 837, 804, 558, 459, 682, 599, 657, 819, 508, 717, 639, 425, 660, 431, 370, 847, 781, 890, 80, 802, 655, 650, 774, 79, 673, 419, 221, 735, 564, 264, 23, 709, 404, 495, 68, 44, 725, 469, 659, 498, 141, 98, 825, 423, 512, 33, 409, 240, 152, 320, 585, 787, 24, 723, 690, 49, 374, 838, 603, 848, 47, 523, 78, 502, 347, 110, 694, 710, 756, 793, 566, 130, 665, 176, 291, 708, 316, 519, 71, 234, 584, 614, 767, 55, 449, 829, 186, 661, 760, 191, 322, 391, 190, 377, 189, 835, 168, 736, 779, 695, 433, 646, 410, 619, 319, 597, 416, 381, 841, 755, 806, 232, 375, 769, 649, 501, 542, 598, 759, 627, 876, 8, 689, 438, 738, 243, 177, 225, 367, 276, 507, 446, 75, 533, 292, 570, 568, 366, 613, 888, 105, 402, 766, 428, 213, 357, 864, 369, 816, 893, 808, 870, 11, 574, 99, 875, 149, 809, 396, 547, 157, 132, 310, 324, 185, 34, 435, 858, 758, 747, 479, 765, 407, 773, 400, 663, 741, 262, 104, 668, 834, 355, 587, 17, 111, 434, 107, 789, 43, 193, 811, 241, 679, 553, 46, 644, 204, 853, 373, 850, 852, 238, 872, 884, 670, 287, 750, 399, 331, 732, 277, 724, 134, 629, 282, 702, 762, 109, 540, 389, 615, 14, 807, 535, 305, 521, 309, 525, 265, 442, 195, 417, 254, 577, 90, 680, 430, 550, 61, 361, 337, 135, 721, 158, 509, 612, 641, 471, 38, 398, 496, 28, 246, 57, 65, 295, 748, 648, 271, 54, 345, 651, 334, 256, 562, 202, 515, 378, 440, 64, 549, 743, 258, 821, 395, 551, 796, 353, 451, 472, 118, 123, 609, 108, 727, 620, 726, 672, 161, 244, 1, 19, 63, 444, 203, 328, 66, 684, 487, 677, 617, 41, 178, 770, 654, 173, 120, 303, 144, 212, 147, 95, 580, 392, 705, 700, 206, 790, 863, 785, 527, 325, 699, 298, 692, 403, 180, 892, 464, 145, 376, 504, 734, 116, 136, 365, 267, 707, 215, 669, 567, 250, 151, 466, 372, 776, 426, 160, 854, 777, 637, 713, 817, 273, 92, 360, 530, 228, 165, 857, 224, 53, 633, 517, 718, 436, 290, 666, 275, 51, 443, 745, 0, 739, 96, 201, 582, 880, 468, 91, 478, 485, 826, 15, 794, 333, 621, 200, 97, 511, 32, 800, 332, 401, 711, 704, 499, 544, 844, 575, 818, 631, 390, 344, 198, 623, 618, 565, 730, 85, 445, 59, 610, 260, 554, 686, 607, 12, 94, 74, 635, 688, 506, 671, 2, 88, 640, 630, 658, 889, 626, 467, 643, 706, 327, 306, 274, 753, 52, 532, 714, 757, 590, 383, 386, 606, 359, 622, 37, 831, 150, 691, 140, 197, 114, 175, 586, 268, 602, 231, 751, 137, 601, 572, 448, 81, 253, 115, 146, 569, 771, 557, 9, 556, 579, 126, 873, 545, 828, 162, 697, 647, 573, 462, 222, 460, 457, 458, 69, 207, 210, 272, 539, 887, 205, 122, 76, 16, 824, 652, 139, 236, 235, 576, 113, 798, 799, 878, 420, 313, 491, 839, 418, 411, 474, 886, 696, 217, 86, 546, 153, 338, 184, 815, 676, 505, 281, 133, 803, 60, 490, 321, 326, 869, 531, 131, 148, 480, 124, 518, 252, 856, 138, 493, 348, 83, 589, 842, 752, 237, 559, 712, 805, 172, 516, 106, 58, 21, 45, 595, 289, 349, 393, 883, 4, 470, 87, 266, 653, 128, 343, 437, 412, 308, 441, 304, 183, 830, 538, 269, 879, 439, 263, 634, 729, 476, 656, 537, 473, 226, 84, 211, 786, 10, 775, 352, 170, 208, 239, 219, 482, 278, 77, 216, 594, 788, 270, 701, 866, 214, 5, 42, 492, 388, 861, 25, 67, 382, 245, 632, 129, 801, 693, 503, 29, 142, 522, 678, 731, 592, 82, 728, 50, 35, 40, 169, 317, 871, 89, 155, 463, 578, 284, 481, 675, 101, 849, 318, 749, 329, 422, 552, 230, 351, 259, 685, 301, 514, 341, 608, 450, 528, 218, 703, 862, 188, 358, 455, 454, 209, 296, 415, 3, 593, 406, 784, 548, 625, 285, 356, 833, 22, 477, 452, 885, 182, 881, 846, 13, 30, 555, 583, 27, 312, 667, 346, 20, 385, 336, 286, 293, 323, 543, 447, 294, 500, 255, 121, 154, 497]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.793, TAw acc= 37.0% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.323, TAw acc= 44.1% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.992, TAw acc= 51.2% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.743, TAw acc= 65.4% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.542, TAw acc= 74.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 320 train exemplars, time=  0.0s
320
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.477 | TAw acc= 79.2%, forg=  0.0%| TAg acc= 79.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.375, TAw acc= 50.0% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.845, TAw acc= 69.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.474, TAw acc= 80.3% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.166, TAw acc= 80.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.925, TAw acc= 81.8% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.922, TAw acc= 81.8% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.920, TAw acc= 81.8% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.917, TAw acc= 81.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.915, TAw acc= 81.8% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.913, TAw acc= 81.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 480 train exemplars, time=  0.0s
480
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.405 | TAw acc= 91.9%, forg=-12.7%| TAg acc= 83.8%, forg= -4.6% <<<
>>> Test on task  1 : loss=1.756 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 61.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.410, TAw acc= 46.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.790, TAw acc= 63.4% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.326, TAw acc= 66.2% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.955, TAw acc= 73.2% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.662, TAw acc= 83.1% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.659, TAw acc= 83.1% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.657, TAw acc= 83.1% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.654, TAw acc= 83.1% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.4s loss=1.652, TAw acc= 84.5% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.4s loss=1.649, TAw acc= 84.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 640 train exemplars, time=  0.0s
640
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.256 | TAw acc= 92.5%, forg= -0.6%| TAg acc= 90.2%, forg= -6.4% <<<
>>> Test on task  1 : loss=1.622 | TAw acc= 95.7%, forg= -6.5%| TAg acc= 66.7%, forg= -5.4% <<<
>>> Test on task  2 : loss=1.739 | TAw acc= 84.8%, forg=  0.0%| TAg acc= 64.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.958, TAw acc= 46.1% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.035, TAw acc= 48.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.380, TAw acc= 61.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.980, TAw acc= 72.4% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.682, TAw acc= 78.9% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.679, TAw acc= 78.9% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.676, TAw acc= 78.9% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.674, TAw acc= 78.9% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.671, TAw acc= 78.9% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.669, TAw acc= 78.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 789 train exemplars, time=  0.0s
789
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.272 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 78.6%, forg= 11.6% <<<
>>> Test on task  1 : loss=1.491 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 72.0%, forg= -5.4% <<<
>>> Test on task  2 : loss=1.641 | TAw acc= 97.0%, forg=-12.1%| TAg acc= 75.8%, forg=-11.1% <<<
>>> Test on task  3 : loss=1.565 | TAw acc= 89.5%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.683, TAw acc= 33.7% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.762, TAw acc= 54.2% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.185, TAw acc= 78.3% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.831, TAw acc= 86.7% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 90.4% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.543, TAw acc= 90.4% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.541, TAw acc= 90.4% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.538, TAw acc= 90.4% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.536, TAw acc= 90.4% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.533, TAw acc= 90.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 929 train exemplars, time=  0.0s
929
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.185 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 82.1%, forg=  8.1% <<<
>>> Test on task  1 : loss=1.252 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 84.9%, forg=-12.9% <<<
>>> Test on task  2 : loss=1.390 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 76.8%, forg= -1.0% <<<
>>> Test on task  3 : loss=1.551 | TAw acc= 98.1%, forg= -8.6%| TAg acc= 75.2%, forg= -1.0% <<<
>>> Test on task  4 : loss=1.464 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 78.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.973, TAw acc= 28.7% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.841, TAw acc= 52.5% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.278, TAw acc= 68.8% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.906, TAw acc= 67.5% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.3s loss=1.651, TAw acc= 76.2% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.648, TAw acc= 76.2% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.646, TAw acc= 76.2% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.644, TAw acc= 75.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.641, TAw acc= 75.0% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.639, TAw acc= 75.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1069 train exemplars, time=  0.0s
1069
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.190 | TAw acc= 94.2%, forg= -1.7%| TAg acc= 82.7%, forg=  7.5% <<<
>>> Test on task  1 : loss=1.131 | TAw acc= 97.8%, forg= -2.2%| TAg acc= 80.6%, forg=  4.3% <<<
>>> Test on task  2 : loss=1.210 | TAw acc= 99.0%, forg= -2.0%| TAg acc= 75.8%, forg=  1.0% <<<
>>> Test on task  3 : loss=1.335 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.9%, forg= -7.6% <<<
>>> Test on task  4 : loss=1.539 | TAw acc= 94.6%, forg= -3.6%| TAg acc= 67.9%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.540 | TAw acc= 79.8%, forg=  0.0%| TAg acc= 67.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.823, TAw acc= 34.0% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.634, TAw acc= 60.6% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.016, TAw acc= 75.5% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.656, TAw acc= 83.0% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.413, TAw acc= 85.1% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.411, TAw acc= 85.1% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.409, TAw acc= 85.1% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.407, TAw acc= 85.1% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.405, TAw acc= 85.1% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.403, TAw acc= 85.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1209 train exemplars, time=  0.0s
1209
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.127 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 83.2%, forg=  6.9% <<<
>>> Test on task  1 : loss=1.056 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 87.1%, forg= -2.2% <<<
>>> Test on task  2 : loss=1.184 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.8%, forg= -1.0% <<<
>>> Test on task  3 : loss=1.265 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 77.1%, forg=  5.7% <<<
>>> Test on task  4 : loss=1.310 | TAw acc= 98.2%, forg= -3.6%| TAg acc= 75.9%, forg=  2.7% <<<
>>> Test on task  5 : loss=1.794 | TAw acc= 91.7%, forg=-11.9%| TAg acc= 60.6%, forg=  6.4% <<<
>>> Test on task  6 : loss=1.419 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 71.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.707, TAw acc= 55.7% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.327, TAw acc= 53.2% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.602, TAw acc= 60.8% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.078, TAw acc= 73.4% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.821, TAw acc= 83.5% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.3s loss=1.819, TAw acc= 83.5% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.817, TAw acc= 83.5% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.814, TAw acc= 83.5% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.812, TAw acc= 83.5% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.810, TAw acc= 83.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1349 train exemplars, time=  0.0s
1349
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.105 | TAw acc= 94.8%, forg= -0.6%| TAg acc= 85.0%, forg=  5.2% <<<
>>> Test on task  1 : loss=1.125 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 79.6%, forg=  7.5% <<<
>>> Test on task  2 : loss=0.994 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 84.8%, forg= -7.1% <<<
>>> Test on task  3 : loss=1.084 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 84.8%, forg= -1.9% <<<
>>> Test on task  4 : loss=1.148 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 84.8%, forg= -6.2% <<<
>>> Test on task  5 : loss=1.575 | TAw acc= 92.7%, forg= -0.9%| TAg acc= 67.9%, forg= -0.9% <<<
>>> Test on task  6 : loss=1.659 | TAw acc= 89.6%, forg= -1.6%| TAg acc= 64.8%, forg=  6.4% <<<
>>> Test on task  7 : loss=1.784 | TAw acc= 85.0%, forg=  0.0%| TAg acc= 59.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=4.828, TAw acc= 28.2% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.458, TAw acc= 46.2% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.563, TAw acc= 66.7% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.080, TAw acc= 73.1% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.762, TAw acc= 76.9% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.756, TAw acc= 76.9% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.751, TAw acc= 76.9% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.746, TAw acc= 76.9% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.742, TAw acc= 76.9% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.737, TAw acc= 76.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1489 train exemplars, time=  0.0s
1489
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.126 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 79.8%, forg= 10.4% <<<
>>> Test on task  1 : loss=1.065 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 87.1%, forg=  0.0% <<<
>>> Test on task  2 : loss=0.914 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 85.9%, forg= -1.0% <<<
>>> Test on task  3 : loss=1.006 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.7%, forg= -1.0% <<<
>>> Test on task  4 : loss=1.073 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 83.9%, forg=  0.9% <<<
>>> Test on task  5 : loss=1.477 | TAw acc= 90.8%, forg=  1.8%| TAg acc= 69.7%, forg= -1.8% <<<
>>> Test on task  6 : loss=1.529 | TAw acc= 94.4%, forg= -4.8%| TAg acc= 69.6%, forg=  1.6% <<<
>>> Test on task  7 : loss=1.959 | TAw acc= 91.6%, forg= -6.5%| TAg acc= 44.9%, forg= 15.0% <<<
>>> Test on task  8 : loss=1.633 | TAw acc= 84.0%, forg=  0.0%| TAg acc= 51.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=5.177, TAw acc= 29.1% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.647, TAw acc= 43.0% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.925, TAw acc= 53.2% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.480, TAw acc= 72.2% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.147, TAw acc= 72.2% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.145, TAw acc= 72.2% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.142, TAw acc= 72.2% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.140, TAw acc= 72.2% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.137, TAw acc= 72.2% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.135, TAw acc= 72.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1629 train exemplars, time=  0.0s
1629
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.134 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 79.2%, forg= 11.0% <<<
>>> Test on task  1 : loss=1.067 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 86.0%, forg=  1.1% <<<
>>> Test on task  2 : loss=0.903 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 84.8%, forg=  1.0% <<<
>>> Test on task  3 : loss=0.938 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 86.7%, forg= -1.0% <<<
>>> Test on task  4 : loss=0.958 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 85.7%, forg= -0.9% <<<
>>> Test on task  5 : loss=1.300 | TAw acc= 95.4%, forg= -2.8%| TAg acc= 74.3%, forg= -4.6% <<<
>>> Test on task  6 : loss=1.377 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 77.6%, forg= -6.4% <<<
>>> Test on task  7 : loss=1.855 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 46.7%, forg= 13.1% <<<
>>> Test on task  8 : loss=1.941 | TAw acc= 89.6%, forg= -5.7%| TAg acc= 37.7%, forg= 14.2% <<<
>>> Test on task  9 : loss=1.749 | TAw acc= 84.3%, forg=  0.0%| TAg acc= 56.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.4s loss=4.121, TAw acc= 42.5% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.4s loss=2.741, TAw acc= 66.7% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.4s loss=2.031, TAw acc= 75.9% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.663, TAw acc= 78.2% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.424, TAw acc= 90.8% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.421, TAw acc= 90.8% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.419, TAw acc= 90.8% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.417, TAw acc= 90.8% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.415, TAw acc= 90.8% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.413, TAw acc= 90.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1769 train exemplars, time=  0.0s
1769
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.104 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 79.8%, forg= 10.4% <<<
>>> Test on task  1 : loss=0.995 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 84.9%, forg=  2.2% <<<
>>> Test on task  2 : loss=0.918 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 81.8%, forg=  4.0% <<<
>>> Test on task  3 : loss=0.906 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 87.6%, forg= -1.0% <<<
>>> Test on task  4 : loss=0.923 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 88.4%, forg= -2.7% <<<
>>> Test on task  5 : loss=1.263 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 78.0%, forg= -3.7% <<<
>>> Test on task  6 : loss=1.318 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 80.0%, forg= -2.4% <<<
>>> Test on task  7 : loss=1.689 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 53.3%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.829 | TAw acc= 86.8%, forg=  2.8%| TAg acc= 35.8%, forg= 16.0% <<<
>>> Test on task  9 : loss=1.965 | TAw acc= 86.1%, forg= -1.9%| TAg acc= 39.8%, forg= 16.7% <<<
>>> Test on task 10 : loss=1.389 | TAw acc= 84.6%, forg=  0.0%| TAg acc= 59.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.3s loss=4.385, TAw acc= 49.0% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.635, TAw acc= 76.0% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.909, TAw acc= 85.0% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.581, TAw acc= 91.0% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.336, TAw acc= 93.0% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.334, TAw acc= 93.0% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.332, TAw acc= 93.0% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.330, TAw acc= 93.0% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.328, TAw acc= 93.0% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.326, TAw acc= 93.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1909 train exemplars, time=  0.0s
1909
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.144 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 77.5%, forg= 12.7% <<<
>>> Test on task  1 : loss=0.998 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 81.7%, forg=  5.4% <<<
>>> Test on task  2 : loss=0.929 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 76.8%, forg=  9.1% <<<
>>> Test on task  3 : loss=0.968 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.9%, forg=  4.8% <<<
>>> Test on task  4 : loss=0.923 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 84.8%, forg=  3.6% <<<
>>> Test on task  5 : loss=1.163 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 81.7%, forg= -3.7% <<<
>>> Test on task  6 : loss=1.239 | TAw acc= 95.2%, forg= -0.8%| TAg acc= 81.6%, forg= -1.6% <<<
>>> Test on task  7 : loss=1.632 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 56.1%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.771 | TAw acc= 87.7%, forg=  1.9%| TAg acc= 39.6%, forg= 12.3% <<<
>>> Test on task  9 : loss=1.728 | TAw acc= 86.1%, forg=  0.0%| TAg acc= 48.1%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.664 | TAw acc= 97.4%, forg=-12.8%| TAg acc= 47.9%, forg= 12.0% <<<
>>> Test on task 11 : loss=1.261 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 72.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.423, TAw acc= 36.7% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=3.651, TAw acc= 41.8% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.837, TAw acc= 63.3% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.323, TAw acc= 74.7% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.910, TAw acc= 74.7% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.908, TAw acc= 74.7% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.905, TAw acc= 74.7% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.903, TAw acc= 74.7% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.901, TAw acc= 74.7% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.899, TAw acc= 74.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2049 train exemplars, time=  0.0s
2049
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.117 | TAw acc= 94.2%, forg=  0.6%| TAg acc= 78.6%, forg= 11.6% <<<
>>> Test on task  1 : loss=1.006 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 80.6%, forg=  6.5% <<<
>>> Test on task  2 : loss=0.919 | TAw acc=100.0%, forg= -1.0%| TAg acc= 84.8%, forg=  1.0% <<<
>>> Test on task  3 : loss=0.934 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 84.8%, forg=  2.9% <<<
>>> Test on task  4 : loss=0.869 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 86.6%, forg=  1.8% <<<
>>> Test on task  5 : loss=1.166 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  6 : loss=1.222 | TAw acc= 96.0%, forg= -0.8%| TAg acc= 83.2%, forg= -1.6% <<<
>>> Test on task  7 : loss=1.590 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 63.6%, forg= -3.7% <<<
>>> Test on task  8 : loss=1.674 | TAw acc= 90.6%, forg= -0.9%| TAg acc= 52.8%, forg= -0.9% <<<
>>> Test on task  9 : loss=1.672 | TAw acc= 87.0%, forg= -0.9%| TAg acc= 54.6%, forg=  1.9% <<<
>>> Test on task 10 : loss=1.476 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 57.3%, forg=  2.6% <<<
>>> Test on task 11 : loss=1.686 | TAw acc= 98.5%, forg= -3.8%| TAg acc= 59.1%, forg= 12.9% <<<
>>> Test on task 12 : loss=1.689 | TAw acc= 86.0%, forg=  0.0%| TAg acc= 57.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=4.226, TAw acc= 54.8% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.389, TAw acc= 68.3% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.776, TAw acc= 80.8% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.498, TAw acc= 90.4% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.273, TAw acc= 93.3% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.3s loss=1.272, TAw acc= 93.3% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.270, TAw acc= 93.3% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=1.268, TAw acc= 93.3% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.266, TAw acc= 93.3% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.3s loss=1.264, TAw acc= 93.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2189 train exemplars, time=  0.0s
2189
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.120 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 79.2%, forg= 11.0% <<<
>>> Test on task  1 : loss=1.041 | TAw acc= 96.8%, forg=  1.1%| TAg acc= 82.8%, forg=  4.3% <<<
>>> Test on task  2 : loss=0.946 | TAw acc=100.0%, forg=  0.0%| TAg acc= 79.8%, forg=  6.1% <<<
>>> Test on task  3 : loss=0.895 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 87.6%, forg=  0.0% <<<
>>> Test on task  4 : loss=0.877 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 84.8%, forg=  3.6% <<<
>>> Test on task  5 : loss=1.126 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  6 : loss=1.285 | TAw acc= 94.4%, forg=  1.6%| TAg acc= 74.4%, forg=  8.8% <<<
>>> Test on task  7 : loss=1.637 | TAw acc= 90.7%, forg=  0.9%| TAg acc= 60.7%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.680 | TAw acc= 88.7%, forg=  1.9%| TAg acc= 44.3%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.509 | TAw acc= 87.0%, forg=  0.0%| TAg acc= 61.1%, forg= -4.6% <<<
>>> Test on task 10 : loss=1.311 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 73.5%, forg=-13.7% <<<
>>> Test on task 11 : loss=1.538 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 56.1%, forg= 15.9% <<<
>>> Test on task 12 : loss=1.966 | TAw acc= 92.5%, forg= -6.5%| TAg acc= 37.4%, forg= 20.6% <<<
>>> Test on task 13 : loss=1.143 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=4.498, TAw acc= 50.0% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.839, TAw acc= 63.0% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.144, TAw acc= 81.5% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.749, TAw acc= 87.0% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.460, TAw acc= 88.0% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.460, TAw acc= 88.0% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.459, TAw acc= 88.0% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.458, TAw acc= 88.0% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.457, TAw acc= 88.0% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.456, TAw acc= 88.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2329 train exemplars, time=  0.0s
2329
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.154 | TAw acc= 96.0%, forg= -1.2%| TAg acc= 79.8%, forg= 10.4% <<<
>>> Test on task  1 : loss=1.045 | TAw acc= 96.8%, forg=  1.1%| TAg acc= 74.2%, forg= 12.9% <<<
>>> Test on task  2 : loss=0.928 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 82.8%, forg=  3.0% <<<
>>> Test on task  3 : loss=0.883 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 91.4%, forg= -3.8% <<<
>>> Test on task  4 : loss=1.014 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 77.7%, forg= 10.7% <<<
>>> Test on task  5 : loss=1.146 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task  6 : loss=1.194 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 77.6%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.518 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 64.5%, forg= -0.9% <<<
>>> Test on task  8 : loss=1.673 | TAw acc= 88.7%, forg=  1.9%| TAg acc= 49.1%, forg=  3.8% <<<
>>> Test on task  9 : loss=1.421 | TAw acc= 88.0%, forg= -0.9%| TAg acc= 63.9%, forg= -2.8% <<<
>>> Test on task 10 : loss=1.319 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 65.0%, forg=  8.5% <<<
>>> Test on task 11 : loss=1.402 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 64.4%, forg=  7.6% <<<
>>> Test on task 12 : loss=1.756 | TAw acc= 94.4%, forg= -1.9%| TAg acc= 45.8%, forg= 12.1% <<<
>>> Test on task 13 : loss=1.651 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 50.7%, forg= 23.5% <<<
>>> Test on task 14 : loss=1.387 | TAw acc= 89.3%, forg=  0.0%| TAg acc= 63.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=4.975, TAw acc= 54.5% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.684, TAw acc= 70.5% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.896, TAw acc= 84.1% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.474, TAw acc= 90.9% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.267, TAw acc= 90.9% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.265, TAw acc= 90.9% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.263, TAw acc= 90.9% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.262, TAw acc= 90.9% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.260, TAw acc= 90.9% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.259, TAw acc= 90.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2469 train exemplars, time=  0.0s
2469
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.157 | TAw acc= 94.2%, forg=  1.7%| TAg acc= 77.5%, forg= 12.7% <<<
>>> Test on task  1 : loss=1.127 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 71.0%, forg= 16.1% <<<
>>> Test on task  2 : loss=0.897 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 82.8%, forg=  3.0% <<<
>>> Test on task  3 : loss=0.899 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 87.6%, forg=  3.8% <<<
>>> Test on task  4 : loss=0.870 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 85.7%, forg=  2.7% <<<
>>> Test on task  5 : loss=1.098 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.143 | TAw acc= 95.2%, forg=  0.8%| TAg acc= 82.4%, forg=  0.8% <<<
>>> Test on task  7 : loss=1.592 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 65.4%, forg= -0.9% <<<
>>> Test on task  8 : loss=1.688 | TAw acc= 90.6%, forg=  0.0%| TAg acc= 46.2%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.402 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 62.0%, forg=  1.9% <<<
>>> Test on task 10 : loss=1.220 | TAw acc= 99.1%, forg= -1.7%| TAg acc= 67.5%, forg=  6.0% <<<
>>> Test on task 11 : loss=1.266 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 69.7%, forg=  2.3% <<<
>>> Test on task 12 : loss=1.633 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 57.9%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.528 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 55.9%, forg= 18.4% <<<
>>> Test on task 14 : loss=1.701 | TAw acc= 95.1%, forg= -5.7%| TAg acc= 50.0%, forg= 13.1% <<<
>>> Test on task 15 : loss=1.322 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 71.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=5.767, TAw acc= 42.7% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=3.587, TAw acc= 64.0% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.366, TAw acc= 84.0% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.848, TAw acc= 90.7% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.532, TAw acc= 90.7% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.530, TAw acc= 90.7% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.529, TAw acc= 90.7% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.527, TAw acc= 90.7% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.526, TAw acc= 90.7% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.524, TAw acc= 90.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2609 train exemplars, time=  0.0s
2609
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.164 | TAw acc= 94.2%, forg=  1.7%| TAg acc= 77.5%, forg= 12.7% <<<
>>> Test on task  1 : loss=1.091 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 77.4%, forg=  9.7% <<<
>>> Test on task  2 : loss=0.900 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 85.9%, forg=  0.0% <<<
>>> Test on task  3 : loss=0.886 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 84.8%, forg=  6.7% <<<
>>> Test on task  4 : loss=0.917 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.6%, forg=  9.8% <<<
>>> Test on task  5 : loss=1.041 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.179 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 84.0%, forg= -0.8% <<<
>>> Test on task  7 : loss=1.559 | TAw acc= 92.5%, forg= -0.9%| TAg acc= 65.4%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.664 | TAw acc= 89.6%, forg=  0.9%| TAg acc= 49.1%, forg=  3.8% <<<
>>> Test on task  9 : loss=1.345 | TAw acc= 87.0%, forg=  0.9%| TAg acc= 67.6%, forg= -3.7% <<<
>>> Test on task 10 : loss=1.176 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 70.9%, forg=  2.6% <<<
>>> Test on task 11 : loss=1.262 | TAw acc= 98.5%, forg=  0.0%| TAg acc= 66.7%, forg=  5.3% <<<
>>> Test on task 12 : loss=1.584 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 51.4%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.361 | TAw acc= 96.3%, forg= -0.7%| TAg acc= 67.6%, forg=  6.6% <<<
>>> Test on task 14 : loss=1.558 | TAw acc= 95.9%, forg= -0.8%| TAg acc= 57.4%, forg=  5.7% <<<
>>> Test on task 15 : loss=1.605 | TAw acc= 94.9%, forg= -5.1%| TAg acc= 50.8%, forg= 20.3% <<<
>>> Test on task 16 : loss=1.732 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 58.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=4.573, TAw acc= 61.4% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.553, TAw acc= 69.9% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.820, TAw acc= 86.7% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.491, TAw acc= 91.6% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 96.4% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 96.4% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.292, TAw acc= 96.4% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.290, TAw acc= 96.4% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.288, TAw acc= 96.4% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2749 train exemplars, time=  0.0s
2749
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.242 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 74.6%, forg= 15.6% <<<
>>> Test on task  1 : loss=1.055 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 73.1%, forg= 14.0% <<<
>>> Test on task  2 : loss=0.966 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 78.8%, forg=  7.1% <<<
>>> Test on task  3 : loss=0.904 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 86.7%, forg=  4.8% <<<
>>> Test on task  4 : loss=0.880 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 84.8%, forg=  3.6% <<<
>>> Test on task  5 : loss=1.036 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 79.8%, forg=  1.8% <<<
>>> Test on task  6 : loss=1.150 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 83.2%, forg=  0.8% <<<
>>> Test on task  7 : loss=1.546 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 69.2%, forg= -3.7% <<<
>>> Test on task  8 : loss=1.638 | TAw acc= 88.7%, forg=  1.9%| TAg acc= 47.2%, forg=  5.7% <<<
>>> Test on task  9 : loss=1.310 | TAw acc= 87.0%, forg=  0.9%| TAg acc= 64.8%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.137 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task 11 : loss=1.149 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 72.0%, forg=  0.0% <<<
>>> Test on task 12 : loss=1.584 | TAw acc= 95.3%, forg= -0.9%| TAg acc= 53.3%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.353 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 60.3%, forg= 14.0% <<<
>>> Test on task 14 : loss=1.502 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 57.4%, forg=  5.7% <<<
>>> Test on task 15 : loss=1.626 | TAw acc= 93.2%, forg=  1.7%| TAg acc= 40.7%, forg= 30.5% <<<
>>> Test on task 16 : loss=1.921 | TAw acc= 91.2%, forg= -1.0%| TAg acc= 45.1%, forg= 13.7% <<<
>>> Test on task 17 : loss=1.510 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 57.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=4.694, TAw acc= 46.7% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=3.010, TAw acc= 64.4% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.116, TAw acc= 73.3% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.637, TAw acc= 81.1% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.462, TAw acc= 82.2% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.459, TAw acc= 82.2% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.456, TAw acc= 82.2% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.453, TAw acc= 82.2% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.451, TAw acc= 82.2% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.448, TAw acc= 82.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2889 train exemplars, time=  0.0s
2889
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.236 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 74.0%, forg= 16.2% <<<
>>> Test on task  1 : loss=1.036 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 79.6%, forg=  7.5% <<<
>>> Test on task  2 : loss=0.976 | TAw acc=100.0%, forg=  0.0%| TAg acc= 76.8%, forg=  9.1% <<<
>>> Test on task  3 : loss=0.911 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.7%, forg=  5.7% <<<
>>> Test on task  4 : loss=0.940 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.0%, forg= 13.4% <<<
>>> Test on task  5 : loss=1.089 | TAw acc= 92.7%, forg=  2.8%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task  6 : loss=1.239 | TAw acc= 95.2%, forg=  0.8%| TAg acc= 74.4%, forg=  9.6% <<<
>>> Test on task  7 : loss=1.526 | TAw acc= 91.6%, forg=  0.9%| TAg acc= 68.2%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.547 | TAw acc= 92.5%, forg= -1.9%| TAg acc= 56.6%, forg= -3.8% <<<
>>> Test on task  9 : loss=1.281 | TAw acc= 87.0%, forg=  0.9%| TAg acc= 64.8%, forg=  2.8% <<<
>>> Test on task 10 : loss=1.090 | TAw acc= 96.6%, forg=  2.6%| TAg acc= 73.5%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.141 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 71.2%, forg=  0.8% <<<
>>> Test on task 12 : loss=1.536 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 57.9%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.305 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.1%, forg=  5.1% <<<
>>> Test on task 14 : loss=1.392 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 63.9%, forg= -0.8% <<<
>>> Test on task 15 : loss=1.413 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 61.9%, forg=  9.3% <<<
>>> Test on task 16 : loss=1.802 | TAw acc= 89.2%, forg=  2.0%| TAg acc= 50.0%, forg=  8.8% <<<
>>> Test on task 17 : loss=1.858 | TAw acc= 98.2%, forg= -8.0%| TAg acc= 44.6%, forg= 12.5% <<<
>>> Test on task 18 : loss=1.470 | TAw acc= 85.8%, forg=  0.0%| TAg acc= 74.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=5.777, TAw acc= 50.0% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=3.387, TAw acc= 63.1% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=2.314, TAw acc= 71.4% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.845, TAw acc= 79.8% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.668, TAw acc= 84.5% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.664, TAw acc= 84.5% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.661, TAw acc= 84.5% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.658, TAw acc= 84.5% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.655, TAw acc= 84.5% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.652, TAw acc= 84.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3029 train exemplars, time=  0.0s
3029
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.282 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 72.3%, forg= 17.9% <<<
>>> Test on task  1 : loss=1.099 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 77.4%, forg=  9.7% <<<
>>> Test on task  2 : loss=0.902 | TAw acc=100.0%, forg=  0.0%| TAg acc= 80.8%, forg=  5.1% <<<
>>> Test on task  3 : loss=0.862 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 87.6%, forg=  3.8% <<<
>>> Test on task  4 : loss=0.895 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.5%, forg=  8.9% <<<
>>> Test on task  5 : loss=1.088 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task  6 : loss=1.239 | TAw acc= 95.2%, forg=  0.8%| TAg acc= 81.6%, forg=  2.4% <<<
>>> Test on task  7 : loss=1.574 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 66.4%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.540 | TAw acc= 89.6%, forg=  2.8%| TAg acc= 48.1%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.354 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 62.0%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.099 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 71.8%, forg=  1.7% <<<
>>> Test on task 11 : loss=1.160 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 68.2%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.477 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 63.6%, forg= -5.6% <<<
>>> Test on task 13 : loss=1.344 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.1%, forg=  5.1% <<<
>>> Test on task 14 : loss=1.323 | TAw acc= 96.7%, forg= -0.8%| TAg acc= 68.9%, forg= -4.9% <<<
>>> Test on task 15 : loss=1.398 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 56.8%, forg= 14.4% <<<
>>> Test on task 16 : loss=1.713 | TAw acc= 91.2%, forg=  0.0%| TAg acc= 54.9%, forg=  3.9% <<<
>>> Test on task 17 : loss=1.594 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 59.8%, forg= -2.7% <<<
>>> Test on task 18 : loss=1.739 | TAw acc= 92.5%, forg= -6.7%| TAg acc= 61.7%, forg= 12.5% <<<
>>> Test on task 19 : loss=1.507 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 57.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=6.173, TAw acc= 18.2% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=3.387, TAw acc= 66.2% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=2.194, TAw acc= 94.8% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.660, TAw acc= 93.5% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 96.1% | *
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 96.1% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 96.1% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 94.8% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 94.8% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.431, TAw acc= 94.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3169 train exemplars, time=  0.0s
3169
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.302 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 71.1%, forg= 19.1% <<<
>>> Test on task  1 : loss=1.125 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 73.1%, forg= 14.0% <<<
>>> Test on task  2 : loss=0.935 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.8%, forg=  7.1% <<<
>>> Test on task  3 : loss=0.918 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 87.6%, forg=  3.8% <<<
>>> Test on task  4 : loss=0.870 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.9%, forg= 12.5% <<<
>>> Test on task  5 : loss=1.044 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 78.9%, forg=  2.8% <<<
>>> Test on task  6 : loss=1.204 | TAw acc= 95.2%, forg=  0.8%| TAg acc= 82.4%, forg=  1.6% <<<
>>> Test on task  7 : loss=1.560 | TAw acc= 90.7%, forg=  1.9%| TAg acc= 66.4%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.557 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 50.0%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.278 | TAw acc= 87.0%, forg=  0.9%| TAg acc= 63.0%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.057 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 73.5%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.109 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 69.7%, forg=  2.3% <<<
>>> Test on task 12 : loss=1.464 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 58.9%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.354 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.4%, forg=  8.8% <<<
>>> Test on task 14 : loss=1.254 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 69.7%, forg= -0.8% <<<
>>> Test on task 15 : loss=1.312 | TAw acc= 93.2%, forg=  1.7%| TAg acc= 64.4%, forg=  6.8% <<<
>>> Test on task 16 : loss=1.597 | TAw acc= 91.2%, forg=  0.0%| TAg acc= 55.9%, forg=  2.9% <<<
>>> Test on task 17 : loss=1.462 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 65.2%, forg= -5.4% <<<
>>> Test on task 18 : loss=1.597 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 71.7%, forg=  2.5% <<<
>>> Test on task 19 : loss=1.849 | TAw acc= 94.7%, forg= -1.8%| TAg acc= 41.6%, forg= 15.9% <<<
>>> Test on task 20 : loss=1.221 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=6.428, TAw acc= 49.2% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=3.657, TAw acc= 72.3% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.461, TAw acc= 80.0% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.769, TAw acc= 78.5% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.492, TAw acc= 83.1% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 83.1% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.495, TAw acc= 84.6% |
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.496, TAw acc= 84.6% |
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.3s loss=1.497, TAw acc= 84.6% |
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.499, TAw acc= 86.2% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3309 train exemplars, time=  0.1s
3309
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.335 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 70.5%, forg= 19.7% <<<
>>> Test on task  1 : loss=1.141 | TAw acc= 98.9%, forg= -1.1%| TAg acc= 66.7%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.058 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=0.920 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.8%, forg=  7.6% <<<
>>> Test on task  4 : loss=0.964 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.9%, forg= 12.5% <<<
>>> Test on task  5 : loss=1.078 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task  6 : loss=1.193 | TAw acc= 96.8%, forg= -0.8%| TAg acc= 81.6%, forg=  2.4% <<<
>>> Test on task  7 : loss=1.573 | TAw acc= 90.7%, forg=  1.9%| TAg acc= 70.1%, forg= -0.9% <<<
>>> Test on task  8 : loss=1.470 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 54.7%, forg=  1.9% <<<
>>> Test on task  9 : loss=1.307 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 65.7%, forg=  1.9% <<<
>>> Test on task 10 : loss=1.064 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 71.8%, forg=  1.7% <<<
>>> Test on task 11 : loss=1.057 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 73.5%, forg= -1.5% <<<
>>> Test on task 12 : loss=1.417 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 63.6%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.250 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 71.3%, forg=  2.9% <<<
>>> Test on task 14 : loss=1.280 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 68.0%, forg=  1.6% <<<
>>> Test on task 15 : loss=1.307 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 66.1%, forg=  5.1% <<<
>>> Test on task 16 : loss=1.595 | TAw acc= 90.2%, forg=  1.0%| TAg acc= 55.9%, forg=  2.9% <<<
>>> Test on task 17 : loss=1.360 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 67.9%, forg= -2.7% <<<
>>> Test on task 18 : loss=1.454 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 76.7%, forg= -2.5% <<<
>>> Test on task 19 : loss=1.693 | TAw acc= 96.5%, forg= -1.8%| TAg acc= 51.3%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.401 | TAw acc= 99.0%, forg= -2.9%| TAg acc= 59.6%, forg=  0.0% <<<
>>> Test on task 21 : loss=1.440 | TAw acc= 84.8%, forg=  0.0%| TAg acc= 64.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.3s loss=6.367, TAw acc= 59.2% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=3.380, TAw acc= 78.9% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.228, TAw acc= 85.5% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.5s loss=1.744, TAw acc= 92.1% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.512, TAw acc= 92.1% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 92.1% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.516, TAw acc= 92.1% |
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.518, TAw acc= 92.1% |
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.520, TAw acc= 92.1% |
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.521, TAw acc= 92.1% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3449 train exemplars, time=  0.0s
3449
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.316 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 74.0%, forg= 16.2% <<<
>>> Test on task  1 : loss=1.109 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 68.8%, forg= 18.3% <<<
>>> Test on task  2 : loss=0.971 | TAw acc=100.0%, forg=  0.0%| TAg acc= 77.8%, forg=  8.1% <<<
>>> Test on task  3 : loss=0.969 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.8%, forg=  7.6% <<<
>>> Test on task  4 : loss=0.847 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 83.0%, forg=  5.4% <<<
>>> Test on task  5 : loss=1.144 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task  6 : loss=1.167 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 81.6%, forg=  2.4% <<<
>>> Test on task  7 : loss=1.590 | TAw acc= 91.6%, forg=  0.9%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.609 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 56.6%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.285 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 66.7%, forg=  0.9% <<<
>>> Test on task 10 : loss=1.076 | TAw acc= 96.6%, forg=  2.6%| TAg acc= 77.8%, forg= -4.3% <<<
>>> Test on task 11 : loss=1.179 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 67.4%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.447 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 61.7%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.324 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.4%, forg=  8.8% <<<
>>> Test on task 14 : loss=1.285 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 67.2%, forg=  2.5% <<<
>>> Test on task 15 : loss=1.247 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 69.5%, forg=  1.7% <<<
>>> Test on task 16 : loss=1.648 | TAw acc= 92.2%, forg= -1.0%| TAg acc= 49.0%, forg=  9.8% <<<
>>> Test on task 17 : loss=1.370 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.2%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.451 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 75.0%, forg=  1.7% <<<
>>> Test on task 19 : loss=1.567 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 60.2%, forg= -2.7% <<<
>>> Test on task 20 : loss=1.345 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
>>> Test on task 21 : loss=1.524 | TAw acc= 91.3%, forg= -6.5%| TAg acc= 50.0%, forg= 14.1% <<<
>>> Test on task 22 : loss=1.470 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 57.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=4.967, TAw acc= 55.3% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=3.037, TAw acc= 59.2% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.355, TAw acc= 75.0% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.814, TAw acc= 85.5% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 85.5% | *
| Epoch   1, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.544, TAw acc= 85.5% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.542, TAw acc= 85.5% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.540, TAw acc= 85.5% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.539, TAw acc= 85.5% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.537, TAw acc= 85.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3589 train exemplars, time=  0.0s
3589
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.321 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 71.1%, forg= 19.1% <<<
>>> Test on task  1 : loss=1.098 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 77.4%, forg=  9.7% <<<
>>> Test on task  2 : loss=0.978 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.7%, forg= 11.1% <<<
>>> Test on task  3 : loss=1.048 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 76.2%, forg= 15.2% <<<
>>> Test on task  4 : loss=0.908 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.0%, forg= 13.4% <<<
>>> Test on task  5 : loss=1.021 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.258 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 78.4%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.574 | TAw acc= 91.6%, forg=  0.9%| TAg acc= 66.4%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.519 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 56.6%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.281 | TAw acc= 88.0%, forg=  0.0%| TAg acc= 67.6%, forg=  0.0% <<<
>>> Test on task 10 : loss=1.056 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 69.2%, forg=  8.5% <<<
>>> Test on task 11 : loss=1.046 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 75.0%, forg= -1.5% <<<
>>> Test on task 12 : loss=1.497 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 58.9%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.231 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.1%, forg=  5.1% <<<
>>> Test on task 14 : loss=1.211 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 73.0%, forg= -3.3% <<<
>>> Test on task 15 : loss=1.312 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 61.0%, forg= 10.2% <<<
>>> Test on task 16 : loss=1.557 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 62.7%, forg= -3.9% <<<
>>> Test on task 17 : loss=1.294 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 67.9%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.402 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 78.3%, forg= -1.7% <<<
>>> Test on task 19 : loss=1.534 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 56.6%, forg=  3.5% <<<
>>> Test on task 20 : loss=1.243 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 62.5%, forg= -2.9% <<<
>>> Test on task 21 : loss=1.503 | TAw acc= 88.0%, forg=  3.3%| TAg acc= 51.1%, forg= 13.0% <<<
>>> Test on task 22 : loss=1.737 | TAw acc= 99.0%, forg= -4.8%| TAg acc= 49.0%, forg=  8.7% <<<
>>> Test on task 23 : loss=1.465 | TAw acc= 88.5%, forg=  0.0%| TAg acc= 67.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=5.142, TAw acc= 47.7% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=3.008, TAw acc= 59.3% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.079, TAw acc= 90.7% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.681, TAw acc= 97.7% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.361, TAw acc=100.0% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.359, TAw acc=100.0% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.356, TAw acc=100.0% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.354, TAw acc=100.0% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.352, TAw acc=100.0% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.349, TAw acc= 98.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3729 train exemplars, time=  0.0s
3729
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.317 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 74.6%, forg= 15.6% <<<
>>> Test on task  1 : loss=1.097 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 74.2%, forg= 12.9% <<<
>>> Test on task  2 : loss=0.976 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.8%, forg= 10.1% <<<
>>> Test on task  3 : loss=0.991 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 84.8%, forg=  6.7% <<<
>>> Test on task  4 : loss=0.949 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.4%, forg= 17.0% <<<
>>> Test on task  5 : loss=1.045 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 79.8%, forg=  1.8% <<<
>>> Test on task  6 : loss=1.210 | TAw acc= 94.4%, forg=  2.4%| TAg acc= 80.0%, forg=  4.0% <<<
>>> Test on task  7 : loss=1.614 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.525 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 54.7%, forg=  1.9% <<<
>>> Test on task  9 : loss=1.352 | TAw acc= 88.9%, forg= -0.9%| TAg acc= 63.0%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.143 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.9%, forg=  6.8% <<<
>>> Test on task 11 : loss=1.173 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 68.9%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.643 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 55.1%, forg=  8.4% <<<
>>> Test on task 13 : loss=1.249 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 70.6%, forg=  3.7% <<<
>>> Test on task 14 : loss=1.267 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 68.0%, forg=  4.9% <<<
>>> Test on task 15 : loss=1.304 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 72.0%, forg= -0.8% <<<
>>> Test on task 16 : loss=1.537 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 63.7%, forg= -1.0% <<<
>>> Test on task 17 : loss=1.267 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 69.6%, forg= -1.8% <<<
>>> Test on task 18 : loss=1.347 | TAw acc= 93.3%, forg= -0.8%| TAg acc= 75.0%, forg=  3.3% <<<
>>> Test on task 19 : loss=1.477 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 61.1%, forg= -0.9% <<<
>>> Test on task 20 : loss=1.254 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 60.6%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.320 | TAw acc= 87.0%, forg=  4.3%| TAg acc= 57.6%, forg=  6.5% <<<
>>> Test on task 22 : loss=1.730 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 43.3%, forg= 14.4% <<<
>>> Test on task 23 : loss=1.647 | TAw acc= 93.3%, forg= -4.8%| TAg acc= 50.0%, forg= 17.3% <<<
>>> Test on task 24 : loss=1.215 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 73.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=7.395, TAw acc= 42.4% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=4.034, TAw acc= 60.6% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.663, TAw acc= 78.8% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.162, TAw acc= 84.8% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.841, TAw acc= 93.9% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.842, TAw acc= 93.9% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.844, TAw acc= 93.9% |
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.845, TAw acc= 95.5% |
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.846, TAw acc= 95.5% |
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.847, TAw acc= 95.5% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3869 train exemplars, time=  0.0s
3869
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.356 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 69.9%, forg= 20.2% <<<
>>> Test on task  1 : loss=1.111 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 69.9%, forg= 17.2% <<<
>>> Test on task  2 : loss=0.980 | TAw acc=100.0%, forg=  0.0%| TAg acc= 74.7%, forg= 11.1% <<<
>>> Test on task  3 : loss=1.055 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.7%, forg=  5.7% <<<
>>> Test on task  4 : loss=0.844 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 85.7%, forg=  2.7% <<<
>>> Test on task  5 : loss=1.008 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 80.7%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.246 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 80.0%, forg=  4.0% <<<
>>> Test on task  7 : loss=1.605 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 64.5%, forg=  5.6% <<<
>>> Test on task  8 : loss=1.633 | TAw acc= 89.6%, forg=  2.8%| TAg acc= 48.1%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.343 | TAw acc= 89.8%, forg= -0.9%| TAg acc= 63.0%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.135 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 70.1%, forg=  7.7% <<<
>>> Test on task 11 : loss=1.162 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 72.7%, forg=  2.3% <<<
>>> Test on task 12 : loss=1.560 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 58.9%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.282 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.1%, forg=  5.1% <<<
>>> Test on task 14 : loss=1.234 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 68.0%, forg=  4.9% <<<
>>> Test on task 15 : loss=1.278 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 72.0%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.555 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 61.8%, forg=  2.0% <<<
>>> Test on task 17 : loss=1.224 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 69.6%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.350 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 75.8%, forg=  2.5% <<<
>>> Test on task 19 : loss=1.411 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 66.4%, forg= -5.3% <<<
>>> Test on task 20 : loss=1.173 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 72.1%, forg= -9.6% <<<
>>> Test on task 21 : loss=1.260 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 65.2%, forg= -1.1% <<<
>>> Test on task 22 : loss=1.643 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 51.0%, forg=  6.7% <<<
>>> Test on task 23 : loss=1.467 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 62.5%, forg=  4.8% <<<
>>> Test on task 24 : loss=1.533 | TAw acc= 98.3%, forg= -2.6%| TAg acc= 56.9%, forg= 16.4% <<<
>>> Test on task 25 : loss=1.562 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 56.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.3s loss=4.986, TAw acc= 53.9% | *
| Epoch   2, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=2.950, TAw acc= 75.0% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=2.105, TAw acc= 86.8% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.661, TAw acc= 90.8% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.476, TAw acc= 94.7% | *
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.469, TAw acc= 94.7% | *
| Epoch   2, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.462, TAw acc= 94.7% | *
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 94.7% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.449, TAw acc= 94.7% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.443, TAw acc= 94.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4009 train exemplars, time=  0.0s
4009
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.383 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 69.4%, forg= 20.8% <<<
>>> Test on task  1 : loss=1.055 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 78.5%, forg=  8.6% <<<
>>> Test on task  2 : loss=0.991 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 74.7%, forg= 11.1% <<<
>>> Test on task  3 : loss=1.036 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 85.7%, forg=  5.7% <<<
>>> Test on task  4 : loss=0.894 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 80.4%, forg=  8.0% <<<
>>> Test on task  5 : loss=1.031 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task  6 : loss=1.156 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 80.8%, forg=  3.2% <<<
>>> Test on task  7 : loss=1.599 | TAw acc= 94.4%, forg= -1.9%| TAg acc= 65.4%, forg=  4.7% <<<
>>> Test on task  8 : loss=1.565 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 50.0%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.376 | TAw acc= 88.0%, forg=  1.9%| TAg acc= 62.0%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.109 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 72.6%, forg=  5.1% <<<
>>> Test on task 11 : loss=1.054 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 73.5%, forg=  1.5% <<<
>>> Test on task 12 : loss=1.570 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.243 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.1%, forg=  2.2% <<<
>>> Test on task 14 : loss=1.199 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 69.7%, forg=  3.3% <<<
>>> Test on task 15 : loss=1.282 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 72.0%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.571 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 63.7%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.229 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 69.6%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.439 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 72.5%, forg=  5.8% <<<
>>> Test on task 19 : loss=1.387 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 66.4%, forg=  0.0% <<<
>>> Test on task 20 : loss=1.210 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.2%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.270 | TAw acc= 93.5%, forg= -2.2%| TAg acc= 64.1%, forg=  1.1% <<<
>>> Test on task 22 : loss=1.531 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 58.7%, forg= -1.0% <<<
>>> Test on task 23 : loss=1.414 | TAw acc= 96.2%, forg= -2.9%| TAg acc= 70.2%, forg= -2.9% <<<
>>> Test on task 24 : loss=1.361 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 63.8%, forg=  9.5% <<<
>>> Test on task 25 : loss=1.782 | TAw acc= 96.7%, forg= -1.1%| TAg acc= 34.8%, forg= 21.7% <<<
>>> Test on task 26 : loss=1.491 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 65.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=4.486, TAw acc= 51.2% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.411, TAw acc= 77.4% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.647, TAw acc= 95.2% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.439, TAw acc= 95.2% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.181, TAw acc= 95.2% | *
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.180, TAw acc= 95.2% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.179, TAw acc= 95.2% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.178, TAw acc= 95.2% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.177, TAw acc= 95.2% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4149 train exemplars, time=  0.0s
4149
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.387 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 71.1%, forg= 19.1% <<<
>>> Test on task  1 : loss=1.137 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 67.7%, forg= 19.4% <<<
>>> Test on task  2 : loss=1.026 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.047 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 86.7%, forg=  4.8% <<<
>>> Test on task  4 : loss=0.997 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 75.0%, forg= 13.4% <<<
>>> Test on task  5 : loss=1.020 | TAw acc= 93.6%, forg=  2.8%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  6 : loss=1.243 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 76.0%, forg=  8.0% <<<
>>> Test on task  7 : loss=1.641 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 62.6%, forg=  7.5% <<<
>>> Test on task  8 : loss=1.471 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 63.2%, forg= -6.6% <<<
>>> Test on task  9 : loss=1.326 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 67.6%, forg=  0.0% <<<
>>> Test on task 10 : loss=1.071 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 76.1%, forg=  1.7% <<<
>>> Test on task 11 : loss=1.124 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 66.7%, forg=  8.3% <<<
>>> Test on task 12 : loss=1.554 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.207 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.9%, forg=  4.4% <<<
>>> Test on task 14 : loss=1.277 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 63.1%, forg=  9.8% <<<
>>> Test on task 15 : loss=1.304 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 73.7%, forg= -1.7% <<<
>>> Test on task 16 : loss=1.611 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 60.8%, forg=  2.9% <<<
>>> Test on task 17 : loss=1.236 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 71.4%, forg= -1.8% <<<
>>> Test on task 18 : loss=1.492 | TAw acc= 94.2%, forg= -0.8%| TAg acc= 66.7%, forg= 11.7% <<<
>>> Test on task 19 : loss=1.353 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 67.3%, forg= -0.9% <<<
>>> Test on task 20 : loss=1.159 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 68.3%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.202 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 70.7%, forg= -5.4% <<<
>>> Test on task 22 : loss=1.524 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 54.8%, forg=  3.8% <<<
>>> Test on task 23 : loss=1.345 | TAw acc= 97.1%, forg= -1.0%| TAg acc= 72.1%, forg= -1.9% <<<
>>> Test on task 24 : loss=1.326 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 63.8%, forg=  9.5% <<<
>>> Test on task 25 : loss=1.651 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 46.7%, forg=  9.8% <<<
>>> Test on task 26 : loss=1.641 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 59.0%, forg=  6.7% <<<
>>> Test on task 27 : loss=1.092 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 73.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=5.399, TAw acc= 59.3% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.3s loss=2.629, TAw acc= 77.8% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.833, TAw acc= 96.3% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.454, TAw acc= 96.3% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.177, TAw acc= 96.3% | *
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.176, TAw acc= 96.3% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 96.3% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.175, TAw acc= 96.3% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.174, TAw acc= 96.3% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.174, TAw acc= 96.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4289 train exemplars, time=  0.0s
4289
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.486 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 66.5%, forg= 23.7% <<<
>>> Test on task  1 : loss=1.124 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 71.0%, forg= 16.1% <<<
>>> Test on task  2 : loss=1.085 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.073 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.9%, forg=  8.6% <<<
>>> Test on task  4 : loss=0.959 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.5%, forg=  8.9% <<<
>>> Test on task  5 : loss=1.010 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  6 : loss=1.180 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 79.2%, forg=  4.8% <<<
>>> Test on task  7 : loss=1.614 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 64.5%, forg=  5.6% <<<
>>> Test on task  8 : loss=1.529 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 56.6%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.400 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 65.7%, forg=  1.9% <<<
>>> Test on task 10 : loss=1.140 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 70.9%, forg=  6.8% <<<
>>> Test on task 11 : loss=1.207 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 65.9%, forg=  9.1% <<<
>>> Test on task 12 : loss=1.604 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 54.2%, forg=  9.3% <<<
>>> Test on task 13 : loss=1.198 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.1%, forg=  2.2% <<<
>>> Test on task 14 : loss=1.209 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 70.5%, forg=  2.5% <<<
>>> Test on task 15 : loss=1.260 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 69.5%, forg=  4.2% <<<
>>> Test on task 16 : loss=1.604 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 60.8%, forg=  2.9% <<<
>>> Test on task 17 : loss=1.237 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 71.4%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.463 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 70.8%, forg=  7.5% <<<
>>> Test on task 19 : loss=1.347 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 69.9%, forg= -2.7% <<<
>>> Test on task 20 : loss=1.176 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 67.3%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.134 | TAw acc= 92.4%, forg=  1.1%| TAg acc= 73.9%, forg= -3.3% <<<
>>> Test on task 22 : loss=1.521 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 57.7%, forg=  1.0% <<<
>>> Test on task 23 : loss=1.241 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 74.0%, forg= -1.9% <<<
>>> Test on task 24 : loss=1.277 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 66.4%, forg=  6.9% <<<
>>> Test on task 25 : loss=1.615 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 53.3%, forg=  3.3% <<<
>>> Test on task 26 : loss=1.642 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 52.4%, forg= 13.3% <<<
>>> Test on task 27 : loss=1.304 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 62.8%, forg= 10.6% <<<
>>> Test on task 28 : loss=1.323 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 67.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.3s loss=4.319, TAw acc= 63.5% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=2.273, TAw acc= 67.3% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.704, TAw acc= 85.6% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.566, TAw acc= 86.5% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.336, TAw acc= 89.4% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.332, TAw acc= 90.4% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.327, TAw acc= 90.4% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.3s loss=1.323, TAw acc= 90.4% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.320, TAw acc= 90.4% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.317, TAw acc= 90.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4429 train exemplars, time=  0.0s
4429
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.487 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 66.5%, forg= 23.7% <<<
>>> Test on task  1 : loss=1.099 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 73.1%, forg= 14.0% <<<
>>> Test on task  2 : loss=1.054 | TAw acc=100.0%, forg=  0.0%| TAg acc= 69.7%, forg= 16.2% <<<
>>> Test on task  3 : loss=1.094 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.8%, forg=  7.6% <<<
>>> Test on task  4 : loss=1.015 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 78.6%, forg=  9.8% <<<
>>> Test on task  5 : loss=1.075 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task  6 : loss=1.284 | TAw acc= 94.4%, forg=  2.4%| TAg acc= 73.6%, forg= 10.4% <<<
>>> Test on task  7 : loss=1.722 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 57.9%, forg= 12.1% <<<
>>> Test on task  8 : loss=1.706 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 50.9%, forg= 12.3% <<<
>>> Test on task  9 : loss=1.468 | TAw acc= 88.9%, forg=  0.9%| TAg acc= 60.2%, forg=  7.4% <<<
>>> Test on task 10 : loss=1.162 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.5%, forg=  4.3% <<<
>>> Test on task 11 : loss=1.189 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 71.2%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.572 | TAw acc= 93.5%, forg=  1.9%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.228 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.8%, forg=  1.5% <<<
>>> Test on task 14 : loss=1.269 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 69.7%, forg=  3.3% <<<
>>> Test on task 15 : loss=1.299 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 68.6%, forg=  5.1% <<<
>>> Test on task 16 : loss=1.718 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 53.9%, forg=  9.8% <<<
>>> Test on task 17 : loss=1.187 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 70.5%, forg=  0.9% <<<
>>> Test on task 18 : loss=1.366 | TAw acc= 93.3%, forg=  0.8%| TAg acc= 74.2%, forg=  4.2% <<<
>>> Test on task 19 : loss=1.306 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 70.8%, forg= -0.9% <<<
>>> Test on task 20 : loss=1.226 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 68.3%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.131 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 71.7%, forg=  2.2% <<<
>>> Test on task 22 : loss=1.594 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 51.0%, forg=  7.7% <<<
>>> Test on task 23 : loss=1.222 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 76.0%, forg= -1.9% <<<
>>> Test on task 24 : loss=1.234 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 73.3%, forg=  0.0% <<<
>>> Test on task 25 : loss=1.608 | TAw acc= 97.8%, forg= -1.1%| TAg acc= 50.0%, forg=  6.5% <<<
>>> Test on task 26 : loss=1.568 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 60.0%, forg=  5.7% <<<
>>> Test on task 27 : loss=1.342 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  7.1% <<<
>>> Test on task 28 : loss=1.675 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 49.1%, forg= 18.2% <<<
>>> Test on task 29 : loss=1.379 | TAw acc= 88.1%, forg=  0.0%| TAg acc= 69.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=5.786, TAw acc= 43.2% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=3.535, TAw acc= 68.9% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=2.496, TAw acc= 81.1% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=2.059, TAw acc= 83.8% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.938, TAw acc= 83.8% | *
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.932, TAw acc= 83.8% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.925, TAw acc= 83.8% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.919, TAw acc= 83.8% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.913, TAw acc= 83.8% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.907, TAw acc= 83.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4569 train exemplars, time=  0.1s
4569
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.482 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 68.8%, forg= 21.4% <<<
>>> Test on task  1 : loss=1.113 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 74.2%, forg= 12.9% <<<
>>> Test on task  2 : loss=1.048 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.087 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 84.8%, forg=  6.7% <<<
>>> Test on task  4 : loss=0.882 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.5%, forg=  8.9% <<<
>>> Test on task  5 : loss=1.049 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task  6 : loss=1.258 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 75.2%, forg=  8.8% <<<
>>> Test on task  7 : loss=1.616 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 66.4%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.637 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 55.7%, forg=  7.5% <<<
>>> Test on task  9 : loss=1.413 | TAw acc= 88.9%, forg=  0.9%| TAg acc= 61.1%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.105 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 73.5%, forg=  4.3% <<<
>>> Test on task 11 : loss=1.109 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 73.5%, forg=  1.5% <<<
>>> Test on task 12 : loss=1.656 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 54.2%, forg=  9.3% <<<
>>> Test on task 13 : loss=1.193 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.0%, forg= -0.7% <<<
>>> Test on task 14 : loss=1.226 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 70.5%, forg=  2.5% <<<
>>> Test on task 15 : loss=1.306 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 70.3%, forg=  3.4% <<<
>>> Test on task 16 : loss=1.709 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 55.9%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.166 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 73.2%, forg= -1.8% <<<
>>> Test on task 18 : loss=1.390 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 74.2%, forg=  4.2% <<<
>>> Test on task 19 : loss=1.299 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task 20 : loss=1.216 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 67.3%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.176 | TAw acc= 92.4%, forg=  1.1%| TAg acc= 71.7%, forg=  2.2% <<<
>>> Test on task 22 : loss=1.505 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 59.6%, forg= -1.0% <<<
>>> Test on task 23 : loss=1.247 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 65.4%, forg= 10.6% <<<
>>> Test on task 24 : loss=1.249 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 69.0%, forg=  4.3% <<<
>>> Test on task 25 : loss=1.539 | TAw acc= 96.7%, forg=  1.1%| TAg acc= 56.5%, forg=  0.0% <<<
>>> Test on task 26 : loss=1.496 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 60.0%, forg=  5.7% <<<
>>> Test on task 27 : loss=1.191 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 71.7%, forg=  1.8% <<<
>>> Test on task 28 : loss=1.509 | TAw acc= 92.7%, forg=  0.9%| TAg acc= 56.4%, forg= 10.9% <<<
>>> Test on task 29 : loss=2.031 | TAw acc= 89.6%, forg= -1.5%| TAg acc= 38.5%, forg= 31.1% <<<
>>> Test on task 30 : loss=1.764 | TAw acc= 87.3%, forg=  0.0%| TAg acc= 50.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=6.019, TAw acc= 52.3% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=3.162, TAw acc= 68.2% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=2.141, TAw acc= 78.4% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.685, TAw acc= 92.0% | *
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.660, TAw acc= 89.8% | *
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.644, TAw acc= 90.9% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.630, TAw acc= 90.9% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.619, TAw acc= 90.9% | *
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.609, TAw acc= 90.9% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.600, TAw acc= 90.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4709 train exemplars, time=  0.0s
4709
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.526 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 70.5%, forg= 19.7% <<<
>>> Test on task  1 : loss=1.158 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 66.7%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.062 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.133 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.9%, forg=  8.6% <<<
>>> Test on task  4 : loss=0.966 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 76.8%, forg= 11.6% <<<
>>> Test on task  5 : loss=1.149 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 68.8%, forg= 12.8% <<<
>>> Test on task  6 : loss=1.214 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 77.6%, forg=  6.4% <<<
>>> Test on task  7 : loss=1.694 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 61.7%, forg=  8.4% <<<
>>> Test on task  8 : loss=1.581 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 56.6%, forg=  6.6% <<<
>>> Test on task  9 : loss=1.386 | TAw acc= 88.9%, forg=  0.9%| TAg acc= 65.7%, forg=  1.9% <<<
>>> Test on task 10 : loss=1.100 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 78.6%, forg= -0.9% <<<
>>> Test on task 11 : loss=1.169 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 67.4%, forg=  7.6% <<<
>>> Test on task 12 : loss=1.625 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 56.1%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.232 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.1%, forg=  2.9% <<<
>>> Test on task 14 : loss=1.240 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 69.7%, forg=  3.3% <<<
>>> Test on task 15 : loss=1.264 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 72.9%, forg=  0.8% <<<
>>> Test on task 16 : loss=1.690 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 55.9%, forg=  7.8% <<<
>>> Test on task 17 : loss=1.236 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 73.2%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.422 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 71.7%, forg=  6.7% <<<
>>> Test on task 19 : loss=1.320 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 72.6%, forg= -1.8% <<<
>>> Test on task 20 : loss=1.094 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 72.1%, forg=  0.0% <<<
>>> Test on task 21 : loss=1.269 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 64.1%, forg=  9.8% <<<
>>> Test on task 22 : loss=1.537 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
>>> Test on task 23 : loss=1.239 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 67.3%, forg=  8.7% <<<
>>> Test on task 24 : loss=1.236 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 67.2%, forg=  6.0% <<<
>>> Test on task 25 : loss=1.485 | TAw acc= 95.7%, forg=  2.2%| TAg acc= 56.5%, forg=  0.0% <<<
>>> Test on task 26 : loss=1.511 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 63.8%, forg=  1.9% <<<
>>> Test on task 27 : loss=1.211 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 68.1%, forg=  5.3% <<<
>>> Test on task 28 : loss=1.525 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 58.2%, forg=  9.1% <<<
>>> Test on task 29 : loss=1.875 | TAw acc= 88.9%, forg=  0.7%| TAg acc= 39.3%, forg= 30.4% <<<
>>> Test on task 30 : loss=2.112 | TAw acc= 87.3%, forg=  0.0%| TAg acc= 26.5%, forg= 23.5% <<<
>>> Test on task 31 : loss=1.589 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 62.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=6.101, TAw acc= 41.4% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=3.260, TAw acc= 72.9% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.876, TAw acc= 84.3% | *
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.635, TAw acc= 84.3% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.497, TAw acc= 84.3% | *
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 84.3% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.489, TAw acc= 84.3% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.486, TAw acc= 84.3% | *
| Epoch   4, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.483, TAw acc= 84.3% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.480, TAw acc= 84.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4849 train exemplars, time=  0.0s
4849
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.545 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 68.8%, forg= 21.4% <<<
>>> Test on task  1 : loss=1.142 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 67.7%, forg= 19.4% <<<
>>> Test on task  2 : loss=1.044 | TAw acc=100.0%, forg=  0.0%| TAg acc= 71.7%, forg= 14.1% <<<
>>> Test on task  3 : loss=1.148 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.9%, forg=  9.5% <<<
>>> Test on task  4 : loss=0.914 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.1%, forg=  6.2% <<<
>>> Test on task  5 : loss=1.057 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 77.1%, forg=  4.6% <<<
>>> Test on task  6 : loss=1.225 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 76.8%, forg=  7.2% <<<
>>> Test on task  7 : loss=1.666 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.640 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 54.7%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.446 | TAw acc= 88.9%, forg=  0.9%| TAg acc= 60.2%, forg=  7.4% <<<
>>> Test on task 10 : loss=1.185 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 66.7%, forg= 12.0% <<<
>>> Test on task 11 : loss=1.145 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 69.7%, forg=  5.3% <<<
>>> Test on task 12 : loss=1.623 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 57.9%, forg=  5.6% <<<
>>> Test on task 13 : loss=1.196 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
>>> Test on task 14 : loss=1.237 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 66.4%, forg=  6.6% <<<
>>> Test on task 15 : loss=1.312 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 69.5%, forg=  4.2% <<<
>>> Test on task 16 : loss=1.712 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 56.9%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.189 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 74.1%, forg= -0.9% <<<
>>> Test on task 18 : loss=1.395 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 75.0%, forg=  3.3% <<<
>>> Test on task 19 : loss=1.244 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 74.3%, forg= -1.8% <<<
>>> Test on task 20 : loss=1.112 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 71.2%, forg=  1.0% <<<
>>> Test on task 21 : loss=1.143 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 72.8%, forg=  1.1% <<<
>>> Test on task 22 : loss=1.521 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
>>> Test on task 23 : loss=1.203 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 74.0%, forg=  1.9% <<<
>>> Test on task 24 : loss=1.159 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 74.1%, forg= -0.9% <<<
>>> Test on task 25 : loss=1.422 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 65.2%, forg= -8.7% <<<
>>> Test on task 26 : loss=1.458 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 64.8%, forg=  1.0% <<<
>>> Test on task 27 : loss=1.152 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 71.7%, forg=  1.8% <<<
>>> Test on task 28 : loss=1.415 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 67.3%, forg=  0.0% <<<
>>> Test on task 29 : loss=1.861 | TAw acc= 88.9%, forg=  0.7%| TAg acc= 40.0%, forg= 29.6% <<<
>>> Test on task 30 : loss=1.973 | TAw acc= 87.3%, forg=  0.0%| TAg acc= 30.4%, forg= 19.6% <<<
>>> Test on task 31 : loss=1.807 | TAw acc= 91.5%, forg=  0.8%| TAg acc= 55.1%, forg=  7.6% <<<
>>> Test on task 32 : loss=1.495 | TAw acc= 85.6%, forg=  0.0%| TAg acc= 53.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=6.057, TAw acc= 50.6% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=3.375, TAw acc= 69.6% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=2.284, TAw acc= 82.3% | *
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.784, TAw acc= 88.6% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 88.6% | *
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 88.6% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.546, TAw acc= 88.6% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.545, TAw acc= 88.6% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.545, TAw acc= 88.6% | *
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.545, TAw acc= 88.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4963 train exemplars, time=  0.0s
4963
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.549 | TAw acc= 94.2%, forg=  1.7%| TAg acc= 67.6%, forg= 22.5% <<<
>>> Test on task  1 : loss=1.202 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 65.6%, forg= 21.5% <<<
>>> Test on task  2 : loss=1.102 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.114 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 82.9%, forg=  8.6% <<<
>>> Test on task  4 : loss=0.918 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 82.1%, forg=  6.2% <<<
>>> Test on task  5 : loss=1.080 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.5%, forg=  9.2% <<<
>>> Test on task  6 : loss=1.249 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 78.4%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.702 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 64.5%, forg=  5.6% <<<
>>> Test on task  8 : loss=1.578 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 55.7%, forg=  7.5% <<<
>>> Test on task  9 : loss=1.436 | TAw acc= 88.9%, forg=  0.9%| TAg acc= 61.1%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.137 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.5%, forg=  5.1% <<<
>>> Test on task 11 : loss=1.169 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 69.7%, forg=  5.3% <<<
>>> Test on task 12 : loss=1.623 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 57.9%, forg=  5.6% <<<
>>> Test on task 13 : loss=1.226 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.8%, forg=  2.2% <<<
>>> Test on task 14 : loss=1.247 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 66.4%, forg=  6.6% <<<
>>> Test on task 15 : loss=1.333 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 68.6%, forg=  5.1% <<<
>>> Test on task 16 : loss=1.702 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 56.9%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.163 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 77.7%, forg= -3.6% <<<
>>> Test on task 18 : loss=1.459 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 74.2%, forg=  4.2% <<<
>>> Test on task 19 : loss=1.318 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 69.0%, forg=  5.3% <<<
>>> Test on task 20 : loss=1.140 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 68.3%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.125 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 71.7%, forg=  2.2% <<<
>>> Test on task 22 : loss=1.491 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 60.6%, forg= -1.0% <<<
>>> Test on task 23 : loss=1.218 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 72.1%, forg=  3.8% <<<
>>> Test on task 24 : loss=1.126 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 70.7%, forg=  3.4% <<<
>>> Test on task 25 : loss=1.461 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 59.8%, forg=  5.4% <<<
>>> Test on task 26 : loss=1.363 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 70.5%, forg= -4.8% <<<
>>> Test on task 27 : loss=1.144 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 73.5%, forg=  0.0% <<<
>>> Test on task 28 : loss=1.406 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 71.8%, forg= -4.5% <<<
>>> Test on task 29 : loss=1.841 | TAw acc= 88.9%, forg=  0.7%| TAg acc= 40.7%, forg= 28.9% <<<
>>> Test on task 30 : loss=1.954 | TAw acc= 90.2%, forg= -2.9%| TAg acc= 39.2%, forg= 10.8% <<<
>>> Test on task 31 : loss=1.737 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 58.5%, forg=  4.2% <<<
>>> Test on task 32 : loss=1.721 | TAw acc= 84.5%, forg=  1.0%| TAg acc= 49.5%, forg=  4.1% <<<
>>> Test on task 33 : loss=1.412 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 63.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=5.320, TAw acc= 52.9% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=2.746, TAw acc= 82.4% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.873, TAw acc= 89.4% | *
| Epoch   4, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.554, TAw acc= 94.1% | *
| Epoch   5, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.590, TAw acc= 95.3% |
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.547, TAw acc= 94.1% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.540, TAw acc= 94.1% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.534, TAw acc= 94.1% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.529, TAw acc= 92.9% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.524, TAw acc= 92.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5063 train exemplars, time=  0.0s
5063
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.575 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 69.4%, forg= 20.8% <<<
>>> Test on task  1 : loss=1.248 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 58.1%, forg= 29.0% <<<
>>> Test on task  2 : loss=1.076 | TAw acc=100.0%, forg=  0.0%| TAg acc= 72.7%, forg= 13.1% <<<
>>> Test on task  3 : loss=1.180 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 83.8%, forg=  7.6% <<<
>>> Test on task  4 : loss=0.975 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 79.5%, forg=  8.9% <<<
>>> Test on task  5 : loss=1.076 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  6 : loss=1.234 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 76.8%, forg=  7.2% <<<
>>> Test on task  7 : loss=1.800 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 61.7%, forg=  8.4% <<<
>>> Test on task  8 : loss=1.670 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 57.5%, forg=  5.7% <<<
>>> Test on task  9 : loss=1.419 | TAw acc= 90.7%, forg= -0.9%| TAg acc= 63.0%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.193 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.9%, forg=  7.7% <<<
>>> Test on task 11 : loss=1.134 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 70.5%, forg=  4.5% <<<
>>> Test on task 12 : loss=1.725 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 55.1%, forg=  8.4% <<<
>>> Test on task 13 : loss=1.362 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.6%, forg=  7.4% <<<
>>> Test on task 14 : loss=1.312 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 63.1%, forg=  9.8% <<<
>>> Test on task 15 : loss=1.331 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 69.5%, forg=  4.2% <<<
>>> Test on task 16 : loss=1.708 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 56.9%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.206 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 72.3%, forg=  5.4% <<<
>>> Test on task 18 : loss=1.427 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 76.7%, forg=  1.7% <<<
>>> Test on task 19 : loss=1.323 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 71.7%, forg=  2.7% <<<
>>> Test on task 20 : loss=1.133 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 73.1%, forg= -1.0% <<<
>>> Test on task 21 : loss=1.146 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 72.8%, forg=  1.1% <<<
>>> Test on task 22 : loss=1.500 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 61.5%, forg= -1.0% <<<
>>> Test on task 23 : loss=1.226 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 72.1%, forg=  3.8% <<<
>>> Test on task 24 : loss=1.233 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 69.0%, forg=  5.2% <<<
>>> Test on task 25 : loss=1.403 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 62.0%, forg=  3.3% <<<
>>> Test on task 26 : loss=1.436 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 63.8%, forg=  6.7% <<<
>>> Test on task 27 : loss=1.137 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 74.3%, forg= -0.9% <<<
>>> Test on task 28 : loss=1.458 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 67.3%, forg=  4.5% <<<
>>> Test on task 29 : loss=1.788 | TAw acc= 88.1%, forg=  1.5%| TAg acc= 43.0%, forg= 26.7% <<<
>>> Test on task 30 : loss=1.796 | TAw acc= 86.3%, forg=  3.9%| TAg acc= 44.1%, forg=  5.9% <<<
>>> Test on task 31 : loss=1.760 | TAw acc= 90.7%, forg=  1.7%| TAg acc= 56.8%, forg=  5.9% <<<
>>> Test on task 32 : loss=1.622 | TAw acc= 85.6%, forg=  0.0%| TAg acc= 48.5%, forg=  5.2% <<<
>>> Test on task 33 : loss=1.856 | TAw acc= 97.2%, forg= -2.8%| TAg acc= 46.3%, forg= 16.7% <<<
>>> Test on task 34 : loss=1.303 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 66.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=6.113, TAw acc= 50.7% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=3.019, TAw acc= 81.7% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.911, TAw acc= 93.0% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.433, TAw acc= 97.2% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 98.6% | *
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.220, TAw acc= 98.6% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.216, TAw acc= 98.6% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 98.6% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.210, TAw acc= 98.6% | *
| Epoch   5, time=  4.6s | Train: skip eval | Valid: time=  0.3s loss=1.207, TAw acc= 98.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5163 train exemplars, time=  0.0s
5163
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.562 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 67.6%, forg= 22.5% <<<
>>> Test on task  1 : loss=1.261 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 62.4%, forg= 24.7% <<<
>>> Test on task  2 : loss=1.114 | TAw acc=100.0%, forg=  0.0%| TAg acc= 69.7%, forg= 16.2% <<<
>>> Test on task  3 : loss=1.182 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.9%, forg=  9.5% <<<
>>> Test on task  4 : loss=0.983 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 80.4%, forg=  8.0% <<<
>>> Test on task  5 : loss=1.047 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task  6 : loss=1.206 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 77.6%, forg=  6.4% <<<
>>> Test on task  7 : loss=1.701 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.573 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 60.4%, forg=  2.8% <<<
>>> Test on task  9 : loss=1.418 | TAw acc= 88.0%, forg=  2.8%| TAg acc= 60.2%, forg=  7.4% <<<
>>> Test on task 10 : loss=1.089 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 76.1%, forg=  2.6% <<<
>>> Test on task 11 : loss=1.092 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 75.0%, forg=  0.0% <<<
>>> Test on task 12 : loss=1.648 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 55.1%, forg=  8.4% <<<
>>> Test on task 13 : loss=1.241 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 71.3%, forg=  3.7% <<<
>>> Test on task 14 : loss=1.259 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 68.0%, forg=  4.9% <<<
>>> Test on task 15 : loss=1.320 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 71.2%, forg=  2.5% <<<
>>> Test on task 16 : loss=1.756 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 56.9%, forg=  6.9% <<<
>>> Test on task 17 : loss=1.182 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 75.9%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.416 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 77.5%, forg=  0.8% <<<
>>> Test on task 19 : loss=1.355 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 69.0%, forg=  5.3% <<<
>>> Test on task 20 : loss=1.151 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 72.1%, forg=  1.0% <<<
>>> Test on task 21 : loss=1.122 | TAw acc= 90.2%, forg=  3.3%| TAg acc= 71.7%, forg=  2.2% <<<
>>> Test on task 22 : loss=1.463 | TAw acc=100.0%, forg= -1.0%| TAg acc= 61.5%, forg=  0.0% <<<
>>> Test on task 23 : loss=1.277 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 70.2%, forg=  5.8% <<<
>>> Test on task 24 : loss=1.116 | TAw acc= 96.6%, forg=  2.6%| TAg acc= 73.3%, forg=  0.9% <<<
>>> Test on task 25 : loss=1.406 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 66.3%, forg= -1.1% <<<
>>> Test on task 26 : loss=1.492 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 62.9%, forg=  7.6% <<<
>>> Test on task 27 : loss=1.172 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 70.8%, forg=  3.5% <<<
>>> Test on task 28 : loss=1.396 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 69.1%, forg=  2.7% <<<
>>> Test on task 29 : loss=1.781 | TAw acc= 88.1%, forg=  1.5%| TAg acc= 41.5%, forg= 28.1% <<<
>>> Test on task 30 : loss=1.875 | TAw acc= 89.2%, forg=  1.0%| TAg acc= 44.1%, forg=  5.9% <<<
>>> Test on task 31 : loss=1.652 | TAw acc= 88.1%, forg=  4.2%| TAg acc= 60.2%, forg=  2.5% <<<
>>> Test on task 32 : loss=1.522 | TAw acc= 86.6%, forg= -1.0%| TAg acc= 54.6%, forg= -1.0% <<<
>>> Test on task 33 : loss=1.765 | TAw acc= 98.1%, forg= -0.9%| TAg acc= 45.4%, forg= 17.6% <<<
>>> Test on task 34 : loss=1.891 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 42.5%, forg= 23.9% <<<
>>> Test on task 35 : loss=1.416 | TAw acc= 93.9%, forg=  0.0%| TAg acc= 63.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=5.404, TAw acc= 50.0% | *
| Epoch   2, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=2.767, TAw acc= 70.2% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.731, TAw acc= 86.9% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 94.0% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.248, TAw acc= 98.8% | *
| Epoch   1, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.245, TAw acc= 98.8% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.242, TAw acc= 98.8% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.239, TAw acc= 98.8% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.236, TAw acc= 98.8% | *
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.234, TAw acc= 98.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5263 train exemplars, time=  0.0s
5263
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.558 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 68.2%, forg= 22.0% <<<
>>> Test on task  1 : loss=1.241 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 66.7%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.075 | TAw acc=100.0%, forg=  0.0%| TAg acc= 73.7%, forg= 12.1% <<<
>>> Test on task  3 : loss=1.164 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.9%, forg=  9.5% <<<
>>> Test on task  4 : loss=0.943 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 80.4%, forg=  8.0% <<<
>>> Test on task  5 : loss=1.112 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  6 : loss=1.243 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 76.0%, forg=  8.0% <<<
>>> Test on task  7 : loss=1.751 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 60.7%, forg=  9.3% <<<
>>> Test on task  8 : loss=1.657 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 54.7%, forg=  8.5% <<<
>>> Test on task  9 : loss=1.473 | TAw acc= 88.0%, forg=  2.8%| TAg acc= 59.3%, forg=  8.3% <<<
>>> Test on task 10 : loss=1.135 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.8%, forg=  6.8% <<<
>>> Test on task 11 : loss=1.099 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 72.0%, forg=  3.0% <<<
>>> Test on task 12 : loss=1.660 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 56.1%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.280 | TAw acc= 97.1%, forg= -0.7%| TAg acc= 69.9%, forg=  5.1% <<<
>>> Test on task 14 : loss=1.263 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 66.4%, forg=  6.6% <<<
>>> Test on task 15 : loss=1.308 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 70.3%, forg=  3.4% <<<
>>> Test on task 16 : loss=1.712 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 54.9%, forg=  8.8% <<<
>>> Test on task 17 : loss=1.246 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 66.1%, forg= 11.6% <<<
>>> Test on task 18 : loss=1.439 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 75.8%, forg=  2.5% <<<
>>> Test on task 19 : loss=1.351 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 69.0%, forg=  5.3% <<<
>>> Test on task 20 : loss=1.140 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 70.2%, forg=  2.9% <<<
>>> Test on task 21 : loss=1.184 | TAw acc= 91.3%, forg=  2.2%| TAg acc= 70.7%, forg=  3.3% <<<
>>> Test on task 22 : loss=1.438 | TAw acc=100.0%, forg=  0.0%| TAg acc= 60.6%, forg=  1.0% <<<
>>> Test on task 23 : loss=1.157 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 71.2%, forg=  4.8% <<<
>>> Test on task 24 : loss=1.126 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 73.3%, forg=  0.9% <<<
>>> Test on task 25 : loss=1.386 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 64.1%, forg=  2.2% <<<
>>> Test on task 26 : loss=1.491 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 64.8%, forg=  5.7% <<<
>>> Test on task 27 : loss=1.138 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 73.5%, forg=  0.9% <<<
>>> Test on task 28 : loss=1.364 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 69.1%, forg=  2.7% <<<
>>> Test on task 29 : loss=1.733 | TAw acc= 89.6%, forg=  0.0%| TAg acc= 42.2%, forg= 27.4% <<<
>>> Test on task 30 : loss=1.889 | TAw acc= 90.2%, forg=  0.0%| TAg acc= 40.2%, forg=  9.8% <<<
>>> Test on task 31 : loss=1.659 | TAw acc= 89.8%, forg=  2.5%| TAg acc= 55.9%, forg=  6.8% <<<
>>> Test on task 32 : loss=1.466 | TAw acc= 86.6%, forg=  0.0%| TAg acc= 58.8%, forg= -4.1% <<<
>>> Test on task 33 : loss=1.740 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 48.1%, forg= 14.8% <<<
>>> Test on task 34 : loss=1.768 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 46.9%, forg= 19.5% <<<
>>> Test on task 35 : loss=1.801 | TAw acc= 92.9%, forg=  1.0%| TAg acc= 46.9%, forg= 16.3% <<<
>>> Test on task 36 : loss=1.099 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 78.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=4.542, TAw acc= 67.5% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=2.371, TAw acc= 83.1% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.752, TAw acc= 96.4% | *
| Epoch   4, time=  4.9s | Train: skip eval | Valid: time=  0.3s loss=1.616, TAw acc= 96.4% | *
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.369, TAw acc= 96.4% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.372, TAw acc= 96.4% | *
| Epoch   2, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.375, TAw acc= 96.4% |
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.378, TAw acc= 96.4% |
| Epoch   4, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.380, TAw acc= 96.4% |
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.383, TAw acc= 96.4% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5363 train exemplars, time=  0.1s
5363
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.605 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 67.1%, forg= 23.1% <<<
>>> Test on task  1 : loss=1.322 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 59.1%, forg= 28.0% <<<
>>> Test on task  2 : loss=1.063 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.8%, forg= 10.1% <<<
>>> Test on task  3 : loss=1.190 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.0%, forg= 10.5% <<<
>>> Test on task  4 : loss=1.134 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.3%, forg= 16.1% <<<
>>> Test on task  5 : loss=1.123 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 68.8%, forg= 12.8% <<<
>>> Test on task  6 : loss=1.229 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 78.4%, forg=  5.6% <<<
>>> Test on task  7 : loss=1.790 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 62.6%, forg=  7.5% <<<
>>> Test on task  8 : loss=1.713 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 52.8%, forg= 10.4% <<<
>>> Test on task  9 : loss=1.456 | TAw acc= 88.0%, forg=  2.8%| TAg acc= 63.0%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.112 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 74.4%, forg=  4.3% <<<
>>> Test on task 11 : loss=1.158 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 69.7%, forg=  5.3% <<<
>>> Test on task 12 : loss=1.686 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 55.1%, forg=  8.4% <<<
>>> Test on task 13 : loss=1.291 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 71.3%, forg=  3.7% <<<
>>> Test on task 14 : loss=1.266 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 68.9%, forg=  4.1% <<<
>>> Test on task 15 : loss=1.360 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 69.5%, forg=  4.2% <<<
>>> Test on task 16 : loss=1.694 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 58.8%, forg=  4.9% <<<
>>> Test on task 17 : loss=1.188 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 74.1%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.479 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 72.5%, forg=  5.8% <<<
>>> Test on task 19 : loss=1.424 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 62.8%, forg= 11.5% <<<
>>> Test on task 20 : loss=1.164 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 72.1%, forg=  1.0% <<<
>>> Test on task 21 : loss=1.154 | TAw acc= 94.6%, forg= -1.1%| TAg acc= 66.3%, forg=  7.6% <<<
>>> Test on task 22 : loss=1.495 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 63.5%, forg= -1.9% <<<
>>> Test on task 23 : loss=1.254 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 71.2%, forg=  4.8% <<<
>>> Test on task 24 : loss=1.123 | TAw acc= 97.4%, forg=  1.7%| TAg acc= 71.6%, forg=  2.6% <<<
>>> Test on task 25 : loss=1.446 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 60.9%, forg=  5.4% <<<
>>> Test on task 26 : loss=1.405 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 67.6%, forg=  2.9% <<<
>>> Test on task 27 : loss=1.129 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 78.8%, forg= -4.4% <<<
>>> Test on task 28 : loss=1.381 | TAw acc= 92.7%, forg=  0.9%| TAg acc= 70.9%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.803 | TAw acc= 87.4%, forg=  2.2%| TAg acc= 40.7%, forg= 28.9% <<<
>>> Test on task 30 : loss=1.815 | TAw acc= 89.2%, forg=  1.0%| TAg acc= 43.1%, forg=  6.9% <<<
>>> Test on task 31 : loss=1.601 | TAw acc= 89.0%, forg=  3.4%| TAg acc= 62.7%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.520 | TAw acc= 86.6%, forg=  0.0%| TAg acc= 55.7%, forg=  3.1% <<<
>>> Test on task 33 : loss=1.643 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 56.5%, forg=  6.5% <<<
>>> Test on task 34 : loss=1.728 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 50.4%, forg= 15.9% <<<
>>> Test on task 35 : loss=1.833 | TAw acc= 91.8%, forg=  2.0%| TAg acc= 44.9%, forg= 18.4% <<<
>>> Test on task 36 : loss=1.474 | TAw acc=100.0%, forg= -0.9%| TAg acc= 57.0%, forg= 21.9% <<<
>>> Test on task 37 : loss=1.321 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 67.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=5.313, TAw acc= 53.2% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=2.496, TAw acc= 78.5% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.696, TAw acc= 87.3% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.480, TAw acc= 87.3% | *
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.322, TAw acc= 87.3% | *
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.322, TAw acc= 87.3% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.322, TAw acc= 87.3% |
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.324, TAw acc= 87.3% |
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.325, TAw acc= 87.3% |
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.327, TAw acc= 87.3% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5463 train exemplars, time=  0.0s
5463
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.647 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 64.7%, forg= 25.4% <<<
>>> Test on task  1 : loss=1.260 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 60.2%, forg= 26.9% <<<
>>> Test on task  2 : loss=1.038 | TAw acc=100.0%, forg=  0.0%| TAg acc= 78.8%, forg=  7.1% <<<
>>> Test on task  3 : loss=1.177 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 81.0%, forg= 10.5% <<<
>>> Test on task  4 : loss=1.141 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 70.5%, forg= 17.9% <<<
>>> Test on task  5 : loss=1.075 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  6 : loss=1.314 | TAw acc= 95.2%, forg=  1.6%| TAg acc= 75.2%, forg=  8.8% <<<
>>> Test on task  7 : loss=1.789 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.727 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 49.1%, forg= 14.2% <<<
>>> Test on task  9 : loss=1.412 | TAw acc= 88.9%, forg=  1.9%| TAg acc= 63.0%, forg=  4.6% <<<
>>> Test on task 10 : loss=1.163 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 70.1%, forg=  8.5% <<<
>>> Test on task 11 : loss=1.074 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 72.7%, forg=  2.3% <<<
>>> Test on task 12 : loss=1.642 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.316 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 69.1%, forg=  5.9% <<<
>>> Test on task 14 : loss=1.226 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 71.3%, forg=  1.6% <<<
>>> Test on task 15 : loss=1.387 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 72.0%, forg=  1.7% <<<
>>> Test on task 16 : loss=1.767 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 57.8%, forg=  5.9% <<<
>>> Test on task 17 : loss=1.303 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 68.8%, forg=  8.9% <<<
>>> Test on task 18 : loss=1.451 | TAw acc= 93.3%, forg=  0.8%| TAg acc= 75.8%, forg=  2.5% <<<
>>> Test on task 19 : loss=1.402 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 67.3%, forg=  7.1% <<<
>>> Test on task 20 : loss=1.082 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 74.0%, forg= -1.0% <<<
>>> Test on task 21 : loss=1.125 | TAw acc= 91.3%, forg=  3.3%| TAg acc= 70.7%, forg=  3.3% <<<
>>> Test on task 22 : loss=1.519 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 61.5%, forg=  1.9% <<<
>>> Test on task 23 : loss=1.270 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 65.4%, forg= 10.6% <<<
>>> Test on task 24 : loss=1.109 | TAw acc= 95.7%, forg=  3.4%| TAg acc= 72.4%, forg=  1.7% <<<
>>> Test on task 25 : loss=1.342 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 64.1%, forg=  2.2% <<<
>>> Test on task 26 : loss=1.503 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 63.8%, forg=  6.7% <<<
>>> Test on task 27 : loss=1.173 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 73.5%, forg=  5.3% <<<
>>> Test on task 28 : loss=1.382 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 67.3%, forg=  4.5% <<<
>>> Test on task 29 : loss=1.774 | TAw acc= 89.6%, forg=  0.0%| TAg acc= 43.0%, forg= 26.7% <<<
>>> Test on task 30 : loss=1.862 | TAw acc= 92.2%, forg= -2.0%| TAg acc= 46.1%, forg=  3.9% <<<
>>> Test on task 31 : loss=1.537 | TAw acc= 88.1%, forg=  4.2%| TAg acc= 64.4%, forg= -1.7% <<<
>>> Test on task 32 : loss=1.441 | TAw acc= 86.6%, forg=  0.0%| TAg acc= 57.7%, forg=  1.0% <<<
>>> Test on task 33 : loss=1.721 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 51.9%, forg= 11.1% <<<
>>> Test on task 34 : loss=1.747 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 48.7%, forg= 17.7% <<<
>>> Test on task 35 : loss=1.628 | TAw acc= 94.9%, forg= -1.0%| TAg acc= 61.2%, forg=  2.0% <<<
>>> Test on task 36 : loss=1.473 | TAw acc=100.0%, forg=  0.0%| TAg acc= 54.4%, forg= 24.6% <<<
>>> Test on task 37 : loss=2.070 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 31.2%, forg= 35.7% <<<
>>> Test on task 38 : loss=1.100 | TAw acc= 91.7%, forg=  0.0%| TAg acc= 72.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=3.745, TAw acc= 75.0% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.594, TAw acc= 78.1% | *
| Epoch   3, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.122, TAw acc= 94.8% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.976, TAw acc= 94.8% | *
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=0.938, TAw acc= 95.8% | *
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.931, TAw acc= 95.8% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=0.925, TAw acc= 95.8% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=0.919, TAw acc= 95.8% | *
| Epoch   4, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.915, TAw acc= 95.8% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=0.910, TAw acc= 95.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5563 train exemplars, time=  0.0s
5563
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.659 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 67.6%, forg= 22.5% <<<
>>> Test on task  1 : loss=1.207 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 66.7%, forg= 20.4% <<<
>>> Test on task  2 : loss=1.112 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.286 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 75.2%, forg= 16.2% <<<
>>> Test on task  4 : loss=1.144 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 72.3%, forg= 16.1% <<<
>>> Test on task  5 : loss=0.973 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  6 : loss=1.289 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 76.0%, forg=  8.0% <<<
>>> Test on task  7 : loss=1.788 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 62.6%, forg=  7.5% <<<
>>> Test on task  8 : loss=1.728 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 52.8%, forg= 10.4% <<<
>>> Test on task  9 : loss=1.496 | TAw acc= 88.9%, forg=  1.9%| TAg acc= 58.3%, forg=  9.3% <<<
>>> Test on task 10 : loss=1.149 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 72.6%, forg=  6.0% <<<
>>> Test on task 11 : loss=1.091 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 75.8%, forg= -0.8% <<<
>>> Test on task 12 : loss=1.699 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 55.1%, forg=  8.4% <<<
>>> Test on task 13 : loss=1.242 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 73.5%, forg=  1.5% <<<
>>> Test on task 14 : loss=1.240 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 70.5%, forg=  2.5% <<<
>>> Test on task 15 : loss=1.311 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 70.3%, forg=  3.4% <<<
>>> Test on task 16 : loss=1.694 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 60.8%, forg=  2.9% <<<
>>> Test on task 17 : loss=1.221 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 74.1%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.471 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 76.7%, forg=  1.7% <<<
>>> Test on task 19 : loss=1.347 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 70.8%, forg=  3.5% <<<
>>> Test on task 20 : loss=1.104 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 71.2%, forg=  2.9% <<<
>>> Test on task 21 : loss=1.178 | TAw acc= 92.4%, forg=  2.2%| TAg acc= 68.5%, forg=  5.4% <<<
>>> Test on task 22 : loss=1.548 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 60.6%, forg=  2.9% <<<
>>> Test on task 23 : loss=1.156 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 75.0%, forg=  1.0% <<<
>>> Test on task 24 : loss=1.131 | TAw acc= 96.6%, forg=  2.6%| TAg acc= 69.0%, forg=  5.2% <<<
>>> Test on task 25 : loss=1.278 | TAw acc= 98.9%, forg= -1.1%| TAg acc= 70.7%, forg= -4.3% <<<
>>> Test on task 26 : loss=1.525 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 58.1%, forg= 12.4% <<<
>>> Test on task 27 : loss=1.153 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 73.5%, forg=  5.3% <<<
>>> Test on task 28 : loss=1.346 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 70.9%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.818 | TAw acc= 88.9%, forg=  0.7%| TAg acc= 43.7%, forg= 25.9% <<<
>>> Test on task 30 : loss=1.769 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 47.1%, forg=  2.9% <<<
>>> Test on task 31 : loss=1.630 | TAw acc= 86.4%, forg=  5.9%| TAg acc= 61.9%, forg=  2.5% <<<
>>> Test on task 32 : loss=1.497 | TAw acc= 86.6%, forg=  0.0%| TAg acc= 56.7%, forg=  2.1% <<<
>>> Test on task 33 : loss=1.740 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 51.9%, forg= 11.1% <<<
>>> Test on task 34 : loss=1.714 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 51.3%, forg= 15.0% <<<
>>> Test on task 35 : loss=1.660 | TAw acc= 95.9%, forg= -1.0%| TAg acc= 57.1%, forg=  6.1% <<<
>>> Test on task 36 : loss=1.339 | TAw acc=100.0%, forg=  0.0%| TAg acc= 64.9%, forg= 14.0% <<<
>>> Test on task 37 : loss=2.107 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 28.6%, forg= 38.4% <<<
>>> Test on task 38 : loss=1.752 | TAw acc= 91.7%, forg=  0.0%| TAg acc= 49.1%, forg= 23.1% <<<
>>> Test on task 39 : loss=0.919 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 85.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=5.255, TAw acc= 54.5% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=2.467, TAw acc= 83.1% | *
| Epoch   3, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.479, TAw acc= 92.2% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.243, TAw acc= 90.9% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.053, TAw acc= 93.5% | *
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.050, TAw acc= 93.5% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.048, TAw acc= 93.5% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.045, TAw acc= 93.5% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.043, TAw acc= 94.8% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.041, TAw acc= 94.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5663 train exemplars, time=  0.0s
5663
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.646 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 66.5%, forg= 23.7% <<<
>>> Test on task  1 : loss=1.256 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 65.6%, forg= 21.5% <<<
>>> Test on task  2 : loss=1.054 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.8%, forg= 10.1% <<<
>>> Test on task  3 : loss=1.218 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 84.8%, forg=  6.7% <<<
>>> Test on task  4 : loss=1.114 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 76.8%, forg= 11.6% <<<
>>> Test on task  5 : loss=1.067 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task  6 : loss=1.305 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 76.8%, forg=  7.2% <<<
>>> Test on task  7 : loss=1.802 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 62.6%, forg=  7.5% <<<
>>> Test on task  8 : loss=1.738 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 52.8%, forg= 10.4% <<<
>>> Test on task  9 : loss=1.462 | TAw acc= 88.9%, forg=  1.9%| TAg acc= 62.0%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.180 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 68.4%, forg= 10.3% <<<
>>> Test on task 11 : loss=1.090 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 71.2%, forg=  4.5% <<<
>>> Test on task 12 : loss=1.739 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.307 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 70.6%, forg=  4.4% <<<
>>> Test on task 14 : loss=1.207 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 70.5%, forg=  2.5% <<<
>>> Test on task 15 : loss=1.375 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 66.1%, forg=  7.6% <<<
>>> Test on task 16 : loss=1.730 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 57.8%, forg=  5.9% <<<
>>> Test on task 17 : loss=1.216 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 75.9%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.493 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 75.8%, forg=  2.5% <<<
>>> Test on task 19 : loss=1.388 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 69.9%, forg=  4.4% <<<
>>> Test on task 20 : loss=1.150 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 69.2%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.196 | TAw acc= 90.2%, forg=  4.3%| TAg acc= 69.6%, forg=  4.3% <<<
>>> Test on task 22 : loss=1.489 | TAw acc=100.0%, forg=  0.0%| TAg acc= 61.5%, forg=  1.9% <<<
>>> Test on task 23 : loss=1.207 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 73.1%, forg=  2.9% <<<
>>> Test on task 24 : loss=1.110 | TAw acc= 96.6%, forg=  2.6%| TAg acc= 73.3%, forg=  0.9% <<<
>>> Test on task 25 : loss=1.335 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 67.4%, forg=  3.3% <<<
>>> Test on task 26 : loss=1.469 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 65.7%, forg=  4.8% <<<
>>> Test on task 27 : loss=1.124 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 74.3%, forg=  4.4% <<<
>>> Test on task 28 : loss=1.370 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 70.9%, forg=  0.9% <<<
>>> Test on task 29 : loss=1.787 | TAw acc= 87.4%, forg=  2.2%| TAg acc= 43.7%, forg= 25.9% <<<
>>> Test on task 30 : loss=1.671 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 47.1%, forg=  2.9% <<<
>>> Test on task 31 : loss=1.583 | TAw acc= 89.0%, forg=  3.4%| TAg acc= 63.6%, forg=  0.8% <<<
>>> Test on task 32 : loss=1.389 | TAw acc= 87.6%, forg= -1.0%| TAg acc= 64.9%, forg= -6.2% <<<
>>> Test on task 33 : loss=1.671 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 56.5%, forg=  6.5% <<<
>>> Test on task 34 : loss=1.690 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 54.0%, forg= 12.4% <<<
>>> Test on task 35 : loss=1.675 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 53.1%, forg= 10.2% <<<
>>> Test on task 36 : loss=1.273 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 67.5%, forg= 11.4% <<<
>>> Test on task 37 : loss=1.893 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 41.1%, forg= 25.9% <<<
>>> Test on task 38 : loss=1.585 | TAw acc= 90.7%, forg=  0.9%| TAg acc= 59.3%, forg= 13.0% <<<
>>> Test on task 39 : loss=1.446 | TAw acc= 98.4%, forg= -1.6%| TAg acc= 57.0%, forg= 28.1% <<<
>>> Test on task 40 : loss=1.320 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 64.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=7.240, TAw acc= 38.8% | *
| Epoch   2, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=3.511, TAw acc= 67.5% | *
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=2.358, TAw acc= 75.0% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.951, TAw acc= 85.0% | *
| Epoch   5, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.752, TAw acc= 93.8% | *
| Epoch   1, time=  5.8s | Train: skip eval | Valid: time=  0.3s loss=1.751, TAw acc= 93.8% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.750, TAw acc= 93.8% | *
| Epoch   3, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.750, TAw acc= 93.8% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.749, TAw acc= 93.8% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.748, TAw acc= 93.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5763 train exemplars, time=  0.0s
5763
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.757 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 61.3%, forg= 28.9% <<<
>>> Test on task  1 : loss=1.435 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 64.5%, forg= 22.6% <<<
>>> Test on task  2 : loss=1.130 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.415 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 78.1%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.094 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 76.8%, forg= 11.6% <<<
>>> Test on task  5 : loss=1.003 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task  6 : loss=1.299 | TAw acc= 96.0%, forg=  0.8%| TAg acc= 73.6%, forg= 10.4% <<<
>>> Test on task  7 : loss=1.788 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 66.4%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.803 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 49.1%, forg= 14.2% <<<
>>> Test on task  9 : loss=1.495 | TAw acc= 88.9%, forg=  1.9%| TAg acc= 62.0%, forg=  5.6% <<<
>>> Test on task 10 : loss=1.134 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 71.8%, forg=  6.8% <<<
>>> Test on task 11 : loss=1.120 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 71.2%, forg=  4.5% <<<
>>> Test on task 12 : loss=1.744 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 56.1%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.307 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 72.1%, forg=  2.9% <<<
>>> Test on task 14 : loss=1.241 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 71.3%, forg=  1.6% <<<
>>> Test on task 15 : loss=1.400 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 68.6%, forg=  5.1% <<<
>>> Test on task 16 : loss=1.695 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 59.8%, forg=  3.9% <<<
>>> Test on task 17 : loss=1.260 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 74.1%, forg=  3.6% <<<
>>> Test on task 18 : loss=1.498 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 75.0%, forg=  3.3% <<<
>>> Test on task 19 : loss=1.459 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 66.4%, forg=  8.0% <<<
>>> Test on task 20 : loss=1.264 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 70.2%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.203 | TAw acc= 90.2%, forg=  4.3%| TAg acc= 71.7%, forg=  2.2% <<<
>>> Test on task 22 : loss=1.713 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 59.6%, forg=  3.8% <<<
>>> Test on task 23 : loss=1.159 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 73.1%, forg=  2.9% <<<
>>> Test on task 24 : loss=1.075 | TAw acc= 95.7%, forg=  3.4%| TAg acc= 74.1%, forg=  0.0% <<<
>>> Test on task 25 : loss=1.508 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 62.0%, forg=  8.7% <<<
>>> Test on task 26 : loss=1.463 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 65.7%, forg=  4.8% <<<
>>> Test on task 27 : loss=1.082 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 79.6%, forg= -0.9% <<<
>>> Test on task 28 : loss=1.320 | TAw acc= 91.8%, forg=  1.8%| TAg acc= 73.6%, forg= -1.8% <<<
>>> Test on task 29 : loss=1.874 | TAw acc= 89.6%, forg=  0.0%| TAg acc= 42.2%, forg= 27.4% <<<
>>> Test on task 30 : loss=1.688 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 48.0%, forg=  2.0% <<<
>>> Test on task 31 : loss=1.579 | TAw acc= 89.0%, forg=  3.4%| TAg acc= 65.3%, forg= -0.8% <<<
>>> Test on task 32 : loss=1.632 | TAw acc= 87.6%, forg=  0.0%| TAg acc= 62.9%, forg=  2.1% <<<
>>> Test on task 33 : loss=1.619 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 58.3%, forg=  4.6% <<<
>>> Test on task 34 : loss=1.731 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 52.2%, forg= 14.2% <<<
>>> Test on task 35 : loss=1.631 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 54.1%, forg=  9.2% <<<
>>> Test on task 36 : loss=1.192 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.4%, forg= 10.5% <<<
>>> Test on task 37 : loss=2.000 | TAw acc= 96.4%, forg=  0.0%| TAg acc= 35.7%, forg= 31.2% <<<
>>> Test on task 38 : loss=1.523 | TAw acc= 90.7%, forg=  0.9%| TAg acc= 67.6%, forg=  4.6% <<<
>>> Test on task 39 : loss=1.347 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 62.5%, forg= 22.7% <<<
>>> Test on task 40 : loss=1.694 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 54.3%, forg= 10.5% <<<
>>> Test on task 41 : loss=1.633 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 51.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=5.173, TAw acc= 67.1% | *
| Epoch   2, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.766, TAw acc= 89.5% | *
| Epoch   3, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.229, TAw acc= 94.7% | *
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.116, TAw acc= 92.1% | *
| Epoch   5, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=0.895, TAw acc= 94.7% | *
| Epoch   1, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.897, TAw acc= 94.7% | *
| Epoch   2, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.899, TAw acc= 94.7% |
| Epoch   3, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.901, TAw acc= 94.7% |
| Epoch   4, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.903, TAw acc= 94.7% |
| Epoch   5, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=0.904, TAw acc= 94.7% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5863 train exemplars, time=  0.1s
5863
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.807 | TAw acc= 94.8%, forg=  1.2%| TAg acc= 61.3%, forg= 28.9% <<<
>>> Test on task  1 : loss=1.312 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 63.4%, forg= 23.7% <<<
>>> Test on task  2 : loss=1.084 | TAw acc=100.0%, forg=  0.0%| TAg acc= 73.7%, forg= 12.1% <<<
>>> Test on task  3 : loss=1.261 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 83.8%, forg=  7.6% <<<
>>> Test on task  4 : loss=0.961 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 83.0%, forg=  5.4% <<<
>>> Test on task  5 : loss=1.101 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  6 : loss=1.282 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 76.8%, forg=  7.2% <<<
>>> Test on task  7 : loss=1.867 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.665 | TAw acc= 93.4%, forg= -0.9%| TAg acc= 57.5%, forg=  5.7% <<<
>>> Test on task  9 : loss=1.563 | TAw acc= 88.9%, forg=  1.9%| TAg acc= 61.1%, forg=  6.5% <<<
>>> Test on task 10 : loss=1.123 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 76.1%, forg=  2.6% <<<
>>> Test on task 11 : loss=1.113 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 68.9%, forg=  6.8% <<<
>>> Test on task 12 : loss=1.702 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 13 : loss=1.309 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 73.5%, forg=  1.5% <<<
>>> Test on task 14 : loss=1.307 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 66.4%, forg=  6.6% <<<
>>> Test on task 15 : loss=1.338 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 72.0%, forg=  1.7% <<<
>>> Test on task 16 : loss=1.812 | TAw acc= 90.2%, forg=  2.0%| TAg acc= 58.8%, forg=  4.9% <<<
>>> Test on task 17 : loss=1.217 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 75.9%, forg=  1.8% <<<
>>> Test on task 18 : loss=1.527 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 73.3%, forg=  5.0% <<<
>>> Test on task 19 : loss=1.439 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 65.5%, forg=  8.8% <<<
>>> Test on task 20 : loss=1.220 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 63.5%, forg= 10.6% <<<
>>> Test on task 21 : loss=1.205 | TAw acc= 90.2%, forg=  4.3%| TAg acc= 68.5%, forg=  5.4% <<<
>>> Test on task 22 : loss=1.565 | TAw acc= 99.0%, forg=  1.0%| TAg acc= 60.6%, forg=  2.9% <<<
>>> Test on task 23 : loss=1.340 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 64.4%, forg= 11.5% <<<
>>> Test on task 24 : loss=1.188 | TAw acc= 95.7%, forg=  3.4%| TAg acc= 69.0%, forg=  5.2% <<<
>>> Test on task 25 : loss=1.398 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 64.1%, forg=  6.5% <<<
>>> Test on task 26 : loss=1.480 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 67.6%, forg=  2.9% <<<
>>> Test on task 27 : loss=1.092 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 74.3%, forg=  5.3% <<<
>>> Test on task 28 : loss=1.345 | TAw acc= 92.7%, forg=  0.9%| TAg acc= 71.8%, forg=  1.8% <<<
>>> Test on task 29 : loss=1.866 | TAw acc= 88.9%, forg=  0.7%| TAg acc= 42.2%, forg= 27.4% <<<
>>> Test on task 30 : loss=1.811 | TAw acc= 93.1%, forg= -1.0%| TAg acc= 51.0%, forg= -1.0% <<<
>>> Test on task 31 : loss=1.571 | TAw acc= 87.3%, forg=  5.1%| TAg acc= 64.4%, forg=  0.8% <<<
>>> Test on task 32 : loss=1.477 | TAw acc= 87.6%, forg=  0.0%| TAg acc= 58.8%, forg=  6.2% <<<
>>> Test on task 33 : loss=1.626 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 58.3%, forg=  4.6% <<<
>>> Test on task 34 : loss=1.639 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 57.5%, forg=  8.8% <<<
>>> Test on task 35 : loss=1.555 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 61.2%, forg=  2.0% <<<
>>> Test on task 36 : loss=1.149 | TAw acc=100.0%, forg=  0.0%| TAg acc= 75.4%, forg=  3.5% <<<
>>> Test on task 37 : loss=2.081 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 32.1%, forg= 34.8% <<<
>>> Test on task 38 : loss=1.475 | TAw acc= 90.7%, forg=  0.9%| TAg acc= 64.8%, forg=  7.4% <<<
>>> Test on task 39 : loss=1.308 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 67.2%, forg= 18.0% <<<
>>> Test on task 40 : loss=1.644 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 55.2%, forg=  9.5% <<<
>>> Test on task 41 : loss=2.364 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 24.1%, forg= 27.8% <<<
>>> Test on task 42 : loss=1.332 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=5.812, TAw acc= 43.2% | *
| Epoch   2, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=2.570, TAw acc= 77.8% | *
| Epoch   3, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.481, TAw acc= 91.4% | *
| Epoch   4, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 93.8% | *
| Epoch   5, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.249, TAw acc= 96.3% | *
| Epoch   1, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.239, TAw acc= 96.3% | *
| Epoch   2, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.229, TAw acc= 96.3% | *
| Epoch   3, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.221, TAw acc= 96.3% | *
| Epoch   4, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 96.3% | *
| Epoch   5, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.206, TAw acc= 96.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5963 train exemplars, time=  0.0s
5963
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.733 | TAw acc= 95.4%, forg=  0.6%| TAg acc= 64.2%, forg= 26.0% <<<
>>> Test on task  1 : loss=1.407 | TAw acc= 97.8%, forg=  1.1%| TAg acc= 60.2%, forg= 26.9% <<<
>>> Test on task  2 : loss=1.115 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.7%, forg= 15.2% <<<
>>> Test on task  3 : loss=1.247 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 83.8%, forg=  7.6% <<<
>>> Test on task  4 : loss=1.039 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 76.8%, forg= 11.6% <<<
>>> Test on task  5 : loss=1.019 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  6 : loss=1.276 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 77.6%, forg=  6.4% <<<
>>> Test on task  7 : loss=1.870 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 63.6%, forg=  6.5% <<<
>>> Test on task  8 : loss=1.717 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 53.8%, forg=  9.4% <<<
>>> Test on task  9 : loss=1.624 | TAw acc= 88.9%, forg=  1.9%| TAg acc= 56.5%, forg= 11.1% <<<
>>> Test on task 10 : loss=1.151 | TAw acc= 98.3%, forg=  0.9%| TAg acc= 72.6%, forg=  6.0% <<<
>>> Test on task 11 : loss=1.071 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 72.0%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.780 | TAw acc= 94.4%, forg=  0.9%| TAg acc= 53.3%, forg= 10.3% <<<
>>> Test on task 13 : loss=1.344 | TAw acc= 96.3%, forg=  0.7%| TAg acc= 73.5%, forg=  1.5% <<<
>>> Test on task 14 : loss=1.213 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 68.9%, forg=  4.1% <<<
>>> Test on task 15 : loss=1.389 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 66.9%, forg=  6.8% <<<
>>> Test on task 16 : loss=1.857 | TAw acc= 91.2%, forg=  1.0%| TAg acc= 57.8%, forg=  5.9% <<<
>>> Test on task 17 : loss=1.275 | TAw acc= 96.4%, forg=  2.7%| TAg acc= 75.0%, forg=  2.7% <<<
>>> Test on task 18 : loss=1.532 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 73.3%, forg=  5.0% <<<
>>> Test on task 19 : loss=1.446 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 68.1%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.135 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 73.1%, forg=  1.0% <<<
>>> Test on task 21 : loss=1.228 | TAw acc= 90.2%, forg=  4.3%| TAg acc= 72.8%, forg=  1.1% <<<
>>> Test on task 22 : loss=1.469 | TAw acc=100.0%, forg=  0.0%| TAg acc= 63.5%, forg=  0.0% <<<
>>> Test on task 23 : loss=1.208 | TAw acc= 96.2%, forg=  1.0%| TAg acc= 73.1%, forg=  2.9% <<<
>>> Test on task 24 : loss=1.074 | TAw acc= 95.7%, forg=  3.4%| TAg acc= 72.4%, forg=  1.7% <<<
>>> Test on task 25 : loss=1.314 | TAw acc= 98.9%, forg=  0.0%| TAg acc= 66.3%, forg=  4.3% <<<
>>> Test on task 26 : loss=1.473 | TAw acc= 95.2%, forg=  1.9%| TAg acc= 68.6%, forg=  1.9% <<<
>>> Test on task 27 : loss=1.133 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 77.0%, forg=  2.7% <<<
>>> Test on task 28 : loss=1.362 | TAw acc= 92.7%, forg=  0.9%| TAg acc= 70.9%, forg=  2.7% <<<
>>> Test on task 29 : loss=1.823 | TAw acc= 88.9%, forg=  0.7%| TAg acc= 45.2%, forg= 24.4% <<<
>>> Test on task 30 : loss=1.696 | TAw acc= 92.2%, forg=  1.0%| TAg acc= 50.0%, forg=  1.0% <<<
>>> Test on task 31 : loss=1.565 | TAw acc= 89.0%, forg=  3.4%| TAg acc= 64.4%, forg=  0.8% <<<
>>> Test on task 32 : loss=1.508 | TAw acc= 84.5%, forg=  3.1%| TAg acc= 58.8%, forg=  6.2% <<<
>>> Test on task 33 : loss=1.638 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 58.3%, forg=  4.6% <<<
>>> Test on task 34 : loss=1.709 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 52.2%, forg= 14.2% <<<
>>> Test on task 35 : loss=1.560 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 58.2%, forg=  5.1% <<<
>>> Test on task 36 : loss=1.179 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.5%, forg= 11.4% <<<
>>> Test on task 37 : loss=2.024 | TAw acc= 95.5%, forg=  0.9%| TAg acc= 38.4%, forg= 28.6% <<<
>>> Test on task 38 : loss=1.470 | TAw acc= 90.7%, forg=  0.9%| TAg acc= 68.5%, forg=  3.7% <<<
>>> Test on task 39 : loss=1.287 | TAw acc= 97.7%, forg=  0.8%| TAg acc= 67.2%, forg= 18.0% <<<
>>> Test on task 40 : loss=1.644 | TAw acc= 93.3%, forg= -1.0%| TAg acc= 56.2%, forg=  8.6% <<<
>>> Test on task 41 : loss=2.364 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 22.2%, forg= 29.6% <<<
>>> Test on task 42 : loss=1.846 | TAw acc= 94.3%, forg= -1.9%| TAg acc= 55.2%, forg= 19.0% <<<
>>> Test on task 43 : loss=1.233 | TAw acc= 94.5%, forg=  0.0%| TAg acc= 66.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 79.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.2% 
	 91.9%  89.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 90.6% 
	 92.5%  95.7%  84.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.0% 
	 92.5%  95.7%  97.0%  89.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.7% 
	 92.5%  95.7%  97.0%  98.1%  91.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 94.2%  97.8%  99.0%  98.1%  94.6%  79.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.9% 
	 94.2%  97.8%  99.0%  98.1%  98.2%  91.7%  88.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 94.8%  97.8%  99.0%  98.1%  98.2%  92.7%  89.6%  85.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.4% 
	 94.8%  97.8%  99.0%  98.1%  98.2%  90.8%  94.4%  91.6%  84.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 94.8%  97.8%  99.0%  98.1%  98.2%  95.4%  94.4%  91.6%  89.6%  84.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 94.8%  97.8%  99.0%  98.1%  98.2%  94.5%  94.4%  91.6%  86.8%  86.1%  84.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.3% 
	 94.8%  97.8%  98.0%  98.1%  98.2%  94.5%  95.2%  91.6%  87.7%  86.1%  97.4%  94.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.5% 
	 94.2%  97.8% 100.0%  98.1%  98.2%  93.6%  96.0%  91.6%  90.6%  87.0%  95.7%  98.5%  86.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.4% 
	 94.8%  96.8% 100.0%  98.1%  98.2%  93.6%  94.4%  90.7%  88.7%  87.0%  95.7%  97.7%  92.5%  95.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 96.0%  96.8%  99.0%  98.1%  98.2%  93.6%  96.0%  91.6%  88.7%  88.0%  96.6%  97.7%  94.4%  95.6%  89.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 94.2%  97.8%  99.0%  98.1%  98.2%  95.4%  95.2%  91.6%  90.6%  88.0%  99.1%  97.7%  92.5%  95.6%  95.1%  89.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 94.2%  97.8%  99.0%  98.1%  98.2%  95.4%  96.0%  92.5%  89.6%  87.0%  98.3%  98.5%  92.5%  96.3%  95.9%  94.9%  90.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 94.8%  97.8%  99.0%  98.1%  98.2%  94.5%  96.0%  92.5%  88.7%  87.0%  97.4%  97.7%  95.3%  96.3%  95.9%  93.2%  91.2%  90.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  92.7%  95.2%  91.6%  92.5%  87.0%  96.6%  97.7%  94.4%  96.3%  95.9%  94.1%  89.2%  98.2%  85.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 94.8%  97.8% 100.0%  98.1%  98.2%  96.3%  95.2%  92.5%  89.6%  88.0%  97.4%  97.7%  93.5%  96.3%  96.7%  94.1%  91.2%  98.2%  92.5%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 94.8%  97.8% 100.0%  98.1%  98.2%  95.4%  95.2%  90.7%  90.6%  87.0%  97.4%  97.7%  94.4%  96.3%  96.7%  93.2%  91.2%  99.1%  92.5%  94.7%  96.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 94.8%  98.9% 100.0%  98.1%  98.2%  96.3%  96.8%  90.7%  90.6%  88.0%  98.3%  97.7%  93.5%  96.3%  96.7%  94.9%  90.2%  99.1%  92.5%  96.5%  99.0%  84.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 95.4%  98.9% 100.0%  98.1%  98.2%  96.3%  96.0%  91.6%  90.6%  88.0%  96.6%  97.7%  94.4%  96.3%  96.7%  94.1%  92.2%  99.1%  92.5%  97.3%  99.0%  91.3%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  96.3%  96.0%  91.6%  91.5%  88.0%  98.3%  97.7%  94.4%  96.3%  96.7%  94.1%  92.2%  97.3%  92.5%  97.3%  99.0%  88.0%  99.0%  88.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 95.4%  98.9% 100.0%  98.1%  98.2%  94.5%  94.4%  92.5%  91.5%  88.9%  99.1%  97.7%  94.4%  96.3%  96.7%  94.9%  92.2%  97.3%  93.3%  97.3%  98.1%  87.0%  99.0%  93.3%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  96.3%  95.2%  92.5%  89.6%  89.8%  98.3%  97.7%  95.3%  96.3%  96.7%  94.1%  91.2%  97.3%  93.3%  95.6%  98.1%  91.3%  99.0%  93.3%  98.3%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 95.4%  98.9%  99.0%  98.1%  98.2%  95.4%  96.8%  94.4%  90.6%  88.0%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  91.2%  97.3%  93.3%  95.6%  99.0%  93.5%  99.0%  96.2%  99.1%  96.7%  97.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  93.6%  96.0%  92.5%  92.5%  89.8%  98.3%  97.7%  95.3%  96.3%  96.7%  94.9%  91.2%  97.3%  94.2%  95.6%  98.1%  91.3%  97.1%  97.1%  99.1%  96.7%  96.2%  97.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 95.4%  98.9% 100.0%  98.1%  98.2%  96.3%  95.2%  92.5%  92.5%  89.8%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  92.2%  98.2%  94.2%  96.5%  98.1%  92.4%  98.1%  96.2%  99.1%  96.7%  96.2%  98.2%  93.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  94.5%  94.4%  94.4%  90.6%  88.9%  99.1%  97.7%  93.5%  96.3%  96.7%  94.9%  90.2%  97.3%  93.3%  96.5%  98.1%  91.3%  98.1%  97.1%  98.3%  97.8%  96.2%  97.3%  93.6%  88.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  96.3%  95.2%  93.5%  90.6%  88.9%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  90.2%  97.3%  94.2%  96.5%  98.1%  92.4%  98.1%  96.2%  99.1%  96.7%  96.2%  97.3%  92.7%  89.6%  87.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 94.8%  98.9% 100.0%  98.1%  98.2%  96.3%  95.2%  92.5%  90.6%  88.9%  98.3%  97.7%  94.4%  96.3%  96.7%  94.1%  90.2%  97.3%  94.2%  96.5%  98.1%  91.3%  99.0%  96.2%  99.1%  95.7%  96.2%  96.5%  91.8%  88.9%  87.3%  92.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 94.8%  98.9% 100.0%  98.1%  98.2%  96.3%  95.2%  92.5%  90.6%  88.9%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  90.2%  97.3%  94.2%  97.3%  98.1%  91.3%  99.0%  96.2%  99.1%  97.8%  96.2%  96.5%  91.8%  88.9%  87.3%  91.5%  85.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 94.2%  98.9% 100.0%  98.1%  98.2%  96.3%  95.2%  92.5%  90.6%  88.9%  99.1%  97.7%  94.4%  96.3%  96.7%  94.9%  92.2%  97.3%  94.2%  96.5%  98.1%  91.3%  99.0%  96.2%  97.4%  97.8%  96.2%  96.5%  91.8%  88.9%  90.2%  89.8%  84.5%  94.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 95.4%  98.9% 100.0%  98.1%  98.2%  94.5%  96.0%  92.5%  91.5%  90.7%  99.1%  97.7%  94.4%  96.3%  96.7%  94.1%  91.2%  97.3%  94.2%  96.5%  98.1%  91.3%  99.0%  95.2%  97.4%  97.8%  96.2%  96.5%  91.8%  88.1%  86.3%  90.7%  85.6%  97.2%  97.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 95.4%  98.9% 100.0%  98.1%  98.2%  96.3%  96.8%  92.5%  92.5%  88.0%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  92.2%  97.3%  94.2%  97.3%  98.1%  90.2% 100.0%  96.2%  96.6%  97.8%  96.2%  96.5%  91.8%  88.1%  89.2%  88.1%  86.6%  98.1%  97.3%  93.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 94.8%  98.9% 100.0%  98.1%  98.2%  95.4%  96.0%  92.5%  91.5%  88.0%  99.1%  97.7%  94.4%  97.1%  96.7%  94.9%  91.2%  97.3%  94.2%  97.3%  98.1%  91.3% 100.0%  95.2%  97.4%  97.8%  95.2%  96.5%  91.8%  89.6%  90.2%  89.8%  86.6%  97.2%  97.3%  92.9%  99.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  96.3%  96.8%  94.4%  91.5%  88.0%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  90.2%  97.3%  94.2%  97.3%  98.1%  94.6%  99.0%  96.2%  97.4%  97.8%  96.2%  96.5%  92.7%  87.4%  89.2%  89.0%  86.6%  98.1%  97.3%  91.8% 100.0%  96.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 95.4%  97.8% 100.0%  98.1%  98.2%  95.4%  95.2%  92.5%  92.5%  88.9%  99.1%  97.7%  95.3%  96.3%  96.7%  94.9%  90.2%  97.3%  93.3%  97.3%  98.1%  91.3%  99.0%  96.2%  95.7%  97.8%  95.2%  96.5%  91.8%  89.6%  92.2%  88.1%  86.6%  98.1%  97.3%  94.9% 100.0%  96.4%  91.7%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 94.8%  97.8% 100.0%  98.1%  98.2%  94.5%  96.0%  93.5%  91.5%  88.9%  99.1%  97.7%  94.4%  96.3%  96.7%  94.9%  91.2%  97.3%  94.2%  97.3%  98.1%  92.4%  99.0%  96.2%  96.6%  98.9%  95.2%  96.5%  91.8%  88.9%  91.2%  86.4%  86.6%  99.1%  97.3%  95.9% 100.0%  95.5%  91.7%  96.9%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 94.8%  97.8% 100.0%  97.1%  98.2%  96.3%  96.0%  93.5%  91.5%  88.9%  98.3%  97.7%  95.3%  96.3%  96.7%  94.9%  90.2%  97.3%  94.2%  97.3%  98.1%  90.2% 100.0%  96.2%  96.6%  97.8%  95.2%  97.3%  91.8%  87.4%  91.2%  89.0%  87.6%  99.1%  98.2%  95.9%  99.1%  96.4%  90.7%  98.4%  92.4%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 95.4%  97.8% 100.0%  97.1%  98.2%  96.3%  96.0%  93.5%  90.6%  88.9%  99.1%  97.7%  94.4%  96.3%  96.7%  94.9%  91.2%  97.3%  94.2%  97.3%  98.1%  90.2%  99.0%  96.2%  95.7%  97.8%  95.2%  96.5%  91.8%  89.6%  91.2%  89.0%  87.6%  99.1%  96.5%  95.9% 100.0%  96.4%  90.7%  98.4%  92.4%  93.5%   0.0%   0.0% 	Avg.: 95.1% 
	 94.8%  97.8% 100.0%  97.1%  98.2%  96.3%  96.8%  93.5%  93.4%  88.9%  99.1%  97.7%  94.4%  96.3%  96.7%  94.9%  90.2%  97.3%  94.2%  97.3%  98.1%  90.2%  99.0%  96.2%  95.7%  98.9%  95.2%  96.5%  92.7%  88.9%  93.1%  87.3%  87.6%  99.1%  97.3%  95.9% 100.0%  95.5%  90.7%  98.4%  92.4%  93.5%  92.4%   0.0% 	Avg.: 95.1% 
	 95.4%  97.8% 100.0%  97.1%  98.2%  96.3%  96.8%  93.5%  92.5%  88.9%  98.3%  97.7%  94.4%  96.3%  96.7%  94.9%  91.2%  96.4%  94.2%  98.2%  98.1%  90.2% 100.0%  96.2%  95.7%  98.9%  95.2%  97.3%  92.7%  88.9%  92.2%  89.0%  84.5%  99.1%  97.3%  95.9% 100.0%  95.5%  90.7%  97.7%  93.3%  93.5%  94.3%  94.5% 	Avg.: 95.1% 
************************************************************************************************************
TAg Acc
	 79.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.2% 
	 83.8%  61.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.6% 
	 90.2%  66.7%  64.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.8% 
	 78.6%  72.0%  75.8%  74.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 75.2% 
	 82.1%  84.9%  76.8%  75.2%  78.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 79.5% 
	 82.7%  80.6%  75.8%  82.9%  67.9%  67.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.1% 
	 83.2%  87.1%  77.8%  77.1%  75.9%  60.6%  71.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.1% 
	 85.0%  79.6%  84.8%  84.8%  84.8%  67.9%  64.8%  59.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.4% 
	 79.8%  87.1%  85.9%  85.7%  83.9%  69.7%  69.6%  44.9%  51.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 73.2% 
	 79.2%  86.0%  84.8%  86.7%  85.7%  74.3%  77.6%  46.7%  37.7%  56.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.5% 
	 79.8%  84.9%  81.8%  87.6%  88.4%  78.0%  80.0%  53.3%  35.8%  39.8%  59.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.9% 
	 77.5%  81.7%  76.8%  82.9%  84.8%  81.7%  81.6%  56.1%  39.6%  48.1%  47.9%  72.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.2% 
	 78.6%  80.6%  84.8%  84.8%  86.6%  78.0%  83.2%  63.6%  52.8%  54.6%  57.3%  59.1%  57.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.9% 
	 79.2%  82.8%  79.8%  87.6%  84.8%  78.0%  74.4%  60.7%  44.3%  61.1%  73.5%  56.1%  37.4%  74.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.6% 
	 79.8%  74.2%  82.8%  91.4%  77.7%  74.3%  77.6%  64.5%  49.1%  63.9%  65.0%  64.4%  45.8%  50.7%  63.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.3% 
	 77.5%  71.0%  82.8%  87.6%  85.7%  77.1%  82.4%  65.4%  46.2%  62.0%  67.5%  69.7%  57.9%  55.9%  50.0%  71.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.4% 
	 77.5%  77.4%  85.9%  84.8%  78.6%  81.7%  84.0%  65.4%  49.1%  67.6%  70.9%  66.7%  51.4%  67.6%  57.4%  50.8%  58.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.1% 
	 74.6%  73.1%  78.8%  86.7%  84.8%  79.8%  83.2%  69.2%  47.2%  64.8%  72.6%  72.0%  53.3%  60.3%  57.4%  40.7%  45.1%  57.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.7% 
	 74.0%  79.6%  76.8%  85.7%  75.0%  74.3%  74.4%  68.2%  56.6%  64.8%  73.5%  71.2%  57.9%  69.1%  63.9%  61.9%  50.0%  44.6%  74.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 72.3%  77.4%  80.8%  87.6%  79.5%  76.1%  81.6%  66.4%  48.1%  62.0%  71.8%  68.2%  63.6%  69.1%  68.9%  56.8%  54.9%  59.8%  61.7%  57.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 71.1%  73.1%  78.8%  87.6%  75.9%  78.9%  82.4%  66.4%  50.0%  63.0%  73.5%  69.7%  58.9%  65.4%  69.7%  64.4%  55.9%  65.2%  71.7%  41.6%  59.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.7% 
	 70.5%  66.7%  70.7%  83.8%  75.9%  71.6%  81.6%  70.1%  54.7%  65.7%  71.8%  73.5%  63.6%  71.3%  68.0%  66.1%  55.9%  67.9%  76.7%  51.3%  59.6%  64.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 74.0%  68.8%  77.8%  83.8%  83.0%  76.1%  81.6%  63.6%  56.6%  66.7%  77.8%  67.4%  61.7%  65.4%  67.2%  69.5%  49.0%  65.2%  75.0%  60.2%  59.6%  50.0%  57.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.7% 
	 71.1%  77.4%  74.7%  76.2%  75.0%  77.1%  78.4%  66.4%  56.6%  67.6%  69.2%  75.0%  58.9%  69.1%  73.0%  61.0%  62.7%  67.9%  78.3%  56.6%  62.5%  51.1%  49.0%  67.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 74.6%  74.2%  75.8%  84.8%  71.4%  79.8%  80.0%  63.6%  54.7%  63.0%  70.9%  68.9%  55.1%  70.6%  68.0%  72.0%  63.7%  69.6%  75.0%  61.1%  60.6%  57.6%  43.3%  50.0%  73.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.3% 
	 69.9%  69.9%  74.7%  85.7%  85.7%  80.7%  80.0%  64.5%  48.1%  63.0%  70.1%  72.7%  58.9%  69.1%  68.0%  72.0%  61.8%  69.6%  75.8%  66.4%  72.1%  65.2%  51.0%  62.5%  56.9%  56.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.1% 
	 69.4%  78.5%  74.7%  85.7%  80.4%  75.2%  80.8%  65.4%  50.0%  62.0%  72.6%  73.5%  57.0%  72.1%  69.7%  72.0%  63.7%  69.6%  72.5%  66.4%  70.2%  64.1%  58.7%  70.2%  63.8%  34.8%  65.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.1% 
	 71.1%  67.7%  70.7%  86.7%  75.0%  73.4%  76.0%  62.6%  63.2%  67.6%  76.1%  66.7%  57.0%  69.9%  63.1%  73.7%  60.8%  71.4%  66.7%  67.3%  68.3%  70.7%  54.8%  72.1%  63.8%  46.7%  59.0%  73.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.7% 
	 66.5%  71.0%  70.7%  82.9%  79.5%  78.0%  79.2%  64.5%  56.6%  65.7%  70.9%  65.9%  54.2%  72.1%  70.5%  69.5%  60.8%  71.4%  70.8%  69.9%  67.3%  73.9%  57.7%  74.0%  66.4%  53.3%  52.4%  62.8%  67.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 66.5%  73.1%  69.7%  83.8%  78.6%  75.2%  73.6%  57.9%  50.9%  60.2%  73.5%  71.2%  57.0%  72.8%  69.7%  68.6%  53.9%  70.5%  74.2%  70.8%  68.3%  71.7%  51.0%  76.0%  73.3%  50.0%  60.0%  66.4%  49.1%  69.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.9% 
	 68.8%  74.2%  70.7%  84.8%  79.5%  75.2%  75.2%  66.4%  55.7%  61.1%  73.5%  73.5%  54.2%  75.0%  70.5%  70.3%  55.9%  73.2%  74.2%  66.4%  67.3%  71.7%  59.6%  65.4%  69.0%  56.5%  60.0%  71.7%  56.4%  38.5%  50.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.6% 
	 70.5%  66.7%  70.7%  82.9%  76.8%  68.8%  77.6%  61.7%  56.6%  65.7%  78.6%  67.4%  56.1%  72.1%  69.7%  72.9%  55.9%  73.2%  71.7%  72.6%  72.1%  64.1%  59.6%  67.3%  67.2%  56.5%  63.8%  68.1%  58.2%  39.3%  26.5%  62.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.4% 
	 68.8%  67.7%  71.7%  81.9%  82.1%  77.1%  76.8%  63.6%  54.7%  60.2%  66.7%  69.7%  57.9%  75.0%  66.4%  69.5%  56.9%  74.1%  75.0%  74.3%  71.2%  72.8%  59.6%  74.0%  74.1%  65.2%  64.8%  71.7%  67.3%  40.0%  30.4%  55.1%  53.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.4% 
	 67.6%  65.6%  70.7%  82.9%  82.1%  72.5%  78.4%  64.5%  55.7%  61.1%  73.5%  69.7%  57.9%  72.8%  66.4%  68.6%  56.9%  77.7%  74.2%  69.0%  68.3%  71.7%  60.6%  72.1%  70.7%  59.8%  70.5%  73.5%  71.8%  40.7%  39.2%  58.5%  49.5%  63.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.4% 
	 69.4%  58.1%  72.7%  83.8%  79.5%  73.4%  76.8%  61.7%  57.5%  63.0%  70.9%  70.5%  55.1%  67.6%  63.1%  69.5%  56.9%  72.3%  76.7%  71.7%  73.1%  72.8%  61.5%  72.1%  69.0%  62.0%  63.8%  74.3%  67.3%  43.0%  44.1%  56.8%  48.5%  46.3%  66.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.5% 
	 67.6%  62.4%  69.7%  81.9%  80.4%  74.3%  77.6%  63.6%  60.4%  60.2%  76.1%  75.0%  55.1%  71.3%  68.0%  71.2%  56.9%  75.9%  77.5%  69.0%  72.1%  71.7%  61.5%  70.2%  73.3%  66.3%  62.9%  70.8%  69.1%  41.5%  44.1%  60.2%  54.6%  45.4%  42.5%  63.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.7% 
	 68.2%  66.7%  73.7%  81.9%  80.4%  73.4%  76.0%  60.7%  54.7%  59.3%  71.8%  72.0%  56.1%  69.9%  66.4%  70.3%  54.9%  66.1%  75.8%  69.0%  70.2%  70.7%  60.6%  71.2%  73.3%  64.1%  64.8%  73.5%  69.1%  42.2%  40.2%  55.9%  58.8%  48.1%  46.9%  46.9%  78.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.9% 
	 67.1%  59.1%  75.8%  81.0%  72.3%  68.8%  78.4%  62.6%  52.8%  63.0%  74.4%  69.7%  55.1%  71.3%  68.9%  69.5%  58.8%  74.1%  72.5%  62.8%  72.1%  66.3%  63.5%  71.2%  71.6%  60.9%  67.6%  78.8%  70.9%  40.7%  43.1%  62.7%  55.7%  56.5%  50.4%  44.9%  57.0%  67.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.7% 
	 64.7%  60.2%  78.8%  81.0%  70.5%  73.4%  75.2%  63.6%  49.1%  63.0%  70.1%  72.7%  57.0%  69.1%  71.3%  72.0%  57.8%  68.8%  75.8%  67.3%  74.0%  70.7%  61.5%  65.4%  72.4%  64.1%  63.8%  73.5%  67.3%  43.0%  46.1%  64.4%  57.7%  51.9%  48.7%  61.2%  54.4%  31.2%  72.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.2% 
	 67.6%  66.7%  70.7%  75.2%  72.3%  78.0%  76.0%  62.6%  52.8%  58.3%  72.6%  75.8%  55.1%  73.5%  70.5%  70.3%  60.8%  74.1%  76.7%  70.8%  71.2%  68.5%  60.6%  75.0%  69.0%  70.7%  58.1%  73.5%  70.9%  43.7%  47.1%  61.9%  56.7%  51.9%  51.3%  57.1%  64.9%  28.6%  49.1%  85.2%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.9% 
	 66.5%  65.6%  75.8%  84.8%  76.8%  74.3%  76.8%  62.6%  52.8%  62.0%  68.4%  71.2%  57.0%  70.6%  70.5%  66.1%  57.8%  75.9%  75.8%  69.9%  69.2%  69.6%  61.5%  73.1%  73.3%  67.4%  65.7%  74.3%  70.9%  43.7%  47.1%  63.6%  64.9%  56.5%  54.0%  53.1%  67.5%  41.1%  59.3%  57.0%  64.8%   0.0%   0.0%   0.0% 	Avg.: 65.3% 
	 61.3%  64.5%  70.7%  78.1%  76.8%  74.3%  73.6%  66.4%  49.1%  62.0%  71.8%  71.2%  56.1%  72.1%  71.3%  68.6%  59.8%  74.1%  75.0%  66.4%  70.2%  71.7%  59.6%  73.1%  74.1%  62.0%  65.7%  79.6%  73.6%  42.2%  48.0%  65.3%  62.9%  58.3%  52.2%  54.1%  68.4%  35.7%  67.6%  62.5%  54.3%  51.9%   0.0%   0.0% 	Avg.: 64.7% 
	 61.3%  63.4%  73.7%  83.8%  83.0%  73.4%  76.8%  63.6%  57.5%  61.1%  76.1%  68.9%  57.0%  73.5%  66.4%  72.0%  58.8%  75.9%  73.3%  65.5%  63.5%  68.5%  60.6%  64.4%  69.0%  64.1%  67.6%  74.3%  71.8%  42.2%  51.0%  64.4%  58.8%  58.3%  57.5%  61.2%  75.4%  32.1%  64.8%  67.2%  55.2%  24.1%  74.3%   0.0% 	Avg.: 64.6% 
	 64.2%  60.2%  70.7%  83.8%  76.8%  78.0%  77.6%  63.6%  53.8%  56.5%  72.6%  72.0%  53.3%  73.5%  68.9%  66.9%  57.8%  75.0%  73.3%  68.1%  73.1%  72.8%  63.5%  73.1%  72.4%  66.3%  68.6%  77.0%  70.9%  45.2%  50.0%  64.4%  58.8%  58.3%  52.2%  58.2%  67.5%  38.4%  68.5%  67.2%  56.2%  22.2%  55.2%  66.4% 	Avg.: 64.4% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	-12.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:-12.7% 
	 -0.6%  -6.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -3.5% 
	  0.0%   0.0% -12.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.0% 
	  0.0%   0.0%   0.0%  -8.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.1% 
	 -1.7%  -2.2%  -2.0%   0.0%  -3.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.9% 
	  0.0%   0.0%   0.0%   0.0%  -3.6% -11.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.6% 
	 -0.6%   0.0%   0.0%   0.0%   0.0%  -0.9%  -1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.4% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   1.8%  -4.8%  -6.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.2% 
	  0.0%   0.0%   0.0%   0.0%   0.0%  -2.8%   0.0%   0.0%  -5.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.9% 
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   2.8%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.0%   0.0%   1.0%   0.0%   0.0%   0.9%  -0.8%   0.0%   1.9%   0.0% -12.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.9% 
	  0.6%   0.0%  -1.0%   0.0%   0.0%   1.8%  -0.8%   0.0%  -0.9%  -0.9%   1.7%  -3.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.3% 
	  0.0%   1.1%   0.0%   0.0%   0.0%   1.8%   1.6%   0.9%   1.9%   0.0%   1.7%   0.8%  -6.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	 -1.2%   1.1%   1.0%   0.0%   0.0%   1.8%   0.0%   0.0%   1.9%  -0.9%   0.9%   0.8%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.7%   0.0%   1.0%   0.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%  -1.7%   0.8%   1.9%   0.0%  -5.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.1% 
	  1.7%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%  -0.9%   0.9%   0.9%   0.9%   0.0%   1.9%  -0.7%  -0.8%  -5.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.0% 
	  1.2%   0.0%   1.0%   0.0%   0.0%   0.9%   0.0%   0.0%   1.9%   0.9%   1.7%   0.8%  -0.9%   0.0%   0.0%   1.7%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   2.8%   0.8%   0.9%  -1.9%   0.9%   2.6%   0.8%   0.9%   0.0%   0.0%   0.8%   2.0%  -8.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.2%   0.0%   0.0%   0.0%   0.0%  -0.9%   0.8%   0.0%   2.8%   0.0%   1.7%   0.8%   1.9%   0.0%  -0.8%   0.8%   0.0%   0.0%  -6.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  1.2%   0.0%   0.0%   0.0%   0.0%   0.9%   0.8%   1.9%   1.9%   0.9%   1.7%   0.8%   0.9%   0.0%   0.0%   1.7%   0.0%  -0.9%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.2%  -1.1%   0.0%   0.0%   0.0%   0.0%  -0.8%   1.9%   1.9%   0.0%   0.9%   0.8%   1.9%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%  -1.8%  -2.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.9%   1.9%   0.0%   2.6%   0.8%   0.9%   0.0%   0.0%   0.8%  -1.0%   0.0%   0.0%  -0.9%   0.0%  -6.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   0.0%   0.8%   0.9%   0.9%   0.0%   0.9%   0.8%   0.9%   0.0%   0.0%   0.8%   0.0%   1.8%   0.0%   0.0%   0.0%   3.3%  -4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   1.8%   2.4%   0.0%   0.9%  -0.9%   0.0%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   1.8%  -0.8%   0.0%   1.0%   4.3%   0.0%  -4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   0.0%   1.6%   0.0%   2.8%  -0.9%   0.9%   0.8%   0.0%   0.0%   0.0%   0.8%   1.0%   1.8%   0.0%   1.8%   1.0%   0.0%   0.0%   0.0%  -2.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.0%   1.0%   0.0%   0.0%   0.9%   0.0%  -1.9%   1.9%   1.9%   0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   1.0%   1.8%   0.0%   1.8%   0.0%  -2.2%   0.0%  -2.9%  -0.9%  -1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   2.8%   0.8%   1.9%   0.0%   0.0%   0.9%   0.8%   0.0%   0.0%   0.0%   0.0%   1.0%   1.8%  -0.8%   1.8%   1.0%   2.2%   1.9%  -1.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   0.0%   1.6%   1.9%   0.0%   0.0%   0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.0%   1.1%   1.0%   1.0%   0.0%   0.0%   1.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   1.8%   2.4%   0.0%   1.9%   0.9%   0.0%   0.8%   1.9%   0.0%   0.0%   0.0%   2.0%   1.8%   0.8%   0.9%   1.0%   2.2%   1.0%   0.0%   0.9%  -1.1%   1.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   0.0%   1.6%   0.9%   1.9%   0.9%   0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   2.0%   1.8%   0.0%   0.9%   1.0%   1.1%   1.0%   1.0%   0.0%   1.1%   1.0%   0.9%   0.9%  -1.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.2%   0.0%   0.0%   0.0%   0.0%   0.0%   1.6%   1.9%   1.9%   0.9%   0.9%   0.8%   0.9%   0.0%   0.0%   0.8%   2.0%   1.8%   0.0%   0.9%   1.0%   2.2%   0.0%   1.0%   0.0%   2.2%   1.0%   1.8%   1.8%   0.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  1.2%   0.0%   0.0%   0.0%   0.0%   0.0%   1.6%   1.9%   1.9%   0.9%   0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   2.0%   1.8%   0.0%   0.0%   1.0%   2.2%   0.0%   1.0%   0.0%   0.0%   1.0%   1.8%   1.8%   0.7%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   1.6%   1.9%   1.9%   0.9%   0.0%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   1.8%   0.0%   0.9%   1.0%   2.2%   0.0%   1.0%   1.7%   0.0%   1.0%   1.8%   1.8%   0.7%  -2.9%   2.5%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   1.8%   0.8%   1.9%   0.9%  -0.9%   0.0%   0.8%   0.9%   0.0%   0.0%   0.8%   1.0%   1.8%   0.0%   0.9%   1.0%   2.2%   0.0%   1.9%   1.7%   0.0%   1.0%   1.8%   1.8%   1.5%   3.9%   1.7%   0.0%  -2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.9%   0.0%   2.8%   0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0%   1.8%   0.0%   0.0%   1.0%   3.3%  -1.0%   1.0%   2.6%   0.0%   1.0%   1.8%   1.8%   1.5%   1.0%   4.2%  -1.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.2%   0.0%   0.0%   0.0%   0.0%   0.9%   0.8%   1.9%   0.9%   2.8%   0.0%   0.8%   0.9%  -0.7%   0.0%   0.0%   1.0%   1.8%   0.0%   0.0%   1.0%   2.2%   0.0%   1.9%   1.7%   0.0%   1.9%   1.8%   1.8%   0.0%   0.0%   2.5%   0.0%   0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   2.8%   0.9%   0.8%   0.9%   0.7%   0.0%   0.0%   2.0%   1.8%   0.0%   0.0%   1.0%  -1.1%   1.0%   1.0%   1.7%   0.0%   1.0%   1.8%   0.9%   2.2%   1.0%   3.4%   0.0%   0.0%   0.0%   2.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   1.1%   0.0%   0.0%   0.0%   0.9%   1.6%   1.9%   0.0%   1.9%   0.0%   0.8%   0.0%   0.7%   0.0%   0.0%   2.0%   1.8%   0.8%   0.0%   1.0%   3.3%   1.0%   1.0%   3.4%   0.0%   1.9%   1.8%   1.8%   0.0%  -2.0%   4.2%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.2%   1.1%   0.0%   0.0%   0.0%   1.8%   0.8%   0.9%   0.9%   1.9%   0.0%   0.8%   0.9%   0.7%   0.0%   0.0%   1.0%   1.8%   0.0%   0.0%   1.0%   2.2%   1.0%   1.0%   2.6%  -1.1%   1.9%   1.8%   1.8%   0.7%   1.0%   5.9%   0.0%  -0.9%   0.0%  -1.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.2%   1.1%   0.0%   1.0%   0.0%   0.0%   0.8%   0.9%   0.9%   1.9%   0.9%   0.8%   0.0%   0.7%   0.0%   0.0%   2.0%   1.8%   0.0%   0.0%   1.0%   4.3%   0.0%   1.0%   2.6%   1.1%   1.9%   0.9%   1.8%   2.2%   1.0%   3.4%  -1.0%   0.0%  -0.9%   0.0%   0.9%   0.0%   0.9%  -1.6%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   1.1%   0.0%   1.0%   0.0%   0.0%   0.8%   0.9%   1.9%   1.9%   0.0%   0.8%   0.9%   0.7%   0.0%   0.0%   1.0%   1.8%   0.0%   0.0%   1.0%   4.3%   1.0%   1.0%   3.4%   1.1%   1.9%   1.8%   1.8%   0.0%   1.0%   3.4%   0.0%   0.0%   1.8%   0.0%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  1.2%   1.1%   0.0%   1.0%   0.0%   0.0%   0.0%   0.9%  -0.9%   1.9%   0.0%   0.8%   0.9%   0.7%   0.0%   0.0%   2.0%   1.8%   0.0%   0.0%   1.0%   4.3%   1.0%   1.0%   3.4%   0.0%   1.9%   1.8%   0.9%   0.7%  -1.0%   5.1%   0.0%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.6%   1.1%   0.0%   1.0%   0.0%   0.0%   0.0%   0.9%   0.9%   1.9%   0.9%   0.8%   0.9%   0.7%   0.0%   0.0%   1.0%   2.7%   0.0%  -0.9%   1.0%   4.3%   0.0%   1.0%   3.4%   0.0%   1.9%   0.9%   0.9%   0.7%   1.0%   3.4%   3.1%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   0.8%  -1.0%   0.0%  -1.9%   0.0% 	Avg.:  0.8% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -4.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.6% 
	 -6.4%  -5.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -5.9% 
	 11.6%  -5.4% -11.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.6% 
	  8.1% -12.9%  -1.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.7% 
	  7.5%   4.3%   1.0%  -7.6%  10.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.2% 
	  6.9%  -2.2%  -1.0%   5.7%   2.7%   6.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.1% 
	  5.2%   7.5%  -7.1%  -1.9%  -6.2%  -0.9%   6.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	 10.4%   0.0%  -1.0%  -1.0%   0.9%  -1.8%   1.6%  15.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.0% 
	 11.0%   1.1%   1.0%  -1.0%  -0.9%  -4.6%  -6.4%  13.1%  14.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.1% 
	 10.4%   2.2%   4.0%  -1.0%  -2.7%  -3.7%  -2.4%   6.5%  16.0%  16.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.6% 
	 12.7%   5.4%   9.1%   4.8%   3.6%  -3.7%  -1.6%   3.7%  12.3%   8.3%  12.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.0% 
	 11.6%   6.5%   1.0%   2.9%   1.8%   3.7%  -1.6%  -3.7%  -0.9%   1.9%   2.6%  12.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.2% 
	 11.0%   4.3%   6.1%   0.0%   3.6%   3.7%   8.8%   2.8%   8.5%  -4.6% -13.7%  15.9%  20.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	 10.4%  12.9%   3.0%  -3.8%  10.7%   7.3%   5.6%  -0.9%   3.8%  -2.8%   8.5%   7.6%  12.1%  23.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.0% 
	 12.7%  16.1%   3.0%   3.8%   2.7%   4.6%   0.8%  -0.9%   6.6%   1.9%   6.0%   2.3%   0.0%  18.4%  13.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	 12.7%   9.7%   0.0%   6.7%   9.8%   0.0%  -0.8%   0.0%   3.8%  -3.7%   2.6%   5.3%   6.5%   6.6%   5.7%  20.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.3% 
	 15.6%  14.0%   7.1%   4.8%   3.6%   1.8%   0.8%  -3.7%   5.7%   2.8%   0.9%   0.0%   4.7%  14.0%   5.7%  30.5%  13.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.2% 
	 16.2%   7.5%   9.1%   5.7%  13.4%   7.3%   9.6%   0.9%  -3.8%   2.8%   0.0%   0.8%   0.0%   5.1%  -0.8%   9.3%   8.8%  12.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	 17.9%   9.7%   5.1%   3.8%   8.9%   5.5%   2.4%   2.8%   8.5%   5.6%   1.7%   3.8%  -5.6%   5.1%  -4.9%  14.4%   3.9%  -2.7%  12.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	 19.1%  14.0%   7.1%   3.8%  12.5%   2.8%   1.6%   2.8%   6.6%   4.6%   0.0%   2.3%   4.7%   8.8%  -0.8%   6.8%   2.9%  -5.4%   2.5%  15.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.6% 
	 19.7%  20.4%  15.2%   7.6%  12.5%  10.1%   2.4%  -0.9%   1.9%   1.9%   1.7%  -1.5%   0.0%   2.9%   1.6%   5.1%   2.9%  -2.7%  -2.5%   6.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.0% 
	 16.2%  18.3%   8.1%   7.6%   5.4%   5.5%   2.4%   6.5%   0.0%   0.9%  -4.3%   6.1%   1.9%   8.8%   2.5%   1.7%   9.8%   2.7%   1.7%  -2.7%   0.0%  14.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	 19.1%   9.7%  11.1%  15.2%  13.4%   4.6%   5.6%   3.7%   0.0%   0.0%   8.5%  -1.5%   4.7%   5.1%  -3.3%  10.2%  -3.9%   0.0%  -1.7%   3.5%  -2.9%  13.0%   8.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.3% 
	 15.6%  12.9%  10.1%   6.7%  17.0%   1.8%   4.0%   6.5%   1.9%   4.6%   6.8%   6.1%   8.4%   3.7%   4.9%  -0.8%  -1.0%  -1.8%   3.3%  -0.9%   1.9%   6.5%  14.4%  17.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	 20.2%  17.2%  11.1%   5.7%   2.7%   0.9%   4.0%   5.6%   8.5%   4.6%   7.7%   2.3%   4.7%   5.1%   4.9%   0.0%   2.0%   0.0%   2.5%  -5.3%  -9.6%  -1.1%   6.7%   4.8%  16.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.9% 
	 20.8%   8.6%  11.1%   5.7%   8.0%   6.4%   3.2%   4.7%   6.6%   5.6%   5.1%   1.5%   6.5%   2.2%   3.3%   0.0%   0.0%   0.0%   5.8%   0.0%   1.9%   1.1%  -1.0%  -2.9%   9.5%  21.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.2% 
	 19.1%  19.4%  15.2%   4.8%  13.4%   8.3%   8.0%   7.5%  -6.6%   0.0%   1.7%   8.3%   6.5%   4.4%   9.8%  -1.7%   2.9%  -1.8%  11.7%  -0.9%   3.8%  -5.4%   3.8%  -1.9%   9.5%   9.8%   6.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	 23.7%  16.1%  15.2%   8.6%   8.9%   3.7%   4.8%   5.6%   6.6%   1.9%   6.8%   9.1%   9.3%   2.2%   2.5%   4.2%   2.9%   0.0%   7.5%  -2.7%   4.8%  -3.3%   1.0%  -1.9%   6.9%   3.3%  13.3%  10.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	 23.7%  14.0%  16.2%   7.6%   9.8%   6.4%  10.4%  12.1%  12.3%   7.4%   4.3%   3.8%   6.5%   1.5%   3.3%   5.1%   9.8%   0.9%   4.2%  -0.9%   3.8%   2.2%   7.7%  -1.9%   0.0%   6.5%   5.7%   7.1%  18.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.2% 
	 21.4%  12.9%  15.2%   6.7%   8.9%   6.4%   8.8%   3.7%   7.5%   6.5%   4.3%   1.5%   9.3%  -0.7%   2.5%   3.4%   7.8%  -1.8%   4.2%   4.4%   4.8%   2.2%  -1.0%  10.6%   4.3%   0.0%   5.7%   1.8%  10.9%  31.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.8% 
	 19.7%  20.4%  15.2%   8.6%  11.6%  12.8%   6.4%   8.4%   6.6%   1.9%  -0.9%   7.6%   7.5%   2.9%   3.3%   0.8%   7.8%   0.0%   6.7%  -1.8%   0.0%   9.8%   0.0%   8.7%   6.0%   0.0%   1.9%   5.3%   9.1%  30.4%  23.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.7% 
	 21.4%  19.4%  14.1%   9.5%   6.2%   4.6%   7.2%   6.5%   8.5%   7.4%  12.0%   5.3%   5.6%   0.0%   6.6%   4.2%   6.9%  -0.9%   3.3%  -1.8%   1.0%   1.1%   0.0%   1.9%  -0.9%  -8.7%   1.0%   1.8%   0.0%  29.6%  19.6%   7.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	 22.5%  21.5%  15.2%   8.6%   6.2%   9.2%   5.6%   5.6%   7.5%   6.5%   5.1%   5.3%   5.6%   2.2%   6.6%   5.1%   6.9%  -3.6%   4.2%   5.3%   3.8%   2.2%  -1.0%   3.8%   3.4%   5.4%  -4.8%   0.0%  -4.5%  28.9%  10.8%   4.2%   4.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	 20.8%  29.0%  13.1%   7.6%   8.9%   8.3%   7.2%   8.4%   5.7%   4.6%   7.7%   4.5%   8.4%   7.4%   9.8%   4.2%   6.9%   5.4%   1.7%   2.7%  -1.0%   1.1%  -1.0%   3.8%   5.2%   3.3%   6.7%  -0.9%   4.5%  26.7%   5.9%   5.9%   5.2%  16.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	 22.5%  24.7%  16.2%   9.5%   8.0%   7.3%   6.4%   6.5%   2.8%   7.4%   2.6%   0.0%   8.4%   3.7%   4.9%   2.5%   6.9%   1.8%   0.8%   5.3%   1.0%   2.2%   0.0%   5.8%   0.9%  -1.1%   7.6%   3.5%   2.7%  28.1%   5.9%   2.5%  -1.0%  17.6%  23.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.1% 
	 22.0%  20.4%  12.1%   9.5%   8.0%   8.3%   8.0%   9.3%   8.5%   8.3%   6.8%   3.0%   7.5%   5.1%   6.6%   3.4%   8.8%  11.6%   2.5%   5.3%   2.9%   3.3%   1.0%   4.8%   0.9%   2.2%   5.7%   0.9%   2.7%  27.4%   9.8%   6.8%  -4.1%  14.8%  19.5%  16.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.1% 
	 23.1%  28.0%  10.1%  10.5%  16.1%  12.8%   5.6%   7.5%  10.4%   4.6%   4.3%   5.3%   8.4%   3.7%   4.1%   4.2%   4.9%   3.6%   5.8%  11.5%   1.0%   7.6%  -1.9%   4.8%   2.6%   5.4%   2.9%  -4.4%   0.9%  28.9%   6.9%   0.0%   3.1%   6.5%  15.9%  18.4%  21.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.2% 
	 25.4%  26.9%   7.1%  10.5%  17.9%   8.3%   8.8%   6.5%  14.2%   4.6%   8.5%   2.3%   6.5%   5.9%   1.6%   1.7%   5.9%   8.9%   2.5%   7.1%  -1.0%   3.3%   1.9%  10.6%   1.7%   2.2%   6.7%   5.3%   4.5%  26.7%   3.9%  -1.7%   1.0%  11.1%  17.7%   2.0%  24.6%  35.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.9% 
	 22.5%  20.4%  15.2%  16.2%  16.1%   3.7%   8.0%   7.5%  10.4%   9.3%   6.0%  -0.8%   8.4%   1.5%   2.5%   3.4%   2.9%   3.6%   1.7%   3.5%   2.9%   5.4%   2.9%   1.0%   5.2%  -4.3%  12.4%   5.3%   0.9%  25.9%   2.9%   2.5%   2.1%  11.1%  15.0%   6.1%  14.0%  38.4%  23.1%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	 23.7%  21.5%  10.1%   6.7%  11.6%   7.3%   7.2%   7.5%  10.4%   5.6%  10.3%   4.5%   6.5%   4.4%   2.5%   7.6%   5.9%   1.8%   2.5%   4.4%   4.8%   4.3%   1.9%   2.9%   0.9%   3.3%   4.8%   4.4%   0.9%  25.9%   2.9%   0.8%  -6.2%   6.5%  12.4%  10.2%  11.4%  25.9%  13.0%  28.1%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.0% 
	 28.9%  22.6%  15.2%  13.3%  11.6%   7.3%  10.4%   3.7%  14.2%   5.6%   6.8%   4.5%   7.5%   2.9%   1.6%   5.1%   3.9%   3.6%   3.3%   8.0%   3.8%   2.2%   3.8%   2.9%   0.0%   8.7%   4.8%  -0.9%  -1.8%  27.4%   2.0%  -0.8%   2.1%   4.6%  14.2%   9.2%  10.5%  31.2%   4.6%  22.7%  10.5%   0.0%   0.0%   0.0% 	Avg.:  8.3% 
	 28.9%  23.7%  12.1%   7.6%   5.4%   8.3%   7.2%   6.5%   5.7%   6.5%   2.6%   6.8%   6.5%   1.5%   6.6%   1.7%   4.9%   1.8%   5.0%   8.8%  10.6%   5.4%   2.9%  11.5%   5.2%   6.5%   2.9%   5.3%   1.8%  27.4%  -1.0%   0.8%   6.2%   4.6%   8.8%   2.0%   3.5%  34.8%   7.4%  18.0%   9.5%  27.8%   0.0%   0.0% 	Avg.:  8.6% 
	 26.0%  26.9%  15.2%   7.6%  11.6%   3.7%   6.4%   6.5%   9.4%  11.1%   6.0%   3.8%  10.3%   1.5%   4.1%   6.8%   5.9%   2.7%   5.0%   6.2%   1.0%   1.1%   0.0%   2.9%   1.7%   4.3%   1.9%   2.7%   2.7%  24.4%   1.0%   0.8%   6.2%   4.6%  14.2%   5.1%  11.4%  28.6%   3.7%  18.0%   8.6%  29.6%  19.0%   0.0% 	Avg.:  8.6% 
************************************************************************************************************
[Elapsed time = 0.4 h]
Done!

f1_score_micro: 0.6451088061826317
f1_score_macro: 0.610381398894291
              precision    recall  f1-score   support

           0       0.86      0.67      0.75         9
           1       0.53      0.89      0.67         9
           2       0.06      0.25      0.10         4
           3       0.09      0.50      0.15         4
           4       0.50      0.75      0.60         4
           5       1.00      1.00      1.00         5
           6       0.09      0.50      0.15         4
           7       0.22      0.40      0.29         5
           8       0.11      0.20      0.14         5
           9       0.50      0.75      0.60         4
          10       0.00      0.00      0.00         4
          11       0.75      0.75      0.75         4
          12       0.67      0.80      0.73         5
          13       0.67      0.67      0.67         9
          14       0.00      0.00      0.00         4
          15       0.67      1.00      0.80         4
          16       1.00      0.80      0.89         5
          17       0.75      0.75      0.75         4
          18       1.00      0.67      0.80         9
          19       0.43      0.75      0.55         4
          20       0.80      1.00      0.89         4
          21       1.00      0.60      0.75         5
          22       1.00      0.50      0.67         4
          23       0.78      0.78      0.78         9
          24       0.67      0.50      0.57         4
          25       0.71      1.00      0.83         5
          26       0.00      0.00      0.00         4
          27       1.00      1.00      1.00         4
          28       0.43      0.75      0.55         4
          29       0.83      0.56      0.67         9
          30       0.75      0.75      0.75         4
          31       0.67      0.50      0.57         4
          32       0.80      1.00      0.89         4
          33       0.25      0.25      0.25         4
          34       0.67      1.00      0.80         4
          35       1.00      0.80      0.89         5
          36       0.00      0.00      0.00         4
          37       0.78      0.78      0.78         9
          38       0.00      0.00      0.00         4
          39       0.00      0.00      0.00         4
          40       0.00      0.00      0.00         4
          41       1.00      1.00      1.00         9
          42       0.20      0.20      0.20         5
          43       0.50      1.00      0.67         4
          44       0.57      1.00      0.73         4
          45       0.67      1.00      0.80         4
          46       1.00      1.00      1.00         4
          47       0.00      0.00      0.00         4
          48       0.00      0.00      0.00         4
          49       0.60      0.75      0.67         4
          50       0.75      0.75      0.75         4
          51       1.00      1.00      1.00         5
          52       1.00      0.75      0.86         4
          53       0.14      0.25      0.18         4
          54       1.00      1.00      1.00         5
          55       0.17      0.25      0.20         4
          56       0.57      1.00      0.73         4
          57       0.57      0.44      0.50         9
          58       0.80      1.00      0.89         4
          59       0.83      1.00      0.91         5
          60       0.80      1.00      0.89         4
          61       0.82      1.00      0.90         9
          62       0.20      0.25      0.22         4
          63       0.75      0.60      0.67         5
          64       0.60      0.75      0.67         4
          65       0.75      0.75      0.75         4
          66       0.43      0.60      0.50         5
          67       1.00      1.00      1.00         4
          68       0.80      1.00      0.89         4
          69       0.43      0.33      0.38         9
          70       0.67      0.50      0.57         4
          71       0.80      1.00      0.89         4
          72       0.20      0.25      0.22         4
          73       0.75      0.75      0.75         4
          74       0.80      1.00      0.89         4
          75       0.71      1.00      0.83         5
          76       0.12      0.75      0.21         4
          77       0.80      0.89      0.84         9
          78       0.75      0.67      0.71         9
          79       0.47      1.00      0.64         9
          80       0.00      0.00      0.00         4
          81       0.67      1.00      0.80         4
          82       0.80      1.00      0.89         4
          83       0.67      1.00      0.80         4
          84       0.82      1.00      0.90         9
          85       0.71      1.00      0.83         5
          86       0.57      0.80      0.67         5
          87       0.75      0.75      0.75         4
          88       0.43      0.75      0.55         4
          89       0.67      1.00      0.80         4
          90       0.60      0.60      0.60         5
          91       0.67      0.50      0.57         4
          92       0.50      1.00      0.67         4
          93       1.00      0.80      0.89         5
          94       0.80      1.00      0.89         4
          95       1.00      1.00      1.00         4
          96       0.40      0.50      0.44         4
          97       0.45      1.00      0.62         5
          98       1.00      0.75      0.86         4
          99       0.67      0.89      0.76         9
         100       1.00      1.00      1.00         5
         101       0.25      0.17      0.20         6
         102       1.00      1.00      1.00         5
         103       1.00      1.00      1.00         4
         104       0.80      1.00      0.89         4
         105       0.86      0.67      0.75         9
         106       0.60      0.75      0.67         4
         107       0.70      0.78      0.74         9
         108       0.67      0.80      0.73         5
         109       0.80      0.80      0.80         5
         110       0.40      0.67      0.50         9
         111       0.89      0.89      0.89         9
         112       1.00      0.75      0.86         4
         113       0.00      0.00      0.00         4
         114       0.43      0.75      0.55         4
         115       0.33      0.25      0.29         4
         116       0.90      1.00      0.95         9
         117       0.57      0.89      0.70         9
         118       0.50      0.44      0.47         9
         119       0.62      1.00      0.77         5
         120       0.57      1.00      0.73         4
         121       0.00      0.00      0.00         9
         122       0.69      1.00      0.82         9
         123       1.00      0.75      0.86         4
         124       1.00      1.00      1.00         4
         125       0.67      1.00      0.80         4
         126       0.57      0.80      0.67         5
         127       1.00      0.75      0.86         4
         128       0.60      0.75      0.67         4
         129       1.00      1.00      1.00         4
         130       1.00      0.80      0.89         5
         131       1.00      1.00      1.00         5
         132       1.00      1.00      1.00         4
         133       1.00      1.00      1.00         4
         134       0.14      0.75      0.23         4
         135       0.83      1.00      0.91         5
         136       0.73      0.89      0.80         9
         137       0.78      0.78      0.78         9
         138       0.50      0.25      0.33         4
         139       1.00      0.33      0.50         9
         140       0.90      1.00      0.95         9
         141       0.80      1.00      0.89         4
         142       0.80      0.80      0.80         5
         143       1.00      1.00      1.00         5
         144       0.71      1.00      0.83         5
         145       0.33      0.25      0.29         4
         146       0.55      0.67      0.60         9
         147       1.00      1.00      1.00         4
         148       1.00      1.00      1.00         5
         149       0.00      0.00      0.00         4
         150       1.00      0.75      0.86         4
         151       0.62      0.89      0.73         9
         152       0.90      1.00      0.95         9
         153       1.00      0.78      0.88         9
         154       0.67      0.89      0.76         9
         155       0.00      0.00      0.00         5
         156       0.25      0.25      0.25         4
         157       0.50      0.60      0.55         5
         158       1.00      0.60      0.75         5
         159       0.25      0.11      0.15         9
         160       1.00      0.44      0.62         9
         161       1.00      1.00      1.00         4
         162       1.00      1.00      1.00         4
         163       0.75      0.75      0.75         4
         164       1.00      1.00      1.00         9
         165       1.00      1.00      1.00         5
         166       0.00      0.00      0.00         4
         167       0.75      0.75      0.75         4
         168       1.00      0.80      0.89         5
         169       0.80      0.80      0.80         5
         170       0.33      1.00      0.50         5
         171       0.80      1.00      0.89         4
         172       0.00      0.00      0.00         4
         173       0.60      0.75      0.67         4
         174       1.00      0.75      0.86         4
         175       0.67      0.50      0.57         4
         176       0.82      1.00      0.90         9
         177       0.60      0.75      0.67         4
         178       0.33      1.00      0.50         4
         179       0.75      0.75      0.75         4
         180       0.12      0.25      0.17         4
         181       0.60      0.33      0.43         9
         182       0.71      1.00      0.83         5
         183       0.00      0.00      0.00         9
         184       0.90      1.00      0.95         9
         185       0.00      0.00      0.00         4
         186       0.11      0.50      0.17         4
         187       0.00      0.00      0.00         4
         188       1.00      1.00      1.00         4
         189       0.14      0.25      0.18         4
         190       0.07      0.11      0.08         9
         191       0.75      0.75      0.75         4
         192       0.00      0.00      0.00         4
         193       0.80      1.00      0.89         4
         194       0.67      0.67      0.67         9
         195       0.75      0.75      0.75         4
         196       0.00      0.00      0.00         9
         197       0.33      0.44      0.38         9
         198       0.00      0.00      0.00         4
         199       1.00      1.00      1.00         5
         200       0.67      1.00      0.80         4
         201       0.83      1.00      0.91         5
         202       1.00      1.00      1.00         4
         203       1.00      0.75      0.86         4
         204       0.00      0.00      0.00         9
         205       0.50      0.60      0.55         5
         206       0.80      1.00      0.89         4
         207       1.00      0.75      0.86         4
         208       1.00      1.00      1.00         4
         209       1.00      0.50      0.67         4
         210       0.50      0.50      0.50         4
         211       1.00      0.56      0.71         9
         212       1.00      1.00      1.00         4
         213       0.00      0.00      0.00         4
         214       1.00      1.00      1.00         4
         215       0.00      0.00      0.00         9
         216       1.00      0.80      0.89         5
         217       0.50      0.75      0.60         4
         218       0.80      1.00      0.89         4
         219       1.00      0.78      0.88         9
         220       0.50      0.50      0.50         4
         221       0.40      0.50      0.44         4
         222       0.00      0.00      0.00         9
         223       1.00      1.00      1.00         5
         224       0.90      1.00      0.95         9
         225       1.00      0.89      0.94         9
         226       1.00      1.00      1.00         4
         227       0.89      0.89      0.89         9
         228       1.00      1.00      1.00         4
         229       0.67      1.00      0.80         4
         230       0.00      0.00      0.00         4
         231       1.00      1.00      1.00         9
         232       0.57      1.00      0.73         4
         233       1.00      1.00      1.00         4
         234       0.60      1.00      0.75         9
         235       0.62      0.56      0.59         9
         236       0.89      0.89      0.89         9
         237       0.80      1.00      0.89         4
         238       0.50      0.20      0.29         5
         239       0.89      0.89      0.89         9
         240       1.00      0.50      0.67         4
         241       0.80      0.89      0.84         9
         242       0.67      0.67      0.67         9
         243       1.00      0.50      0.67         4
         244       1.00      1.00      1.00         9
         245       0.78      0.78      0.78         9
         246       1.00      1.00      1.00         4
         247       0.50      1.00      0.67         5
         248       0.80      1.00      0.89         4
         249       0.00      0.00      0.00         4
         250       0.80      1.00      0.89         4
         251       0.00      0.00      0.00         9
         252       0.00      0.00      0.00         4
         253       0.75      1.00      0.86         9
         254       0.80      1.00      0.89         4
         255       0.75      0.75      0.75         4
         256       1.00      0.75      0.86         4
         257       0.25      0.25      0.25         4
         258       0.67      0.67      0.67         9
         259       0.00      0.00      0.00         9
         260       0.80      0.89      0.84         9
         261       0.60      0.75      0.67         4
         262       1.00      1.00      1.00         4
         263       1.00      0.50      0.67         4
         264       0.25      0.25      0.25         4
         265       0.43      0.75      0.55         4
         266       0.00      0.00      0.00         4
         267       1.00      0.80      0.89         5
         268       0.44      0.80      0.57         5
         269       1.00      0.75      0.86         4
         270       0.75      0.75      0.75         4
         271       0.00      0.00      0.00         9
         272       0.00      0.00      0.00         4
         273       0.62      0.56      0.59         9
         274       1.00      0.78      0.88         9
         275       1.00      0.78      0.88         9
         276       1.00      1.00      1.00         9
         277       0.67      0.50      0.57         4
         278       0.70      0.78      0.74         9
         279       1.00      0.78      0.88         9
         280       0.75      0.60      0.67         5
         281       0.75      0.67      0.71         9
         282       0.43      0.75      0.55         4
         283       0.83      1.00      0.91         5
         284       1.00      0.75      0.86         4
         285       1.00      0.60      0.75         5
         286       0.00      0.00      0.00         4
         287       0.70      1.00      0.82         7
         288       0.82      1.00      0.90         9
         289       0.50      1.00      0.67         4
         290       0.67      0.22      0.33         9
         291       0.73      0.89      0.80         9
         292       1.00      0.89      0.94         9
         293       0.00      0.00      0.00         4
         294       0.11      0.25      0.15         4
         295       0.67      0.67      0.67         9
         296       1.00      0.50      0.67         4
         297       0.38      0.75      0.50         4
         298       1.00      0.50      0.67         4
         299       0.60      0.75      0.67         4
         300       0.62      0.56      0.59         9
         301       0.56      1.00      0.71         5
         302       0.00      0.00      0.00         4
         303       0.82      1.00      0.90         9
         304       0.67      1.00      0.80         4
         305       1.00      1.00      1.00         9
         306       0.71      0.56      0.63         9
         307       0.50      0.67      0.57         9
         308       0.50      0.33      0.40         9
         309       1.00      0.89      0.94         9
         310       1.00      1.00      1.00         4
         311       0.60      0.75      0.67         4
         312       0.60      0.60      0.60         5
         313       1.00      0.75      0.86         4
         314       0.00      0.00      0.00         4
         315       0.67      0.50      0.57         4
         316       1.00      0.89      0.94         9
         317       1.00      0.75      0.86         4
         318       1.00      0.78      0.88         9
         319       0.50      0.75      0.60         4
         320       0.25      0.75      0.38         4
         321       1.00      0.78      0.88         9
         322       1.00      0.80      0.89         5
         323       0.88      0.78      0.82         9
         324       0.60      0.75      0.67         4
         325       1.00      1.00      1.00         5
         326       0.75      1.00      0.86         9
         327       1.00      0.25      0.40         4
         328       0.00      0.00      0.00         4
         329       0.33      0.25      0.29         4
         330       0.80      0.80      0.80         5
         331       0.50      0.75      0.60         4
         332       1.00      0.44      0.62         9
         333       0.71      0.56      0.63         9
         334       0.00      0.00      0.00         4
         335       0.83      0.56      0.67         9
         336       0.00      0.00      0.00         4
         337       0.67      0.50      0.57         4
         338       1.00      1.00      1.00         4
         339       1.00      0.50      0.67         4
         340       0.83      1.00      0.91         5
         341       0.00      0.00      0.00         4
         342       0.57      0.44      0.50         9
         343       0.75      0.75      0.75         4
         344       1.00      0.50      0.67         4
         345       0.80      1.00      0.89         4
         346       0.00      0.00      0.00         4
         347       0.82      1.00      0.90         9
         348       0.67      0.50      0.57         4
         349       1.00      1.00      1.00         9
         350       0.67      1.00      0.80         4
         351       0.00      0.00      0.00         4
         352       0.00      0.00      0.00         4
         353       1.00      0.80      0.89         5
         354       1.00      0.25      0.40         4
         355       0.80      1.00      0.89         4
         356       0.07      0.25      0.11         4
         357       0.67      1.00      0.80         4
         358       0.75      0.60      0.67         5
         359       1.00      1.00      1.00         4
         360       0.54      0.78      0.64         9
         361       0.00      0.00      0.00         4
         362       0.83      1.00      0.91         5
         363       0.50      0.50      0.50         4
         364       1.00      0.89      0.94         9
         365       1.00      1.00      1.00         4
         366       0.88      0.78      0.82         9
         367       1.00      1.00      1.00         4
         368       0.00      0.00      0.00         4
         369       0.33      0.50      0.40         4
         370       0.80      1.00      0.89         4
         371       0.82      1.00      0.90         9
         372       0.89      0.89      0.89         9
         373       0.88      0.78      0.82         9
         374       0.67      1.00      0.80         4
         375       0.50      0.11      0.18         9
         376       0.44      1.00      0.62         4
         377       1.00      0.89      0.94         9
         378       0.80      0.80      0.80         5
         379       0.86      0.67      0.75         9
         380       1.00      0.50      0.67         4
         381       1.00      1.00      1.00         4
         382       0.50      0.33      0.40         9
         383       1.00      0.75      0.86         4
         384       0.60      0.75      0.67         4
         385       0.89      0.89      0.89         9
         386       1.00      1.00      1.00         4
         387       1.00      0.89      0.94         9
         388       0.56      0.56      0.56         9
         389       0.67      1.00      0.80         4
         390       0.75      0.75      0.75         4
         391       0.80      1.00      0.89         8
         392       0.33      0.50      0.40         4
         393       0.80      1.00      0.89         4
         394       0.75      0.75      0.75         4
         395       1.00      1.00      1.00         4
         396       0.38      0.75      0.50         4
         397       0.88      0.78      0.82         9
         398       0.67      1.00      0.80         4
         399       0.82      1.00      0.90         9
         400       0.38      0.60      0.46         5
         401       0.00      0.00      0.00         4
         402       0.57      0.89      0.70         9
         403       0.40      0.67      0.50         9
         404       1.00      1.00      1.00         5
         405       0.12      0.25      0.17         4
         406       0.00      0.00      0.00         4
         407       0.60      0.67      0.63         9
         408       0.80      1.00      0.89         4
         409       1.00      0.78      0.88         9
         410       0.00      0.00      0.00         4
         411       0.67      0.50      0.57         4
         412       0.44      1.00      0.62         4
         413       0.50      0.20      0.29         5
         414       0.90      1.00      0.95         9
         415       0.50      0.40      0.44         5
         416       0.75      0.75      0.75         4
         417       0.00      0.00      0.00         4
         418       1.00      1.00      1.00         9
         419       0.50      1.00      0.67         4
         420       0.40      0.50      0.44         4
         421       1.00      1.00      1.00         5
         422       0.00      0.00      0.00         4
         423       0.11      0.25      0.15         4
         424       0.88      0.78      0.82         9
         425       0.67      1.00      0.80         4
         426       1.00      1.00      1.00         4
         427       0.00      0.00      0.00         4
         428       0.12      0.25      0.17         4
         429       0.80      1.00      0.89         4
         430       0.71      1.00      0.83         5
         431       0.73      0.89      0.80         9
         432       1.00      1.00      1.00         4
         433       1.00      0.80      0.89         5
         434       0.83      1.00      0.91         5
         435       1.00      1.00      1.00         4
         436       1.00      1.00      1.00         5
         437       1.00      1.00      1.00         5
         438       0.00      0.00      0.00         4
         439       1.00      0.80      0.89         5
         440       0.33      0.25      0.29         4
         441       1.00      1.00      1.00         5
         442       0.67      0.50      0.57         4
         443       0.75      0.75      0.75         4
         444       1.00      1.00      1.00         5
         445       0.75      0.75      0.75         4
         446       0.60      0.75      0.67         4
         447       0.67      1.00      0.80         4
         448       0.60      0.75      0.67         4
         449       0.80      1.00      0.89         4
         450       0.67      0.40      0.50         5
         451       0.60      0.75      0.67         4
         452       0.50      0.67      0.57         9
         453       0.00      0.00      0.00         4
         454       0.60      0.75      0.67         4
         455       1.00      0.78      0.88         9
         456       0.00      0.00      0.00         4
         457       1.00      1.00      1.00         9
         458       0.36      1.00      0.53         5
         459       0.50      0.50      0.50         4
         460       0.67      0.50      0.57         4
         461       0.75      0.75      0.75         4
         462       0.75      0.60      0.67         5
         463       0.27      0.60      0.37         5
         464       1.00      0.75      0.86         4
         465       1.00      0.89      0.94         9
         466       0.00      0.00      0.00         9
         467       1.00      1.00      1.00         4
         468       0.40      0.50      0.44         4
         469       1.00      0.50      0.67         4
         470       0.75      0.75      0.75         4
         471       0.50      1.00      0.67         4
         472       0.06      0.25      0.10         4
         473       1.00      0.40      0.57         5
         474       1.00      1.00      1.00         5
         475       1.00      1.00      1.00         4
         476       0.00      0.00      0.00         4
         477       0.75      0.60      0.67         5
         478       0.20      0.20      0.20         5
         479       0.40      0.50      0.44         4
         480       1.00      1.00      1.00         4
         481       0.43      0.33      0.38         9
         482       0.88      0.78      0.82         9
         483       0.75      0.75      0.75         4
         484       1.00      0.75      0.86         4
         485       0.70      0.78      0.74         9
         486       0.33      1.00      0.50         4
         487       0.89      0.89      0.89         9
         488       0.50      1.00      0.67         4
         489       0.75      0.75      0.75         4
         490       1.00      1.00      1.00         4
         491       0.80      1.00      0.89         4
         492       0.56      1.00      0.71         5
         493       0.11      0.50      0.17         4
         494       0.89      0.89      0.89         9
         495       1.00      1.00      1.00         5
         496       0.40      0.50      0.44         4
         497       0.67      0.89      0.76         9
         498       0.06      0.11      0.08         9
         499       1.00      1.00      1.00         4
         500       0.60      0.75      0.67         4
         501       0.38      1.00      0.56         5
         502       0.78      0.78      0.78         9
         503       1.00      1.00      1.00         9
         504       0.50      0.25      0.33         4
         505       0.50      0.75      0.60         4
         506       0.67      0.40      0.50         5
         507       0.62      1.00      0.77         5
         508       0.60      0.60      0.60         5
         509       0.50      1.00      0.67         4
         510       0.50      0.50      0.50         4
         511       0.60      0.60      0.60         5
         512       0.20      0.25      0.22         4
         513       0.89      0.89      0.89         9
         514       1.00      0.75      0.86         4
         515       1.00      0.50      0.67         4
         516       0.64      1.00      0.78         9
         517       0.75      0.75      0.75         4
         518       0.43      0.75      0.55         4
         519       0.83      1.00      0.91         5
         520       0.80      1.00      0.89         4
         521       0.06      0.11      0.07         9
         522       1.00      0.50      0.67         4
         523       0.80      1.00      0.89         4
         524       0.60      0.75      0.67         4
         525       1.00      0.50      0.67         4
         526       1.00      0.80      0.89         5
         527       0.80      1.00      0.89         4
         528       1.00      0.25      0.40         4
         529       0.67      1.00      0.80         4
         530       0.75      0.75      0.75         4
         531       0.00      0.00      0.00         4
         532       0.00      0.00      0.00         4
         533       1.00      1.00      1.00         4
         534       0.56      0.56      0.56         9
         535       0.83      0.71      0.77         7
         536       0.33      0.75      0.46         4
         537       0.00      0.00      0.00         4
         538       0.90      1.00      0.95         9
         539       0.00      0.00      0.00         4
         540       0.57      1.00      0.73         4
         541       0.75      0.75      0.75         4
         542       0.57      1.00      0.73         4
         543       0.00      0.00      0.00         4
         544       0.67      0.22      0.33         9
         545       1.00      1.00      1.00         5
         546       1.00      0.75      0.86         4
         547       0.80      1.00      0.89         4
         548       0.80      1.00      0.89         4
         549       0.75      0.75      0.75         4
         550       0.90      1.00      0.95         9
         551       0.67      0.50      0.57         4
         552       0.50      0.50      0.50         4
         553       1.00      1.00      1.00         5
         554       0.64      1.00      0.78         9
         555       0.50      0.40      0.44         5
         556       1.00      1.00      1.00         4
         557       0.58      0.78      0.67         9
         558       1.00      0.89      0.94         9
         559       0.80      0.80      0.80         5
         560       0.17      0.25      0.20         4
         561       0.67      1.00      0.80         4
         562       0.29      0.50      0.36         4
         563       1.00      1.00      1.00         9
         564       0.67      1.00      0.80         4
         565       0.80      0.89      0.84         9
         566       0.67      1.00      0.80         4
         567       0.00      0.00      0.00         4
         568       1.00      1.00      1.00         4
         569       1.00      0.75      0.86         4
         570       0.58      0.78      0.67         9
         571       0.40      0.50      0.44         4
         572       1.00      1.00      1.00         5
         573       0.00      0.00      0.00         4
         574       1.00      1.00      1.00         9
         575       1.00      0.50      0.67         4
         576       0.75      0.75      0.75         4
         577       1.00      0.75      0.86         4
         578       0.80      1.00      0.89         4
         579       0.17      0.22      0.19         9
         580       0.33      0.25      0.29         4
         581       0.83      0.56      0.67         9
         582       0.80      1.00      0.89         4
         583       0.40      0.50      0.44         4
         584       1.00      0.88      0.93         8
         585       1.00      0.75      0.86         4
         586       0.78      0.78      0.78         9
         587       0.86      0.67      0.75         9
         588       1.00      0.75      0.86         4
         589       1.00      0.75      0.86         4
         590       1.00      0.75      0.86         4
         591       0.67      0.80      0.73         5
         592       0.60      0.75      0.67         4
         593       1.00      1.00      1.00         4
         594       0.67      0.44      0.53         9
         595       0.00      0.00      0.00         4
         596       0.00      0.00      0.00         9
         597       0.00      0.00      0.00         4
         598       0.67      0.50      0.57         4
         599       0.67      0.89      0.76         9
         600       0.05      0.11      0.06         9
         601       0.00      0.00      0.00         4
         602       0.88      0.78      0.82         9
         603       1.00      0.89      0.94         9
         604       0.56      0.56      0.56         9
         605       0.33      0.11      0.17         9
         606       0.08      0.25      0.12         4
         607       0.67      0.50      0.57         4
         608       1.00      1.00      1.00         4
         609       0.60      0.75      0.67         4
         610       0.64      0.78      0.70         9
         611       0.57      1.00      0.73         4
         612       0.10      0.11      0.11         9
         613       0.50      0.33      0.40         9
         614       0.44      1.00      0.62         4
         615       0.75      0.75      0.75         4
         616       1.00      0.40      0.57         5
         617       0.00      0.00      0.00         9
         618       0.67      1.00      0.80         4
         619       0.67      0.50      0.57         4
         620       0.71      0.56      0.63         9
         621       0.43      0.75      0.55         4
         622       0.80      1.00      0.89         4
         623       0.00      0.00      0.00         4
         624       0.83      1.00      0.91         5
         625       0.00      0.00      0.00         4
         626       1.00      0.67      0.80         9
         627       1.00      0.75      0.86         4
         628       0.75      0.75      0.75         4
         629       0.00      0.00      0.00         9
         630       0.00      0.00      0.00         4
         631       0.50      0.50      0.50         4
         632       0.33      0.25      0.29         4
         633       1.00      1.00      1.00         4
         634       1.00      0.25      0.40         4
         635       0.60      0.33      0.43         9
         636       0.80      0.89      0.84         9
         637       0.90      1.00      0.95         9
         638       0.00      0.00      0.00         4
         639       0.25      0.75      0.38         4
         640       0.73      0.89      0.80         9
         641       0.40      0.40      0.40         5
         642       0.67      0.80      0.73         5
         643       1.00      0.25      0.40         4
         644       1.00      0.80      0.89         5
         645       0.75      0.75      0.75         4
         646       0.75      0.75      0.75         4
         647       0.20      0.14      0.17         7
         648       0.44      0.78      0.56         9
         649       0.82      1.00      0.90         9
         650       1.00      0.60      0.75         5
         651       0.00      0.00      0.00         4
         652       1.00      0.80      0.89         5
         653       0.60      0.75      0.67         4
         654       1.00      0.75      0.86         4
         655       0.25      0.25      0.25         4
         656       1.00      1.00      1.00         4
         657       0.00      0.00      0.00         4
         658       0.58      0.78      0.67         9
         659       0.90      1.00      0.95         9
         660       1.00      0.75      0.86         4
         661       0.00      0.00      0.00         4
         662       0.80      1.00      0.89         4
         663       0.00      0.00      0.00         4
         664       0.80      0.80      0.80         5
         665       0.80      1.00      0.89         4
         666       0.40      0.50      0.44         4
         667       1.00      0.50      0.67         4
         668       0.86      0.67      0.75         9
         669       0.00      0.00      0.00         4
         670       0.00      0.00      0.00         4
         671       0.00      0.00      0.00         4
         672       1.00      0.80      0.89         5
         673       1.00      1.00      1.00         4
         674       0.00      0.00      0.00         4
         675       1.00      0.56      0.71         9
         676       0.71      0.56      0.63         9
         677       0.73      0.89      0.80         9
         678       0.50      0.60      0.55         5
         679       1.00      0.75      0.86         4
         680       1.00      0.89      0.94         9
         681       0.00      0.00      0.00         4
         682       1.00      0.50      0.67         4
         683       1.00      0.80      0.89         5
         684       1.00      0.78      0.88         9
         685       1.00      1.00      1.00         4
         686       0.40      0.50      0.44         4
         687       1.00      0.50      0.67         4
         688       0.00      0.00      0.00         4
         689       0.67      1.00      0.80         4
         690       0.75      0.60      0.67         5
         691       0.75      0.75      0.75         4
         692       0.00      0.00      0.00         4
         693       0.00      0.00      0.00         4
         694       0.00      0.00      0.00         5
         695       0.80      0.44      0.57         9
         696       1.00      0.50      0.67         4
         697       0.00      0.00      0.00         4
         698       0.67      1.00      0.80         4
         699       1.00      0.67      0.80         9
         700       1.00      1.00      1.00         4
         701       0.00      0.00      0.00         9
         702       0.67      0.50      0.57         4
         703       1.00      1.00      1.00         5
         704       1.00      0.40      0.57         5
         705       0.88      0.78      0.82         9
         706       1.00      0.50      0.67         4
         707       0.00      0.00      0.00         4
         708       0.56      1.00      0.72         9
         709       0.00      0.00      0.00         4
         710       1.00      0.75      0.86         4
         711       0.73      0.89      0.80         9
         712       0.50      0.25      0.33         4
         713       0.00      0.00      0.00         4
         714       0.75      1.00      0.86         9
         715       1.00      0.75      0.86         4
         716       0.00      0.00      0.00         4
         717       1.00      1.00      1.00         4
         718       1.00      0.25      0.40         4
         719       0.50      0.25      0.33         4
         720       1.00      0.75      0.86         4
         721       0.00      0.00      0.00         4
         722       1.00      1.00      1.00         4
         723       0.00      0.00      0.00         4
         724       1.00      1.00      1.00         4
         725       0.00      0.00      0.00         4
         726       1.00      0.50      0.67         4
         727       1.00      1.00      1.00         5
         728       0.67      0.40      0.50         5
         729       1.00      0.89      0.94         9
         730       0.54      0.78      0.64         9
         731       0.00      0.00      0.00         5
         732       0.00      0.00      0.00         4
         733       0.80      1.00      0.89         4
         734       0.00      0.00      0.00         4
         735       1.00      0.80      0.89         5
         736       0.00      0.00      0.00         4
         737       0.83      0.56      0.67         9
         738       1.00      1.00      1.00         4
         739       0.71      1.00      0.83         5
         740       1.00      1.00      1.00         9
         741       0.80      1.00      0.89         4
         742       1.00      1.00      1.00         9
         743       0.67      1.00      0.80         4
         744       0.75      0.67      0.71         9
         745       0.50      0.11      0.18         9
         746       0.00      0.00      0.00         5
         747       1.00      1.00      1.00         4
         748       0.80      1.00      0.89         4
         749       0.60      0.75      0.67         4
         750       1.00      0.75      0.86         4
         751       1.00      0.20      0.33         5
         752       1.00      0.78      0.88         9
         753       1.00      1.00      1.00         4
         754       0.75      0.60      0.67         5
         755       1.00      0.22      0.36         9
         756       0.00      0.00      0.00         4
         757       0.00      0.00      0.00         4
         758       1.00      0.22      0.36         9
         759       0.25      0.40      0.31         5
         760       1.00      1.00      1.00         4
         761       1.00      1.00      1.00         4
         762       1.00      0.56      0.71         9
         763       0.57      1.00      0.73         4
         764       0.67      1.00      0.80         4
         765       0.42      0.56      0.48         9
         766       0.00      0.00      0.00         4
         767       0.67      1.00      0.80         4
         768       1.00      0.25      0.40         4
         769       1.00      0.50      0.67         4
         770       0.00      0.00      0.00         9
         771       1.00      0.11      0.20         9
         772       0.00      0.00      0.00         4
         773       0.00      0.00      0.00         4
         774       0.67      1.00      0.80         4
         775       0.00      0.00      0.00         4
         776       1.00      1.00      1.00         9
         777       0.00      0.00      0.00         4
         778       1.00      1.00      1.00         4
         779       0.80      0.80      0.80         5
         780       0.75      0.75      0.75         4
         781       0.00      0.00      0.00         4
         782       1.00      1.00      1.00         9
         783       1.00      1.00      1.00         4
         784       1.00      0.50      0.67         4
         785       0.67      1.00      0.80         4
         786       0.80      1.00      0.89         4
         787       0.75      0.60      0.67         5
         788       1.00      0.56      0.71         9
         789       0.71      0.56      0.63         9
         790       0.67      0.89      0.76         9
         791       0.83      1.00      0.91         5
         792       1.00      0.25      0.40         4
         793       0.00      0.00      0.00         4
         794       1.00      1.00      1.00         5
         795       1.00      1.00      1.00         9
         796       1.00      0.89      0.94         9
         797       1.00      0.22      0.36         9
         798       1.00      1.00      1.00         4
         799       1.00      1.00      1.00         4
         800       1.00      1.00      1.00         5
         801       0.80      1.00      0.89         4
         802       1.00      0.67      0.80         9
         803       1.00      0.75      0.86         4
         804       1.00      0.50      0.67         4
         805       1.00      0.20      0.33         5
         806       1.00      0.78      0.88         9
         807       0.80      1.00      0.89         4
         808       1.00      1.00      1.00         4
         809       1.00      0.78      0.88         9
         810       0.00      0.00      0.00         9
         811       0.78      0.78      0.78         9
         812       1.00      1.00      1.00         4
         813       0.00      0.00      0.00         9
         814       1.00      0.50      0.67         4
         815       0.86      0.67      0.75         9
         816       1.00      1.00      1.00         4
         817       1.00      0.75      0.86         4
         818       1.00      0.89      0.94         9
         819       0.50      0.25      0.33         4
         820       0.00      0.00      0.00         5
         821       0.88      0.78      0.82         9
         822       0.82      1.00      0.90         9
         823       1.00      0.80      0.89         5
         824       0.00      0.00      0.00         5
         825       0.50      0.25      0.33         4
         826       0.67      0.50      0.57         4
         827       0.50      0.25      0.33         4
         828       1.00      0.75      0.86         4
         829       0.00      0.00      0.00         4
         830       0.75      0.60      0.67         5
         831       0.80      1.00      0.89         4
         832       0.00      0.00      0.00         5
         833       0.33      0.25      0.29         4
         834       0.00      0.00      0.00         4
         835       1.00      0.20      0.33         5
         836       0.00      0.00      0.00         4
         837       0.00      0.00      0.00         4
         838       0.00      0.00      0.00         4
         839       0.75      0.33      0.46         9
         840       1.00      0.25      0.40         4
         841       0.75      0.33      0.46         9
         842       1.00      0.67      0.80         9
         843       0.00      0.00      0.00         9
         844       1.00      0.75      0.86         4
         845       0.00      0.00      0.00         4
         846       1.00      0.20      0.33         5
         847       1.00      0.50      0.67         4
         848       1.00      0.20      0.33         5
         849       0.60      0.33      0.43         9
         850       0.00      0.00      0.00         4
         851       0.00      0.00      0.00         4
         852       0.00      0.00      0.00         4
         853       0.00      0.00      0.00         4
         854       0.75      0.67      0.71         9
         855       0.33      0.40      0.36         5
         856       1.00      1.00      1.00         5
         857       0.50      0.20      0.29         5
         858       0.50      0.25      0.33         4
         859       0.80      0.44      0.57         9
         860       1.00      0.80      0.89         5
         861       0.00      0.00      0.00         4
         862       1.00      0.50      0.67         4
         863       1.00      0.25      0.40         4
         864       1.00      0.75      0.86         4
         865       1.00      1.00      1.00         4
         866       0.60      0.75      0.67         4
         867       0.50      0.50      0.50         4
         868       1.00      0.89      0.94         9
         869       1.00      0.67      0.80         9
         870       1.00      0.75      0.86         4
         871       0.50      0.20      0.29         5
         872       0.00      0.00      0.00         4
         873       1.00      0.50      0.67         4
         874       0.00      0.00      0.00         4
         875       0.00      0.00      0.00         4
         876       0.69      1.00      0.82         9
         877       0.00      0.00      0.00         4
         878       0.40      0.50      0.44         4
         879       0.62      0.89      0.73         9
         880       0.50      1.00      0.67         5
         881       0.00      0.00      0.00         4
         882       1.00      0.75      0.86         4
         883       1.00      0.75      0.86         4
         884       0.12      0.60      0.20         5
         885       0.53      1.00      0.69         9
         886       0.71      1.00      0.83         5
         887       0.67      0.40      0.50         5
         888       0.50      0.56      0.53         9
         889       0.50      0.40      0.44         5
         890       0.50      1.00      0.67         5
         891       0.00      0.00      0.00         4
         892       1.00      1.00      1.00         4
         893       0.89      1.00      0.94         8

    accuracy                           0.65      4917
   macro avg       0.64      0.63      0.61      4917
weighted avg       0.66      0.65      0.63      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
