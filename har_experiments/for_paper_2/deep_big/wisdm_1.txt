CLASS_ORDER: [249, 499, 529, 560, 878, 525, 578, 351, 555, 243, 592, 537, 439, 632, 118, 332, 428, 337, 11, 553, 35, 793, 534, 290, 639, 203, 595, 691, 687, 831, 446, 354, 305, 330, 49, 823, 824, 811, 324, 542, 678, 15, 242, 705, 774, 862, 140, 654, 844, 879, 638, 228, 652, 281, 672, 220, 768, 505, 409, 480, 887, 590, 458, 727, 710, 574, 544, 144, 23, 739, 857, 80, 40, 121, 311, 481, 751, 629, 548, 25, 850, 215, 741, 692, 386, 738, 453, 63, 662, 487, 699, 109, 257, 828, 466, 31, 430, 422, 837, 771, 117, 754, 787, 640, 170, 690, 661, 803, 291, 1, 874, 434, 317, 120, 812, 182, 637, 788, 51, 411, 609, 441, 103, 490, 883, 394, 365, 540, 195, 443, 235, 686, 792, 138, 763, 621, 847, 339, 716, 750, 760, 404, 819, 192, 605, 834, 549, 580, 238, 868, 379, 626, 573, 85, 566, 832, 767, 740, 18, 137, 733, 211, 769, 468, 259, 761, 387, 454, 234, 876, 802, 341, 359, 194, 55, 190, 500, 510, 519, 643, 456, 617, 322, 646, 304, 0, 571, 675, 382, 522, 799, 315, 298, 250, 849, 488, 506, 435, 303, 848, 28, 408, 711, 722, 173, 563, 358, 688, 405, 462, 677, 482, 300, 703, 602, 533, 301, 712, 360, 758, 223, 132, 355, 162, 429, 269, 56, 671, 27, 159, 526, 308, 20, 74, 448, 567, 594, 204, 283, 101, 297, 455, 425, 312, 649, 821, 861, 889, 286, 631, 495, 427, 502, 285, 668, 804, 381, 701, 752, 489, 82, 483, 272, 279, 814, 552, 232, 520, 4, 313, 624, 370, 496, 723, 402, 607, 340, 866, 9, 197, 361, 372, 369, 836, 158, 88, 352, 839, 778, 663, 829, 296, 59, 888, 745, 418, 99, 808, 613, 870, 107, 419, 820, 199, 486, 306, 586, 452, 676, 345, 719, 442, 622, 766, 673, 75, 151, 735, 485, 513, 21, 614, 222, 790, 601, 189, 288, 432, 30, 98, 512, 50, 390, 299, 467, 797, 145, 217, 287, 436, 396, 809, 57, 780, 698, 516, 538, 106, 368, 83, 356, 41, 153, 321, 134, 244, 625, 597, 603, 125, 753, 114, 765, 732, 155, 575, 474, 68, 535, 581, 852, 414, 447, 100, 196, 278, 67, 589, 292, 689, 184, 373, 460, 328, 782, 413, 801, 813, 148, 3, 755, 150, 696, 336, 73, 241, 334, 116, 176, 660, 438, 252, 433, 268, 657, 648, 64, 309, 5, 314, 395, 757, 772, 131, 362, 859, 645, 599, 666, 856, 90, 826, 42, 843, 149, 872, 606, 777, 664, 263, 582, 655, 397, 172, 407, 656, 19, 247, 475, 806, 871, 697, 700, 213, 770, 48, 275, 146, 171, 865, 127, 6, 547, 476, 713, 93, 129, 152, 421, 276, 180, 807, 721, 126, 473, 307, 245, 642, 479, 112, 325, 737, 344, 97, 618, 508, 33, 378, 385, 877, 659, 521, 295, 776, 45, 417, 720, 160, 229, 320, 348, 110, 484, 175, 319, 584, 683, 511, 724, 377, 718, 230, 556, 96, 450, 810, 681, 633, 130, 667, 610, 464, 789, 636, 265, 251, 187, 840, 647, 119, 695, 498, 391, 76, 891, 188, 709, 437, 161, 564, 423, 685, 561, 102, 682, 679, 323, 875, 156, 260, 246, 593, 284, 531, 69, 255, 846, 142, 845, 43, 557, 294, 873, 225, 518, 670, 644, 587, 128, 604, 619, 543, 166, 756, 842, 34, 24, 262, 729, 528, 233, 86, 280, 492, 816, 104, 181, 406, 384, 239, 759, 224, 570, 207, 154, 431, 318, 380, 256, 412, 669, 415, 445, 202, 725, 198, 343, 218, 420, 157, 890, 289, 273, 178, 37, 517, 58, 815, 562, 551, 572, 830, 335, 583, 227, 46, 212, 376, 693, 282, 514, 882, 749, 623, 493, 347, 326, 715, 706, 169, 17, 52, 858, 366, 70, 558, 44, 568, 612, 477, 165, 501, 680, 253, 478, 717, 14, 136, 89, 208, 236, 833, 611, 108, 497, 608, 261, 122, 141, 704, 708, 841, 817, 105, 78, 791, 22, 892, 747, 825, 7, 880, 503, 504, 674, 403, 237, 585, 746, 509, 13, 164, 357, 94, 743, 440, 569, 527, 775, 209, 494, 191, 133, 350, 827, 168, 216, 333, 728, 851, 410, 219, 2, 884, 363, 818, 860, 444, 426, 885, 258, 375, 12, 541, 773, 389, 277, 559, 371, 864, 545, 353, 786, 79, 658, 539, 36, 449, 615, 163, 424, 600, 392, 349, 95, 635, 742, 200, 65, 762, 736, 62, 794, 867, 32, 507, 271, 491, 240, 796, 393, 748, 193, 338, 316, 26, 254, 174, 123, 388, 54, 302, 469, 367, 472, 374, 530, 186, 167, 532, 113, 653, 29, 400, 8, 61, 694, 206, 579, 805, 554, 627, 205, 346, 226, 863, 783, 536, 270, 591, 744, 785, 800, 71, 550, 183, 714, 730, 39, 72, 620, 115, 264, 457, 471, 838, 854, 598, 342, 523, 707, 81, 734, 398, 596, 641, 47, 92, 179, 470, 702, 588, 893, 781, 177, 869, 10, 38, 231, 784, 248, 764, 16, 124, 147, 795, 416, 616, 798, 853, 60, 630, 577, 401, 565, 310, 267, 461, 465, 451, 91, 399, 185, 684, 546, 822, 634, 66, 628, 665, 135, 327, 201, 274, 463, 651, 383, 881, 650, 53, 77, 364, 524, 293, 210, 139, 84, 855, 835, 329, 459, 111, 331, 886, 515, 214, 726, 731, 266, 576, 87, 221, 779, 143]
class_group: [(249, 499, 529, 560, 878, 525, 578, 351, 555, 243, 592, 537, 439, 632, 118, 332, 428, 337, 11, 553, 35, 793, 534, 290, 639, 203, 595, 691, 687, 831, 446, 354, 305, 330), (49, 823, 824, 811, 324, 542, 678, 15, 242, 705, 774, 862, 140, 654, 844, 879, 638, 228, 652, 281), (672, 220, 768, 505, 409, 480, 887, 590, 458, 727, 710, 574, 544, 144, 23, 739, 857, 80, 40, 121), (311, 481, 751, 629, 548, 25, 850, 215, 741, 692, 386, 738, 453, 63, 662, 487, 699, 109, 257, 828), (466, 31, 430, 422, 837, 771, 117, 754, 787, 640, 170, 690, 661, 803, 291, 1, 874, 434, 317, 120), (812, 182, 637, 788, 51, 411, 609, 441, 103, 490, 883, 394, 365, 540, 195, 443, 235, 686, 792, 138), (763, 621, 847, 339, 716, 750, 760, 404, 819, 192, 605, 834, 549, 580, 238, 868, 379, 626, 573, 85), (566, 832, 767, 740, 18, 137, 733, 211, 769, 468, 259, 761, 387, 454, 234, 876, 802, 341, 359, 194), (55, 190, 500, 510, 519, 643, 456, 617, 322, 646, 304, 0, 571, 675, 382, 522, 799, 315, 298, 250), (849, 488, 506, 435, 303, 848, 28, 408, 711, 722, 173, 563, 358, 688, 405, 462, 677, 482, 300, 703), (602, 533, 301, 712, 360, 758, 223, 132, 355, 162, 429, 269, 56, 671, 27, 159, 526, 308, 20, 74), (448, 567, 594, 204, 283, 101, 297, 455, 425, 312, 649, 821, 861, 889, 286, 631, 495, 427, 502, 285), (668, 804, 381, 701, 752, 489, 82, 483, 272, 279, 814, 552, 232, 520, 4, 313, 624, 370, 496, 723), (402, 607, 340, 866, 9, 197, 361, 372, 369, 836, 158, 88, 352, 839, 778, 663, 829, 296, 59, 888), (745, 418, 99, 808, 613, 870, 107, 419, 820, 199, 486, 306, 586, 452, 676, 345, 719, 442, 622, 766), (673, 75, 151, 735, 485, 513, 21, 614, 222, 790, 601, 189, 288, 432, 30, 98, 512, 50, 390, 299), (467, 797, 145, 217, 287, 436, 396, 809, 57, 780, 698, 516, 538, 106, 368, 83, 356, 41, 153, 321), (134, 244, 625, 597, 603, 125, 753, 114, 765, 732, 155, 575, 474, 68, 535, 581, 852, 414, 447, 100), (196, 278, 67, 589, 292, 689, 184, 373, 460, 328, 782, 413, 801, 813, 148, 3, 755, 150, 696, 336), (73, 241, 334, 116, 176, 660, 438, 252, 433, 268, 657, 648, 64, 309, 5, 314, 395, 757, 772, 131), (362, 859, 645, 599, 666, 856, 90, 826, 42, 843, 149, 872, 606, 777, 664, 263, 582, 655, 397, 172), (407, 656, 19, 247, 475, 806, 871, 697, 700, 213, 770, 48, 275, 146, 171, 865, 127, 6, 547, 476), (713, 93, 129, 152, 421, 276, 180, 807, 721, 126, 473, 307, 245, 642, 479, 112, 325, 737, 344, 97), (618, 508, 33, 378, 385, 877, 659, 521, 295, 776, 45, 417, 720, 160, 229, 320, 348, 110, 484, 175), (319, 584, 683, 511, 724, 377, 718, 230, 556, 96, 450, 810, 681, 633, 130, 667, 610, 464, 789, 636), (265, 251, 187, 840, 647, 119, 695, 498, 391, 76, 891, 188, 709, 437, 161, 564, 423, 685, 561, 102), (682, 679, 323, 875, 156, 260, 246, 593, 284, 531, 69, 255, 846, 142, 845, 43, 557, 294, 873, 225), (518, 670, 644, 587, 128, 604, 619, 543, 166, 756, 842, 34, 24, 262, 729, 528, 233, 86, 280, 492), (816, 104, 181, 406, 384, 239, 759, 224, 570, 207, 154, 431, 318, 380, 256, 412, 669, 415, 445, 202), (725, 198, 343, 218, 420, 157, 890, 289, 273, 178, 37, 517, 58, 815, 562, 551, 572, 830, 335, 583), (227, 46, 212, 376, 693, 282, 514, 882, 749, 623, 493, 347, 326, 715, 706, 169, 17, 52, 858, 366), (70, 558, 44, 568, 612, 477, 165, 501, 680, 253, 478, 717, 14, 136, 89, 208, 236, 833, 611, 108), (497, 608, 261, 122, 141, 704, 708, 841, 817, 105, 78, 791, 22, 892, 747, 825, 7, 880, 503, 504), (674, 403, 237, 585, 746, 509, 13, 164, 357, 94, 743, 440, 569, 527, 775, 209, 494, 191, 133, 350), (827, 168, 216, 333, 728, 851, 410, 219, 2, 884, 363, 818, 860, 444, 426, 885, 258, 375, 12, 541), (773, 389, 277, 559, 371, 864, 545, 353, 786, 79, 658, 539, 36, 449, 615, 163, 424, 600, 392, 349), (95, 635, 742, 200, 65, 762, 736, 62, 794, 867, 32, 507, 271, 491, 240, 796, 393, 748, 193, 338), (316, 26, 254, 174, 123, 388, 54, 302, 469, 367, 472, 374, 530, 186, 167, 532, 113, 653, 29, 400), (8, 61, 694, 206, 579, 805, 554, 627, 205, 346, 226, 863, 783, 536, 270, 591, 744, 785, 800, 71), (550, 183, 714, 730, 39, 72, 620, 115, 264, 457, 471, 838, 854, 598, 342, 523, 707, 81, 734, 398), (596, 641, 47, 92, 179, 470, 702, 588, 893, 781, 177, 869, 10, 38, 231, 784, 248, 764, 16, 124), (147, 795, 416, 616, 798, 853, 60, 630, 577, 401, 565, 310, 267, 461, 465, 451, 91, 399, 185, 684), (546, 822, 634, 66, 628, 665, 135, 327, 201, 274, 463, 651, 383, 881, 650, 53, 77, 364, 524, 293), (210, 139, 84, 855, 835, 329, 459, 111, 331, 886, 515, 214, 726, 731, 266, 576, 87, 221, 779, 143)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.5559	step1_train_accuracy: 66.5546
STEP-1	Epoch: 20/50	loss: 0.7595	step1_train_accuracy: 88.0672
STEP-1	Epoch: 30/50	loss: 0.4440	step1_train_accuracy: 93.9496
STEP-1	Epoch: 40/50	loss: 0.2991	step1_train_accuracy: 96.4706
STEP-1	Epoch: 50/50	loss: 0.2225	step1_train_accuracy: 97.1429
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.0826	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.0715	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.0673	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0653	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0641	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0634	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0629	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0625	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0622	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0619	gate_loss: 0.0000	step2_classification_accuracy: 95.5882	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 90.6040	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.4954	step1_train_accuracy: 62.5683
STEP-1	Epoch: 20/50	loss: 0.6813	step1_train_accuracy: 89.0710
STEP-1	Epoch: 30/50	loss: 0.3685	step1_train_accuracy: 95.6284
STEP-1	Epoch: 40/50	loss: 0.2277	step1_train_accuracy: 98.3607
STEP-1	Epoch: 50/50	loss: 0.1616	step1_train_accuracy: 99.1803
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1721	gate_loss: 0.4276	step2_classification_accuracy: 94.6296	step_2_gate_accuracy: 87.5926
STEP-2	Epoch: 40/200	classification_loss: 0.0934	gate_loss: 0.2080	step2_classification_accuracy: 96.6667	step_2_gate_accuracy: 98.3333
STEP-2	Epoch: 60/200	classification_loss: 0.0767	gate_loss: 0.1113	step2_classification_accuracy: 97.4074	step_2_gate_accuracy: 99.2593
STEP-2	Epoch: 80/200	classification_loss: 0.0640	gate_loss: 0.0587	step2_classification_accuracy: 97.5926	step_2_gate_accuracy: 99.6296
STEP-2	Epoch: 100/200	classification_loss: 0.0701	gate_loss: 0.0452	step2_classification_accuracy: 97.5926	step_2_gate_accuracy: 99.6296
STEP-2	Epoch: 120/200	classification_loss: 0.0582	gate_loss: 0.0297	step2_classification_accuracy: 97.7778	step_2_gate_accuracy: 99.4444
STEP-2	Epoch: 140/200	classification_loss: 0.0563	gate_loss: 0.0264	step2_classification_accuracy: 97.9630	step_2_gate_accuracy: 99.8148
STEP-2	Epoch: 160/200	classification_loss: 0.0583	gate_loss: 0.0219	step2_classification_accuracy: 97.7778	step_2_gate_accuracy: 99.8148
STEP-2	Epoch: 180/200	classification_loss: 0.0526	gate_loss: 0.0151	step2_classification_accuracy: 97.9630	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0516	gate_loss: 0.0140	step2_classification_accuracy: 97.9630	step_2_gate_accuracy: 99.8148
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 87.2483	gate_accuracy: 95.9732
	Task-1	val_accuracy: 92.3913	gate_accuracy: 96.7391
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 96.2656


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 1.7352	step1_train_accuracy: 53.4819
STEP-1	Epoch: 20/50	loss: 0.7851	step1_train_accuracy: 86.9081
STEP-1	Epoch: 30/50	loss: 0.4414	step1_train_accuracy: 95.5432
STEP-1	Epoch: 40/50	loss: 0.2800	step1_train_accuracy: 97.4930
STEP-1	Epoch: 50/50	loss: 0.1964	step1_train_accuracy: 98.6072
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2150	gate_loss: 0.6539	step2_classification_accuracy: 92.4324	step_2_gate_accuracy: 79.5946
STEP-2	Epoch: 40/200	classification_loss: 0.1241	gate_loss: 0.3052	step2_classification_accuracy: 95.2703	step_2_gate_accuracy: 94.3243
STEP-2	Epoch: 60/200	classification_loss: 0.1667	gate_loss: 0.1862	step2_classification_accuracy: 94.3243	step_2_gate_accuracy: 95.5405
STEP-2	Epoch: 80/200	classification_loss: 0.1165	gate_loss: 0.1057	step2_classification_accuracy: 96.0811	step_2_gate_accuracy: 98.2432
STEP-2	Epoch: 100/200	classification_loss: 0.0887	gate_loss: 0.0693	step2_classification_accuracy: 96.2162	step_2_gate_accuracy: 98.7838
STEP-2	Epoch: 120/200	classification_loss: 0.0736	gate_loss: 0.0529	step2_classification_accuracy: 96.3514	step_2_gate_accuracy: 98.7838
STEP-2	Epoch: 140/200	classification_loss: 0.0639	gate_loss: 0.0369	step2_classification_accuracy: 96.8919	step_2_gate_accuracy: 99.4595
STEP-2	Epoch: 160/200	classification_loss: 0.0719	gate_loss: 0.0371	step2_classification_accuracy: 96.4865	step_2_gate_accuracy: 99.1892
STEP-2	Epoch: 180/200	classification_loss: 0.0680	gate_loss: 0.0312	step2_classification_accuracy: 96.6216	step_2_gate_accuracy: 99.1892
STEP-2	Epoch: 200/200	classification_loss: 0.0691	gate_loss: 0.0291	step2_classification_accuracy: 96.4865	step_2_gate_accuracy: 99.1892
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.5638	gate_accuracy: 93.2886
	Task-1	val_accuracy: 83.6957	gate_accuracy: 90.2174
	Task-2	val_accuracy: 92.2222	gate_accuracy: 92.2222
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 92.1450


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 2.2045	step1_train_accuracy: 47.5728
STEP-1	Epoch: 20/50	loss: 0.9157	step1_train_accuracy: 94.1748
STEP-1	Epoch: 30/50	loss: 0.4543	step1_train_accuracy: 98.0583
STEP-1	Epoch: 40/50	loss: 0.2673	step1_train_accuracy: 99.0291
STEP-1	Epoch: 50/50	loss: 0.1733	step1_train_accuracy: 99.6764
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1779	gate_loss: 0.8375	step2_classification_accuracy: 93.2979	step_2_gate_accuracy: 71.8085
STEP-2	Epoch: 40/200	classification_loss: 0.1257	gate_loss: 0.3901	step2_classification_accuracy: 95.8511	step_2_gate_accuracy: 91.5957
STEP-2	Epoch: 60/200	classification_loss: 0.1020	gate_loss: 0.1963	step2_classification_accuracy: 96.4894	step_2_gate_accuracy: 96.2766
STEP-2	Epoch: 80/200	classification_loss: 0.0885	gate_loss: 0.1189	step2_classification_accuracy: 96.7021	step_2_gate_accuracy: 97.7660
STEP-2	Epoch: 100/200	classification_loss: 0.0809	gate_loss: 0.0855	step2_classification_accuracy: 97.1277	step_2_gate_accuracy: 98.2979
STEP-2	Epoch: 120/200	classification_loss: 0.0743	gate_loss: 0.0630	step2_classification_accuracy: 97.2340	step_2_gate_accuracy: 99.4681
STEP-2	Epoch: 140/200	classification_loss: 0.0658	gate_loss: 0.0491	step2_classification_accuracy: 97.4468	step_2_gate_accuracy: 99.0425
STEP-2	Epoch: 160/200	classification_loss: 0.0760	gate_loss: 0.0463	step2_classification_accuracy: 97.1277	step_2_gate_accuracy: 99.1489
STEP-2	Epoch: 180/200	classification_loss: 0.0636	gate_loss: 0.0369	step2_classification_accuracy: 97.6596	step_2_gate_accuracy: 99.3617
STEP-2	Epoch: 200/200	classification_loss: 0.0633	gate_loss: 0.0335	step2_classification_accuracy: 97.5532	step_2_gate_accuracy: 99.1489
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.1812	gate_accuracy: 85.9060
	Task-1	val_accuracy: 85.8696	gate_accuracy: 86.9565
	Task-2	val_accuracy: 92.2222	gate_accuracy: 88.8889
	Task-3	val_accuracy: 81.8182	gate_accuracy: 84.4156
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 86.5196


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.1356	step1_train_accuracy: 58.3851
STEP-1	Epoch: 20/50	loss: 0.9644	step1_train_accuracy: 81.0559
STEP-1	Epoch: 30/50	loss: 0.5181	step1_train_accuracy: 91.9255
STEP-1	Epoch: 40/50	loss: 0.3179	step1_train_accuracy: 95.6522
STEP-1	Epoch: 50/50	loss: 0.2341	step1_train_accuracy: 96.2733
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2623	gate_loss: 1.0487	step2_classification_accuracy: 90.7018	step_2_gate_accuracy: 65.0877
STEP-2	Epoch: 40/200	classification_loss: 0.1742	gate_loss: 0.4952	step2_classification_accuracy: 93.8596	step_2_gate_accuracy: 89.8246
STEP-2	Epoch: 60/200	classification_loss: 0.1178	gate_loss: 0.2370	step2_classification_accuracy: 95.7018	step_2_gate_accuracy: 96.3158
STEP-2	Epoch: 80/200	classification_loss: 0.1075	gate_loss: 0.1430	step2_classification_accuracy: 95.7895	step_2_gate_accuracy: 97.8947
STEP-2	Epoch: 100/200	classification_loss: 0.1019	gate_loss: 0.0976	step2_classification_accuracy: 95.7895	step_2_gate_accuracy: 98.4211
STEP-2	Epoch: 120/200	classification_loss: 0.0968	gate_loss: 0.0800	step2_classification_accuracy: 95.7895	step_2_gate_accuracy: 98.4211
STEP-2	Epoch: 140/200	classification_loss: 0.0876	gate_loss: 0.0603	step2_classification_accuracy: 96.4912	step_2_gate_accuracy: 99.2982
STEP-2	Epoch: 160/200	classification_loss: 0.0771	gate_loss: 0.0509	step2_classification_accuracy: 96.4912	step_2_gate_accuracy: 99.0351
STEP-2	Epoch: 180/200	classification_loss: 0.0809	gate_loss: 0.0427	step2_classification_accuracy: 96.7544	step_2_gate_accuracy: 99.4737
STEP-2	Epoch: 200/200	classification_loss: 0.0825	gate_loss: 0.0401	step2_classification_accuracy: 96.4035	step_2_gate_accuracy: 99.2982
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.5369	gate_accuracy: 87.2483
	Task-1	val_accuracy: 83.6957	gate_accuracy: 89.1304
	Task-2	val_accuracy: 90.0000	gate_accuracy: 88.8889
	Task-3	val_accuracy: 80.5195	gate_accuracy: 80.5195
	Task-4	val_accuracy: 87.6543	gate_accuracy: 91.3580
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 87.5256


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 1.5520	step1_train_accuracy: 67.9157
STEP-1	Epoch: 20/50	loss: 0.6669	step1_train_accuracy: 85.4801
STEP-1	Epoch: 30/50	loss: 0.4101	step1_train_accuracy: 91.5691
STEP-1	Epoch: 40/50	loss: 0.2832	step1_train_accuracy: 94.1452
STEP-1	Epoch: 50/50	loss: 0.2156	step1_train_accuracy: 96.0187
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3234	gate_loss: 1.1791	step2_classification_accuracy: 89.1791	step_2_gate_accuracy: 65.8209
STEP-2	Epoch: 40/200	classification_loss: 0.1866	gate_loss: 0.4931	step2_classification_accuracy: 93.2836	step_2_gate_accuracy: 89.5522
STEP-2	Epoch: 60/200	classification_loss: 0.1488	gate_loss: 0.2367	step2_classification_accuracy: 94.8507	step_2_gate_accuracy: 94.7761
STEP-2	Epoch: 80/200	classification_loss: 0.1229	gate_loss: 0.1447	step2_classification_accuracy: 95.2239	step_2_gate_accuracy: 97.1642
STEP-2	Epoch: 100/200	classification_loss: 0.1085	gate_loss: 0.1019	step2_classification_accuracy: 95.8955	step_2_gate_accuracy: 97.2388
STEP-2	Epoch: 120/200	classification_loss: 0.0975	gate_loss: 0.0757	step2_classification_accuracy: 95.9701	step_2_gate_accuracy: 98.2090
STEP-2	Epoch: 140/200	classification_loss: 0.0997	gate_loss: 0.0655	step2_classification_accuracy: 95.9701	step_2_gate_accuracy: 98.4328
STEP-2	Epoch: 160/200	classification_loss: 0.0888	gate_loss: 0.0527	step2_classification_accuracy: 96.1940	step_2_gate_accuracy: 98.5075
STEP-2	Epoch: 180/200	classification_loss: 0.0877	gate_loss: 0.0493	step2_classification_accuracy: 95.9701	step_2_gate_accuracy: 98.7313
STEP-2	Epoch: 200/200	classification_loss: 0.0862	gate_loss: 0.0446	step2_classification_accuracy: 96.0448	step_2_gate_accuracy: 98.8060
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.1275	gate_accuracy: 78.5235
	Task-1	val_accuracy: 82.6087	gate_accuracy: 88.0435
	Task-2	val_accuracy: 88.8889	gate_accuracy: 84.4444
	Task-3	val_accuracy: 80.5195	gate_accuracy: 81.8182
	Task-4	val_accuracy: 77.7778	gate_accuracy: 81.4815
	Task-5	val_accuracy: 85.0467	gate_accuracy: 86.9159
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 83.2215


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 1.8822	step1_train_accuracy: 64.6067
STEP-1	Epoch: 20/50	loss: 0.8385	step1_train_accuracy: 86.5169
STEP-1	Epoch: 30/50	loss: 0.4800	step1_train_accuracy: 92.6966
STEP-1	Epoch: 40/50	loss: 0.3211	step1_train_accuracy: 93.8202
STEP-1	Epoch: 50/50	loss: 0.2305	step1_train_accuracy: 97.4719
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3686	gate_loss: 1.2841	step2_classification_accuracy: 87.4026	step_2_gate_accuracy: 62.9221
STEP-2	Epoch: 40/200	classification_loss: 0.2515	gate_loss: 0.5622	step2_classification_accuracy: 90.7143	step_2_gate_accuracy: 86.4935
STEP-2	Epoch: 60/200	classification_loss: 0.1719	gate_loss: 0.2735	step2_classification_accuracy: 93.4416	step_2_gate_accuracy: 94.9351
STEP-2	Epoch: 80/200	classification_loss: 0.1621	gate_loss: 0.1756	step2_classification_accuracy: 93.9610	step_2_gate_accuracy: 96.2987
STEP-2	Epoch: 100/200	classification_loss: 0.1383	gate_loss: 0.1226	step2_classification_accuracy: 94.7403	step_2_gate_accuracy: 97.3377
STEP-2	Epoch: 120/200	classification_loss: 0.1287	gate_loss: 0.0943	step2_classification_accuracy: 94.9351	step_2_gate_accuracy: 97.7922
STEP-2	Epoch: 140/200	classification_loss: 0.1191	gate_loss: 0.0742	step2_classification_accuracy: 95.0000	step_2_gate_accuracy: 98.3117
STEP-2	Epoch: 160/200	classification_loss: 0.1154	gate_loss: 0.0617	step2_classification_accuracy: 94.9351	step_2_gate_accuracy: 98.8312
STEP-2	Epoch: 180/200	classification_loss: 0.1194	gate_loss: 0.0590	step2_classification_accuracy: 95.2597	step_2_gate_accuracy: 98.6364
STEP-2	Epoch: 200/200	classification_loss: 0.1074	gate_loss: 0.0498	step2_classification_accuracy: 95.1299	step_2_gate_accuracy: 98.7013
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.1678	gate_accuracy: 79.1946
	Task-1	val_accuracy: 75.0000	gate_accuracy: 79.3478
	Task-2	val_accuracy: 86.6667	gate_accuracy: 85.5556
	Task-3	val_accuracy: 81.8182	gate_accuracy: 81.8182
	Task-4	val_accuracy: 76.5432	gate_accuracy: 82.7160
	Task-5	val_accuracy: 79.4393	gate_accuracy: 80.3738
	Task-6	val_accuracy: 84.2697	gate_accuracy: 85.3933
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.7518


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 2.7325	step1_train_accuracy: 55.2239
STEP-1	Epoch: 20/50	loss: 1.2536	step1_train_accuracy: 82.4627
STEP-1	Epoch: 30/50	loss: 0.6595	step1_train_accuracy: 91.4179
STEP-1	Epoch: 40/50	loss: 0.4748	step1_train_accuracy: 92.5373
STEP-1	Epoch: 50/50	loss: 0.3464	step1_train_accuracy: 94.4030
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4084	gate_loss: 1.3492	step2_classification_accuracy: 87.1264	step_2_gate_accuracy: 59.8851
STEP-2	Epoch: 40/200	classification_loss: 0.2611	gate_loss: 0.5581	step2_classification_accuracy: 91.1494	step_2_gate_accuracy: 85.8621
STEP-2	Epoch: 60/200	classification_loss: 0.1977	gate_loss: 0.2705	step2_classification_accuracy: 93.2759	step_2_gate_accuracy: 94.3103
STEP-2	Epoch: 80/200	classification_loss: 0.1788	gate_loss: 0.1741	step2_classification_accuracy: 93.8506	step_2_gate_accuracy: 96.6092
STEP-2	Epoch: 100/200	classification_loss: 0.1452	gate_loss: 0.1163	step2_classification_accuracy: 95.2874	step_2_gate_accuracy: 97.8736
STEP-2	Epoch: 120/200	classification_loss: 0.1396	gate_loss: 0.0881	step2_classification_accuracy: 95.0000	step_2_gate_accuracy: 98.5057
STEP-2	Epoch: 140/200	classification_loss: 0.1261	gate_loss: 0.0711	step2_classification_accuracy: 95.2874	step_2_gate_accuracy: 98.7931
STEP-2	Epoch: 160/200	classification_loss: 0.1264	gate_loss: 0.0601	step2_classification_accuracy: 95.5172	step_2_gate_accuracy: 99.0230
STEP-2	Epoch: 180/200	classification_loss: 0.1230	gate_loss: 0.0532	step2_classification_accuracy: 95.3448	step_2_gate_accuracy: 98.6207
STEP-2	Epoch: 200/200	classification_loss: 0.1193	gate_loss: 0.0489	step2_classification_accuracy: 95.2874	step_2_gate_accuracy: 98.8506
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.8389	gate_accuracy: 83.2215
	Task-1	val_accuracy: 76.0870	gate_accuracy: 82.6087
	Task-2	val_accuracy: 87.7778	gate_accuracy: 86.6667
	Task-3	val_accuracy: 75.3247	gate_accuracy: 72.7273
	Task-4	val_accuracy: 82.7160	gate_accuracy: 82.7160
	Task-5	val_accuracy: 78.5047	gate_accuracy: 77.5701
	Task-6	val_accuracy: 80.8989	gate_accuracy: 83.1461
	Task-7	val_accuracy: 88.0597	gate_accuracy: 91.0448
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.3138


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.2418	step1_train_accuracy: 54.1401
STEP-1	Epoch: 20/50	loss: 0.9138	step1_train_accuracy: 81.5287
STEP-1	Epoch: 30/50	loss: 0.5030	step1_train_accuracy: 89.8089
STEP-1	Epoch: 40/50	loss: 0.3289	step1_train_accuracy: 94.9045
STEP-1	Epoch: 50/50	loss: 0.2465	step1_train_accuracy: 96.8153
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4212	gate_loss: 1.4288	step2_classification_accuracy: 85.7732	step_2_gate_accuracy: 55.4639
STEP-2	Epoch: 40/200	classification_loss: 0.2685	gate_loss: 0.5860	step2_classification_accuracy: 91.1340	step_2_gate_accuracy: 84.7938
STEP-2	Epoch: 60/200	classification_loss: 0.1925	gate_loss: 0.2848	step2_classification_accuracy: 93.8660	step_2_gate_accuracy: 93.4021
STEP-2	Epoch: 80/200	classification_loss: 0.1674	gate_loss: 0.1819	step2_classification_accuracy: 94.0722	step_2_gate_accuracy: 95.6701
STEP-2	Epoch: 100/200	classification_loss: 0.1505	gate_loss: 0.1252	step2_classification_accuracy: 94.7423	step_2_gate_accuracy: 97.6804
STEP-2	Epoch: 120/200	classification_loss: 0.1561	gate_loss: 0.1062	step2_classification_accuracy: 94.3299	step_2_gate_accuracy: 97.2165
STEP-2	Epoch: 140/200	classification_loss: 0.1285	gate_loss: 0.0776	step2_classification_accuracy: 95.3608	step_2_gate_accuracy: 97.9381
STEP-2	Epoch: 160/200	classification_loss: 0.1397	gate_loss: 0.0784	step2_classification_accuracy: 94.7938	step_2_gate_accuracy: 97.4742
STEP-2	Epoch: 180/200	classification_loss: 0.1253	gate_loss: 0.0612	step2_classification_accuracy: 95.1546	step_2_gate_accuracy: 98.6598
STEP-2	Epoch: 200/200	classification_loss: 0.1206	gate_loss: 0.0539	step2_classification_accuracy: 95.3093	step_2_gate_accuracy: 98.5567
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.8121	gate_accuracy: 79.1946
	Task-1	val_accuracy: 77.1739	gate_accuracy: 84.7826
	Task-2	val_accuracy: 86.6667	gate_accuracy: 85.5556
	Task-3	val_accuracy: 76.6234	gate_accuracy: 74.0260
	Task-4	val_accuracy: 82.7160	gate_accuracy: 83.9506
	Task-5	val_accuracy: 76.6355	gate_accuracy: 66.3551
	Task-6	val_accuracy: 80.8989	gate_accuracy: 83.1461
	Task-7	val_accuracy: 88.0597	gate_accuracy: 89.5522
	Task-8	val_accuracy: 80.7692	gate_accuracy: 78.2051
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.0000


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.8944	step1_train_accuracy: 51.4925
STEP-1	Epoch: 20/50	loss: 1.0713	step1_train_accuracy: 84.3284
STEP-1	Epoch: 30/50	loss: 0.5204	step1_train_accuracy: 92.9104
STEP-1	Epoch: 40/50	loss: 0.3432	step1_train_accuracy: 94.0299
STEP-1	Epoch: 50/50	loss: 0.2569	step1_train_accuracy: 94.7761
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4596	gate_loss: 1.6580	step2_classification_accuracy: 84.2056	step_2_gate_accuracy: 50.5140
STEP-2	Epoch: 40/200	classification_loss: 0.3185	gate_loss: 0.7523	step2_classification_accuracy: 88.5047	step_2_gate_accuracy: 81.3551
STEP-2	Epoch: 60/200	classification_loss: 0.2486	gate_loss: 0.3778	step2_classification_accuracy: 90.7009	step_2_gate_accuracy: 91.2150
STEP-2	Epoch: 80/200	classification_loss: 0.2126	gate_loss: 0.2352	step2_classification_accuracy: 92.3364	step_2_gate_accuracy: 94.3925
STEP-2	Epoch: 100/200	classification_loss: 0.2022	gate_loss: 0.1804	step2_classification_accuracy: 92.3364	step_2_gate_accuracy: 94.9533
STEP-2	Epoch: 120/200	classification_loss: 0.1803	gate_loss: 0.1391	step2_classification_accuracy: 93.3645	step_2_gate_accuracy: 95.9813
STEP-2	Epoch: 140/200	classification_loss: 0.1670	gate_loss: 0.1197	step2_classification_accuracy: 93.7383	step_2_gate_accuracy: 96.5888
STEP-2	Epoch: 160/200	classification_loss: 0.1539	gate_loss: 0.1008	step2_classification_accuracy: 93.8785	step_2_gate_accuracy: 96.9626
STEP-2	Epoch: 180/200	classification_loss: 0.1496	gate_loss: 0.0912	step2_classification_accuracy: 94.3925	step_2_gate_accuracy: 97.1495
STEP-2	Epoch: 200/200	classification_loss: 0.1458	gate_loss: 0.0865	step2_classification_accuracy: 94.1121	step_2_gate_accuracy: 97.1495
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.4430	gate_accuracy: 73.1544
	Task-1	val_accuracy: 71.7391	gate_accuracy: 78.2609
	Task-2	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-3	val_accuracy: 76.6234	gate_accuracy: 74.0260
	Task-4	val_accuracy: 81.4815	gate_accuracy: 82.7160
	Task-5	val_accuracy: 74.7664	gate_accuracy: 71.9626
	Task-6	val_accuracy: 85.3933	gate_accuracy: 85.3933
	Task-7	val_accuracy: 77.6119	gate_accuracy: 79.1045
	Task-8	val_accuracy: 79.4872	gate_accuracy: 78.2051
	Task-9	val_accuracy: 64.1791	gate_accuracy: 59.7015
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.2542


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.8798	step1_train_accuracy: 47.3684
STEP-1	Epoch: 20/50	loss: 1.2713	step1_train_accuracy: 75.9398
STEP-1	Epoch: 30/50	loss: 0.6407	step1_train_accuracy: 88.7218
STEP-1	Epoch: 40/50	loss: 0.4108	step1_train_accuracy: 92.4812
STEP-1	Epoch: 50/50	loss: 0.3040	step1_train_accuracy: 94.7368
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5047	gate_loss: 1.7191	step2_classification_accuracy: 83.5470	step_2_gate_accuracy: 48.7179
STEP-2	Epoch: 40/200	classification_loss: 0.3555	gate_loss: 0.7324	step2_classification_accuracy: 87.8632	step_2_gate_accuracy: 82.6923
STEP-2	Epoch: 60/200	classification_loss: 0.2747	gate_loss: 0.3613	step2_classification_accuracy: 90.4274	step_2_gate_accuracy: 90.6410
STEP-2	Epoch: 80/200	classification_loss: 0.2195	gate_loss: 0.2301	step2_classification_accuracy: 92.0513	step_2_gate_accuracy: 94.1880
STEP-2	Epoch: 100/200	classification_loss: 0.2060	gate_loss: 0.1763	step2_classification_accuracy: 93.0342	step_2_gate_accuracy: 95.4274
STEP-2	Epoch: 120/200	classification_loss: 0.1963	gate_loss: 0.1400	step2_classification_accuracy: 92.8632	step_2_gate_accuracy: 95.8974
STEP-2	Epoch: 140/200	classification_loss: 0.1721	gate_loss: 0.1158	step2_classification_accuracy: 93.7607	step_2_gate_accuracy: 96.4957
STEP-2	Epoch: 160/200	classification_loss: 0.1728	gate_loss: 0.1063	step2_classification_accuracy: 93.8034	step_2_gate_accuracy: 97.0513
STEP-2	Epoch: 180/200	classification_loss: 0.1579	gate_loss: 0.0916	step2_classification_accuracy: 94.1453	step_2_gate_accuracy: 97.3504
STEP-2	Epoch: 200/200	classification_loss: 0.1467	gate_loss: 0.0842	step2_classification_accuracy: 94.3590	step_2_gate_accuracy: 97.6068
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 68.4564	gate_accuracy: 75.1678
	Task-1	val_accuracy: 70.6522	gate_accuracy: 76.0870
	Task-2	val_accuracy: 82.2222	gate_accuracy: 84.4444
	Task-3	val_accuracy: 67.5325	gate_accuracy: 66.2338
	Task-4	val_accuracy: 83.9506	gate_accuracy: 90.1235
	Task-5	val_accuracy: 77.5701	gate_accuracy: 74.7664
	Task-6	val_accuracy: 79.7753	gate_accuracy: 77.5281
	Task-7	val_accuracy: 80.5970	gate_accuracy: 80.5970
	Task-8	val_accuracy: 79.4872	gate_accuracy: 83.3333
	Task-9	val_accuracy: 65.6716	gate_accuracy: 56.7164
	Task-10	val_accuracy: 63.6364	gate_accuracy: 72.7273
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.4278


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.5607	step1_train_accuracy: 53.0612
STEP-1	Epoch: 20/50	loss: 1.0180	step1_train_accuracy: 85.0340
STEP-1	Epoch: 30/50	loss: 0.4500	step1_train_accuracy: 95.9184
STEP-1	Epoch: 40/50	loss: 0.2725	step1_train_accuracy: 97.6190
STEP-1	Epoch: 50/50	loss: 0.1955	step1_train_accuracy: 98.2993
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5498	gate_loss: 1.7867	step2_classification_accuracy: 81.9685	step_2_gate_accuracy: 45.7874
STEP-2	Epoch: 40/200	classification_loss: 0.3889	gate_loss: 0.7736	step2_classification_accuracy: 86.6929	step_2_gate_accuracy: 79.4488
STEP-2	Epoch: 60/200	classification_loss: 0.3180	gate_loss: 0.4077	step2_classification_accuracy: 89.5669	step_2_gate_accuracy: 89.2913
STEP-2	Epoch: 80/200	classification_loss: 0.2588	gate_loss: 0.2696	step2_classification_accuracy: 91.2205	step_2_gate_accuracy: 92.4803
STEP-2	Epoch: 100/200	classification_loss: 0.2314	gate_loss: 0.2017	step2_classification_accuracy: 92.1654	step_2_gate_accuracy: 94.7244
STEP-2	Epoch: 120/200	classification_loss: 0.2018	gate_loss: 0.1528	step2_classification_accuracy: 92.6378	step_2_gate_accuracy: 95.5512
STEP-2	Epoch: 140/200	classification_loss: 0.2133	gate_loss: 0.1439	step2_classification_accuracy: 92.4803	step_2_gate_accuracy: 95.6299
STEP-2	Epoch: 160/200	classification_loss: 0.1671	gate_loss: 0.1103	step2_classification_accuracy: 93.8189	step_2_gate_accuracy: 96.6142
STEP-2	Epoch: 180/200	classification_loss: 0.1663	gate_loss: 0.1028	step2_classification_accuracy: 93.7402	step_2_gate_accuracy: 97.0472
STEP-2	Epoch: 200/200	classification_loss: 0.1628	gate_loss: 0.0930	step2_classification_accuracy: 94.1339	step_2_gate_accuracy: 97.2047
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.1007	gate_accuracy: 75.8389
	Task-1	val_accuracy: 68.4783	gate_accuracy: 80.4348
	Task-2	val_accuracy: 75.5556	gate_accuracy: 73.3333
	Task-3	val_accuracy: 63.6364	gate_accuracy: 63.6364
	Task-4	val_accuracy: 75.3086	gate_accuracy: 77.7778
	Task-5	val_accuracy: 71.0280	gate_accuracy: 71.0280
	Task-6	val_accuracy: 80.8989	gate_accuracy: 80.8989
	Task-7	val_accuracy: 80.5970	gate_accuracy: 82.0896
	Task-8	val_accuracy: 78.2051	gate_accuracy: 79.4872
	Task-9	val_accuracy: 68.6567	gate_accuracy: 56.7164
	Task-10	val_accuracy: 69.6970	gate_accuracy: 77.2727
	Task-11	val_accuracy: 83.7838	gate_accuracy: 78.3784
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.9277


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.2212	step1_train_accuracy: 51.3245
STEP-1	Epoch: 20/50	loss: 0.8549	step1_train_accuracy: 87.7483
STEP-1	Epoch: 30/50	loss: 0.4419	step1_train_accuracy: 96.0265
STEP-1	Epoch: 40/50	loss: 0.3206	step1_train_accuracy: 98.0132
STEP-1	Epoch: 50/50	loss: 0.2000	step1_train_accuracy: 99.0066
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5363	gate_loss: 1.8105	step2_classification_accuracy: 82.5912	step_2_gate_accuracy: 45.2555
STEP-2	Epoch: 40/200	classification_loss: 0.3916	gate_loss: 0.7542	step2_classification_accuracy: 87.1898	step_2_gate_accuracy: 79.2336
STEP-2	Epoch: 60/200	classification_loss: 0.3159	gate_loss: 0.3964	step2_classification_accuracy: 89.4891	step_2_gate_accuracy: 89.4526
STEP-2	Epoch: 80/200	classification_loss: 0.2467	gate_loss: 0.2637	step2_classification_accuracy: 91.5328	step_2_gate_accuracy: 91.9708
STEP-2	Epoch: 100/200	classification_loss: 0.2255	gate_loss: 0.2035	step2_classification_accuracy: 91.7883	step_2_gate_accuracy: 94.2701
STEP-2	Epoch: 120/200	classification_loss: 0.2076	gate_loss: 0.1601	step2_classification_accuracy: 92.6642	step_2_gate_accuracy: 95.3650
STEP-2	Epoch: 140/200	classification_loss: 0.1827	gate_loss: 0.1324	step2_classification_accuracy: 93.2117	step_2_gate_accuracy: 95.7664
STEP-2	Epoch: 160/200	classification_loss: 0.1737	gate_loss: 0.1185	step2_classification_accuracy: 93.6131	step_2_gate_accuracy: 96.3869
STEP-2	Epoch: 180/200	classification_loss: 0.1641	gate_loss: 0.1064	step2_classification_accuracy: 93.9051	step_2_gate_accuracy: 96.3869
STEP-2	Epoch: 200/200	classification_loss: 0.1541	gate_loss: 0.0979	step2_classification_accuracy: 94.3431	step_2_gate_accuracy: 97.1533
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.1275	gate_accuracy: 77.1812
	Task-1	val_accuracy: 72.8261	gate_accuracy: 76.0870
	Task-2	val_accuracy: 83.3333	gate_accuracy: 81.1111
	Task-3	val_accuracy: 68.8312	gate_accuracy: 71.4286
	Task-4	val_accuracy: 70.3704	gate_accuracy: 72.8395
	Task-5	val_accuracy: 71.9626	gate_accuracy: 71.9626
	Task-6	val_accuracy: 76.4045	gate_accuracy: 80.8989
	Task-7	val_accuracy: 83.5821	gate_accuracy: 83.5821
	Task-8	val_accuracy: 80.7692	gate_accuracy: 80.7692
	Task-9	val_accuracy: 55.2239	gate_accuracy: 50.7463
	Task-10	val_accuracy: 66.6667	gate_accuracy: 77.2727
	Task-11	val_accuracy: 75.6757	gate_accuracy: 72.9730
	Task-12	val_accuracy: 88.1579	gate_accuracy: 80.2632
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.4717


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 1.9544	step1_train_accuracy: 57.6705
STEP-1	Epoch: 20/50	loss: 0.8466	step1_train_accuracy: 81.8182
STEP-1	Epoch: 30/50	loss: 0.4144	step1_train_accuracy: 97.1591
STEP-1	Epoch: 40/50	loss: 0.2538	step1_train_accuracy: 97.7273
STEP-1	Epoch: 50/50	loss: 0.1790	step1_train_accuracy: 97.7273
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5466	gate_loss: 1.8872	step2_classification_accuracy: 81.7687	step_2_gate_accuracy: 42.0748
STEP-2	Epoch: 40/200	classification_loss: 0.4126	gate_loss: 0.8157	step2_classification_accuracy: 86.3946	step_2_gate_accuracy: 77.6190
STEP-2	Epoch: 60/200	classification_loss: 0.3208	gate_loss: 0.4279	step2_classification_accuracy: 89.6259	step_2_gate_accuracy: 89.0476
STEP-2	Epoch: 80/200	classification_loss: 0.2614	gate_loss: 0.2753	step2_classification_accuracy: 91.4966	step_2_gate_accuracy: 92.2789
STEP-2	Epoch: 100/200	classification_loss: 0.2317	gate_loss: 0.2094	step2_classification_accuracy: 92.1429	step_2_gate_accuracy: 93.8776
STEP-2	Epoch: 120/200	classification_loss: 0.2209	gate_loss: 0.1722	step2_classification_accuracy: 92.5510	step_2_gate_accuracy: 94.6258
STEP-2	Epoch: 140/200	classification_loss: 0.1893	gate_loss: 0.1460	step2_classification_accuracy: 93.6735	step_2_gate_accuracy: 95.1361
STEP-2	Epoch: 160/200	classification_loss: 0.1950	gate_loss: 0.1338	step2_classification_accuracy: 93.3673	step_2_gate_accuracy: 95.5782
STEP-2	Epoch: 180/200	classification_loss: 0.1865	gate_loss: 0.1269	step2_classification_accuracy: 93.3333	step_2_gate_accuracy: 95.5102
STEP-2	Epoch: 200/200	classification_loss: 0.1778	gate_loss: 0.1163	step2_classification_accuracy: 93.5374	step_2_gate_accuracy: 95.6463
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 67.7852	gate_accuracy: 74.4966
	Task-1	val_accuracy: 65.2174	gate_accuracy: 75.0000
	Task-2	val_accuracy: 70.0000	gate_accuracy: 72.2222
	Task-3	val_accuracy: 59.7403	gate_accuracy: 58.4416
	Task-4	val_accuracy: 74.0741	gate_accuracy: 85.1852
	Task-5	val_accuracy: 65.4206	gate_accuracy: 69.1589
	Task-6	val_accuracy: 75.2809	gate_accuracy: 77.5281
	Task-7	val_accuracy: 77.6119	gate_accuracy: 82.0896
	Task-8	val_accuracy: 75.6410	gate_accuracy: 79.4872
	Task-9	val_accuracy: 65.6716	gate_accuracy: 56.7164
	Task-10	val_accuracy: 62.1212	gate_accuracy: 68.1818
	Task-11	val_accuracy: 79.7297	gate_accuracy: 78.3784
	Task-12	val_accuracy: 89.4737	gate_accuracy: 84.2105
	Task-13	val_accuracy: 75.0000	gate_accuracy: 76.1364
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.1882


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.2195	step1_train_accuracy: 49.8542
STEP-1	Epoch: 20/50	loss: 0.7532	step1_train_accuracy: 86.5889
STEP-1	Epoch: 30/50	loss: 0.4601	step1_train_accuracy: 94.1691
STEP-1	Epoch: 40/50	loss: 0.2957	step1_train_accuracy: 95.9184
STEP-1	Epoch: 50/50	loss: 0.2090	step1_train_accuracy: 97.6676
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5857	gate_loss: 2.0237	step2_classification_accuracy: 80.4777	step_2_gate_accuracy: 40.9873
STEP-2	Epoch: 40/200	classification_loss: 0.4128	gate_loss: 0.8648	step2_classification_accuracy: 86.5924	step_2_gate_accuracy: 75.8599
STEP-2	Epoch: 60/200	classification_loss: 0.3182	gate_loss: 0.4296	step2_classification_accuracy: 88.9809	step_2_gate_accuracy: 88.6306
STEP-2	Epoch: 80/200	classification_loss: 0.2468	gate_loss: 0.2705	step2_classification_accuracy: 91.8790	step_2_gate_accuracy: 92.9936
STEP-2	Epoch: 100/200	classification_loss: 0.2207	gate_loss: 0.2024	step2_classification_accuracy: 92.3567	step_2_gate_accuracy: 94.4904
STEP-2	Epoch: 120/200	classification_loss: 0.2054	gate_loss: 0.1649	step2_classification_accuracy: 93.0573	step_2_gate_accuracy: 95.4459
STEP-2	Epoch: 140/200	classification_loss: 0.1905	gate_loss: 0.1411	step2_classification_accuracy: 93.2484	step_2_gate_accuracy: 95.8599
STEP-2	Epoch: 160/200	classification_loss: 0.1795	gate_loss: 0.1239	step2_classification_accuracy: 93.4713	step_2_gate_accuracy: 96.1783
STEP-2	Epoch: 180/200	classification_loss: 0.1704	gate_loss: 0.1126	step2_classification_accuracy: 93.5032	step_2_gate_accuracy: 95.9236
STEP-2	Epoch: 200/200	classification_loss: 0.1599	gate_loss: 0.1026	step2_classification_accuracy: 94.1083	step_2_gate_accuracy: 96.8790
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.7584	gate_accuracy: 73.8255
	Task-1	val_accuracy: 75.0000	gate_accuracy: 82.6087
	Task-2	val_accuracy: 76.6667	gate_accuracy: 78.8889
	Task-3	val_accuracy: 59.7403	gate_accuracy: 54.5455
	Task-4	val_accuracy: 69.1358	gate_accuracy: 76.5432
	Task-5	val_accuracy: 71.9626	gate_accuracy: 74.7664
	Task-6	val_accuracy: 85.3933	gate_accuracy: 86.5169
	Task-7	val_accuracy: 79.1045	gate_accuracy: 79.1045
	Task-8	val_accuracy: 76.9231	gate_accuracy: 78.2051
	Task-9	val_accuracy: 58.2090	gate_accuracy: 55.2239
	Task-10	val_accuracy: 56.0606	gate_accuracy: 66.6667
	Task-11	val_accuracy: 78.3784	gate_accuracy: 77.0270
	Task-12	val_accuracy: 86.8421	gate_accuracy: 80.2632
	Task-13	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-14	val_accuracy: 77.9070	gate_accuracy: 73.2558
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.5921


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 1.9525	step1_train_accuracy: 58.7131
STEP-1	Epoch: 20/50	loss: 0.7092	step1_train_accuracy: 90.6166
STEP-1	Epoch: 30/50	loss: 0.3473	step1_train_accuracy: 97.8552
STEP-1	Epoch: 40/50	loss: 0.2110	step1_train_accuracy: 98.9276
STEP-1	Epoch: 50/50	loss: 0.1455	step1_train_accuracy: 99.1957
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6100	gate_loss: 2.0057	step2_classification_accuracy: 79.5808	step_2_gate_accuracy: 40.7784
STEP-2	Epoch: 40/200	classification_loss: 0.4367	gate_loss: 0.8512	step2_classification_accuracy: 85.7784	step_2_gate_accuracy: 76.4671
STEP-2	Epoch: 60/200	classification_loss: 0.3385	gate_loss: 0.4518	step2_classification_accuracy: 88.9820	step_2_gate_accuracy: 87.1856
STEP-2	Epoch: 80/200	classification_loss: 0.2586	gate_loss: 0.2883	step2_classification_accuracy: 91.5868	step_2_gate_accuracy: 92.4251
STEP-2	Epoch: 100/200	classification_loss: 0.2470	gate_loss: 0.2254	step2_classification_accuracy: 91.7365	step_2_gate_accuracy: 93.7425
STEP-2	Epoch: 120/200	classification_loss: 0.2101	gate_loss: 0.1731	step2_classification_accuracy: 92.7246	step_2_gate_accuracy: 94.9701
STEP-2	Epoch: 140/200	classification_loss: 0.1896	gate_loss: 0.1413	step2_classification_accuracy: 93.3832	step_2_gate_accuracy: 96.1976
STEP-2	Epoch: 160/200	classification_loss: 0.1774	gate_loss: 0.1285	step2_classification_accuracy: 93.4431	step_2_gate_accuracy: 95.9581
STEP-2	Epoch: 180/200	classification_loss: 0.1720	gate_loss: 0.1128	step2_classification_accuracy: 93.8623	step_2_gate_accuracy: 96.6467
STEP-2	Epoch: 200/200	classification_loss: 0.1619	gate_loss: 0.1068	step2_classification_accuracy: 94.1317	step_2_gate_accuracy: 96.5868
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 68.4564	gate_accuracy: 77.1812
	Task-1	val_accuracy: 66.3043	gate_accuracy: 77.1739
	Task-2	val_accuracy: 73.3333	gate_accuracy: 77.7778
	Task-3	val_accuracy: 66.2338	gate_accuracy: 63.6364
	Task-4	val_accuracy: 69.1358	gate_accuracy: 80.2469
	Task-5	val_accuracy: 70.0935	gate_accuracy: 71.0280
	Task-6	val_accuracy: 76.4045	gate_accuracy: 75.2809
	Task-7	val_accuracy: 77.6119	gate_accuracy: 79.1045
	Task-8	val_accuracy: 85.8974	gate_accuracy: 88.4615
	Task-9	val_accuracy: 58.2090	gate_accuracy: 52.2388
	Task-10	val_accuracy: 66.6667	gate_accuracy: 77.2727
	Task-11	val_accuracy: 78.3784	gate_accuracy: 71.6216
	Task-12	val_accuracy: 88.1579	gate_accuracy: 82.8947
	Task-13	val_accuracy: 71.5909	gate_accuracy: 71.5909
	Task-14	val_accuracy: 84.8837	gate_accuracy: 79.0698
	Task-15	val_accuracy: 88.1720	gate_accuracy: 87.0968
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.0145


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.3751	step1_train_accuracy: 52.3669
STEP-1	Epoch: 20/50	loss: 1.0507	step1_train_accuracy: 73.0769
STEP-1	Epoch: 30/50	loss: 0.4932	step1_train_accuracy: 95.8580
STEP-1	Epoch: 40/50	loss: 0.2871	step1_train_accuracy: 96.7456
STEP-1	Epoch: 50/50	loss: 0.1860	step1_train_accuracy: 98.5207
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6166	gate_loss: 2.1263	step2_classification_accuracy: 80.1977	step_2_gate_accuracy: 38.1356
STEP-2	Epoch: 40/200	classification_loss: 0.4390	gate_loss: 0.8906	step2_classification_accuracy: 85.8192	step_2_gate_accuracy: 75.2825
STEP-2	Epoch: 60/200	classification_loss: 0.3199	gate_loss: 0.4474	step2_classification_accuracy: 89.4633	step_2_gate_accuracy: 87.9096
STEP-2	Epoch: 80/200	classification_loss: 0.2905	gate_loss: 0.3028	step2_classification_accuracy: 90.3672	step_2_gate_accuracy: 91.6667
STEP-2	Epoch: 100/200	classification_loss: 0.2408	gate_loss: 0.2158	step2_classification_accuracy: 92.2599	step_2_gate_accuracy: 94.0960
STEP-2	Epoch: 120/200	classification_loss: 0.2173	gate_loss: 0.1726	step2_classification_accuracy: 92.5989	step_2_gate_accuracy: 94.9435
STEP-2	Epoch: 140/200	classification_loss: 0.1917	gate_loss: 0.1420	step2_classification_accuracy: 93.1356	step_2_gate_accuracy: 95.8192
STEP-2	Epoch: 160/200	classification_loss: 0.1824	gate_loss: 0.1226	step2_classification_accuracy: 93.7288	step_2_gate_accuracy: 96.4689
STEP-2	Epoch: 180/200	classification_loss: 0.1778	gate_loss: 0.1158	step2_classification_accuracy: 93.6441	step_2_gate_accuracy: 96.7232
STEP-2	Epoch: 200/200	classification_loss: 0.1616	gate_loss: 0.0999	step2_classification_accuracy: 94.3785	step_2_gate_accuracy: 96.9492
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.4832	gate_accuracy: 79.8658
	Task-1	val_accuracy: 65.2174	gate_accuracy: 76.0870
	Task-2	val_accuracy: 73.3333	gate_accuracy: 71.1111
	Task-3	val_accuracy: 50.6494	gate_accuracy: 50.6494
	Task-4	val_accuracy: 67.9012	gate_accuracy: 76.5432
	Task-5	val_accuracy: 60.7477	gate_accuracy: 62.6168
	Task-6	val_accuracy: 73.0337	gate_accuracy: 74.1573
	Task-7	val_accuracy: 77.6119	gate_accuracy: 83.5821
	Task-8	val_accuracy: 83.3333	gate_accuracy: 84.6154
	Task-9	val_accuracy: 47.7612	gate_accuracy: 44.7761
	Task-10	val_accuracy: 60.6061	gate_accuracy: 66.6667
	Task-11	val_accuracy: 83.7838	gate_accuracy: 79.7297
	Task-12	val_accuracy: 84.2105	gate_accuracy: 78.9474
	Task-13	val_accuracy: 75.0000	gate_accuracy: 71.5909
	Task-14	val_accuracy: 81.3953	gate_accuracy: 74.4186
	Task-15	val_accuracy: 93.5484	gate_accuracy: 90.3226
	Task-16	val_accuracy: 84.7059	gate_accuracy: 82.3529
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.9249


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 3.1765	step1_train_accuracy: 31.2268
STEP-1	Epoch: 20/50	loss: 1.4399	step1_train_accuracy: 65.0558
STEP-1	Epoch: 30/50	loss: 0.8617	step1_train_accuracy: 82.1561
STEP-1	Epoch: 40/50	loss: 0.5916	step1_train_accuracy: 91.0781
STEP-1	Epoch: 50/50	loss: 0.4338	step1_train_accuracy: 94.4238
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6057	gate_loss: 2.1185	step2_classification_accuracy: 80.6417	step_2_gate_accuracy: 40.9626
STEP-2	Epoch: 40/200	classification_loss: 0.4348	gate_loss: 0.8458	step2_classification_accuracy: 86.2032	step_2_gate_accuracy: 78.4225
STEP-2	Epoch: 60/200	classification_loss: 0.3192	gate_loss: 0.4242	step2_classification_accuracy: 90.2139	step_2_gate_accuracy: 89.0374
STEP-2	Epoch: 80/200	classification_loss: 0.2706	gate_loss: 0.2781	step2_classification_accuracy: 91.3904	step_2_gate_accuracy: 92.4332
STEP-2	Epoch: 100/200	classification_loss: 0.2310	gate_loss: 0.2041	step2_classification_accuracy: 92.5668	step_2_gate_accuracy: 94.3048
STEP-2	Epoch: 120/200	classification_loss: 0.2070	gate_loss: 0.1610	step2_classification_accuracy: 93.2620	step_2_gate_accuracy: 95.8021
STEP-2	Epoch: 140/200	classification_loss: 0.1908	gate_loss: 0.1390	step2_classification_accuracy: 93.7701	step_2_gate_accuracy: 96.3102
STEP-2	Epoch: 160/200	classification_loss: 0.1844	gate_loss: 0.1204	step2_classification_accuracy: 93.7968	step_2_gate_accuracy: 96.6578
STEP-2	Epoch: 180/200	classification_loss: 0.1786	gate_loss: 0.1083	step2_classification_accuracy: 93.9840	step_2_gate_accuracy: 96.8717
STEP-2	Epoch: 200/200	classification_loss: 0.1675	gate_loss: 0.0966	step2_classification_accuracy: 94.2246	step_2_gate_accuracy: 97.2193
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 67.1141	gate_accuracy: 75.1678
	Task-1	val_accuracy: 66.3043	gate_accuracy: 73.9130
	Task-2	val_accuracy: 80.0000	gate_accuracy: 81.1111
	Task-3	val_accuracy: 54.5455	gate_accuracy: 62.3377
	Task-4	val_accuracy: 74.0741	gate_accuracy: 79.0123
	Task-5	val_accuracy: 69.1589	gate_accuracy: 64.4860
	Task-6	val_accuracy: 71.9101	gate_accuracy: 73.0337
	Task-7	val_accuracy: 73.1343	gate_accuracy: 77.6119
	Task-8	val_accuracy: 74.3590	gate_accuracy: 74.3590
	Task-9	val_accuracy: 50.7463	gate_accuracy: 49.2537
	Task-10	val_accuracy: 56.0606	gate_accuracy: 62.1212
	Task-11	val_accuracy: 79.7297	gate_accuracy: 78.3784
	Task-12	val_accuracy: 75.0000	gate_accuracy: 68.4211
	Task-13	val_accuracy: 75.0000	gate_accuracy: 71.5909
	Task-14	val_accuracy: 83.7209	gate_accuracy: 73.2558
	Task-15	val_accuracy: 84.9462	gate_accuracy: 81.7204
	Task-16	val_accuracy: 91.7647	gate_accuracy: 85.8824
	Task-17	val_accuracy: 79.1045	gate_accuracy: 77.6119
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.1071


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 2.2262	step1_train_accuracy: 60.1246
STEP-1	Epoch: 20/50	loss: 0.7441	step1_train_accuracy: 91.5888
STEP-1	Epoch: 30/50	loss: 0.3861	step1_train_accuracy: 93.7695
STEP-1	Epoch: 40/50	loss: 0.2955	step1_train_accuracy: 95.0156
STEP-1	Epoch: 50/50	loss: 0.2132	step1_train_accuracy: 95.0156
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6493	gate_loss: 2.2032	step2_classification_accuracy: 79.8223	step_2_gate_accuracy: 35.9898
STEP-2	Epoch: 40/200	classification_loss: 0.4716	gate_loss: 0.9215	step2_classification_accuracy: 85.4315	step_2_gate_accuracy: 74.9746
STEP-2	Epoch: 60/200	classification_loss: 0.3477	gate_loss: 0.4765	step2_classification_accuracy: 88.4518	step_2_gate_accuracy: 87.0305
STEP-2	Epoch: 80/200	classification_loss: 0.2799	gate_loss: 0.3034	step2_classification_accuracy: 90.9645	step_2_gate_accuracy: 91.9289
STEP-2	Epoch: 100/200	classification_loss: 0.2309	gate_loss: 0.2245	step2_classification_accuracy: 92.1827	step_2_gate_accuracy: 93.7563
STEP-2	Epoch: 120/200	classification_loss: 0.2127	gate_loss: 0.1777	step2_classification_accuracy: 92.7665	step_2_gate_accuracy: 94.8731
STEP-2	Epoch: 140/200	classification_loss: 0.2026	gate_loss: 0.1501	step2_classification_accuracy: 93.2741	step_2_gate_accuracy: 95.5584
STEP-2	Epoch: 160/200	classification_loss: 0.1860	gate_loss: 0.1352	step2_classification_accuracy: 93.4772	step_2_gate_accuracy: 95.8629
STEP-2	Epoch: 180/200	classification_loss: 0.1775	gate_loss: 0.1236	step2_classification_accuracy: 93.5787	step_2_gate_accuracy: 96.2690
STEP-2	Epoch: 200/200	classification_loss: 0.1625	gate_loss: 0.1076	step2_classification_accuracy: 94.0102	step_2_gate_accuracy: 96.4721
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.0738	gate_accuracy: 76.5101
	Task-1	val_accuracy: 73.9130	gate_accuracy: 81.5217
	Task-2	val_accuracy: 75.5556	gate_accuracy: 80.0000
	Task-3	val_accuracy: 62.3377	gate_accuracy: 66.2338
	Task-4	val_accuracy: 76.5432	gate_accuracy: 79.0123
	Task-5	val_accuracy: 69.1589	gate_accuracy: 69.1589
	Task-6	val_accuracy: 77.5281	gate_accuracy: 75.2809
	Task-7	val_accuracy: 71.6418	gate_accuracy: 70.1493
	Task-8	val_accuracy: 73.0769	gate_accuracy: 75.6410
	Task-9	val_accuracy: 56.7164	gate_accuracy: 53.7313
	Task-10	val_accuracy: 62.1212	gate_accuracy: 74.2424
	Task-11	val_accuracy: 83.7838	gate_accuracy: 79.7297
	Task-12	val_accuracy: 76.3158	gate_accuracy: 73.6842
	Task-13	val_accuracy: 69.3182	gate_accuracy: 65.9091
	Task-14	val_accuracy: 81.3953	gate_accuracy: 77.9070
	Task-15	val_accuracy: 82.7957	gate_accuracy: 79.5699
	Task-16	val_accuracy: 88.2353	gate_accuracy: 87.0588
	Task-17	val_accuracy: 61.1940	gate_accuracy: 64.1791
	Task-18	val_accuracy: 68.7500	gate_accuracy: 61.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.6973


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.7180	step1_train_accuracy: 44.2997
STEP-1	Epoch: 20/50	loss: 1.0177	step1_train_accuracy: 79.1531
STEP-1	Epoch: 30/50	loss: 0.5397	step1_train_accuracy: 87.9479
STEP-1	Epoch: 40/50	loss: 0.3865	step1_train_accuracy: 87.9479
STEP-1	Epoch: 50/50	loss: 0.3134	step1_train_accuracy: 88.9251
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7114	gate_loss: 2.2387	step2_classification_accuracy: 77.0048	step_2_gate_accuracy: 36.8116
STEP-2	Epoch: 40/200	classification_loss: 0.5125	gate_loss: 0.9266	step2_classification_accuracy: 83.4058	step_2_gate_accuracy: 75.2899
STEP-2	Epoch: 60/200	classification_loss: 0.3760	gate_loss: 0.4731	step2_classification_accuracy: 87.6570	step_2_gate_accuracy: 87.4396
STEP-2	Epoch: 80/200	classification_loss: 0.2984	gate_loss: 0.3027	step2_classification_accuracy: 90.0000	step_2_gate_accuracy: 91.7633
STEP-2	Epoch: 100/200	classification_loss: 0.2565	gate_loss: 0.2257	step2_classification_accuracy: 91.3527	step_2_gate_accuracy: 93.7440
STEP-2	Epoch: 120/200	classification_loss: 0.2288	gate_loss: 0.1791	step2_classification_accuracy: 92.3671	step_2_gate_accuracy: 95.1208
STEP-2	Epoch: 140/200	classification_loss: 0.2106	gate_loss: 0.1509	step2_classification_accuracy: 92.6087	step_2_gate_accuracy: 95.6039
STEP-2	Epoch: 160/200	classification_loss: 0.1947	gate_loss: 0.1309	step2_classification_accuracy: 93.0435	step_2_gate_accuracy: 96.1111
STEP-2	Epoch: 180/200	classification_loss: 0.1895	gate_loss: 0.1205	step2_classification_accuracy: 92.9227	step_2_gate_accuracy: 96.4734
STEP-2	Epoch: 200/200	classification_loss: 0.1760	gate_loss: 0.1073	step2_classification_accuracy: 93.5990	step_2_gate_accuracy: 96.8599
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.7181	gate_accuracy: 68.4564
	Task-1	val_accuracy: 65.2174	gate_accuracy: 71.7391
	Task-2	val_accuracy: 77.7778	gate_accuracy: 85.5556
	Task-3	val_accuracy: 49.3506	gate_accuracy: 54.5455
	Task-4	val_accuracy: 70.3704	gate_accuracy: 81.4815
	Task-5	val_accuracy: 63.5514	gate_accuracy: 65.4206
	Task-6	val_accuracy: 70.7865	gate_accuracy: 71.9101
	Task-7	val_accuracy: 70.1493	gate_accuracy: 76.1194
	Task-8	val_accuracy: 71.7949	gate_accuracy: 76.9231
	Task-9	val_accuracy: 58.2090	gate_accuracy: 55.2239
	Task-10	val_accuracy: 57.5758	gate_accuracy: 68.1818
	Task-11	val_accuracy: 77.0270	gate_accuracy: 72.9730
	Task-12	val_accuracy: 82.8947	gate_accuracy: 76.3158
	Task-13	val_accuracy: 75.0000	gate_accuracy: 72.7273
	Task-14	val_accuracy: 79.0698	gate_accuracy: 73.2558
	Task-15	val_accuracy: 84.9462	gate_accuracy: 87.0968
	Task-16	val_accuracy: 84.7059	gate_accuracy: 84.7059
	Task-17	val_accuracy: 76.1194	gate_accuracy: 73.1343
	Task-18	val_accuracy: 62.5000	gate_accuracy: 57.5000
	Task-19	val_accuracy: 51.9481	gate_accuracy: 53.2468
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.5216


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.5350	step1_train_accuracy: 53.8922
STEP-1	Epoch: 20/50	loss: 0.9406	step1_train_accuracy: 85.6287
STEP-1	Epoch: 30/50	loss: 0.4958	step1_train_accuracy: 89.2216
STEP-1	Epoch: 40/50	loss: 0.3334	step1_train_accuracy: 93.4132
STEP-1	Epoch: 50/50	loss: 0.2550	step1_train_accuracy: 94.6108
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7285	gate_loss: 2.2786	step2_classification_accuracy: 76.4516	step_2_gate_accuracy: 35.7604
STEP-2	Epoch: 40/200	classification_loss: 0.5321	gate_loss: 0.9387	step2_classification_accuracy: 82.8802	step_2_gate_accuracy: 74.2627
STEP-2	Epoch: 60/200	classification_loss: 0.3917	gate_loss: 0.4880	step2_classification_accuracy: 87.0737	step_2_gate_accuracy: 86.5668
STEP-2	Epoch: 80/200	classification_loss: 0.3266	gate_loss: 0.3244	step2_classification_accuracy: 89.7696	step_2_gate_accuracy: 90.9447
STEP-2	Epoch: 100/200	classification_loss: 0.2831	gate_loss: 0.2435	step2_classification_accuracy: 90.9677	step_2_gate_accuracy: 92.8571
STEP-2	Epoch: 120/200	classification_loss: 0.2594	gate_loss: 0.2024	step2_classification_accuracy: 91.4286	step_2_gate_accuracy: 94.0092
STEP-2	Epoch: 140/200	classification_loss: 0.2444	gate_loss: 0.1728	step2_classification_accuracy: 91.2673	step_2_gate_accuracy: 94.5392
STEP-2	Epoch: 160/200	classification_loss: 0.2291	gate_loss: 0.1537	step2_classification_accuracy: 92.2120	step_2_gate_accuracy: 95.1843
STEP-2	Epoch: 180/200	classification_loss: 0.2143	gate_loss: 0.1394	step2_classification_accuracy: 92.3963	step_2_gate_accuracy: 95.6912
STEP-2	Epoch: 200/200	classification_loss: 0.2047	gate_loss: 0.1269	step2_classification_accuracy: 92.8341	step_2_gate_accuracy: 95.6221
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.7584	gate_accuracy: 72.4832
	Task-1	val_accuracy: 63.0435	gate_accuracy: 69.5652
	Task-2	val_accuracy: 67.7778	gate_accuracy: 66.6667
	Task-3	val_accuracy: 53.2468	gate_accuracy: 55.8442
	Task-4	val_accuracy: 66.6667	gate_accuracy: 75.3086
	Task-5	val_accuracy: 63.5514	gate_accuracy: 62.6168
	Task-6	val_accuracy: 74.1573	gate_accuracy: 76.4045
	Task-7	val_accuracy: 67.1642	gate_accuracy: 73.1343
	Task-8	val_accuracy: 76.9231	gate_accuracy: 78.2051
	Task-9	val_accuracy: 61.1940	gate_accuracy: 58.2090
	Task-10	val_accuracy: 57.5758	gate_accuracy: 69.6970
	Task-11	val_accuracy: 77.0270	gate_accuracy: 71.6216
	Task-12	val_accuracy: 89.4737	gate_accuracy: 84.2105
	Task-13	val_accuracy: 73.8636	gate_accuracy: 65.9091
	Task-14	val_accuracy: 80.2326	gate_accuracy: 77.9070
	Task-15	val_accuracy: 80.6452	gate_accuracy: 77.4194
	Task-16	val_accuracy: 85.8824	gate_accuracy: 85.8824
	Task-17	val_accuracy: 73.1343	gate_accuracy: 68.6567
	Task-18	val_accuracy: 66.2500	gate_accuracy: 63.7500
	Task-19	val_accuracy: 50.6494	gate_accuracy: 44.1558
	Task-20	val_accuracy: 53.5714	gate_accuracy: 52.3810
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.2611


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.7148	step1_train_accuracy: 52.1739
STEP-1	Epoch: 20/50	loss: 1.0318	step1_train_accuracy: 77.9264
STEP-1	Epoch: 30/50	loss: 0.4815	step1_train_accuracy: 92.9766
STEP-1	Epoch: 40/50	loss: 0.2848	step1_train_accuracy: 94.9833
STEP-1	Epoch: 50/50	loss: 0.2033	step1_train_accuracy: 97.6589
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7268	gate_loss: 2.2968	step2_classification_accuracy: 77.0925	step_2_gate_accuracy: 36.2115
STEP-2	Epoch: 40/200	classification_loss: 0.5502	gate_loss: 0.9451	step2_classification_accuracy: 82.6432	step_2_gate_accuracy: 74.7357
STEP-2	Epoch: 60/200	classification_loss: 0.4103	gate_loss: 0.4977	step2_classification_accuracy: 86.4758	step_2_gate_accuracy: 86.0573
STEP-2	Epoch: 80/200	classification_loss: 0.3352	gate_loss: 0.3322	step2_classification_accuracy: 89.0529	step_2_gate_accuracy: 90.5286
STEP-2	Epoch: 100/200	classification_loss: 0.2855	gate_loss: 0.2525	step2_classification_accuracy: 90.4626	step_2_gate_accuracy: 92.6872
STEP-2	Epoch: 120/200	classification_loss: 0.2706	gate_loss: 0.2106	step2_classification_accuracy: 90.8370	step_2_gate_accuracy: 93.7445
STEP-2	Epoch: 140/200	classification_loss: 0.2381	gate_loss: 0.1737	step2_classification_accuracy: 91.9824	step_2_gate_accuracy: 94.6696
STEP-2	Epoch: 160/200	classification_loss: 0.2239	gate_loss: 0.1568	step2_classification_accuracy: 92.3128	step_2_gate_accuracy: 95.3524
STEP-2	Epoch: 180/200	classification_loss: 0.2097	gate_loss: 0.1423	step2_classification_accuracy: 92.4670	step_2_gate_accuracy: 95.3965
STEP-2	Epoch: 200/200	classification_loss: 0.2009	gate_loss: 0.1303	step2_classification_accuracy: 92.8414	step_2_gate_accuracy: 95.7489
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.4027	gate_accuracy: 73.1544
	Task-1	val_accuracy: 66.3043	gate_accuracy: 76.0870
	Task-2	val_accuracy: 71.1111	gate_accuracy: 75.5556
	Task-3	val_accuracy: 48.0519	gate_accuracy: 54.5455
	Task-4	val_accuracy: 58.0247	gate_accuracy: 64.1975
	Task-5	val_accuracy: 60.7477	gate_accuracy: 62.6168
	Task-6	val_accuracy: 69.6629	gate_accuracy: 73.0337
	Task-7	val_accuracy: 64.1791	gate_accuracy: 67.1642
	Task-8	val_accuracy: 78.2051	gate_accuracy: 74.3590
	Task-9	val_accuracy: 53.7313	gate_accuracy: 53.7313
	Task-10	val_accuracy: 60.6061	gate_accuracy: 68.1818
	Task-11	val_accuracy: 79.7297	gate_accuracy: 74.3243
	Task-12	val_accuracy: 77.6316	gate_accuracy: 69.7368
	Task-13	val_accuracy: 75.0000	gate_accuracy: 70.4545
	Task-14	val_accuracy: 76.7442	gate_accuracy: 72.0930
	Task-15	val_accuracy: 86.0215	gate_accuracy: 80.6452
	Task-16	val_accuracy: 89.4118	gate_accuracy: 89.4118
	Task-17	val_accuracy: 71.6418	gate_accuracy: 65.6716
	Task-18	val_accuracy: 60.0000	gate_accuracy: 52.5000
	Task-19	val_accuracy: 45.4545	gate_accuracy: 38.9610
	Task-20	val_accuracy: 59.5238	gate_accuracy: 64.2857
	Task-21	val_accuracy: 70.6667	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.1818


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.3480	step1_train_accuracy: 57.5419
STEP-1	Epoch: 20/50	loss: 0.9092	step1_train_accuracy: 82.6816
STEP-1	Epoch: 30/50	loss: 0.4780	step1_train_accuracy: 93.0168
STEP-1	Epoch: 40/50	loss: 0.3283	step1_train_accuracy: 94.9721
STEP-1	Epoch: 50/50	loss: 0.2447	step1_train_accuracy: 95.2514
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7874	gate_loss: 2.3556	step2_classification_accuracy: 75.7173	step_2_gate_accuracy: 34.8312
STEP-2	Epoch: 40/200	classification_loss: 0.5759	gate_loss: 0.9782	step2_classification_accuracy: 81.7722	step_2_gate_accuracy: 73.6498
STEP-2	Epoch: 60/200	classification_loss: 0.4423	gate_loss: 0.5240	step2_classification_accuracy: 85.8650	step_2_gate_accuracy: 84.6413
STEP-2	Epoch: 80/200	classification_loss: 0.3573	gate_loss: 0.3558	step2_classification_accuracy: 88.7975	step_2_gate_accuracy: 89.6202
STEP-2	Epoch: 100/200	classification_loss: 0.3062	gate_loss: 0.2739	step2_classification_accuracy: 90.1477	step_2_gate_accuracy: 91.8776
STEP-2	Epoch: 120/200	classification_loss: 0.2775	gate_loss: 0.2241	step2_classification_accuracy: 90.9705	step_2_gate_accuracy: 93.1224
STEP-2	Epoch: 140/200	classification_loss: 0.2627	gate_loss: 0.1950	step2_classification_accuracy: 91.7300	step_2_gate_accuracy: 93.6498
STEP-2	Epoch: 160/200	classification_loss: 0.2394	gate_loss: 0.1719	step2_classification_accuracy: 92.1308	step_2_gate_accuracy: 94.5148
STEP-2	Epoch: 180/200	classification_loss: 0.2282	gate_loss: 0.1553	step2_classification_accuracy: 92.4051	step_2_gate_accuracy: 95.1477
STEP-2	Epoch: 200/200	classification_loss: 0.2140	gate_loss: 0.1409	step2_classification_accuracy: 92.7215	step_2_gate_accuracy: 95.6329
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.4161	gate_accuracy: 73.1544
	Task-1	val_accuracy: 69.5652	gate_accuracy: 77.1739
	Task-2	val_accuracy: 63.3333	gate_accuracy: 68.8889
	Task-3	val_accuracy: 54.5455	gate_accuracy: 59.7403
	Task-4	val_accuracy: 69.1358	gate_accuracy: 72.8395
	Task-5	val_accuracy: 69.1589	gate_accuracy: 68.2243
	Task-6	val_accuracy: 75.2809	gate_accuracy: 78.6517
	Task-7	val_accuracy: 64.1791	gate_accuracy: 70.1493
	Task-8	val_accuracy: 70.5128	gate_accuracy: 73.0769
	Task-9	val_accuracy: 65.6716	gate_accuracy: 62.6866
	Task-10	val_accuracy: 65.1515	gate_accuracy: 72.7273
	Task-11	val_accuracy: 83.7838	gate_accuracy: 81.0811
	Task-12	val_accuracy: 80.2632	gate_accuracy: 77.6316
	Task-13	val_accuracy: 68.1818	gate_accuracy: 65.9091
	Task-14	val_accuracy: 86.0465	gate_accuracy: 74.4186
	Task-15	val_accuracy: 86.0215	gate_accuracy: 78.4946
	Task-16	val_accuracy: 88.2353	gate_accuracy: 85.8824
	Task-17	val_accuracy: 73.1343	gate_accuracy: 77.6119
	Task-18	val_accuracy: 72.5000	gate_accuracy: 60.0000
	Task-19	val_accuracy: 45.4545	gate_accuracy: 46.7532
	Task-20	val_accuracy: 53.5714	gate_accuracy: 50.0000
	Task-21	val_accuracy: 73.3333	gate_accuracy: 72.0000
	Task-22	val_accuracy: 69.6629	gate_accuracy: 69.6629
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.4698


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 2.7375	step1_train_accuracy: 54.9669
STEP-1	Epoch: 20/50	loss: 1.0908	step1_train_accuracy: 86.4238
STEP-1	Epoch: 30/50	loss: 0.4308	step1_train_accuracy: 98.3444
STEP-1	Epoch: 40/50	loss: 0.2601	step1_train_accuracy: 98.6755
STEP-1	Epoch: 50/50	loss: 0.1850	step1_train_accuracy: 98.6755
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7748	gate_loss: 2.4523	step2_classification_accuracy: 74.9798	step_2_gate_accuracy: 32.6923
STEP-2	Epoch: 40/200	classification_loss: 0.5745	gate_loss: 1.0553	step2_classification_accuracy: 80.9919	step_2_gate_accuracy: 70.6478
STEP-2	Epoch: 60/200	classification_loss: 0.4304	gate_loss: 0.5629	step2_classification_accuracy: 86.6397	step_2_gate_accuracy: 84.4939
STEP-2	Epoch: 80/200	classification_loss: 0.3692	gate_loss: 0.3837	step2_classification_accuracy: 88.0567	step_2_gate_accuracy: 89.1498
STEP-2	Epoch: 100/200	classification_loss: 0.3075	gate_loss: 0.2905	step2_classification_accuracy: 90.1012	step_2_gate_accuracy: 91.7814
STEP-2	Epoch: 120/200	classification_loss: 0.2784	gate_loss: 0.2371	step2_classification_accuracy: 90.5668	step_2_gate_accuracy: 93.0364
STEP-2	Epoch: 140/200	classification_loss: 0.2502	gate_loss: 0.2056	step2_classification_accuracy: 91.0121	step_2_gate_accuracy: 93.6640
STEP-2	Epoch: 160/200	classification_loss: 0.2294	gate_loss: 0.1776	step2_classification_accuracy: 92.2065	step_2_gate_accuracy: 94.7368
STEP-2	Epoch: 180/200	classification_loss: 0.2149	gate_loss: 0.1612	step2_classification_accuracy: 92.3279	step_2_gate_accuracy: 94.9798
STEP-2	Epoch: 200/200	classification_loss: 0.2194	gate_loss: 0.1564	step2_classification_accuracy: 92.1053	step_2_gate_accuracy: 95.0810
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.0738	gate_accuracy: 71.8121
	Task-1	val_accuracy: 63.0435	gate_accuracy: 66.3043
	Task-2	val_accuracy: 64.4444	gate_accuracy: 64.4444
	Task-3	val_accuracy: 49.3506	gate_accuracy: 49.3506
	Task-4	val_accuracy: 56.7901	gate_accuracy: 67.9012
	Task-5	val_accuracy: 70.0935	gate_accuracy: 75.7009
	Task-6	val_accuracy: 68.5393	gate_accuracy: 67.4157
	Task-7	val_accuracy: 67.1642	gate_accuracy: 68.6567
	Task-8	val_accuracy: 70.5128	gate_accuracy: 67.9487
	Task-9	val_accuracy: 47.7612	gate_accuracy: 41.7910
	Task-10	val_accuracy: 63.6364	gate_accuracy: 74.2424
	Task-11	val_accuracy: 83.7838	gate_accuracy: 75.6757
	Task-12	val_accuracy: 78.9474	gate_accuracy: 77.6316
	Task-13	val_accuracy: 63.6364	gate_accuracy: 61.3636
	Task-14	val_accuracy: 79.0698	gate_accuracy: 76.7442
	Task-15	val_accuracy: 77.4194	gate_accuracy: 78.4946
	Task-16	val_accuracy: 83.5294	gate_accuracy: 78.8235
	Task-17	val_accuracy: 64.1791	gate_accuracy: 59.7015
	Task-18	val_accuracy: 65.0000	gate_accuracy: 56.2500
	Task-19	val_accuracy: 48.0519	gate_accuracy: 40.2597
	Task-20	val_accuracy: 52.3810	gate_accuracy: 48.8095
	Task-21	val_accuracy: 70.6667	gate_accuracy: 66.6667
	Task-22	val_accuracy: 73.0337	gate_accuracy: 66.2921
	Task-23	val_accuracy: 65.3333	gate_accuracy: 62.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.8052


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.7919	step1_train_accuracy: 45.1613
STEP-1	Epoch: 20/50	loss: 1.0317	step1_train_accuracy: 82.2581
STEP-1	Epoch: 30/50	loss: 0.4618	step1_train_accuracy: 96.7742
STEP-1	Epoch: 40/50	loss: 0.2925	step1_train_accuracy: 98.0645
STEP-1	Epoch: 50/50	loss: 0.2143	step1_train_accuracy: 98.0645
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8057	gate_loss: 2.4871	step2_classification_accuracy: 74.5136	step_2_gate_accuracy: 30.0389
STEP-2	Epoch: 40/200	classification_loss: 0.6024	gate_loss: 1.0543	step2_classification_accuracy: 81.7315	step_2_gate_accuracy: 70.7977
STEP-2	Epoch: 60/200	classification_loss: 0.4622	gate_loss: 0.5615	step2_classification_accuracy: 85.4280	step_2_gate_accuracy: 84.3580
STEP-2	Epoch: 80/200	classification_loss: 0.3843	gate_loss: 0.3891	step2_classification_accuracy: 88.2879	step_2_gate_accuracy: 88.4630
STEP-2	Epoch: 100/200	classification_loss: 0.3248	gate_loss: 0.2961	step2_classification_accuracy: 89.6304	step_2_gate_accuracy: 91.3619
STEP-2	Epoch: 120/200	classification_loss: 0.2899	gate_loss: 0.2404	step2_classification_accuracy: 90.4864	step_2_gate_accuracy: 92.5875
STEP-2	Epoch: 140/200	classification_loss: 0.2711	gate_loss: 0.2105	step2_classification_accuracy: 91.1673	step_2_gate_accuracy: 93.4241
STEP-2	Epoch: 160/200	classification_loss: 0.2498	gate_loss: 0.1840	step2_classification_accuracy: 91.7510	step_2_gate_accuracy: 94.4747
STEP-2	Epoch: 180/200	classification_loss: 0.2344	gate_loss: 0.1657	step2_classification_accuracy: 92.1012	step_2_gate_accuracy: 94.9805
STEP-2	Epoch: 200/200	classification_loss: 0.2222	gate_loss: 0.1521	step2_classification_accuracy: 92.3541	step_2_gate_accuracy: 95.0778
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.7181	gate_accuracy: 68.4564
	Task-1	val_accuracy: 69.5652	gate_accuracy: 76.0870
	Task-2	val_accuracy: 71.1111	gate_accuracy: 76.6667
	Task-3	val_accuracy: 51.9481	gate_accuracy: 57.1429
	Task-4	val_accuracy: 50.6173	gate_accuracy: 62.9630
	Task-5	val_accuracy: 65.4206	gate_accuracy: 62.6168
	Task-6	val_accuracy: 69.6629	gate_accuracy: 73.0337
	Task-7	val_accuracy: 67.1642	gate_accuracy: 68.6567
	Task-8	val_accuracy: 67.9487	gate_accuracy: 66.6667
	Task-9	val_accuracy: 58.2090	gate_accuracy: 55.2239
	Task-10	val_accuracy: 50.0000	gate_accuracy: 53.0303
	Task-11	val_accuracy: 78.3784	gate_accuracy: 74.3243
	Task-12	val_accuracy: 82.8947	gate_accuracy: 75.0000
	Task-13	val_accuracy: 70.4545	gate_accuracy: 69.3182
	Task-14	val_accuracy: 80.2326	gate_accuracy: 70.9302
	Task-15	val_accuracy: 84.9462	gate_accuracy: 84.9462
	Task-16	val_accuracy: 85.8824	gate_accuracy: 81.1765
	Task-17	val_accuracy: 76.1194	gate_accuracy: 76.1194
	Task-18	val_accuracy: 62.5000	gate_accuracy: 60.0000
	Task-19	val_accuracy: 42.8571	gate_accuracy: 44.1558
	Task-20	val_accuracy: 50.0000	gate_accuracy: 48.8095
	Task-21	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-22	val_accuracy: 71.9101	gate_accuracy: 61.7978
	Task-23	val_accuracy: 72.0000	gate_accuracy: 69.3333
	Task-24	val_accuracy: 67.9487	gate_accuracy: 67.9487
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.1770


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 2.2775	step1_train_accuracy: 54.2328
STEP-1	Epoch: 20/50	loss: 0.8810	step1_train_accuracy: 80.6878
STEP-1	Epoch: 30/50	loss: 0.4494	step1_train_accuracy: 92.8571
STEP-1	Epoch: 40/50	loss: 0.2983	step1_train_accuracy: 94.7090
STEP-1	Epoch: 50/50	loss: 0.2253	step1_train_accuracy: 94.9735
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8170	gate_loss: 2.4272	step2_classification_accuracy: 73.4644	step_2_gate_accuracy: 32.2472
STEP-2	Epoch: 40/200	classification_loss: 0.6149	gate_loss: 1.0580	step2_classification_accuracy: 80.7116	step_2_gate_accuracy: 69.1011
STEP-2	Epoch: 60/200	classification_loss: 0.4698	gate_loss: 0.5915	step2_classification_accuracy: 85.3933	step_2_gate_accuracy: 82.6404
STEP-2	Epoch: 80/200	classification_loss: 0.3893	gate_loss: 0.4100	step2_classification_accuracy: 87.3783	step_2_gate_accuracy: 87.6217
STEP-2	Epoch: 100/200	classification_loss: 0.3286	gate_loss: 0.3117	step2_classification_accuracy: 89.3446	step_2_gate_accuracy: 90.7303
STEP-2	Epoch: 120/200	classification_loss: 0.2891	gate_loss: 0.2582	step2_classification_accuracy: 90.6554	step_2_gate_accuracy: 92.3034
STEP-2	Epoch: 140/200	classification_loss: 0.2675	gate_loss: 0.2263	step2_classification_accuracy: 91.2360	step_2_gate_accuracy: 92.9213
STEP-2	Epoch: 160/200	classification_loss: 0.2470	gate_loss: 0.1934	step2_classification_accuracy: 91.9663	step_2_gate_accuracy: 94.0824
STEP-2	Epoch: 180/200	classification_loss: 0.2486	gate_loss: 0.1901	step2_classification_accuracy: 92.0225	step_2_gate_accuracy: 94.0075
STEP-2	Epoch: 200/200	classification_loss: 0.2207	gate_loss: 0.1685	step2_classification_accuracy: 92.6030	step_2_gate_accuracy: 94.6629
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 54.3624	gate_accuracy: 69.1275
	Task-1	val_accuracy: 61.9565	gate_accuracy: 73.9130
	Task-2	val_accuracy: 56.6667	gate_accuracy: 60.0000
	Task-3	val_accuracy: 50.6494	gate_accuracy: 53.2468
	Task-4	val_accuracy: 51.8519	gate_accuracy: 54.3210
	Task-5	val_accuracy: 67.2897	gate_accuracy: 68.2243
	Task-6	val_accuracy: 75.2809	gate_accuracy: 78.6517
	Task-7	val_accuracy: 65.6716	gate_accuracy: 70.1493
	Task-8	val_accuracy: 74.3590	gate_accuracy: 69.2308
	Task-9	val_accuracy: 59.7015	gate_accuracy: 55.2239
	Task-10	val_accuracy: 56.0606	gate_accuracy: 63.6364
	Task-11	val_accuracy: 79.7297	gate_accuracy: 71.6216
	Task-12	val_accuracy: 71.0526	gate_accuracy: 64.4737
	Task-13	val_accuracy: 69.3182	gate_accuracy: 64.7727
	Task-14	val_accuracy: 73.2558	gate_accuracy: 63.9535
	Task-15	val_accuracy: 78.4946	gate_accuracy: 74.1935
	Task-16	val_accuracy: 82.3529	gate_accuracy: 78.8235
	Task-17	val_accuracy: 73.1343	gate_accuracy: 73.1343
	Task-18	val_accuracy: 57.5000	gate_accuracy: 45.0000
	Task-19	val_accuracy: 44.1558	gate_accuracy: 42.8571
	Task-20	val_accuracy: 51.1905	gate_accuracy: 46.4286
	Task-21	val_accuracy: 70.6667	gate_accuracy: 66.6667
	Task-22	val_accuracy: 78.6517	gate_accuracy: 67.4157
	Task-23	val_accuracy: 57.3333	gate_accuracy: 52.0000
	Task-24	val_accuracy: 67.9487	gate_accuracy: 58.9744
	Task-25	val_accuracy: 72.3404	gate_accuracy: 61.7021
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.7821


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 2.8728	step1_train_accuracy: 60.3226
STEP-1	Epoch: 20/50	loss: 0.9527	step1_train_accuracy: 82.5806
STEP-1	Epoch: 30/50	loss: 0.5100	step1_train_accuracy: 93.5484
STEP-1	Epoch: 40/50	loss: 0.3535	step1_train_accuracy: 94.8387
STEP-1	Epoch: 50/50	loss: 0.2580	step1_train_accuracy: 97.4194
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8452	gate_loss: 2.4678	step2_classification_accuracy: 74.2599	step_2_gate_accuracy: 31.2094
STEP-2	Epoch: 40/200	classification_loss: 0.6454	gate_loss: 1.0643	step2_classification_accuracy: 79.2780	step_2_gate_accuracy: 69.6931
STEP-2	Epoch: 60/200	classification_loss: 0.5069	gate_loss: 0.5991	step2_classification_accuracy: 84.0433	step_2_gate_accuracy: 82.1480
STEP-2	Epoch: 80/200	classification_loss: 0.4117	gate_loss: 0.4182	step2_classification_accuracy: 86.5343	step_2_gate_accuracy: 87.4729
STEP-2	Epoch: 100/200	classification_loss: 0.3451	gate_loss: 0.3236	step2_classification_accuracy: 88.7004	step_2_gate_accuracy: 90.4513
STEP-2	Epoch: 120/200	classification_loss: 0.3049	gate_loss: 0.2656	step2_classification_accuracy: 90.0181	step_2_gate_accuracy: 92.0036
STEP-2	Epoch: 140/200	classification_loss: 0.2830	gate_loss: 0.2267	step2_classification_accuracy: 90.8303	step_2_gate_accuracy: 93.1588
STEP-2	Epoch: 160/200	classification_loss: 0.2723	gate_loss: 0.2144	step2_classification_accuracy: 90.9747	step_2_gate_accuracy: 93.4116
STEP-2	Epoch: 180/200	classification_loss: 0.2411	gate_loss: 0.1786	step2_classification_accuracy: 91.7690	step_2_gate_accuracy: 94.4404
STEP-2	Epoch: 200/200	classification_loss: 0.2309	gate_loss: 0.1661	step2_classification_accuracy: 91.8592	step_2_gate_accuracy: 94.4404
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.3490	gate_accuracy: 71.1409
	Task-1	val_accuracy: 63.0435	gate_accuracy: 68.4783
	Task-2	val_accuracy: 75.5556	gate_accuracy: 74.4444
	Task-3	val_accuracy: 54.5455	gate_accuracy: 61.0390
	Task-4	val_accuracy: 56.7901	gate_accuracy: 64.1975
	Task-5	val_accuracy: 62.6168	gate_accuracy: 62.6168
	Task-6	val_accuracy: 61.7978	gate_accuracy: 64.0449
	Task-7	val_accuracy: 70.1493	gate_accuracy: 73.1343
	Task-8	val_accuracy: 66.6667	gate_accuracy: 64.1026
	Task-9	val_accuracy: 58.2090	gate_accuracy: 53.7313
	Task-10	val_accuracy: 56.0606	gate_accuracy: 57.5758
	Task-11	val_accuracy: 75.6757	gate_accuracy: 72.9730
	Task-12	val_accuracy: 69.7368	gate_accuracy: 65.7895
	Task-13	val_accuracy: 73.8636	gate_accuracy: 69.3182
	Task-14	val_accuracy: 73.2558	gate_accuracy: 65.1163
	Task-15	val_accuracy: 78.4946	gate_accuracy: 75.2688
	Task-16	val_accuracy: 82.3529	gate_accuracy: 77.6471
	Task-17	val_accuracy: 68.6567	gate_accuracy: 65.6716
	Task-18	val_accuracy: 68.7500	gate_accuracy: 62.5000
	Task-19	val_accuracy: 53.2468	gate_accuracy: 42.8571
	Task-20	val_accuracy: 51.1905	gate_accuracy: 44.0476
	Task-21	val_accuracy: 73.3333	gate_accuracy: 73.3333
	Task-22	val_accuracy: 74.1573	gate_accuracy: 66.2921
	Task-23	val_accuracy: 64.0000	gate_accuracy: 58.6667
	Task-24	val_accuracy: 67.9487	gate_accuracy: 62.8205
	Task-25	val_accuracy: 51.0638	gate_accuracy: 47.8723
	Task-26	val_accuracy: 74.0260	gate_accuracy: 67.5325
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.4405


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.3014	step1_train_accuracy: 53.4483
STEP-1	Epoch: 20/50	loss: 0.8889	step1_train_accuracy: 83.0460
STEP-1	Epoch: 30/50	loss: 0.4846	step1_train_accuracy: 91.6667
STEP-1	Epoch: 40/50	loss: 0.3360	step1_train_accuracy: 94.2529
STEP-1	Epoch: 50/50	loss: 0.2530	step1_train_accuracy: 96.8391
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8578	gate_loss: 2.4800	step2_classification_accuracy: 73.7282	step_2_gate_accuracy: 31.8815
STEP-2	Epoch: 40/200	classification_loss: 0.6442	gate_loss: 1.0533	step2_classification_accuracy: 80.0000	step_2_gate_accuracy: 70.5401
STEP-2	Epoch: 60/200	classification_loss: 0.4993	gate_loss: 0.5995	step2_classification_accuracy: 84.3554	step_2_gate_accuracy: 81.6899
STEP-2	Epoch: 80/200	classification_loss: 0.3974	gate_loss: 0.4160	step2_classification_accuracy: 87.5261	step_2_gate_accuracy: 87.9443
STEP-2	Epoch: 100/200	classification_loss: 0.3547	gate_loss: 0.3273	step2_classification_accuracy: 88.7979	step_2_gate_accuracy: 90.2787
STEP-2	Epoch: 120/200	classification_loss: 0.3169	gate_loss: 0.2738	step2_classification_accuracy: 89.9303	step_2_gate_accuracy: 91.5331
STEP-2	Epoch: 140/200	classification_loss: 0.3012	gate_loss: 0.2442	step2_classification_accuracy: 90.4530	step_2_gate_accuracy: 92.6481
STEP-2	Epoch: 160/200	classification_loss: 0.2726	gate_loss: 0.2110	step2_classification_accuracy: 91.1150	step_2_gate_accuracy: 93.4146
STEP-2	Epoch: 180/200	classification_loss: 0.2633	gate_loss: 0.1963	step2_classification_accuracy: 91.4111	step_2_gate_accuracy: 93.7108
STEP-2	Epoch: 200/200	classification_loss: 0.2411	gate_loss: 0.1740	step2_classification_accuracy: 91.8641	step_2_gate_accuracy: 94.3380
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.7047	gate_accuracy: 66.4430
	Task-1	val_accuracy: 60.8696	gate_accuracy: 61.9565
	Task-2	val_accuracy: 64.4444	gate_accuracy: 70.0000
	Task-3	val_accuracy: 53.2468	gate_accuracy: 53.2468
	Task-4	val_accuracy: 56.7901	gate_accuracy: 65.4321
	Task-5	val_accuracy: 67.2897	gate_accuracy: 74.7664
	Task-6	val_accuracy: 64.0449	gate_accuracy: 65.1685
	Task-7	val_accuracy: 58.2090	gate_accuracy: 61.1940
	Task-8	val_accuracy: 76.9231	gate_accuracy: 70.5128
	Task-9	val_accuracy: 52.2388	gate_accuracy: 50.7463
	Task-10	val_accuracy: 56.0606	gate_accuracy: 62.1212
	Task-11	val_accuracy: 71.6216	gate_accuracy: 75.6757
	Task-12	val_accuracy: 77.6316	gate_accuracy: 68.4211
	Task-13	val_accuracy: 64.7727	gate_accuracy: 56.8182
	Task-14	val_accuracy: 82.5581	gate_accuracy: 80.2326
	Task-15	val_accuracy: 81.7204	gate_accuracy: 83.8710
	Task-16	val_accuracy: 88.2353	gate_accuracy: 85.8824
	Task-17	val_accuracy: 71.6418	gate_accuracy: 65.6716
	Task-18	val_accuracy: 57.5000	gate_accuracy: 48.7500
	Task-19	val_accuracy: 42.8571	gate_accuracy: 40.2597
	Task-20	val_accuracy: 58.3333	gate_accuracy: 54.7619
	Task-21	val_accuracy: 61.3333	gate_accuracy: 64.0000
	Task-22	val_accuracy: 74.1573	gate_accuracy: 71.9101
	Task-23	val_accuracy: 56.0000	gate_accuracy: 56.0000
	Task-24	val_accuracy: 75.6410	gate_accuracy: 71.7949
	Task-25	val_accuracy: 73.4043	gate_accuracy: 67.0213
	Task-26	val_accuracy: 79.2208	gate_accuracy: 71.4286
	Task-27	val_accuracy: 72.4138	gate_accuracy: 68.9655
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.9284


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 3.0989	step1_train_accuracy: 48.0645
STEP-1	Epoch: 20/50	loss: 1.0951	step1_train_accuracy: 75.1613
STEP-1	Epoch: 30/50	loss: 0.4528	step1_train_accuracy: 94.8387
STEP-1	Epoch: 40/50	loss: 0.2690	step1_train_accuracy: 98.0645
STEP-1	Epoch: 50/50	loss: 0.1793	step1_train_accuracy: 98.3871
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8693	gate_loss: 2.4966	step2_classification_accuracy: 73.5859	step_2_gate_accuracy: 31.0774
STEP-2	Epoch: 40/200	classification_loss: 0.6694	gate_loss: 1.0541	step2_classification_accuracy: 79.9158	step_2_gate_accuracy: 70.2694
STEP-2	Epoch: 60/200	classification_loss: 0.5080	gate_loss: 0.5996	step2_classification_accuracy: 84.5286	step_2_gate_accuracy: 82.2896
STEP-2	Epoch: 80/200	classification_loss: 0.4187	gate_loss: 0.4200	step2_classification_accuracy: 87.3064	step_2_gate_accuracy: 87.0370
STEP-2	Epoch: 100/200	classification_loss: 0.3625	gate_loss: 0.3330	step2_classification_accuracy: 88.6532	step_2_gate_accuracy: 89.5960
STEP-2	Epoch: 120/200	classification_loss: 0.3244	gate_loss: 0.2735	step2_classification_accuracy: 90.0000	step_2_gate_accuracy: 91.7845
STEP-2	Epoch: 140/200	classification_loss: 0.2979	gate_loss: 0.2371	step2_classification_accuracy: 90.5724	step_2_gate_accuracy: 92.9798
STEP-2	Epoch: 160/200	classification_loss: 0.2681	gate_loss: 0.2065	step2_classification_accuracy: 91.4478	step_2_gate_accuracy: 93.3333
STEP-2	Epoch: 180/200	classification_loss: 0.2578	gate_loss: 0.1867	step2_classification_accuracy: 91.6498	step_2_gate_accuracy: 94.1582
STEP-2	Epoch: 200/200	classification_loss: 0.2419	gate_loss: 0.1755	step2_classification_accuracy: 92.1044	step_2_gate_accuracy: 94.7643
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.0604	gate_accuracy: 76.5101
	Task-1	val_accuracy: 59.7826	gate_accuracy: 64.1304
	Task-2	val_accuracy: 63.3333	gate_accuracy: 73.3333
	Task-3	val_accuracy: 58.4416	gate_accuracy: 53.2468
	Task-4	val_accuracy: 60.4938	gate_accuracy: 71.6049
	Task-5	val_accuracy: 65.4206	gate_accuracy: 68.2243
	Task-6	val_accuracy: 70.7865	gate_accuracy: 76.4045
	Task-7	val_accuracy: 61.1940	gate_accuracy: 64.1791
	Task-8	val_accuracy: 69.2308	gate_accuracy: 70.5128
	Task-9	val_accuracy: 55.2239	gate_accuracy: 52.2388
	Task-10	val_accuracy: 59.0909	gate_accuracy: 63.6364
	Task-11	val_accuracy: 81.0811	gate_accuracy: 72.9730
	Task-12	val_accuracy: 76.3158	gate_accuracy: 65.7895
	Task-13	val_accuracy: 64.7727	gate_accuracy: 60.2273
	Task-14	val_accuracy: 80.2326	gate_accuracy: 70.9302
	Task-15	val_accuracy: 76.3441	gate_accuracy: 79.5699
	Task-16	val_accuracy: 83.5294	gate_accuracy: 77.6471
	Task-17	val_accuracy: 71.6418	gate_accuracy: 65.6716
	Task-18	val_accuracy: 56.2500	gate_accuracy: 53.7500
	Task-19	val_accuracy: 50.6494	gate_accuracy: 48.0519
	Task-20	val_accuracy: 48.8095	gate_accuracy: 45.2381
	Task-21	val_accuracy: 70.6667	gate_accuracy: 70.6667
	Task-22	val_accuracy: 71.9101	gate_accuracy: 69.6629
	Task-23	val_accuracy: 57.3333	gate_accuracy: 57.3333
	Task-24	val_accuracy: 70.5128	gate_accuracy: 65.3846
	Task-25	val_accuracy: 69.1489	gate_accuracy: 67.0213
	Task-26	val_accuracy: 72.7273	gate_accuracy: 71.4286
	Task-27	val_accuracy: 78.1609	gate_accuracy: 80.4598
	Task-28	val_accuracy: 83.1169	gate_accuracy: 79.2208
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.2990


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.8055	step1_train_accuracy: 57.0909
STEP-1	Epoch: 20/50	loss: 1.0989	step1_train_accuracy: 80.3636
STEP-1	Epoch: 30/50	loss: 0.6283	step1_train_accuracy: 85.8182
STEP-1	Epoch: 40/50	loss: 0.4424	step1_train_accuracy: 88.3636
STEP-1	Epoch: 50/50	loss: 0.3411	step1_train_accuracy: 93.4545
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8742	gate_loss: 2.5946	step2_classification_accuracy: 73.1433	step_2_gate_accuracy: 28.1596
STEP-2	Epoch: 40/200	classification_loss: 0.6644	gate_loss: 1.0814	step2_classification_accuracy: 79.9837	step_2_gate_accuracy: 70.1303
STEP-2	Epoch: 60/200	classification_loss: 0.5154	gate_loss: 0.6173	step2_classification_accuracy: 84.1205	step_2_gate_accuracy: 82.1173
STEP-2	Epoch: 80/200	classification_loss: 0.4249	gate_loss: 0.4343	step2_classification_accuracy: 86.7590	step_2_gate_accuracy: 86.9381
STEP-2	Epoch: 100/200	classification_loss: 0.3631	gate_loss: 0.3333	step2_classification_accuracy: 88.5993	step_2_gate_accuracy: 89.9837
STEP-2	Epoch: 120/200	classification_loss: 0.3222	gate_loss: 0.2738	step2_classification_accuracy: 90.0326	step_2_gate_accuracy: 91.6938
STEP-2	Epoch: 140/200	classification_loss: 0.2898	gate_loss: 0.2358	step2_classification_accuracy: 90.6840	step_2_gate_accuracy: 92.7362
STEP-2	Epoch: 160/200	classification_loss: 0.2835	gate_loss: 0.2187	step2_classification_accuracy: 90.7980	step_2_gate_accuracy: 93.2573
STEP-2	Epoch: 180/200	classification_loss: 0.2543	gate_loss: 0.1934	step2_classification_accuracy: 91.6612	step_2_gate_accuracy: 93.8111
STEP-2	Epoch: 200/200	classification_loss: 0.2392	gate_loss: 0.1764	step2_classification_accuracy: 92.0195	step_2_gate_accuracy: 94.4951
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 48.9933	gate_accuracy: 63.0872
	Task-1	val_accuracy: 61.9565	gate_accuracy: 70.6522
	Task-2	val_accuracy: 57.7778	gate_accuracy: 70.0000
	Task-3	val_accuracy: 49.3506	gate_accuracy: 50.6494
	Task-4	val_accuracy: 62.9630	gate_accuracy: 67.9012
	Task-5	val_accuracy: 63.5514	gate_accuracy: 67.2897
	Task-6	val_accuracy: 65.1685	gate_accuracy: 69.6629
	Task-7	val_accuracy: 59.7015	gate_accuracy: 64.1791
	Task-8	val_accuracy: 71.7949	gate_accuracy: 73.0769
	Task-9	val_accuracy: 59.7015	gate_accuracy: 50.7463
	Task-10	val_accuracy: 54.5455	gate_accuracy: 48.4848
	Task-11	val_accuracy: 75.6757	gate_accuracy: 71.6216
	Task-12	val_accuracy: 78.9474	gate_accuracy: 77.6316
	Task-13	val_accuracy: 70.4545	gate_accuracy: 64.7727
	Task-14	val_accuracy: 87.2093	gate_accuracy: 81.3953
	Task-15	val_accuracy: 77.4194	gate_accuracy: 74.1935
	Task-16	val_accuracy: 80.0000	gate_accuracy: 75.2941
	Task-17	val_accuracy: 67.1642	gate_accuracy: 65.6716
	Task-18	val_accuracy: 62.5000	gate_accuracy: 55.0000
	Task-19	val_accuracy: 48.0519	gate_accuracy: 42.8571
	Task-20	val_accuracy: 50.0000	gate_accuracy: 46.4286
	Task-21	val_accuracy: 57.3333	gate_accuracy: 56.0000
	Task-22	val_accuracy: 73.0337	gate_accuracy: 66.2921
	Task-23	val_accuracy: 53.3333	gate_accuracy: 49.3333
	Task-24	val_accuracy: 62.8205	gate_accuracy: 62.8205
	Task-25	val_accuracy: 62.7660	gate_accuracy: 56.3830
	Task-26	val_accuracy: 71.4286	gate_accuracy: 66.2338
	Task-27	val_accuracy: 72.4138	gate_accuracy: 72.4138
	Task-28	val_accuracy: 77.9221	gate_accuracy: 74.0260
	Task-29	val_accuracy: 68.1159	gate_accuracy: 73.9130
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.5549


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 3.2476	step1_train_accuracy: 46.6443
STEP-1	Epoch: 20/50	loss: 1.1825	step1_train_accuracy: 75.8389
STEP-1	Epoch: 30/50	loss: 0.5979	step1_train_accuracy: 91.2752
STEP-1	Epoch: 40/50	loss: 0.3879	step1_train_accuracy: 96.3087
STEP-1	Epoch: 50/50	loss: 0.2728	step1_train_accuracy: 97.3154
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9041	gate_loss: 2.5822	step2_classification_accuracy: 72.4448	step_2_gate_accuracy: 27.9495
STEP-2	Epoch: 40/200	classification_loss: 0.6901	gate_loss: 1.0918	step2_classification_accuracy: 78.9590	step_2_gate_accuracy: 68.6751
STEP-2	Epoch: 60/200	classification_loss: 0.5302	gate_loss: 0.6200	step2_classification_accuracy: 84.0536	step_2_gate_accuracy: 81.7981
STEP-2	Epoch: 80/200	classification_loss: 0.4497	gate_loss: 0.4497	step2_classification_accuracy: 86.4038	step_2_gate_accuracy: 86.6246
STEP-2	Epoch: 100/200	classification_loss: 0.3759	gate_loss: 0.3436	step2_classification_accuracy: 88.4858	step_2_gate_accuracy: 89.8423
STEP-2	Epoch: 120/200	classification_loss: 0.3425	gate_loss: 0.2922	step2_classification_accuracy: 89.3533	step_2_gate_accuracy: 90.9622
STEP-2	Epoch: 140/200	classification_loss: 0.3186	gate_loss: 0.2539	step2_classification_accuracy: 90.1577	step_2_gate_accuracy: 92.0662
STEP-2	Epoch: 160/200	classification_loss: 0.2983	gate_loss: 0.2286	step2_classification_accuracy: 90.8517	step_2_gate_accuracy: 92.7760
STEP-2	Epoch: 180/200	classification_loss: 0.2626	gate_loss: 0.1966	step2_classification_accuracy: 91.6719	step_2_gate_accuracy: 93.8959
STEP-2	Epoch: 200/200	classification_loss: 0.2577	gate_loss: 0.1854	step2_classification_accuracy: 91.5931	step_2_gate_accuracy: 94.1956
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.4027	gate_accuracy: 73.1544
	Task-1	val_accuracy: 58.6957	gate_accuracy: 65.2174
	Task-2	val_accuracy: 67.7778	gate_accuracy: 77.7778
	Task-3	val_accuracy: 49.3506	gate_accuracy: 50.6494
	Task-4	val_accuracy: 53.0864	gate_accuracy: 65.4321
	Task-5	val_accuracy: 65.4206	gate_accuracy: 66.3551
	Task-6	val_accuracy: 69.6629	gate_accuracy: 71.9101
	Task-7	val_accuracy: 61.1940	gate_accuracy: 59.7015
	Task-8	val_accuracy: 73.0769	gate_accuracy: 76.9231
	Task-9	val_accuracy: 56.7164	gate_accuracy: 53.7313
	Task-10	val_accuracy: 59.0909	gate_accuracy: 69.6970
	Task-11	val_accuracy: 79.7297	gate_accuracy: 68.9189
	Task-12	val_accuracy: 72.3684	gate_accuracy: 65.7895
	Task-13	val_accuracy: 61.3636	gate_accuracy: 60.2273
	Task-14	val_accuracy: 67.4419	gate_accuracy: 58.1395
	Task-15	val_accuracy: 74.1935	gate_accuracy: 69.8925
	Task-16	val_accuracy: 84.7059	gate_accuracy: 80.0000
	Task-17	val_accuracy: 79.1045	gate_accuracy: 74.6269
	Task-18	val_accuracy: 58.7500	gate_accuracy: 52.5000
	Task-19	val_accuracy: 49.3506	gate_accuracy: 44.1558
	Task-20	val_accuracy: 57.1429	gate_accuracy: 52.3810
	Task-21	val_accuracy: 62.6667	gate_accuracy: 62.6667
	Task-22	val_accuracy: 74.1573	gate_accuracy: 58.4270
	Task-23	val_accuracy: 61.3333	gate_accuracy: 54.6667
	Task-24	val_accuracy: 65.3846	gate_accuracy: 65.3846
	Task-25	val_accuracy: 68.0851	gate_accuracy: 64.8936
	Task-26	val_accuracy: 63.6364	gate_accuracy: 59.7403
	Task-27	val_accuracy: 74.7126	gate_accuracy: 73.5632
	Task-28	val_accuracy: 79.2208	gate_accuracy: 77.9221
	Task-29	val_accuracy: 75.3623	gate_accuracy: 75.3623
	Task-30	val_accuracy: 66.6667	gate_accuracy: 65.3333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.3172


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 3.4275	step1_train_accuracy: 54.6763
STEP-1	Epoch: 20/50	loss: 1.5615	step1_train_accuracy: 61.5108
STEP-1	Epoch: 30/50	loss: 0.7240	step1_train_accuracy: 90.2878
STEP-1	Epoch: 40/50	loss: 0.4149	step1_train_accuracy: 97.4820
STEP-1	Epoch: 50/50	loss: 0.2770	step1_train_accuracy: 98.5611
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9374	gate_loss: 2.5212	step2_classification_accuracy: 71.4526	step_2_gate_accuracy: 31.9725
STEP-2	Epoch: 40/200	classification_loss: 0.7300	gate_loss: 1.0727	step2_classification_accuracy: 77.7064	step_2_gate_accuracy: 68.6697
STEP-2	Epoch: 60/200	classification_loss: 0.5563	gate_loss: 0.6338	step2_classification_accuracy: 83.1040	step_2_gate_accuracy: 80.9480
STEP-2	Epoch: 80/200	classification_loss: 0.4647	gate_loss: 0.4602	step2_classification_accuracy: 85.8563	step_2_gate_accuracy: 85.7492
STEP-2	Epoch: 100/200	classification_loss: 0.3969	gate_loss: 0.3606	step2_classification_accuracy: 87.8135	step_2_gate_accuracy: 88.7768
STEP-2	Epoch: 120/200	classification_loss: 0.3515	gate_loss: 0.2983	step2_classification_accuracy: 89.2661	step_2_gate_accuracy: 90.4434
STEP-2	Epoch: 140/200	classification_loss: 0.3278	gate_loss: 0.2612	step2_classification_accuracy: 89.6330	step_2_gate_accuracy: 91.7890
STEP-2	Epoch: 160/200	classification_loss: 0.3032	gate_loss: 0.2310	step2_classification_accuracy: 90.5199	step_2_gate_accuracy: 92.5841
STEP-2	Epoch: 180/200	classification_loss: 0.2760	gate_loss: 0.2070	step2_classification_accuracy: 91.3303	step_2_gate_accuracy: 93.4404
STEP-2	Epoch: 200/200	classification_loss: 0.2593	gate_loss: 0.1876	step2_classification_accuracy: 91.4985	step_2_gate_accuracy: 93.8991
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 54.3624	gate_accuracy: 72.4832
	Task-1	val_accuracy: 65.2174	gate_accuracy: 72.8261
	Task-2	val_accuracy: 58.8889	gate_accuracy: 66.6667
	Task-3	val_accuracy: 53.2468	gate_accuracy: 55.8442
	Task-4	val_accuracy: 54.3210	gate_accuracy: 66.6667
	Task-5	val_accuracy: 62.6168	gate_accuracy: 65.4206
	Task-6	val_accuracy: 74.1573	gate_accuracy: 74.1573
	Task-7	val_accuracy: 55.2239	gate_accuracy: 61.1940
	Task-8	val_accuracy: 67.9487	gate_accuracy: 66.6667
	Task-9	val_accuracy: 53.7313	gate_accuracy: 49.2537
	Task-10	val_accuracy: 54.5455	gate_accuracy: 66.6667
	Task-11	val_accuracy: 79.7297	gate_accuracy: 75.6757
	Task-12	val_accuracy: 77.6316	gate_accuracy: 72.3684
	Task-13	val_accuracy: 68.1818	gate_accuracy: 59.0909
	Task-14	val_accuracy: 76.7442	gate_accuracy: 67.4419
	Task-15	val_accuracy: 76.3441	gate_accuracy: 72.0430
	Task-16	val_accuracy: 81.1765	gate_accuracy: 74.1176
	Task-17	val_accuracy: 65.6716	gate_accuracy: 62.6866
	Task-18	val_accuracy: 57.5000	gate_accuracy: 47.5000
	Task-19	val_accuracy: 45.4545	gate_accuracy: 41.5584
	Task-20	val_accuracy: 57.1429	gate_accuracy: 60.7143
	Task-21	val_accuracy: 57.3333	gate_accuracy: 54.6667
	Task-22	val_accuracy: 68.5393	gate_accuracy: 62.9213
	Task-23	val_accuracy: 61.3333	gate_accuracy: 64.0000
	Task-24	val_accuracy: 70.5128	gate_accuracy: 65.3846
	Task-25	val_accuracy: 67.0213	gate_accuracy: 67.0213
	Task-26	val_accuracy: 74.0260	gate_accuracy: 66.2338
	Task-27	val_accuracy: 68.9655	gate_accuracy: 71.2644
	Task-28	val_accuracy: 79.2208	gate_accuracy: 72.7273
	Task-29	val_accuracy: 59.4203	gate_accuracy: 63.7681
	Task-30	val_accuracy: 78.6667	gate_accuracy: 66.6667
	Task-31	val_accuracy: 63.7681	gate_accuracy: 59.4203
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.0114


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 2.1711	step1_train_accuracy: 46.1318
STEP-1	Epoch: 20/50	loss: 0.8082	step1_train_accuracy: 81.3754
STEP-1	Epoch: 30/50	loss: 0.5060	step1_train_accuracy: 87.1060
STEP-1	Epoch: 40/50	loss: 0.3786	step1_train_accuracy: 89.1117
STEP-1	Epoch: 50/50	loss: 0.3230	step1_train_accuracy: 88.8252
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9047	gate_loss: 2.6674	step2_classification_accuracy: 72.3739	step_2_gate_accuracy: 28.9318
STEP-2	Epoch: 40/200	classification_loss: 0.7129	gate_loss: 1.1238	step2_classification_accuracy: 78.4125	step_2_gate_accuracy: 67.8932
STEP-2	Epoch: 60/200	classification_loss: 0.5560	gate_loss: 0.6549	step2_classification_accuracy: 82.9970	step_2_gate_accuracy: 80.3116
STEP-2	Epoch: 80/200	classification_loss: 0.4571	gate_loss: 0.4705	step2_classification_accuracy: 85.9792	step_2_gate_accuracy: 85.7863
STEP-2	Epoch: 100/200	classification_loss: 0.3913	gate_loss: 0.3622	step2_classification_accuracy: 87.9080	step_2_gate_accuracy: 89.2433
STEP-2	Epoch: 120/200	classification_loss: 0.3505	gate_loss: 0.3012	step2_classification_accuracy: 89.0504	step_2_gate_accuracy: 91.1128
STEP-2	Epoch: 140/200	classification_loss: 0.3109	gate_loss: 0.2516	step2_classification_accuracy: 90.2671	step_2_gate_accuracy: 92.1365
STEP-2	Epoch: 160/200	classification_loss: 0.3059	gate_loss: 0.2401	step2_classification_accuracy: 90.4451	step_2_gate_accuracy: 92.4036
STEP-2	Epoch: 180/200	classification_loss: 0.2858	gate_loss: 0.2169	step2_classification_accuracy: 90.9644	step_2_gate_accuracy: 93.3086
STEP-2	Epoch: 200/200	classification_loss: 0.2594	gate_loss: 0.1891	step2_classification_accuracy: 91.7656	step_2_gate_accuracy: 94.2137
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.3893	gate_accuracy: 66.4430
	Task-1	val_accuracy: 60.8696	gate_accuracy: 70.6522
	Task-2	val_accuracy: 57.7778	gate_accuracy: 64.4444
	Task-3	val_accuracy: 46.7532	gate_accuracy: 55.8442
	Task-4	val_accuracy: 59.2593	gate_accuracy: 71.6049
	Task-5	val_accuracy: 55.1402	gate_accuracy: 57.9439
	Task-6	val_accuracy: 61.7978	gate_accuracy: 62.9213
	Task-7	val_accuracy: 53.7313	gate_accuracy: 58.2090
	Task-8	val_accuracy: 74.3590	gate_accuracy: 69.2308
	Task-9	val_accuracy: 52.2388	gate_accuracy: 47.7612
	Task-10	val_accuracy: 54.5455	gate_accuracy: 54.5455
	Task-11	val_accuracy: 75.6757	gate_accuracy: 75.6757
	Task-12	val_accuracy: 77.6316	gate_accuracy: 67.1053
	Task-13	val_accuracy: 67.0455	gate_accuracy: 64.7727
	Task-14	val_accuracy: 62.7907	gate_accuracy: 56.9767
	Task-15	val_accuracy: 75.2688	gate_accuracy: 76.3441
	Task-16	val_accuracy: 82.3529	gate_accuracy: 77.6471
	Task-17	val_accuracy: 65.6716	gate_accuracy: 64.1791
	Task-18	val_accuracy: 58.7500	gate_accuracy: 47.5000
	Task-19	val_accuracy: 44.1558	gate_accuracy: 50.6494
	Task-20	val_accuracy: 45.2381	gate_accuracy: 41.6667
	Task-21	val_accuracy: 69.3333	gate_accuracy: 64.0000
	Task-22	val_accuracy: 67.4157	gate_accuracy: 57.3034
	Task-23	val_accuracy: 64.0000	gate_accuracy: 57.3333
	Task-24	val_accuracy: 67.9487	gate_accuracy: 58.9744
	Task-25	val_accuracy: 67.0213	gate_accuracy: 64.8936
	Task-26	val_accuracy: 64.9351	gate_accuracy: 66.2338
	Task-27	val_accuracy: 68.9655	gate_accuracy: 66.6667
	Task-28	val_accuracy: 79.2208	gate_accuracy: 76.6234
	Task-29	val_accuracy: 72.4638	gate_accuracy: 69.5652
	Task-30	val_accuracy: 69.3333	gate_accuracy: 68.0000
	Task-31	val_accuracy: 59.4203	gate_accuracy: 59.4203
	Task-32	val_accuracy: 62.0690	gate_accuracy: 62.0690
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.0459


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.3025	step1_train_accuracy: 56.9364
STEP-1	Epoch: 20/50	loss: 0.8965	step1_train_accuracy: 78.6127
STEP-1	Epoch: 30/50	loss: 0.4349	step1_train_accuracy: 95.6647
STEP-1	Epoch: 40/50	loss: 0.2604	step1_train_accuracy: 98.5549
STEP-1	Epoch: 50/50	loss: 0.1837	step1_train_accuracy: 98.5549
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9637	gate_loss: 2.6496	step2_classification_accuracy: 70.4611	step_2_gate_accuracy: 29.5821
STEP-2	Epoch: 40/200	classification_loss: 0.7577	gate_loss: 1.1480	step2_classification_accuracy: 76.4265	step_2_gate_accuracy: 67.0317
STEP-2	Epoch: 60/200	classification_loss: 0.5904	gate_loss: 0.6848	step2_classification_accuracy: 81.7291	step_2_gate_accuracy: 79.5245
STEP-2	Epoch: 80/200	classification_loss: 0.4986	gate_loss: 0.5048	step2_classification_accuracy: 84.5389	step_2_gate_accuracy: 84.6542
STEP-2	Epoch: 100/200	classification_loss: 0.4258	gate_loss: 0.3919	step2_classification_accuracy: 87.0029	step_2_gate_accuracy: 87.8098
STEP-2	Epoch: 120/200	classification_loss: 0.3780	gate_loss: 0.3310	step2_classification_accuracy: 88.2133	step_2_gate_accuracy: 89.8559
STEP-2	Epoch: 140/200	classification_loss: 0.3497	gate_loss: 0.2900	step2_classification_accuracy: 88.9769	step_2_gate_accuracy: 90.6772
STEP-2	Epoch: 160/200	classification_loss: 0.3202	gate_loss: 0.2543	step2_classification_accuracy: 89.7983	step_2_gate_accuracy: 92.0461
STEP-2	Epoch: 180/200	classification_loss: 0.3015	gate_loss: 0.2307	step2_classification_accuracy: 90.3458	step_2_gate_accuracy: 92.7666
STEP-2	Epoch: 200/200	classification_loss: 0.2861	gate_loss: 0.2112	step2_classification_accuracy: 90.8790	step_2_gate_accuracy: 93.2133
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.0201	gate_accuracy: 72.4832
	Task-1	val_accuracy: 59.7826	gate_accuracy: 64.1304
	Task-2	val_accuracy: 48.8889	gate_accuracy: 57.7778
	Task-3	val_accuracy: 48.0519	gate_accuracy: 46.7532
	Task-4	val_accuracy: 56.7901	gate_accuracy: 65.4321
	Task-5	val_accuracy: 68.2243	gate_accuracy: 71.0280
	Task-6	val_accuracy: 73.0337	gate_accuracy: 73.0337
	Task-7	val_accuracy: 58.2090	gate_accuracy: 62.6866
	Task-8	val_accuracy: 75.6410	gate_accuracy: 70.5128
	Task-9	val_accuracy: 55.2239	gate_accuracy: 52.2388
	Task-10	val_accuracy: 46.9697	gate_accuracy: 51.5152
	Task-11	val_accuracy: 70.2703	gate_accuracy: 72.9730
	Task-12	val_accuracy: 61.8421	gate_accuracy: 55.2632
	Task-13	val_accuracy: 69.3182	gate_accuracy: 65.9091
	Task-14	val_accuracy: 73.2558	gate_accuracy: 69.7674
	Task-15	val_accuracy: 73.1183	gate_accuracy: 72.0430
	Task-16	val_accuracy: 84.7059	gate_accuracy: 85.8824
	Task-17	val_accuracy: 76.1194	gate_accuracy: 77.6119
	Task-18	val_accuracy: 61.2500	gate_accuracy: 56.2500
	Task-19	val_accuracy: 49.3506	gate_accuracy: 45.4545
	Task-20	val_accuracy: 50.0000	gate_accuracy: 46.4286
	Task-21	val_accuracy: 64.0000	gate_accuracy: 60.0000
	Task-22	val_accuracy: 69.6629	gate_accuracy: 60.6742
	Task-23	val_accuracy: 64.0000	gate_accuracy: 58.6667
	Task-24	val_accuracy: 62.8205	gate_accuracy: 53.8462
	Task-25	val_accuracy: 67.0213	gate_accuracy: 63.8298
	Task-26	val_accuracy: 70.1299	gate_accuracy: 68.8312
	Task-27	val_accuracy: 68.9655	gate_accuracy: 65.5172
	Task-28	val_accuracy: 79.2208	gate_accuracy: 77.9221
	Task-29	val_accuracy: 71.0145	gate_accuracy: 69.5652
	Task-30	val_accuracy: 68.0000	gate_accuracy: 64.0000
	Task-31	val_accuracy: 63.7681	gate_accuracy: 65.2174
	Task-32	val_accuracy: 57.4713	gate_accuracy: 49.4253
	Task-33	val_accuracy: 67.8161	gate_accuracy: 64.3678
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.8336


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 3.2474	step1_train_accuracy: 42.3333
STEP-1	Epoch: 20/50	loss: 1.0949	step1_train_accuracy: 81.6667
STEP-1	Epoch: 30/50	loss: 0.5177	step1_train_accuracy: 92.3333
STEP-1	Epoch: 40/50	loss: 0.3366	step1_train_accuracy: 95.0000
STEP-1	Epoch: 50/50	loss: 0.2502	step1_train_accuracy: 96.3333
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0071	gate_loss: 2.5546	step2_classification_accuracy: 70.0840	step_2_gate_accuracy: 30.5742
STEP-2	Epoch: 40/200	classification_loss: 0.7827	gate_loss: 1.0884	step2_classification_accuracy: 77.1709	step_2_gate_accuracy: 68.6555
STEP-2	Epoch: 60/200	classification_loss: 0.6154	gate_loss: 0.6674	step2_classification_accuracy: 81.6947	step_2_gate_accuracy: 79.9440
STEP-2	Epoch: 80/200	classification_loss: 0.4979	gate_loss: 0.4817	step2_classification_accuracy: 84.9860	step_2_gate_accuracy: 85.3922
STEP-2	Epoch: 100/200	classification_loss: 0.4370	gate_loss: 0.3928	step2_classification_accuracy: 86.7367	step_2_gate_accuracy: 87.7871
STEP-2	Epoch: 120/200	classification_loss: 0.3933	gate_loss: 0.3294	step2_classification_accuracy: 87.3529	step_2_gate_accuracy: 89.7479
STEP-2	Epoch: 140/200	classification_loss: 0.3582	gate_loss: 0.2891	step2_classification_accuracy: 88.7675	step_2_gate_accuracy: 90.9944
STEP-2	Epoch: 160/200	classification_loss: 0.3384	gate_loss: 0.2628	step2_classification_accuracy: 89.2157	step_2_gate_accuracy: 91.4006
STEP-2	Epoch: 180/200	classification_loss: 0.3145	gate_loss: 0.2348	step2_classification_accuracy: 89.9720	step_2_gate_accuracy: 92.7171
STEP-2	Epoch: 200/200	classification_loss: 0.3139	gate_loss: 0.2292	step2_classification_accuracy: 90.0280	step_2_gate_accuracy: 92.8291
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.7315	gate_accuracy: 77.8523
	Task-1	val_accuracy: 67.3913	gate_accuracy: 71.7391
	Task-2	val_accuracy: 65.5556	gate_accuracy: 70.0000
	Task-3	val_accuracy: 49.3506	gate_accuracy: 55.8442
	Task-4	val_accuracy: 51.8519	gate_accuracy: 58.0247
	Task-5	val_accuracy: 66.3551	gate_accuracy: 63.5514
	Task-6	val_accuracy: 68.5393	gate_accuracy: 68.5393
	Task-7	val_accuracy: 52.2388	gate_accuracy: 55.2239
	Task-8	val_accuracy: 65.3846	gate_accuracy: 62.8205
	Task-9	val_accuracy: 52.2388	gate_accuracy: 44.7761
	Task-10	val_accuracy: 57.5758	gate_accuracy: 62.1212
	Task-11	val_accuracy: 71.6216	gate_accuracy: 66.2162
	Task-12	val_accuracy: 68.4211	gate_accuracy: 67.1053
	Task-13	val_accuracy: 72.7273	gate_accuracy: 67.0455
	Task-14	val_accuracy: 80.2326	gate_accuracy: 69.7674
	Task-15	val_accuracy: 77.4194	gate_accuracy: 78.4946
	Task-16	val_accuracy: 78.8235	gate_accuracy: 75.2941
	Task-17	val_accuracy: 62.6866	gate_accuracy: 49.2537
	Task-18	val_accuracy: 66.2500	gate_accuracy: 60.0000
	Task-19	val_accuracy: 49.3506	gate_accuracy: 48.0519
	Task-20	val_accuracy: 45.2381	gate_accuracy: 40.4762
	Task-21	val_accuracy: 62.6667	gate_accuracy: 61.3333
	Task-22	val_accuracy: 69.6629	gate_accuracy: 60.6742
	Task-23	val_accuracy: 61.3333	gate_accuracy: 60.0000
	Task-24	val_accuracy: 65.3846	gate_accuracy: 60.2564
	Task-25	val_accuracy: 57.4468	gate_accuracy: 57.4468
	Task-26	val_accuracy: 54.5455	gate_accuracy: 54.5455
	Task-27	val_accuracy: 67.8161	gate_accuracy: 64.3678
	Task-28	val_accuracy: 81.8182	gate_accuracy: 79.2208
	Task-29	val_accuracy: 69.5652	gate_accuracy: 71.0145
	Task-30	val_accuracy: 65.3333	gate_accuracy: 61.3333
	Task-31	val_accuracy: 73.9130	gate_accuracy: 69.5652
	Task-32	val_accuracy: 58.6207	gate_accuracy: 60.9195
	Task-33	val_accuracy: 63.2184	gate_accuracy: 57.4713
	Task-34	val_accuracy: 60.0000	gate_accuracy: 46.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.8680


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 1.9846	step1_train_accuracy: 61.8497
STEP-1	Epoch: 20/50	loss: 0.6727	step1_train_accuracy: 89.3064
STEP-1	Epoch: 30/50	loss: 0.3405	step1_train_accuracy: 95.9538
STEP-1	Epoch: 40/50	loss: 0.2359	step1_train_accuracy: 95.3757
STEP-1	Epoch: 50/50	loss: 0.1743	step1_train_accuracy: 96.5318
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9879	gate_loss: 2.5885	step2_classification_accuracy: 69.8365	step_2_gate_accuracy: 30.5313
STEP-2	Epoch: 40/200	classification_loss: 0.7583	gate_loss: 1.0845	step2_classification_accuracy: 77.3161	step_2_gate_accuracy: 69.4550
STEP-2	Epoch: 60/200	classification_loss: 0.5937	gate_loss: 0.6617	step2_classification_accuracy: 81.9891	step_2_gate_accuracy: 79.7956
STEP-2	Epoch: 80/200	classification_loss: 0.4874	gate_loss: 0.4854	step2_classification_accuracy: 85.2044	step_2_gate_accuracy: 84.9864
STEP-2	Epoch: 100/200	classification_loss: 0.4331	gate_loss: 0.3943	step2_classification_accuracy: 86.5531	step_2_gate_accuracy: 87.8065
STEP-2	Epoch: 120/200	classification_loss: 0.3955	gate_loss: 0.3397	step2_classification_accuracy: 87.4251	step_2_gate_accuracy: 89.3188
STEP-2	Epoch: 140/200	classification_loss: 0.3549	gate_loss: 0.2904	step2_classification_accuracy: 88.5967	step_2_gate_accuracy: 90.9946
STEP-2	Epoch: 160/200	classification_loss: 0.3330	gate_loss: 0.2603	step2_classification_accuracy: 89.3869	step_2_gate_accuracy: 91.6485
STEP-2	Epoch: 180/200	classification_loss: 0.3157	gate_loss: 0.2407	step2_classification_accuracy: 89.7275	step_2_gate_accuracy: 92.5068
STEP-2	Epoch: 200/200	classification_loss: 0.2871	gate_loss: 0.2181	step2_classification_accuracy: 90.5313	step_2_gate_accuracy: 92.9973
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.6913	gate_accuracy: 69.1275
	Task-1	val_accuracy: 65.2174	gate_accuracy: 73.9130
	Task-2	val_accuracy: 61.1111	gate_accuracy: 64.4444
	Task-3	val_accuracy: 54.5455	gate_accuracy: 61.0390
	Task-4	val_accuracy: 58.0247	gate_accuracy: 66.6667
	Task-5	val_accuracy: 57.9439	gate_accuracy: 58.8785
	Task-6	val_accuracy: 64.0449	gate_accuracy: 66.2921
	Task-7	val_accuracy: 56.7164	gate_accuracy: 59.7015
	Task-8	val_accuracy: 69.2308	gate_accuracy: 67.9487
	Task-9	val_accuracy: 52.2388	gate_accuracy: 40.2985
	Task-10	val_accuracy: 51.5152	gate_accuracy: 54.5455
	Task-11	val_accuracy: 79.7297	gate_accuracy: 75.6757
	Task-12	val_accuracy: 77.6316	gate_accuracy: 67.1053
	Task-13	val_accuracy: 65.9091	gate_accuracy: 62.5000
	Task-14	val_accuracy: 74.4186	gate_accuracy: 68.6047
	Task-15	val_accuracy: 74.1935	gate_accuracy: 73.1183
	Task-16	val_accuracy: 83.5294	gate_accuracy: 76.4706
	Task-17	val_accuracy: 56.7164	gate_accuracy: 56.7164
	Task-18	val_accuracy: 62.5000	gate_accuracy: 57.5000
	Task-19	val_accuracy: 42.8571	gate_accuracy: 40.2597
	Task-20	val_accuracy: 48.8095	gate_accuracy: 46.4286
	Task-21	val_accuracy: 64.0000	gate_accuracy: 60.0000
	Task-22	val_accuracy: 73.0337	gate_accuracy: 66.2921
	Task-23	val_accuracy: 56.0000	gate_accuracy: 50.6667
	Task-24	val_accuracy: 58.9744	gate_accuracy: 53.8462
	Task-25	val_accuracy: 65.9574	gate_accuracy: 63.8298
	Task-26	val_accuracy: 66.2338	gate_accuracy: 64.9351
	Task-27	val_accuracy: 66.6667	gate_accuracy: 65.5172
	Task-28	val_accuracy: 81.8182	gate_accuracy: 79.2208
	Task-29	val_accuracy: 65.2174	gate_accuracy: 66.6667
	Task-30	val_accuracy: 70.6667	gate_accuracy: 65.3333
	Task-31	val_accuracy: 63.7681	gate_accuracy: 60.8696
	Task-32	val_accuracy: 65.5172	gate_accuracy: 66.6667
	Task-33	val_accuracy: 55.1724	gate_accuracy: 49.4253
	Task-34	val_accuracy: 61.3333	gate_accuracy: 60.0000
	Task-35	val_accuracy: 58.6207	gate_accuracy: 54.0230
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.4748


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 2.5002	step1_train_accuracy: 55.2023
STEP-1	Epoch: 20/50	loss: 0.8886	step1_train_accuracy: 81.2139
STEP-1	Epoch: 30/50	loss: 0.4374	step1_train_accuracy: 93.6416
STEP-1	Epoch: 40/50	loss: 0.2824	step1_train_accuracy: 94.5087
STEP-1	Epoch: 50/50	loss: 0.2150	step1_train_accuracy: 94.7977
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0259	gate_loss: 2.6361	step2_classification_accuracy: 69.2838	step_2_gate_accuracy: 28.2891
STEP-2	Epoch: 40/200	classification_loss: 0.8120	gate_loss: 1.1635	step2_classification_accuracy: 76.1804	step_2_gate_accuracy: 66.1141
STEP-2	Epoch: 60/200	classification_loss: 0.6366	gate_loss: 0.7142	step2_classification_accuracy: 81.4589	step_2_gate_accuracy: 78.6207
STEP-2	Epoch: 80/200	classification_loss: 0.5298	gate_loss: 0.5190	step2_classification_accuracy: 84.2573	step_2_gate_accuracy: 84.2440
STEP-2	Epoch: 100/200	classification_loss: 0.4557	gate_loss: 0.4180	step2_classification_accuracy: 86.2732	step_2_gate_accuracy: 86.9231
STEP-2	Epoch: 120/200	classification_loss: 0.4006	gate_loss: 0.3440	step2_classification_accuracy: 87.9841	step_2_gate_accuracy: 89.2838
STEP-2	Epoch: 140/200	classification_loss: 0.3711	gate_loss: 0.3011	step2_classification_accuracy: 88.7798	step_2_gate_accuracy: 90.5836
STEP-2	Epoch: 160/200	classification_loss: 0.3503	gate_loss: 0.2758	step2_classification_accuracy: 89.2308	step_2_gate_accuracy: 91.3528
STEP-2	Epoch: 180/200	classification_loss: 0.3129	gate_loss: 0.2400	step2_classification_accuracy: 90.2785	step_2_gate_accuracy: 92.3873
STEP-2	Epoch: 200/200	classification_loss: 0.3026	gate_loss: 0.2246	step2_classification_accuracy: 90.7427	step_2_gate_accuracy: 93.0637
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 46.9799	gate_accuracy: 63.0872
	Task-1	val_accuracy: 61.9565	gate_accuracy: 71.7391
	Task-2	val_accuracy: 63.3333	gate_accuracy: 70.0000
	Task-3	val_accuracy: 45.4545	gate_accuracy: 53.2468
	Task-4	val_accuracy: 59.2593	gate_accuracy: 64.1975
	Task-5	val_accuracy: 57.0093	gate_accuracy: 57.0093
	Task-6	val_accuracy: 66.2921	gate_accuracy: 67.4157
	Task-7	val_accuracy: 56.7164	gate_accuracy: 58.2090
	Task-8	val_accuracy: 62.8205	gate_accuracy: 62.8205
	Task-9	val_accuracy: 47.7612	gate_accuracy: 37.3134
	Task-10	val_accuracy: 51.5152	gate_accuracy: 63.6364
	Task-11	val_accuracy: 72.9730	gate_accuracy: 71.6216
	Task-12	val_accuracy: 63.1579	gate_accuracy: 51.3158
	Task-13	val_accuracy: 69.3182	gate_accuracy: 62.5000
	Task-14	val_accuracy: 77.9070	gate_accuracy: 66.2791
	Task-15	val_accuracy: 78.4946	gate_accuracy: 74.1935
	Task-16	val_accuracy: 78.8235	gate_accuracy: 72.9412
	Task-17	val_accuracy: 56.7164	gate_accuracy: 55.2239
	Task-18	val_accuracy: 56.2500	gate_accuracy: 50.0000
	Task-19	val_accuracy: 44.1558	gate_accuracy: 38.9610
	Task-20	val_accuracy: 45.2381	gate_accuracy: 45.2381
	Task-21	val_accuracy: 60.0000	gate_accuracy: 61.3333
	Task-22	val_accuracy: 73.0337	gate_accuracy: 65.1685
	Task-23	val_accuracy: 52.0000	gate_accuracy: 45.3333
	Task-24	val_accuracy: 64.1026	gate_accuracy: 62.8205
	Task-25	val_accuracy: 65.9574	gate_accuracy: 63.8298
	Task-26	val_accuracy: 71.4286	gate_accuracy: 70.1299
	Task-27	val_accuracy: 73.5632	gate_accuracy: 77.0115
	Task-28	val_accuracy: 76.6234	gate_accuracy: 72.7273
	Task-29	val_accuracy: 63.7681	gate_accuracy: 62.3188
	Task-30	val_accuracy: 65.3333	gate_accuracy: 60.0000
	Task-31	val_accuracy: 68.1159	gate_accuracy: 71.0145
	Task-32	val_accuracy: 66.6667	gate_accuracy: 63.2184
	Task-33	val_accuracy: 60.9195	gate_accuracy: 57.4713
	Task-34	val_accuracy: 54.6667	gate_accuracy: 52.0000
	Task-35	val_accuracy: 59.7701	gate_accuracy: 49.4253
	Task-36	val_accuracy: 69.7674	gate_accuracy: 60.4651
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.1765


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.2570	step1_train_accuracy: 52.8249
STEP-1	Epoch: 20/50	loss: 0.9662	step1_train_accuracy: 81.9209
STEP-1	Epoch: 30/50	loss: 0.5317	step1_train_accuracy: 90.6780
STEP-1	Epoch: 40/50	loss: 0.3723	step1_train_accuracy: 93.7853
STEP-1	Epoch: 50/50	loss: 0.2798	step1_train_accuracy: 94.6328
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0201	gate_loss: 2.8196	step2_classification_accuracy: 68.8889	step_2_gate_accuracy: 24.6899
STEP-2	Epoch: 40/200	classification_loss: 0.8079	gate_loss: 1.2067	step2_classification_accuracy: 75.7106	step_2_gate_accuracy: 65.7881
STEP-2	Epoch: 60/200	classification_loss: 0.6319	gate_loss: 0.7171	step2_classification_accuracy: 81.3824	step_2_gate_accuracy: 78.6305
STEP-2	Epoch: 80/200	classification_loss: 0.5246	gate_loss: 0.5269	step2_classification_accuracy: 84.3798	step_2_gate_accuracy: 83.9793
STEP-2	Epoch: 100/200	classification_loss: 0.4455	gate_loss: 0.4124	step2_classification_accuracy: 86.4987	step_2_gate_accuracy: 87.5323
STEP-2	Epoch: 120/200	classification_loss: 0.4108	gate_loss: 0.3510	step2_classification_accuracy: 87.2093	step_2_gate_accuracy: 89.2506
STEP-2	Epoch: 140/200	classification_loss: 0.3665	gate_loss: 0.3043	step2_classification_accuracy: 88.5659	step_2_gate_accuracy: 90.6460
STEP-2	Epoch: 160/200	classification_loss: 0.3403	gate_loss: 0.2704	step2_classification_accuracy: 89.1602	step_2_gate_accuracy: 91.5891
STEP-2	Epoch: 180/200	classification_loss: 0.3200	gate_loss: 0.2444	step2_classification_accuracy: 89.9096	step_2_gate_accuracy: 92.3385
STEP-2	Epoch: 200/200	classification_loss: 0.3099	gate_loss: 0.2343	step2_classification_accuracy: 90.4264	step_2_gate_accuracy: 92.6486
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 54.3624	gate_accuracy: 68.4564
	Task-1	val_accuracy: 64.1304	gate_accuracy: 71.7391
	Task-2	val_accuracy: 61.1111	gate_accuracy: 68.8889
	Task-3	val_accuracy: 41.5584	gate_accuracy: 50.6494
	Task-4	val_accuracy: 64.1975	gate_accuracy: 71.6049
	Task-5	val_accuracy: 56.0748	gate_accuracy: 59.8131
	Task-6	val_accuracy: 62.9213	gate_accuracy: 68.5393
	Task-7	val_accuracy: 55.2239	gate_accuracy: 64.1791
	Task-8	val_accuracy: 58.9744	gate_accuracy: 62.8205
	Task-9	val_accuracy: 43.2836	gate_accuracy: 34.3284
	Task-10	val_accuracy: 53.0303	gate_accuracy: 59.0909
	Task-11	val_accuracy: 72.9730	gate_accuracy: 68.9189
	Task-12	val_accuracy: 65.7895	gate_accuracy: 55.2632
	Task-13	val_accuracy: 63.6364	gate_accuracy: 62.5000
	Task-14	val_accuracy: 68.6047	gate_accuracy: 66.2791
	Task-15	val_accuracy: 72.0430	gate_accuracy: 72.0430
	Task-16	val_accuracy: 76.4706	gate_accuracy: 70.5882
	Task-17	val_accuracy: 58.2090	gate_accuracy: 52.2388
	Task-18	val_accuracy: 57.5000	gate_accuracy: 52.5000
	Task-19	val_accuracy: 42.8571	gate_accuracy: 37.6623
	Task-20	val_accuracy: 52.3810	gate_accuracy: 51.1905
	Task-21	val_accuracy: 62.6667	gate_accuracy: 65.3333
	Task-22	val_accuracy: 61.7978	gate_accuracy: 50.5618
	Task-23	val_accuracy: 57.3333	gate_accuracy: 57.3333
	Task-24	val_accuracy: 62.8205	gate_accuracy: 60.2564
	Task-25	val_accuracy: 63.8298	gate_accuracy: 61.7021
	Task-26	val_accuracy: 68.8312	gate_accuracy: 64.9351
	Task-27	val_accuracy: 71.2644	gate_accuracy: 70.1149
	Task-28	val_accuracy: 79.2208	gate_accuracy: 70.1299
	Task-29	val_accuracy: 59.4203	gate_accuracy: 68.1159
	Task-30	val_accuracy: 64.0000	gate_accuracy: 62.6667
	Task-31	val_accuracy: 55.0725	gate_accuracy: 52.1739
	Task-32	val_accuracy: 57.4713	gate_accuracy: 52.8736
	Task-33	val_accuracy: 67.8161	gate_accuracy: 59.7701
	Task-34	val_accuracy: 53.3333	gate_accuracy: 46.6667
	Task-35	val_accuracy: 56.3218	gate_accuracy: 49.4253
	Task-36	val_accuracy: 69.7674	gate_accuracy: 69.7674
	Task-37	val_accuracy: 77.2727	gate_accuracy: 75.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.1817


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 1.9325	step1_train_accuracy: 64.7696
STEP-1	Epoch: 20/50	loss: 0.6541	step1_train_accuracy: 86.7209
STEP-1	Epoch: 30/50	loss: 0.3269	step1_train_accuracy: 97.8320
STEP-1	Epoch: 40/50	loss: 0.1910	step1_train_accuracy: 98.9160
STEP-1	Epoch: 50/50	loss: 0.1269	step1_train_accuracy: 99.4580
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0198	gate_loss: 2.6670	step2_classification_accuracy: 69.0176	step_2_gate_accuracy: 28.3627
STEP-2	Epoch: 40/200	classification_loss: 0.7910	gate_loss: 1.1194	step2_classification_accuracy: 76.3854	step_2_gate_accuracy: 67.9471
STEP-2	Epoch: 60/200	classification_loss: 0.6159	gate_loss: 0.6854	step2_classification_accuracy: 81.4106	step_2_gate_accuracy: 79.5088
STEP-2	Epoch: 80/200	classification_loss: 0.4965	gate_loss: 0.4934	step2_classification_accuracy: 84.7733	step_2_gate_accuracy: 84.9244
STEP-2	Epoch: 100/200	classification_loss: 0.4370	gate_loss: 0.3962	step2_classification_accuracy: 86.3980	step_2_gate_accuracy: 87.7582
STEP-2	Epoch: 120/200	classification_loss: 0.3868	gate_loss: 0.3348	step2_classification_accuracy: 87.8086	step_2_gate_accuracy: 89.4962
STEP-2	Epoch: 140/200	classification_loss: 0.3591	gate_loss: 0.2999	step2_classification_accuracy: 88.5139	step_2_gate_accuracy: 90.7431
STEP-2	Epoch: 160/200	classification_loss: 0.3339	gate_loss: 0.2655	step2_classification_accuracy: 89.5088	step_2_gate_accuracy: 91.6373
STEP-2	Epoch: 180/200	classification_loss: 0.3144	gate_loss: 0.2447	step2_classification_accuracy: 89.7859	step_2_gate_accuracy: 92.4685
STEP-2	Epoch: 200/200	classification_loss: 0.2883	gate_loss: 0.2164	step2_classification_accuracy: 90.7305	step_2_gate_accuracy: 93.2494
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.3758	gate_accuracy: 69.1275
	Task-1	val_accuracy: 63.0435	gate_accuracy: 68.4783
	Task-2	val_accuracy: 48.8889	gate_accuracy: 57.7778
	Task-3	val_accuracy: 46.7532	gate_accuracy: 48.0519
	Task-4	val_accuracy: 59.2593	gate_accuracy: 67.9012
	Task-5	val_accuracy: 66.3551	gate_accuracy: 69.1589
	Task-6	val_accuracy: 60.6742	gate_accuracy: 69.6629
	Task-7	val_accuracy: 56.7164	gate_accuracy: 52.2388
	Task-8	val_accuracy: 58.9744	gate_accuracy: 65.3846
	Task-9	val_accuracy: 47.7612	gate_accuracy: 46.2687
	Task-10	val_accuracy: 56.0606	gate_accuracy: 65.1515
	Task-11	val_accuracy: 74.3243	gate_accuracy: 68.9189
	Task-12	val_accuracy: 65.7895	gate_accuracy: 57.8947
	Task-13	val_accuracy: 63.6364	gate_accuracy: 57.9545
	Task-14	val_accuracy: 70.9302	gate_accuracy: 63.9535
	Task-15	val_accuracy: 76.3441	gate_accuracy: 75.2688
	Task-16	val_accuracy: 75.2941	gate_accuracy: 67.0588
	Task-17	val_accuracy: 61.1940	gate_accuracy: 59.7015
	Task-18	val_accuracy: 61.2500	gate_accuracy: 57.5000
	Task-19	val_accuracy: 38.9610	gate_accuracy: 35.0649
	Task-20	val_accuracy: 57.1429	gate_accuracy: 52.3810
	Task-21	val_accuracy: 61.3333	gate_accuracy: 64.0000
	Task-22	val_accuracy: 61.7978	gate_accuracy: 59.5506
	Task-23	val_accuracy: 57.3333	gate_accuracy: 52.0000
	Task-24	val_accuracy: 62.8205	gate_accuracy: 61.5385
	Task-25	val_accuracy: 63.8298	gate_accuracy: 60.6383
	Task-26	val_accuracy: 67.5325	gate_accuracy: 64.9351
	Task-27	val_accuracy: 63.2184	gate_accuracy: 67.8161
	Task-28	val_accuracy: 81.8182	gate_accuracy: 77.9221
	Task-29	val_accuracy: 68.1159	gate_accuracy: 71.0145
	Task-30	val_accuracy: 72.0000	gate_accuracy: 72.0000
	Task-31	val_accuracy: 60.8696	gate_accuracy: 59.4203
	Task-32	val_accuracy: 59.7701	gate_accuracy: 59.7701
	Task-33	val_accuracy: 60.9195	gate_accuracy: 56.3218
	Task-34	val_accuracy: 58.6667	gate_accuracy: 50.6667
	Task-35	val_accuracy: 58.6207	gate_accuracy: 50.5747
	Task-36	val_accuracy: 65.1163	gate_accuracy: 61.6279
	Task-37	val_accuracy: 63.6364	gate_accuracy: 65.9091
	Task-38	val_accuracy: 78.2609	gate_accuracy: 75.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.0988


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 2.5207	step1_train_accuracy: 54.3662
STEP-1	Epoch: 20/50	loss: 0.8206	step1_train_accuracy: 80.5634
STEP-1	Epoch: 30/50	loss: 0.4260	step1_train_accuracy: 96.0563
STEP-1	Epoch: 40/50	loss: 0.2706	step1_train_accuracy: 98.3099
STEP-1	Epoch: 50/50	loss: 0.1886	step1_train_accuracy: 98.5916
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0422	gate_loss: 2.6715	step2_classification_accuracy: 68.8329	step_2_gate_accuracy: 28.9558
STEP-2	Epoch: 40/200	classification_loss: 0.8214	gate_loss: 1.1569	step2_classification_accuracy: 75.1474	step_2_gate_accuracy: 66.5356
STEP-2	Epoch: 60/200	classification_loss: 0.6367	gate_loss: 0.7108	step2_classification_accuracy: 80.7125	step_2_gate_accuracy: 78.0713
STEP-2	Epoch: 80/200	classification_loss: 0.5355	gate_loss: 0.5365	step2_classification_accuracy: 83.9681	step_2_gate_accuracy: 83.6732
STEP-2	Epoch: 100/200	classification_loss: 0.4588	gate_loss: 0.4274	step2_classification_accuracy: 85.6388	step_2_gate_accuracy: 86.6830
STEP-2	Epoch: 120/200	classification_loss: 0.4126	gate_loss: 0.3600	step2_classification_accuracy: 86.9656	step_2_gate_accuracy: 88.7592
STEP-2	Epoch: 140/200	classification_loss: 0.3735	gate_loss: 0.3116	step2_classification_accuracy: 88.6978	step_2_gate_accuracy: 90.3194
STEP-2	Epoch: 160/200	classification_loss: 0.3507	gate_loss: 0.2781	step2_classification_accuracy: 89.0295	step_2_gate_accuracy: 91.1425
STEP-2	Epoch: 180/200	classification_loss: 0.3239	gate_loss: 0.2525	step2_classification_accuracy: 89.9754	step_2_gate_accuracy: 92.0885
STEP-2	Epoch: 200/200	classification_loss: 0.3144	gate_loss: 0.2395	step2_classification_accuracy: 90.0614	step_2_gate_accuracy: 92.6904
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.0336	gate_accuracy: 68.4564
	Task-1	val_accuracy: 55.4348	gate_accuracy: 66.3043
	Task-2	val_accuracy: 63.3333	gate_accuracy: 71.1111
	Task-3	val_accuracy: 44.1558	gate_accuracy: 45.4545
	Task-4	val_accuracy: 56.7901	gate_accuracy: 69.1358
	Task-5	val_accuracy: 54.2056	gate_accuracy: 51.4019
	Task-6	val_accuracy: 64.0449	gate_accuracy: 60.6742
	Task-7	val_accuracy: 59.7015	gate_accuracy: 62.6866
	Task-8	val_accuracy: 65.3846	gate_accuracy: 64.1026
	Task-9	val_accuracy: 55.2239	gate_accuracy: 44.7761
	Task-10	val_accuracy: 46.9697	gate_accuracy: 46.9697
	Task-11	val_accuracy: 63.5135	gate_accuracy: 60.8108
	Task-12	val_accuracy: 71.0526	gate_accuracy: 57.8947
	Task-13	val_accuracy: 64.7727	gate_accuracy: 55.6818
	Task-14	val_accuracy: 63.9535	gate_accuracy: 50.0000
	Task-15	val_accuracy: 69.8925	gate_accuracy: 67.7419
	Task-16	val_accuracy: 81.1765	gate_accuracy: 76.4706
	Task-17	val_accuracy: 58.2090	gate_accuracy: 47.7612
	Task-18	val_accuracy: 51.2500	gate_accuracy: 37.5000
	Task-19	val_accuracy: 50.6494	gate_accuracy: 38.9610
	Task-20	val_accuracy: 50.0000	gate_accuracy: 54.7619
	Task-21	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-22	val_accuracy: 67.4157	gate_accuracy: 66.2921
	Task-23	val_accuracy: 50.6667	gate_accuracy: 45.3333
	Task-24	val_accuracy: 62.8205	gate_accuracy: 55.1282
	Task-25	val_accuracy: 68.0851	gate_accuracy: 62.7660
	Task-26	val_accuracy: 64.9351	gate_accuracy: 59.7403
	Task-27	val_accuracy: 66.6667	gate_accuracy: 71.2644
	Task-28	val_accuracy: 75.3247	gate_accuracy: 66.2338
	Task-29	val_accuracy: 59.4203	gate_accuracy: 57.9710
	Task-30	val_accuracy: 62.6667	gate_accuracy: 53.3333
	Task-31	val_accuracy: 63.7681	gate_accuracy: 60.8696
	Task-32	val_accuracy: 65.5172	gate_accuracy: 70.1149
	Task-33	val_accuracy: 65.5172	gate_accuracy: 63.2184
	Task-34	val_accuracy: 53.3333	gate_accuracy: 52.0000
	Task-35	val_accuracy: 42.5287	gate_accuracy: 40.2299
	Task-36	val_accuracy: 63.9535	gate_accuracy: 59.3023
	Task-37	val_accuracy: 72.7273	gate_accuracy: 71.5909
	Task-38	val_accuracy: 72.8261	gate_accuracy: 65.2174
	Task-39	val_accuracy: 60.6742	gate_accuracy: 52.8090
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 58.9967


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 2.7531	step1_train_accuracy: 53.7313
STEP-1	Epoch: 20/50	loss: 0.8796	step1_train_accuracy: 82.9851
STEP-1	Epoch: 30/50	loss: 0.4139	step1_train_accuracy: 94.3284
STEP-1	Epoch: 40/50	loss: 0.2749	step1_train_accuracy: 95.5224
STEP-1	Epoch: 50/50	loss: 0.2037	step1_train_accuracy: 95.8209
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1008	gate_loss: 2.6861	step2_classification_accuracy: 66.7746	step_2_gate_accuracy: 27.7698
STEP-2	Epoch: 40/200	classification_loss: 0.8820	gate_loss: 1.1793	step2_classification_accuracy: 73.5971	step_2_gate_accuracy: 65.4197
STEP-2	Epoch: 60/200	classification_loss: 0.7007	gate_loss: 0.7534	step2_classification_accuracy: 79.1727	step_2_gate_accuracy: 76.7746
STEP-2	Epoch: 80/200	classification_loss: 0.5840	gate_loss: 0.5713	step2_classification_accuracy: 82.0983	step_2_gate_accuracy: 82.1343
STEP-2	Epoch: 100/200	classification_loss: 0.5065	gate_loss: 0.4645	step2_classification_accuracy: 84.3165	step_2_gate_accuracy: 85.1679
STEP-2	Epoch: 120/200	classification_loss: 0.4518	gate_loss: 0.3890	step2_classification_accuracy: 86.1751	step_2_gate_accuracy: 87.5180
STEP-2	Epoch: 140/200	classification_loss: 0.4122	gate_loss: 0.3420	step2_classification_accuracy: 86.9784	step_2_gate_accuracy: 89.0168
STEP-2	Epoch: 160/200	classification_loss: 0.3758	gate_loss: 0.3013	step2_classification_accuracy: 87.9856	step_2_gate_accuracy: 89.8801
STEP-2	Epoch: 180/200	classification_loss: 0.3447	gate_loss: 0.2663	step2_classification_accuracy: 88.9568	step_2_gate_accuracy: 91.2230
STEP-2	Epoch: 200/200	classification_loss: 0.3307	gate_loss: 0.2519	step2_classification_accuracy: 89.2446	step_2_gate_accuracy: 91.8465
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 45.6376	gate_accuracy: 63.0872
	Task-1	val_accuracy: 57.6087	gate_accuracy: 65.2174
	Task-2	val_accuracy: 67.7778	gate_accuracy: 72.2222
	Task-3	val_accuracy: 46.7532	gate_accuracy: 49.3506
	Task-4	val_accuracy: 58.0247	gate_accuracy: 69.1358
	Task-5	val_accuracy: 59.8131	gate_accuracy: 53.2710
	Task-6	val_accuracy: 68.5393	gate_accuracy: 68.5393
	Task-7	val_accuracy: 59.7015	gate_accuracy: 64.1791
	Task-8	val_accuracy: 74.3590	gate_accuracy: 75.6410
	Task-9	val_accuracy: 37.3134	gate_accuracy: 35.8209
	Task-10	val_accuracy: 54.5455	gate_accuracy: 59.0909
	Task-11	val_accuracy: 72.9730	gate_accuracy: 70.2703
	Task-12	val_accuracy: 72.3684	gate_accuracy: 65.7895
	Task-13	val_accuracy: 69.3182	gate_accuracy: 67.0455
	Task-14	val_accuracy: 69.7674	gate_accuracy: 60.4651
	Task-15	val_accuracy: 75.2688	gate_accuracy: 70.9677
	Task-16	val_accuracy: 80.0000	gate_accuracy: 77.6471
	Task-17	val_accuracy: 59.7015	gate_accuracy: 52.2388
	Task-18	val_accuracy: 53.7500	gate_accuracy: 47.5000
	Task-19	val_accuracy: 46.7532	gate_accuracy: 49.3506
	Task-20	val_accuracy: 52.3810	gate_accuracy: 50.0000
	Task-21	val_accuracy: 65.3333	gate_accuracy: 66.6667
	Task-22	val_accuracy: 67.4157	gate_accuracy: 62.9213
	Task-23	val_accuracy: 50.6667	gate_accuracy: 45.3333
	Task-24	val_accuracy: 60.2564	gate_accuracy: 55.1282
	Task-25	val_accuracy: 56.3830	gate_accuracy: 51.0638
	Task-26	val_accuracy: 64.9351	gate_accuracy: 59.7403
	Task-27	val_accuracy: 64.3678	gate_accuracy: 63.2184
	Task-28	val_accuracy: 74.0260	gate_accuracy: 68.8312
	Task-29	val_accuracy: 66.6667	gate_accuracy: 71.0145
	Task-30	val_accuracy: 60.0000	gate_accuracy: 58.6667
	Task-31	val_accuracy: 56.5217	gate_accuracy: 56.5217
	Task-32	val_accuracy: 60.9195	gate_accuracy: 60.9195
	Task-33	val_accuracy: 63.2184	gate_accuracy: 55.1724
	Task-34	val_accuracy: 53.3333	gate_accuracy: 46.6667
	Task-35	val_accuracy: 55.1724	gate_accuracy: 47.1264
	Task-36	val_accuracy: 65.1163	gate_accuracy: 56.9767
	Task-37	val_accuracy: 69.3182	gate_accuracy: 70.4545
	Task-38	val_accuracy: 82.6087	gate_accuracy: 75.0000
	Task-39	val_accuracy: 65.1685	gate_accuracy: 58.4270
	Task-40	val_accuracy: 65.4762	gate_accuracy: 59.5238
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.6505


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 2.6504	step1_train_accuracy: 47.9751
STEP-1	Epoch: 20/50	loss: 0.9843	step1_train_accuracy: 81.9315
STEP-1	Epoch: 30/50	loss: 0.5476	step1_train_accuracy: 91.2773
STEP-1	Epoch: 40/50	loss: 0.3515	step1_train_accuracy: 94.3925
STEP-1	Epoch: 50/50	loss: 0.3760	step1_train_accuracy: 96.5732
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1001	gate_loss: 2.6594	step2_classification_accuracy: 67.2482	step_2_gate_accuracy: 29.6955
STEP-2	Epoch: 40/200	classification_loss: 0.8824	gate_loss: 1.1848	step2_classification_accuracy: 74.4028	step_2_gate_accuracy: 65.9719
STEP-2	Epoch: 60/200	classification_loss: 0.6958	gate_loss: 0.7556	step2_classification_accuracy: 79.4145	step_2_gate_accuracy: 76.8735
STEP-2	Epoch: 80/200	classification_loss: 0.5804	gate_loss: 0.5621	step2_classification_accuracy: 82.3770	step_2_gate_accuracy: 82.4707
STEP-2	Epoch: 100/200	classification_loss: 0.5081	gate_loss: 0.4566	step2_classification_accuracy: 84.3794	step_2_gate_accuracy: 85.7494
STEP-2	Epoch: 120/200	classification_loss: 0.4549	gate_loss: 0.3834	step2_classification_accuracy: 86.2881	step_2_gate_accuracy: 88.3255
STEP-2	Epoch: 140/200	classification_loss: 0.4181	gate_loss: 0.3416	step2_classification_accuracy: 87.0726	step_2_gate_accuracy: 89.2389
STEP-2	Epoch: 160/200	classification_loss: 0.3782	gate_loss: 0.2993	step2_classification_accuracy: 88.2904	step_2_gate_accuracy: 90.7143
STEP-2	Epoch: 180/200	classification_loss: 0.3586	gate_loss: 0.2765	step2_classification_accuracy: 88.7237	step_2_gate_accuracy: 91.0773
STEP-2	Epoch: 200/200	classification_loss: 0.3355	gate_loss: 0.2525	step2_classification_accuracy: 89.3443	step_2_gate_accuracy: 92.0609
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 49.6644	gate_accuracy: 65.1007
	Task-1	val_accuracy: 59.7826	gate_accuracy: 66.3043
	Task-2	val_accuracy: 60.0000	gate_accuracy: 63.3333
	Task-3	val_accuracy: 54.5455	gate_accuracy: 62.3377
	Task-4	val_accuracy: 56.7901	gate_accuracy: 66.6667
	Task-5	val_accuracy: 55.1402	gate_accuracy: 55.1402
	Task-6	val_accuracy: 71.9101	gate_accuracy: 74.1573
	Task-7	val_accuracy: 50.7463	gate_accuracy: 49.2537
	Task-8	val_accuracy: 64.1026	gate_accuracy: 61.5385
	Task-9	val_accuracy: 41.7910	gate_accuracy: 41.7910
	Task-10	val_accuracy: 53.0303	gate_accuracy: 59.0909
	Task-11	val_accuracy: 72.9730	gate_accuracy: 66.2162
	Task-12	val_accuracy: 55.2632	gate_accuracy: 48.6842
	Task-13	val_accuracy: 63.6364	gate_accuracy: 57.9545
	Task-14	val_accuracy: 68.6047	gate_accuracy: 56.9767
	Task-15	val_accuracy: 63.4409	gate_accuracy: 65.5914
	Task-16	val_accuracy: 84.7059	gate_accuracy: 81.1765
	Task-17	val_accuracy: 61.1940	gate_accuracy: 58.2090
	Task-18	val_accuracy: 55.0000	gate_accuracy: 50.0000
	Task-19	val_accuracy: 45.4545	gate_accuracy: 41.5584
	Task-20	val_accuracy: 46.4286	gate_accuracy: 47.6190
	Task-21	val_accuracy: 72.0000	gate_accuracy: 69.3333
	Task-22	val_accuracy: 67.4157	gate_accuracy: 57.3034
	Task-23	val_accuracy: 41.3333	gate_accuracy: 38.6667
	Task-24	val_accuracy: 67.9487	gate_accuracy: 62.8205
	Task-25	val_accuracy: 69.1489	gate_accuracy: 71.2766
	Task-26	val_accuracy: 67.5325	gate_accuracy: 63.6364
	Task-27	val_accuracy: 70.1149	gate_accuracy: 60.9195
	Task-28	val_accuracy: 76.6234	gate_accuracy: 77.9221
	Task-29	val_accuracy: 57.9710	gate_accuracy: 65.2174
	Task-30	val_accuracy: 61.3333	gate_accuracy: 61.3333
	Task-31	val_accuracy: 53.6232	gate_accuracy: 47.8261
	Task-32	val_accuracy: 71.2644	gate_accuracy: 68.9655
	Task-33	val_accuracy: 66.6667	gate_accuracy: 59.7701
	Task-34	val_accuracy: 48.0000	gate_accuracy: 42.6667
	Task-35	val_accuracy: 64.3678	gate_accuracy: 59.7701
	Task-36	val_accuracy: 62.7907	gate_accuracy: 66.2791
	Task-37	val_accuracy: 75.0000	gate_accuracy: 73.8636
	Task-38	val_accuracy: 71.7391	gate_accuracy: 70.6522
	Task-39	val_accuracy: 66.2921	gate_accuracy: 59.5506
	Task-40	val_accuracy: 58.3333	gate_accuracy: 52.3810
	Task-41	val_accuracy: 58.7500	gate_accuracy: 51.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.4638


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 3.4824	step1_train_accuracy: 34.8837
STEP-1	Epoch: 20/50	loss: 1.2058	step1_train_accuracy: 81.0078
STEP-1	Epoch: 30/50	loss: 0.6952	step1_train_accuracy: 92.2481
STEP-1	Epoch: 40/50	loss: 0.4625	step1_train_accuracy: 96.1240
STEP-1	Epoch: 50/50	loss: 0.3321	step1_train_accuracy: 98.8372
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1354	gate_loss: 2.6842	step2_classification_accuracy: 66.7391	step_2_gate_accuracy: 29.6110
STEP-2	Epoch: 40/200	classification_loss: 0.9116	gate_loss: 1.1834	step2_classification_accuracy: 73.2838	step_2_gate_accuracy: 65.5378
STEP-2	Epoch: 60/200	classification_loss: 0.7211	gate_loss: 0.7604	step2_classification_accuracy: 78.9474	step_2_gate_accuracy: 76.8764
STEP-2	Epoch: 80/200	classification_loss: 0.5967	gate_loss: 0.5669	step2_classification_accuracy: 82.7803	step_2_gate_accuracy: 82.6773
STEP-2	Epoch: 100/200	classification_loss: 0.5203	gate_loss: 0.4603	step2_classification_accuracy: 84.4622	step_2_gate_accuracy: 85.2517
STEP-2	Epoch: 120/200	classification_loss: 0.4703	gate_loss: 0.3935	step2_classification_accuracy: 86.1556	step_2_gate_accuracy: 87.6545
STEP-2	Epoch: 140/200	classification_loss: 0.4300	gate_loss: 0.3483	step2_classification_accuracy: 87.0481	step_2_gate_accuracy: 89.0046
STEP-2	Epoch: 160/200	classification_loss: 0.3860	gate_loss: 0.3035	step2_classification_accuracy: 88.1579	step_2_gate_accuracy: 90.6178
STEP-2	Epoch: 180/200	classification_loss: 0.3691	gate_loss: 0.2797	step2_classification_accuracy: 88.9130	step_2_gate_accuracy: 91.2243
STEP-2	Epoch: 200/200	classification_loss: 0.3392	gate_loss: 0.2538	step2_classification_accuracy: 89.3936	step_2_gate_accuracy: 92.0137
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 45.6376	gate_accuracy: 59.7315
	Task-1	val_accuracy: 55.4348	gate_accuracy: 66.3043
	Task-2	val_accuracy: 47.7778	gate_accuracy: 57.7778
	Task-3	val_accuracy: 35.0649	gate_accuracy: 33.7662
	Task-4	val_accuracy: 49.3827	gate_accuracy: 64.1975
	Task-5	val_accuracy: 62.6168	gate_accuracy: 64.4860
	Task-6	val_accuracy: 66.2921	gate_accuracy: 68.5393
	Task-7	val_accuracy: 53.7313	gate_accuracy: 53.7313
	Task-8	val_accuracy: 57.6923	gate_accuracy: 65.3846
	Task-9	val_accuracy: 40.2985	gate_accuracy: 35.8209
	Task-10	val_accuracy: 45.4545	gate_accuracy: 59.0909
	Task-11	val_accuracy: 62.1622	gate_accuracy: 60.8108
	Task-12	val_accuracy: 52.6316	gate_accuracy: 42.1053
	Task-13	val_accuracy: 67.0455	gate_accuracy: 67.0455
	Task-14	val_accuracy: 72.0930	gate_accuracy: 59.3023
	Task-15	val_accuracy: 78.4946	gate_accuracy: 77.4194
	Task-16	val_accuracy: 77.6471	gate_accuracy: 74.1176
	Task-17	val_accuracy: 59.7015	gate_accuracy: 56.7164
	Task-18	val_accuracy: 53.7500	gate_accuracy: 51.2500
	Task-19	val_accuracy: 42.8571	gate_accuracy: 35.0649
	Task-20	val_accuracy: 45.2381	gate_accuracy: 41.6667
	Task-21	val_accuracy: 73.3333	gate_accuracy: 70.6667
	Task-22	val_accuracy: 68.5393	gate_accuracy: 66.2921
	Task-23	val_accuracy: 53.3333	gate_accuracy: 54.6667
	Task-24	val_accuracy: 64.1026	gate_accuracy: 50.0000
	Task-25	val_accuracy: 64.8936	gate_accuracy: 61.7021
	Task-26	val_accuracy: 67.5325	gate_accuracy: 59.7403
	Task-27	val_accuracy: 59.7701	gate_accuracy: 60.9195
	Task-28	val_accuracy: 75.3247	gate_accuracy: 70.1299
	Task-29	val_accuracy: 57.9710	gate_accuracy: 65.2174
	Task-30	val_accuracy: 64.0000	gate_accuracy: 62.6667
	Task-31	val_accuracy: 55.0725	gate_accuracy: 59.4203
	Task-32	val_accuracy: 62.0690	gate_accuracy: 70.1149
	Task-33	val_accuracy: 67.8161	gate_accuracy: 64.3678
	Task-34	val_accuracy: 58.6667	gate_accuracy: 53.3333
	Task-35	val_accuracy: 58.6207	gate_accuracy: 48.2759
	Task-36	val_accuracy: 56.9767	gate_accuracy: 52.3256
	Task-37	val_accuracy: 73.8636	gate_accuracy: 71.5909
	Task-38	val_accuracy: 78.2609	gate_accuracy: 75.0000
	Task-39	val_accuracy: 62.9213	gate_accuracy: 51.6854
	Task-40	val_accuracy: 65.4762	gate_accuracy: 57.1429
	Task-41	val_accuracy: 58.7500	gate_accuracy: 60.0000
	Task-42	val_accuracy: 81.2500	gate_accuracy: 71.8750
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.6851


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 3.5408	step1_train_accuracy: 39.2617
STEP-1	Epoch: 20/50	loss: 1.3872	step1_train_accuracy: 71.8121
STEP-1	Epoch: 30/50	loss: 0.5947	step1_train_accuracy: 91.9463
STEP-1	Epoch: 40/50	loss: 0.3545	step1_train_accuracy: 93.9597
STEP-1	Epoch: 50/50	loss: 0.2484	step1_train_accuracy: 95.3020
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10, 874: 10, 875: 10, 876: 10, 877: 10, 878: 10, 879: 10, 880: 10, 881: 10, 882: 10, 883: 10, 884: 10, 885: 10, 886: 10, 887: 10, 888: 10, 889: 10, 890: 10, 891: 10, 892: 10, 893: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1721	gate_loss: 2.6909	step2_classification_accuracy: 65.0447	step_2_gate_accuracy: 27.8747
STEP-2	Epoch: 40/200	classification_loss: 0.9519	gate_loss: 1.2503	step2_classification_accuracy: 72.8747	step_2_gate_accuracy: 63.6242
STEP-2	Epoch: 60/200	classification_loss: 0.7544	gate_loss: 0.8082	step2_classification_accuracy: 78.1991	step_2_gate_accuracy: 75.2237
STEP-2	Epoch: 80/200	classification_loss: 0.6335	gate_loss: 0.6066	step2_classification_accuracy: 81.5101	step_2_gate_accuracy: 80.9955
STEP-2	Epoch: 100/200	classification_loss: 0.5371	gate_loss: 0.4839	step2_classification_accuracy: 83.7919	step_2_gate_accuracy: 84.8770
STEP-2	Epoch: 120/200	classification_loss: 0.4803	gate_loss: 0.4093	step2_classification_accuracy: 85.3579	step_2_gate_accuracy: 87.0694
STEP-2	Epoch: 140/200	classification_loss: 0.4416	gate_loss: 0.3620	step2_classification_accuracy: 86.5324	step_2_gate_accuracy: 89.0157
STEP-2	Epoch: 160/200	classification_loss: 0.4099	gate_loss: 0.3182	step2_classification_accuracy: 87.4385	step_2_gate_accuracy: 89.9553
STEP-2	Epoch: 180/200	classification_loss: 0.3817	gate_loss: 0.2893	step2_classification_accuracy: 88.1655	step_2_gate_accuracy: 90.5481
STEP-2	Epoch: 200/200	classification_loss: 0.3586	gate_loss: 0.2687	step2_classification_accuracy: 88.8031	step_2_gate_accuracy: 91.3758
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.0738	gate_accuracy: 72.4832
	Task-1	val_accuracy: 51.0870	gate_accuracy: 59.7826
	Task-2	val_accuracy: 46.6667	gate_accuracy: 52.2222
	Task-3	val_accuracy: 45.4545	gate_accuracy: 40.2597
	Task-4	val_accuracy: 50.6173	gate_accuracy: 60.4938
	Task-5	val_accuracy: 55.1402	gate_accuracy: 56.0748
	Task-6	val_accuracy: 59.5506	gate_accuracy: 60.6742
	Task-7	val_accuracy: 38.8060	gate_accuracy: 34.3284
	Task-8	val_accuracy: 61.5385	gate_accuracy: 64.1026
	Task-9	val_accuracy: 44.7761	gate_accuracy: 38.8060
	Task-10	val_accuracy: 43.9394	gate_accuracy: 50.0000
	Task-11	val_accuracy: 70.2703	gate_accuracy: 66.2162
	Task-12	val_accuracy: 60.5263	gate_accuracy: 55.2632
	Task-13	val_accuracy: 64.7727	gate_accuracy: 62.5000
	Task-14	val_accuracy: 76.7442	gate_accuracy: 58.1395
	Task-15	val_accuracy: 78.4946	gate_accuracy: 78.4946
	Task-16	val_accuracy: 78.8235	gate_accuracy: 69.4118
	Task-17	val_accuracy: 61.1940	gate_accuracy: 55.2239
	Task-18	val_accuracy: 56.2500	gate_accuracy: 50.0000
	Task-19	val_accuracy: 49.3506	gate_accuracy: 42.8571
	Task-20	val_accuracy: 46.4286	gate_accuracy: 45.2381
	Task-21	val_accuracy: 54.6667	gate_accuracy: 56.0000
	Task-22	val_accuracy: 62.9213	gate_accuracy: 53.9326
	Task-23	val_accuracy: 48.0000	gate_accuracy: 45.3333
	Task-24	val_accuracy: 60.2564	gate_accuracy: 55.1282
	Task-25	val_accuracy: 56.3830	gate_accuracy: 55.3191
	Task-26	val_accuracy: 62.3377	gate_accuracy: 61.0390
	Task-27	val_accuracy: 62.0690	gate_accuracy: 54.0230
	Task-28	val_accuracy: 71.4286	gate_accuracy: 68.8312
	Task-29	val_accuracy: 47.8261	gate_accuracy: 56.5217
	Task-30	val_accuracy: 64.0000	gate_accuracy: 56.0000
	Task-31	val_accuracy: 50.7246	gate_accuracy: 46.3768
	Task-32	val_accuracy: 57.4713	gate_accuracy: 56.3218
	Task-33	val_accuracy: 63.2184	gate_accuracy: 58.6207
	Task-34	val_accuracy: 57.3333	gate_accuracy: 53.3333
	Task-35	val_accuracy: 51.7241	gate_accuracy: 43.6782
	Task-36	val_accuracy: 76.7442	gate_accuracy: 69.7674
	Task-37	val_accuracy: 73.8636	gate_accuracy: 69.3182
	Task-38	val_accuracy: 80.4348	gate_accuracy: 79.3478
	Task-39	val_accuracy: 66.2921	gate_accuracy: 55.0562
	Task-40	val_accuracy: 73.8095	gate_accuracy: 70.2381
	Task-41	val_accuracy: 57.5000	gate_accuracy: 56.2500
	Task-42	val_accuracy: 48.4375	gate_accuracy: 40.6250
	Task-43	val_accuracy: 66.2162	gate_accuracy: 60.8108
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 57.4773


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 06:33:27	Wall time: 05:46:54
CPU time: 23607.194188251	Wall time: 20814.662655830383
FAA: 73.68145856327412
FF: 31.48956381937199

TRAINER.METRIC.ACCURACY
0: [90.60402684563759]
1: [87.24832214765101, 92.3913043478261]
2: [84.56375838926175, 83.69565217391305, 92.22222222222223]
3: [77.18120805369128, 85.86956521739131, 92.22222222222223, 81.81818181818183]
4: [80.53691275167785, 83.69565217391305, 90.0, 80.51948051948052, 87.65432098765432]
5: [69.12751677852349, 82.6086956521739, 88.88888888888889, 80.51948051948052, 77.77777777777779, 85.04672897196261]
6: [75.16778523489933, 75.0, 86.66666666666667, 81.81818181818183, 76.5432098765432, 79.43925233644859, 84.26966292134831]
7: [75.83892617449665, 76.08695652173914, 87.77777777777777, 75.32467532467533, 82.71604938271605, 78.50467289719626, 80.89887640449437, 88.05970149253731]
8: [71.81208053691275, 77.17391304347827, 86.66666666666667, 76.62337662337663, 82.71604938271605, 76.63551401869158, 80.89887640449437, 88.05970149253731, 80.76923076923077]
9: [66.44295302013423, 71.73913043478261, 80.0, 76.62337662337663, 81.48148148148148, 74.76635514018692, 85.39325842696628, 77.61194029850746, 79.48717948717949, 64.17910447761194]
10: [68.45637583892618, 70.65217391304348, 82.22222222222221, 67.53246753246754, 83.9506172839506, 77.57009345794393, 79.7752808988764, 80.59701492537313, 79.48717948717949, 65.67164179104478, 63.63636363636363]
11: [65.1006711409396, 68.47826086956522, 75.55555555555556, 63.63636363636363, 75.30864197530865, 71.02803738317756, 80.89887640449437, 80.59701492537313, 78.2051282051282, 68.65671641791045, 69.6969696969697, 83.78378378378379]
12: [69.12751677852349, 72.82608695652173, 83.33333333333334, 68.83116883116884, 70.37037037037037, 71.96261682242991, 76.40449438202246, 83.5820895522388, 80.76923076923077, 55.223880597014926, 66.66666666666666, 75.67567567567568, 88.1578947368421]
13: [67.78523489932886, 65.21739130434783, 70.0, 59.74025974025974, 74.07407407407408, 65.42056074766354, 75.28089887640449, 77.61194029850746, 75.64102564102564, 65.67164179104478, 62.121212121212125, 79.72972972972973, 89.47368421052632, 75.0]
14: [63.758389261744966, 75.0, 76.66666666666667, 59.74025974025974, 69.1358024691358, 71.96261682242991, 85.39325842696628, 79.1044776119403, 76.92307692307693, 58.2089552238806, 56.060606060606055, 78.37837837837837, 86.8421052631579, 75.0, 77.90697674418605]
15: [68.45637583892618, 66.30434782608695, 73.33333333333333, 66.23376623376623, 69.1358024691358, 70.09345794392523, 76.40449438202246, 77.61194029850746, 85.8974358974359, 58.2089552238806, 66.66666666666666, 78.37837837837837, 88.1578947368421, 71.5909090909091, 84.88372093023256, 88.17204301075269]
16: [72.48322147651007, 65.21739130434783, 73.33333333333333, 50.649350649350644, 67.90123456790124, 60.747663551401864, 73.03370786516854, 77.61194029850746, 83.33333333333334, 47.76119402985074, 60.60606060606061, 83.78378378378379, 84.21052631578947, 75.0, 81.3953488372093, 93.54838709677419, 84.70588235294117]
17: [67.11409395973155, 66.30434782608695, 80.0, 54.54545454545454, 74.07407407407408, 69.1588785046729, 71.91011235955057, 73.13432835820896, 74.35897435897436, 50.74626865671642, 56.060606060606055, 79.72972972972973, 75.0, 75.0, 83.72093023255815, 84.94623655913979, 91.76470588235294, 79.1044776119403]
18: [61.07382550335571, 73.91304347826086, 75.55555555555556, 62.33766233766234, 76.5432098765432, 69.1588785046729, 77.52808988764045, 71.64179104477611, 73.07692307692307, 56.71641791044776, 62.121212121212125, 83.78378378378379, 76.31578947368422, 69.31818181818183, 81.3953488372093, 82.79569892473118, 88.23529411764706, 61.19402985074627, 68.75]
19: [57.71812080536913, 65.21739130434783, 77.77777777777779, 49.35064935064935, 70.37037037037037, 63.55140186915887, 70.78651685393258, 70.1492537313433, 71.7948717948718, 58.2089552238806, 57.57575757575758, 77.02702702702703, 82.89473684210526, 75.0, 79.06976744186046, 84.94623655913979, 84.70588235294117, 76.11940298507463, 62.5, 51.94805194805194]
20: [63.758389261744966, 63.04347826086957, 67.77777777777779, 53.246753246753244, 66.66666666666666, 63.55140186915887, 74.15730337078652, 67.16417910447761, 76.92307692307693, 61.19402985074627, 57.57575757575758, 77.02702702702703, 89.47368421052632, 73.86363636363636, 80.23255813953489, 80.64516129032258, 85.88235294117646, 73.13432835820896, 66.25, 50.649350649350644, 53.57142857142857]
21: [60.40268456375839, 66.30434782608695, 71.11111111111111, 48.05194805194805, 58.0246913580247, 60.747663551401864, 69.66292134831461, 64.17910447761194, 78.2051282051282, 53.73134328358209, 60.60606060606061, 79.72972972972973, 77.63157894736842, 75.0, 76.74418604651163, 86.02150537634408, 89.41176470588236, 71.64179104477611, 60.0, 45.45454545454545, 59.523809523809526, 70.66666666666667]
22: [62.41610738255034, 69.56521739130434, 63.33333333333333, 54.54545454545454, 69.1358024691358, 69.1588785046729, 75.28089887640449, 64.17910447761194, 70.51282051282051, 65.67164179104478, 65.15151515151516, 83.78378378378379, 80.26315789473685, 68.18181818181817, 86.04651162790698, 86.02150537634408, 88.23529411764706, 73.13432835820896, 72.5, 45.45454545454545, 53.57142857142857, 73.33333333333333, 69.66292134831461]
23: [61.07382550335571, 63.04347826086957, 64.44444444444444, 49.35064935064935, 56.79012345679012, 70.09345794392523, 68.53932584269663, 67.16417910447761, 70.51282051282051, 47.76119402985074, 63.63636363636363, 83.78378378378379, 78.94736842105263, 63.63636363636363, 79.06976744186046, 77.41935483870968, 83.52941176470588, 64.17910447761194, 65.0, 48.05194805194805, 52.38095238095239, 70.66666666666667, 73.03370786516854, 65.33333333333333]
24: [57.71812080536913, 69.56521739130434, 71.11111111111111, 51.94805194805194, 50.617283950617285, 65.42056074766354, 69.66292134831461, 67.16417910447761, 67.94871794871796, 58.2089552238806, 50.0, 78.37837837837837, 82.89473684210526, 70.45454545454545, 80.23255813953489, 84.94623655913979, 85.88235294117646, 76.11940298507463, 62.5, 42.857142857142854, 50.0, 66.66666666666666, 71.91011235955057, 72.0, 67.94871794871796]
25: [54.36241610738255, 61.95652173913043, 56.666666666666664, 50.649350649350644, 51.85185185185185, 67.28971962616822, 75.28089887640449, 65.67164179104478, 74.35897435897436, 59.70149253731343, 56.060606060606055, 79.72972972972973, 71.05263157894737, 69.31818181818183, 73.25581395348837, 78.49462365591397, 82.35294117647058, 73.13432835820896, 57.49999999999999, 44.15584415584416, 51.19047619047619, 70.66666666666667, 78.65168539325843, 57.333333333333336, 67.94871794871796, 72.3404255319149]
26: [52.348993288590606, 63.04347826086957, 75.55555555555556, 54.54545454545454, 56.79012345679012, 62.616822429906534, 61.79775280898876, 70.1492537313433, 66.66666666666666, 58.2089552238806, 56.060606060606055, 75.67567567567568, 69.73684210526315, 73.86363636363636, 73.25581395348837, 78.49462365591397, 82.35294117647058, 68.65671641791045, 68.75, 53.246753246753244, 51.19047619047619, 73.33333333333333, 74.15730337078652, 64.0, 67.94871794871796, 51.06382978723404, 74.02597402597402]
27: [55.70469798657718, 60.86956521739131, 64.44444444444444, 53.246753246753244, 56.79012345679012, 67.28971962616822, 64.04494382022472, 58.2089552238806, 76.92307692307693, 52.23880597014925, 56.060606060606055, 71.62162162162163, 77.63157894736842, 64.77272727272727, 82.55813953488372, 81.72043010752688, 88.23529411764706, 71.64179104477611, 57.49999999999999, 42.857142857142854, 58.333333333333336, 61.33333333333333, 74.15730337078652, 56.00000000000001, 75.64102564102564, 73.40425531914893, 79.22077922077922, 72.41379310344827]
28: [59.06040268456376, 59.78260869565217, 63.33333333333333, 58.44155844155844, 60.49382716049383, 65.42056074766354, 70.78651685393258, 61.19402985074627, 69.23076923076923, 55.223880597014926, 59.09090909090909, 81.08108108108108, 76.31578947368422, 64.77272727272727, 80.23255813953489, 76.34408602150538, 83.52941176470588, 71.64179104477611, 56.25, 50.649350649350644, 48.80952380952381, 70.66666666666667, 71.91011235955057, 57.333333333333336, 70.51282051282051, 69.14893617021278, 72.72727272727273, 78.16091954022988, 83.11688311688312]
29: [48.99328859060403, 61.95652173913043, 57.77777777777777, 49.35064935064935, 62.96296296296296, 63.55140186915887, 65.1685393258427, 59.70149253731343, 71.7948717948718, 59.70149253731343, 54.54545454545454, 75.67567567567568, 78.94736842105263, 70.45454545454545, 87.20930232558139, 77.41935483870968, 80.0, 67.16417910447761, 62.5, 48.05194805194805, 50.0, 57.333333333333336, 73.03370786516854, 53.333333333333336, 62.82051282051282, 62.76595744680851, 71.42857142857143, 72.41379310344827, 77.92207792207793, 68.11594202898551]
30: [60.40268456375839, 58.69565217391305, 67.77777777777779, 49.35064935064935, 53.086419753086425, 65.42056074766354, 69.66292134831461, 61.19402985074627, 73.07692307692307, 56.71641791044776, 59.09090909090909, 79.72972972972973, 72.36842105263158, 61.36363636363637, 67.44186046511628, 74.19354838709677, 84.70588235294117, 79.1044776119403, 58.75, 49.35064935064935, 57.14285714285714, 62.66666666666667, 74.15730337078652, 61.33333333333333, 65.38461538461539, 68.08510638297872, 63.63636363636363, 74.71264367816092, 79.22077922077922, 75.36231884057972, 66.66666666666666]
31: [54.36241610738255, 65.21739130434783, 58.88888888888889, 53.246753246753244, 54.32098765432099, 62.616822429906534, 74.15730337078652, 55.223880597014926, 67.94871794871796, 53.73134328358209, 54.54545454545454, 79.72972972972973, 77.63157894736842, 68.18181818181817, 76.74418604651163, 76.34408602150538, 81.17647058823529, 65.67164179104478, 57.49999999999999, 45.45454545454545, 57.14285714285714, 57.333333333333336, 68.53932584269663, 61.33333333333333, 70.51282051282051, 67.02127659574468, 74.02597402597402, 68.96551724137932, 79.22077922077922, 59.42028985507246, 78.66666666666666, 63.76811594202898]
32: [58.38926174496645, 60.86956521739131, 57.77777777777777, 46.75324675324675, 59.25925925925925, 55.140186915887845, 61.79775280898876, 53.73134328358209, 74.35897435897436, 52.23880597014925, 54.54545454545454, 75.67567567567568, 77.63157894736842, 67.04545454545455, 62.7906976744186, 75.26881720430107, 82.35294117647058, 65.67164179104478, 58.75, 44.15584415584416, 45.23809523809524, 69.33333333333334, 67.41573033707866, 64.0, 67.94871794871796, 67.02127659574468, 64.93506493506493, 68.96551724137932, 79.22077922077922, 72.46376811594203, 69.33333333333334, 59.42028985507246, 62.06896551724138]
33: [53.02013422818792, 59.78260869565217, 48.888888888888886, 48.05194805194805, 56.79012345679012, 68.22429906542055, 73.03370786516854, 58.2089552238806, 75.64102564102564, 55.223880597014926, 46.96969696969697, 70.27027027027027, 61.8421052631579, 69.31818181818183, 73.25581395348837, 73.11827956989248, 84.70588235294117, 76.11940298507463, 61.25000000000001, 49.35064935064935, 50.0, 64.0, 69.66292134831461, 64.0, 62.82051282051282, 67.02127659574468, 70.12987012987013, 68.96551724137932, 79.22077922077922, 71.01449275362319, 68.0, 63.76811594202898, 57.47126436781609, 67.81609195402298]
34: [59.73154362416108, 67.3913043478261, 65.55555555555556, 49.35064935064935, 51.85185185185185, 66.35514018691589, 68.53932584269663, 52.23880597014925, 65.38461538461539, 52.23880597014925, 57.57575757575758, 71.62162162162163, 68.42105263157895, 72.72727272727273, 80.23255813953489, 77.41935483870968, 78.82352941176471, 62.68656716417911, 66.25, 49.35064935064935, 45.23809523809524, 62.66666666666667, 69.66292134831461, 61.33333333333333, 65.38461538461539, 57.446808510638306, 54.54545454545454, 67.81609195402298, 81.81818181818183, 69.56521739130434, 65.33333333333333, 73.91304347826086, 58.620689655172406, 63.2183908045977, 60.0]
35: [53.691275167785236, 65.21739130434783, 61.111111111111114, 54.54545454545454, 58.0246913580247, 57.943925233644855, 64.04494382022472, 56.71641791044776, 69.23076923076923, 52.23880597014925, 51.515151515151516, 79.72972972972973, 77.63157894736842, 65.9090909090909, 74.4186046511628, 74.19354838709677, 83.52941176470588, 56.71641791044776, 62.5, 42.857142857142854, 48.80952380952381, 64.0, 73.03370786516854, 56.00000000000001, 58.97435897435898, 65.95744680851064, 66.23376623376623, 66.66666666666666, 81.81818181818183, 65.21739130434783, 70.66666666666667, 63.76811594202898, 65.51724137931035, 55.172413793103445, 61.33333333333333, 58.620689655172406]
36: [46.97986577181208, 61.95652173913043, 63.33333333333333, 45.45454545454545, 59.25925925925925, 57.009345794392516, 66.29213483146067, 56.71641791044776, 62.82051282051282, 47.76119402985074, 51.515151515151516, 72.97297297297297, 63.1578947368421, 69.31818181818183, 77.90697674418605, 78.49462365591397, 78.82352941176471, 56.71641791044776, 56.25, 44.15584415584416, 45.23809523809524, 60.0, 73.03370786516854, 52.0, 64.1025641025641, 65.95744680851064, 71.42857142857143, 73.5632183908046, 76.62337662337663, 63.76811594202898, 65.33333333333333, 68.11594202898551, 66.66666666666666, 60.91954022988506, 54.666666666666664, 59.77011494252874, 69.76744186046511]
37: [54.36241610738255, 64.13043478260869, 61.111111111111114, 41.55844155844156, 64.19753086419753, 56.074766355140184, 62.92134831460674, 55.223880597014926, 58.97435897435898, 43.28358208955223, 53.03030303030303, 72.97297297297297, 65.78947368421053, 63.63636363636363, 68.6046511627907, 72.04301075268818, 76.47058823529412, 58.2089552238806, 57.49999999999999, 42.857142857142854, 52.38095238095239, 62.66666666666667, 61.79775280898876, 57.333333333333336, 62.82051282051282, 63.829787234042556, 68.83116883116884, 71.26436781609196, 79.22077922077922, 59.42028985507246, 64.0, 55.072463768115945, 57.47126436781609, 67.81609195402298, 53.333333333333336, 56.32183908045977, 69.76744186046511, 77.27272727272727]
38: [56.375838926174495, 63.04347826086957, 48.888888888888886, 46.75324675324675, 59.25925925925925, 66.35514018691589, 60.67415730337079, 56.71641791044776, 58.97435897435898, 47.76119402985074, 56.060606060606055, 74.32432432432432, 65.78947368421053, 63.63636363636363, 70.93023255813954, 76.34408602150538, 75.29411764705883, 61.19402985074627, 61.25000000000001, 38.961038961038966, 57.14285714285714, 61.33333333333333, 61.79775280898876, 57.333333333333336, 62.82051282051282, 63.829787234042556, 67.53246753246754, 63.2183908045977, 81.81818181818183, 68.11594202898551, 72.0, 60.86956521739131, 59.77011494252874, 60.91954022988506, 58.666666666666664, 58.620689655172406, 65.11627906976744, 63.63636363636363, 78.26086956521739]
39: [55.033557046979865, 55.434782608695656, 63.33333333333333, 44.15584415584416, 56.79012345679012, 54.20560747663551, 64.04494382022472, 59.70149253731343, 65.38461538461539, 55.223880597014926, 46.96969696969697, 63.51351351351351, 71.05263157894737, 64.77272727272727, 63.95348837209303, 69.89247311827957, 81.17647058823529, 58.2089552238806, 51.24999999999999, 50.649350649350644, 50.0, 66.66666666666666, 67.41573033707866, 50.66666666666667, 62.82051282051282, 68.08510638297872, 64.93506493506493, 66.66666666666666, 75.32467532467533, 59.42028985507246, 62.66666666666667, 63.76811594202898, 65.51724137931035, 65.51724137931035, 53.333333333333336, 42.5287356321839, 63.95348837209303, 72.72727272727273, 72.82608695652173, 60.67415730337079]
40: [45.63758389261745, 57.608695652173914, 67.77777777777779, 46.75324675324675, 58.0246913580247, 59.813084112149525, 68.53932584269663, 59.70149253731343, 74.35897435897436, 37.3134328358209, 54.54545454545454, 72.97297297297297, 72.36842105263158, 69.31818181818183, 69.76744186046511, 75.26881720430107, 80.0, 59.70149253731343, 53.75, 46.75324675324675, 52.38095238095239, 65.33333333333333, 67.41573033707866, 50.66666666666667, 60.256410256410255, 56.38297872340425, 64.93506493506493, 64.36781609195403, 74.02597402597402, 66.66666666666666, 60.0, 56.52173913043478, 60.91954022988506, 63.2183908045977, 53.333333333333336, 55.172413793103445, 65.11627906976744, 69.31818181818183, 82.6086956521739, 65.1685393258427, 65.47619047619048]
41: [49.664429530201346, 59.78260869565217, 60.0, 54.54545454545454, 56.79012345679012, 55.140186915887845, 71.91011235955057, 50.74626865671642, 64.1025641025641, 41.7910447761194, 53.03030303030303, 72.97297297297297, 55.26315789473685, 63.63636363636363, 68.6046511627907, 63.44086021505376, 84.70588235294117, 61.19402985074627, 55.00000000000001, 45.45454545454545, 46.42857142857143, 72.0, 67.41573033707866, 41.333333333333336, 67.94871794871796, 69.14893617021278, 67.53246753246754, 70.11494252873564, 76.62337662337663, 57.971014492753625, 61.33333333333333, 53.62318840579711, 71.26436781609196, 66.66666666666666, 48.0, 64.36781609195403, 62.7906976744186, 75.0, 71.73913043478261, 66.29213483146067, 58.333333333333336, 58.75]
42: [45.63758389261745, 55.434782608695656, 47.77777777777778, 35.064935064935064, 49.382716049382715, 62.616822429906534, 66.29213483146067, 53.73134328358209, 57.692307692307686, 40.298507462686565, 45.45454545454545, 62.16216216216216, 52.63157894736842, 67.04545454545455, 72.09302325581395, 78.49462365591397, 77.64705882352942, 59.70149253731343, 53.75, 42.857142857142854, 45.23809523809524, 73.33333333333333, 68.53932584269663, 53.333333333333336, 64.1025641025641, 64.8936170212766, 67.53246753246754, 59.77011494252874, 75.32467532467533, 57.971014492753625, 64.0, 55.072463768115945, 62.06896551724138, 67.81609195402298, 58.666666666666664, 58.620689655172406, 56.97674418604651, 73.86363636363636, 78.26086956521739, 62.92134831460674, 65.47619047619048, 58.75, 81.25]
43: [61.07382550335571, 51.08695652173913, 46.666666666666664, 45.45454545454545, 50.617283950617285, 55.140186915887845, 59.55056179775281, 38.80597014925373, 61.53846153846154, 44.776119402985074, 43.93939393939394, 70.27027027027027, 60.526315789473685, 64.77272727272727, 76.74418604651163, 78.49462365591397, 78.82352941176471, 61.19402985074627, 56.25, 49.35064935064935, 46.42857142857143, 54.666666666666664, 62.92134831460674, 48.0, 60.256410256410255, 56.38297872340425, 62.33766233766234, 62.06896551724138, 71.42857142857143, 47.82608695652174, 64.0, 50.72463768115942, 57.47126436781609, 63.2183908045977, 57.333333333333336, 51.724137931034484, 76.74418604651163, 73.86363636363636, 80.43478260869566, 66.29213483146067, 73.80952380952381, 57.49999999999999, 48.4375, 66.21621621621621]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 59.2965
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 50.0000
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 39.6694
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 47.6190
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 47.7064
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 56.8345
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 56.7797
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 41.9355
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 59.2593
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 56.9892
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 54.8387
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 66.3366
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 53.8462
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 63.2479
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 72.1739
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 77.2358
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 68.4211
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 55.3191
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 60.5505
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 51.9231
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 56.2500
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 60.1942
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 67.2269
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 47.5728
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 57.1429
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 64.2857
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 66.0377
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 66.6667
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 76.6355
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 54.1667
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 53.3981
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 53.1250
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 58.1197
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 62.0690
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 56.7308
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 54.7009
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 69.5652
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 73.9496
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 73.7705
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 64.7059
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 69.0265
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 53.7037
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 64.8352
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 61.7647

====================

f1_score(micro): 59.914582062233066
f1_score(macro): 56.615806459817655
Classification report:
              precision    recall  f1-score   support

           0       0.80      0.80      0.80         5
           1       0.67      0.50      0.57         4
           2       0.90      1.00      0.95         9
           3       1.00      0.25      0.40         4
           4       0.50      0.25      0.33         4
           5       1.00      0.50      0.67         4
           6       0.57      1.00      0.73         4
           7       0.33      0.25      0.29         4
           8       1.00      0.00      0.00         4
           9       1.00      0.44      0.62         9
          10       1.00      0.67      0.80         9
          11       0.86      0.67      0.75         9
          12       1.00      0.78      0.88         9
          13       0.67      0.50      0.57         4
          14       1.00      0.50      0.67         4
          15       0.67      0.50      0.57         4
          16       0.25      0.50      0.33         4
          17       0.64      0.78      0.70         9
          18       1.00      0.00      0.00         4
          19       1.00      0.44      0.62         9
          20       0.80      1.00      0.89         4
          21       1.00      0.78      0.88         9
          22       0.50      0.25      0.33         4
          23       0.80      0.89      0.84         9
          24       0.38      0.33      0.35         9
          25       0.29      0.40      0.33         5
          26       0.80      1.00      0.89         4
          27       0.43      0.33      0.38         9
          28       0.83      0.56      0.67         9
          29       0.80      1.00      0.89         4
          30       1.00      0.25      0.40         4
          31       1.00      0.60      0.75         5
          32       0.80      1.00      0.89         4
          33       0.75      0.75      0.75         4
          34       0.86      0.67      0.75         9
          35       1.00      1.00      1.00         4
          36       1.00      0.50      0.67         4
          37       1.00      0.75      0.86         4
          38       0.90      1.00      0.95         9
          39       1.00      0.00      0.00         9
          40       1.00      0.67      0.80         9
          41       1.00      0.00      0.00         4
          42       0.43      0.33      0.38         9
          43       0.00      0.00      0.00         9
          44       0.75      0.75      0.75         4
          45       1.00      0.40      0.57         5
          46       1.00      0.44      0.62         9
          47       0.50      0.25      0.33         4
          48       1.00      0.75      0.86         4
          49       1.00      0.50      0.67         4
          50       1.00      0.56      0.71         9
          51       1.00      0.75      0.86         4
          52       1.00      0.20      0.33         5
          53       1.00      1.00      1.00         4
          54       1.00      1.00      1.00         4
          55       0.57      0.44      0.50         9
          56       1.00      0.33      0.50         9
          57       1.00      0.00      0.00         4
          58       0.00      0.00      0.00         4
          59       1.00      0.50      0.67         4
          60       1.00      0.44      0.62         9
          61       1.00      0.67      0.80         9
          62       0.00      0.00      0.00         4
          63       1.00      0.00      0.00         4
          64       0.00      0.00      0.00         9
          65       1.00      0.00      0.00         4
          66       0.67      0.67      0.67         9
          67       1.00      0.40      0.57         5
          68       0.80      1.00      0.89         4
          69       0.71      0.56      0.63         9
          70       0.00      0.00      0.00         4
          71       0.50      0.25      0.33         4
          72       1.00      0.67      0.80         9
          73       1.00      0.25      0.40         4
          74       0.00      0.00      0.00         4
          75       1.00      0.50      0.67         4
          76       1.00      0.60      0.75         5
          77       0.67      0.22      0.33         9
          78       1.00      1.00      1.00         5
          79       1.00      0.25      0.40         4
          80       1.00      0.75      0.86         4
          81       1.00      0.25      0.40         4
          82       0.33      0.22      0.27         9
          83       1.00      0.00      0.00         5
          84       1.00      1.00      1.00         4
          85       0.40      0.44      0.42         9
          86       1.00      1.00      1.00         4
          87       0.00      0.00      0.00         4
          88       1.00      1.00      1.00         5
          89       0.88      0.78      0.82         9
          90       0.40      0.40      0.40         5
          91       1.00      0.00      0.00         4
          92       0.50      0.25      0.33         4
          93       1.00      1.00      1.00         4
          94       1.00      0.75      0.86         4
          95       0.80      1.00      0.89         4
          96       1.00      0.25      0.40         4
          97       0.50      0.25      0.33         4
          98       0.64      1.00      0.78         9
          99       0.67      0.50      0.57         4
         100       0.38      0.60      0.46         5
         101       1.00      0.00      0.00         4
         102       1.00      0.56      0.71         9
         103       0.89      0.89      0.89         9
         104       1.00      0.50      0.67         4
         105       1.00      0.00      0.00         5
         106       0.12      0.11      0.12         9
         107       0.00      0.00      0.00         4
         108       0.57      0.44      0.50         9
         109       1.00      0.80      0.89         5
         110       0.80      1.00      0.89         4
         111       0.00      0.00      0.00         4
         112       0.50      0.20      0.29         5
         113       1.00      0.00      0.00         4
         114       1.00      0.60      0.75         5
         115       1.00      0.78      0.88         9
         116       0.20      0.11      0.14         9
         117       0.70      0.78      0.74         9
         118       1.00      1.00      1.00         5
         119       0.00      0.00      0.00         4
         120       0.20      0.22      0.21         9
         121       0.00      0.00      0.00         9
         122       1.00      0.25      0.40         4
         123       0.82      1.00      0.90         9
         124       0.40      0.50      0.44         4
         125       0.42      0.89      0.57         9
         126       1.00      0.60      0.75         5
         127       0.67      0.44      0.53         9
         128       0.89      0.89      0.89         9
         129       0.90      1.00      0.95         9
         130       0.33      0.25      0.29         4
         131       0.62      0.56      0.59         9
         132       0.67      0.80      0.73         5
         133       1.00      0.00      0.00         4
         134       0.33      0.20      0.25         5
         135       1.00      0.44      0.62         9
         136       1.00      0.80      0.89         5
         137       0.73      0.89      0.80         9
         138       1.00      1.00      1.00         4
         139       1.00      0.50      0.67         4
         140       0.12      0.25      0.17         4
         141       1.00      0.80      0.89         5
         142       1.00      0.50      0.67         4
         143       0.60      0.67      0.63         9
         144       0.12      0.11      0.12         9
         145       0.78      0.78      0.78         9
         146       0.43      0.75      0.55         4
         147       1.00      0.75      0.86         4
         148       0.82      1.00      0.90         9
         149       0.36      0.44      0.40         9
         150       1.00      0.50      0.67         4
         151       0.00      0.00      0.00         4
         152       0.67      0.50      0.57         4
         153       1.00      0.00      0.00         4
         154       1.00      1.00      1.00         4
         155       1.00      0.75      0.86         4
         156       1.00      0.00      0.00         4
         157       0.25      0.20      0.22         5
         158       1.00      0.80      0.89         5
         159       0.00      0.00      0.00         4
         160       1.00      0.00      0.00         4
         161       1.00      0.00      0.00         4
         162       0.67      0.50      0.57         4
         163       0.50      0.25      0.33         4
         164       0.40      0.50      0.44         4
         165       1.00      0.75      0.86         4
         166       0.18      0.22      0.20         9
         167       1.00      0.00      0.00         4
         168       1.00      0.50      0.67         4
         169       1.00      0.50      0.67         4
         170       1.00      1.00      1.00         4
         171       1.00      0.78      0.88         9
         172       0.00      0.00      0.00         4
         173       0.33      0.40      0.36         5
         174       1.00      0.80      0.89         5
         175       1.00      0.89      0.94         9
         176       0.71      1.00      0.83         5
         177       0.00      0.00      0.00         4
         178       0.33      0.50      0.40         4
         179       0.30      0.60      0.40         5
         180       1.00      0.00      0.00         4
         181       1.00      0.75      0.86         4
         182       1.00      0.67      0.80         9
         183       0.67      0.80      0.73         5
         184       0.60      0.60      0.60         5
         185       0.80      1.00      0.89         4
         186       0.56      0.56      0.56         9
         187       1.00      0.75      0.86         4
         188       1.00      0.75      0.86         4
         189       0.67      0.44      0.53         9
         190       1.00      0.80      0.89         5
         191       1.00      0.00      0.00         5
         192       0.67      0.40      0.50         5
         193       1.00      0.25      0.40         4
         194       1.00      0.40      0.57         5
         195       0.88      0.78      0.82         9
         196       1.00      0.00      0.00         4
         197       1.00      0.75      0.86         4
         198       1.00      0.75      0.86         4
         199       0.33      0.25      0.29         4
         200       1.00      1.00      1.00         4
         201       1.00      0.00      0.00         4
         202       0.80      1.00      0.89         4
         203       1.00      0.75      0.86         4
         204       1.00      1.00      1.00         4
         205       0.38      0.60      0.46         5
         206       1.00      0.00      0.00         4
         207       0.29      0.22      0.25         9
         208       1.00      0.00      0.00         4
         209       0.67      0.50      0.57         4
         210       0.60      0.75      0.67         4
         211       0.75      0.75      0.75         4
         212       0.83      1.00      0.91         5
         213       0.80      1.00      0.89         4
         214       0.60      0.60      0.60         5
         215       0.25      0.25      0.25         4
         216       0.43      0.75      0.55         4
         217       1.00      0.80      0.89         5
         218       0.00      0.00      0.00         4
         219       0.00      0.00      0.00         4
         220       1.00      0.50      0.67         4
         221       1.00      0.75      0.86         4
         222       1.00      0.75      0.86         4
         223       0.12      0.50      0.20         4
         224       1.00      0.25      0.40         4
         225       0.75      0.75      0.75         4
         226       1.00      1.00      1.00         4
         227       0.43      0.75      0.55         4
         228       0.73      0.89      0.80         9
         229       0.00      0.00      0.00         4
         230       1.00      0.25      0.40         4
         231       0.00      0.00      0.00         4
         232       0.38      0.60      0.46         5
         233       1.00      0.78      0.88         9
         234       0.64      0.78      0.70         9
         235       1.00      0.25      0.40         4
         236       0.80      0.89      0.84         9
         237       0.60      0.75      0.67         4
         238       0.75      0.75      0.75         4
         239       0.25      0.50      0.33         4
         240       0.80      1.00      0.89         4
         241       0.17      0.25      0.20         4
         242       1.00      0.75      0.86         4
         243       0.00      0.00      0.00         4
         244       0.33      0.25      0.29         4
         245       1.00      0.89      0.94         9
         246       0.80      1.00      0.89         4
         247       0.25      0.33      0.29         9
         248       1.00      1.00      1.00         4
         249       1.00      1.00      1.00         4
         250       0.57      1.00      0.73         4
         251       0.50      0.75      0.60         4
         252       0.00      0.00      0.00         5
         253       0.80      1.00      0.89         4
         254       0.67      0.50      0.57         4
         255       0.00      0.00      0.00         4
         256       1.00      0.44      0.62         9
         257       0.80      0.80      0.80         5
         258       0.50      0.75      0.60         4
         259       0.25      0.11      0.15         9
         260       0.75      0.75      0.75         4
         261       0.50      0.25      0.33         4
         262       1.00      0.78      0.88         9
         263       1.00      0.50      0.67         4
         264       0.83      1.00      0.91         5
         265       1.00      0.80      0.89         5
         266       1.00      0.44      0.62         9
         267       0.00      0.00      0.00         4
         268       0.80      1.00      0.89         4
         269       0.00      0.00      0.00         4
         270       0.80      1.00      0.89         4
         271       0.00      0.00      0.00         4
         272       0.67      0.80      0.73         5
         273       0.80      1.00      0.89         4
         274       1.00      1.00      1.00         5
         275       0.00      0.00      0.00         9
         276       1.00      0.44      0.62         9
         277       0.67      0.50      0.57         4
         278       0.80      1.00      0.89         4
         279       0.89      0.89      0.89         9
         280       1.00      0.00      0.00         4
         281       1.00      0.50      0.67         4
         282       0.67      0.50      0.57         4
         283       0.75      1.00      0.86         9
         284       1.00      0.00      0.00         4
         285       0.67      1.00      0.80         4
         286       0.67      0.80      0.73         5
         287       0.62      0.56      0.59         9
         288       1.00      0.75      0.86         4
         289       0.80      1.00      0.89         4
         290       1.00      1.00      1.00         4
         291       0.80      0.89      0.84         9
         292       0.00      0.00      0.00         4
         293       0.60      0.67      0.63         9
         294       0.50      1.00      0.67         5
         295       0.75      0.75      0.75         4
         296       1.00      1.00      1.00         4
         297       0.00      0.00      0.00         4
         298       0.80      0.80      0.80         5
         299       0.71      0.56      0.63         9
         300       0.50      0.50      0.50         4
         301       0.60      0.75      0.67         4
         302       0.64      0.78      0.70         9
         303       1.00      0.80      0.89         5
         304       0.78      0.78      0.78         9
         305       0.40      0.50      0.44         4
         306       0.78      0.78      0.78         9
         307       0.50      0.50      0.50         4
         308       0.75      0.67      0.71         9
         309       0.83      1.00      0.91         5
         310       0.80      1.00      0.89         4
         311       0.29      0.40      0.33         5
         312       0.67      0.50      0.57         4
         313       0.56      1.00      0.72         9
         314       0.75      0.75      0.75         4
         315       0.57      0.44      0.50         9
         316       1.00      0.80      0.89         5
         317       0.67      0.89      0.76         9
         318       0.78      0.78      0.78         9
         319       0.62      1.00      0.77         5
         320       0.80      1.00      0.89         4
         321       1.00      0.00      0.00         4
         322       0.55      0.67      0.60         9
         323       1.00      0.80      0.89         5
         324       0.80      1.00      0.89         4
         325       1.00      0.78      0.88         9
         326       0.45      0.56      0.50         9
         327       0.67      1.00      0.80         4
         328       0.80      1.00      0.89         4
         329       0.89      0.89      0.89         9
         330       0.50      0.25      0.33         4
         331       1.00      1.00      1.00         4
         332       0.75      1.00      0.86         9
         333       1.00      1.00      1.00         4
         334       0.67      0.50      0.57         4
         335       1.00      0.60      0.75         5
         336       1.00      0.33      0.50         9
         337       0.50      0.56      0.53         9
         338       0.67      1.00      0.80         4
         339       1.00      1.00      1.00         4
         340       0.78      0.78      0.78         9
         341       0.00      0.00      0.00         4
         342       1.00      1.00      1.00         5
         343       0.80      1.00      0.89         4
         344       0.50      0.75      0.60         4
         345       1.00      1.00      1.00         4
         346       0.55      0.67      0.60         9
         347       1.00      0.80      0.89         5
         348       0.80      1.00      0.89         4
         349       0.67      0.89      0.76         9
         350       0.25      0.25      0.25         4
         351       0.89      0.89      0.89         9
         352       0.67      0.40      0.50         5
         353       0.50      0.25      0.33         4
         354       0.44      1.00      0.62         4
         355       0.20      0.40      0.27         5
         356       1.00      0.25      0.40         4
         357       0.67      0.50      0.57         4
         358       0.55      0.67      0.60         9
         359       0.00      0.00      0.00         4
         360       0.67      0.50      0.57         4
         361       0.09      0.25      0.13         4
         362       1.00      1.00      1.00         4
         363       1.00      0.25      0.40         4
         364       0.71      1.00      0.83         5
         365       0.80      1.00      0.89         4
         366       0.80      1.00      0.89         4
         367       1.00      0.75      0.86         4
         368       0.50      0.33      0.40         9
         369       0.00      0.00      0.00         4
         370       0.57      1.00      0.73         4
         371       0.33      0.25      0.29         4
         372       0.30      0.60      0.40         5
         373       0.67      0.40      0.50         5
         374       1.00      0.60      0.75         5
         375       1.00      1.00      1.00         4
         376       0.71      1.00      0.83         5
         377       0.70      0.78      0.74         9
         378       0.73      0.89      0.80         9
         379       0.46      0.67      0.55         9
         380       0.50      1.00      0.67         4
         381       0.20      0.11      0.14         9
         382       1.00      0.00      0.00         4
         383       0.80      1.00      0.89         4
         384       0.75      0.75      0.75         4
         385       0.00      0.00      0.00         4
         386       0.36      0.80      0.50         5
         387       0.33      0.22      0.27         9
         388       0.67      1.00      0.80         4
         389       0.50      1.00      0.67         5
         390       0.00      0.00      0.00         4
         391       0.50      0.50      0.50         4
         392       0.00      0.00      0.00         4
         393       0.80      1.00      0.89         4
         394       0.57      1.00      0.73         4
         395       0.43      0.33      0.38         9
         396       1.00      1.00      1.00         4
         397       0.00      0.00      0.00         4
         398       0.67      0.50      0.57         4
         399       0.00      0.00      0.00         9
         400       0.73      0.89      0.80         9
         401       1.00      1.00      1.00         4
         402       0.40      0.50      0.44         4
         403       0.60      0.60      0.60         5
         404       0.08      0.22      0.11         9
         405       0.67      0.40      0.50         5
         406       0.18      0.75      0.29         4
         407       0.00      0.00      0.00         4
         408       1.00      1.00      1.00         5
         409       1.00      1.00      1.00         4
         410       1.00      1.00      1.00         5
         411       0.00      0.00      0.00         4
         412       1.00      0.25      0.40         4
         413       0.50      0.50      0.50         4
         414       0.00      0.00      0.00         4
         415       1.00      0.00      0.00         4
         416       1.00      1.00      1.00         4
         417       0.00      0.00      0.00         4
         418       0.67      0.50      0.57         4
         419       0.00      0.00      0.00         4
         420       1.00      0.89      0.94         9
         421       1.00      1.00      1.00         4
         422       1.00      0.89      0.94         9
         423       0.80      0.89      0.84         9
         424       1.00      1.00      1.00         5
         425       1.00      1.00      1.00         4
         426       0.33      0.11      0.17         9
         427       0.57      1.00      0.73         4
         428       0.50      0.60      0.55         5
         429       0.00      0.00      0.00         4
         430       0.12      0.25      0.17         4
         431       0.00      0.00      0.00         9
         432       0.43      0.75      0.55         4
         433       0.57      0.89      0.70         9
         434       0.00      0.00      0.00         4
         435       0.00      0.00      0.00         9
         436       0.80      1.00      0.89         4
         437       1.00      0.67      0.80         9
         438       0.60      0.75      0.67         4
         439       0.00      0.00      0.00         4
         440       0.64      1.00      0.78         9
         441       0.62      1.00      0.77         5
         442       1.00      0.25      0.40         4
         443       0.00      0.00      0.00         4
         444       1.00      0.67      0.80         9
         445       0.67      0.80      0.73         5
         446       0.60      0.75      0.67         4
         447       0.80      0.80      0.80         5
         448       0.43      0.75      0.55         4
         449       1.00      0.25      0.40         4
         450       0.80      1.00      0.89         4
         451       1.00      1.00      1.00         4
         452       0.75      0.75      0.75         4
         453       0.40      0.50      0.44         4
         454       1.00      0.57      0.73         7
         455       0.80      0.89      0.84         9
         456       0.40      0.50      0.44         4
         457       0.67      0.50      0.57         4
         458       0.70      0.78      0.74         9
         459       0.31      1.00      0.47         4
         460       0.40      0.50      0.44         4
         461       0.25      0.25      0.25         4
         462       1.00      0.75      0.86         4
         463       0.90      1.00      0.95         9
         464       0.43      0.75      0.55         4
         465       0.00      0.00      0.00         4
         466       1.00      0.78      0.88         9
         467       0.67      0.89      0.76         9
         468       0.33      0.50      0.40         4
         469       1.00      0.00      0.00         4
         470       0.57      1.00      0.73         4
         471       0.40      0.22      0.29         9
         472       0.89      0.89      0.89         9
         473       0.67      0.80      0.73         5
         474       0.67      1.00      0.80         4
         475       1.00      0.00      0.00         4
         476       0.80      1.00      0.89         4
         477       0.00      0.00      0.00         4
         478       0.75      0.75      0.75         4
         479       1.00      1.00      1.00         4
         480       0.00      0.00      0.00         9
         481       0.83      0.56      0.67         9
         482       1.00      0.40      0.57         5
         483       1.00      0.50      0.67         4
         484       0.90      1.00      0.95         9
         485       0.50      0.40      0.44         5
         486       1.00      0.56      0.71         9
         487       1.00      0.00      0.00         4
         488       1.00      0.00      0.00         4
         489       0.38      0.75      0.50         4
         490       0.60      0.60      0.60         5
         491       0.00      0.00      0.00         4
         492       1.00      0.00      0.00         4
         493       0.50      0.75      0.60         4
         494       0.89      0.89      0.89         9
         495       0.00      0.00      0.00         4
         496       0.75      0.75      0.75         4
         497       1.00      0.00      0.00         4
         498       0.40      0.50      0.44         4
         499       0.80      1.00      0.89         4
         500       1.00      1.00      1.00         4
         501       0.50      0.50      0.50         4
         502       1.00      0.00      0.00         4
         503       1.00      0.78      0.88         9
         504       0.75      0.75      0.75         4
         505       0.00      0.00      0.00         4
         506       0.80      1.00      0.89         4
         507       1.00      0.50      0.67         4
         508       1.00      0.75      0.86         4
         509       1.00      0.00      0.00         4
         510       0.10      0.11      0.11         9
         511       0.25      0.25      0.25         4
         512       1.00      0.89      0.94         9
         513       1.00      0.89      0.94         9
         514       0.80      1.00      0.89         4
         515       1.00      0.80      0.89         5
         516       0.60      0.75      0.67         4
         517       0.56      1.00      0.71         5
         518       1.00      0.75      0.86         4
         519       0.25      0.40      0.31         5
         520       0.57      0.80      0.67         5
         521       0.83      1.00      0.91         5
         522       0.88      0.78      0.82         9
         523       1.00      0.89      0.94         9
         524       0.67      0.67      0.67         9
         525       0.18      0.22      0.20         9
         526       0.00      0.00      0.00         9
         527       0.50      0.44      0.47         9
         528       1.00      0.25      0.40         4
         529       1.00      1.00      1.00         4
         530       0.75      1.00      0.86         9
         531       0.58      0.78      0.67         9
         532       0.12      0.25      0.17         4
         533       0.67      0.40      0.50         5
         534       0.40      1.00      0.57         4
         535       0.50      1.00      0.67         4
         536       0.67      0.67      0.67         9
         537       0.75      0.75      0.75         4
         538       1.00      0.75      0.86         4
         539       0.00      0.00      0.00         4
         540       1.00      1.00      1.00         5
         541       0.80      0.80      0.80         5
         542       1.00      0.25      0.40         4
         543       1.00      0.75      0.86         4
         544       0.80      0.80      0.80         5
         545       0.67      0.80      0.73         5
         546       0.40      0.40      0.40         5
         547       0.38      0.89      0.53         9
         548       0.60      0.60      0.60         5
         549       0.54      0.78      0.64         9
         550       0.00      0.00      0.00         4
         551       1.00      0.56      0.71         9
         552       1.00      0.50      0.67         4
         553       0.50      0.50      0.50         4
         554       0.00      0.00      0.00         4
         555       0.47      0.78      0.58         9
         556       0.38      0.67      0.48         9
         557       1.00      0.67      0.80         9
         558       0.75      0.75      0.75         4
         559       0.50      0.60      0.55         5
         560       0.62      0.71      0.67         7
         561       0.71      1.00      0.83         5
         562       1.00      0.20      0.33         5
         563       0.12      0.50      0.19         4
         564       0.83      1.00      0.91         5
         565       0.78      0.78      0.78         9
         566       1.00      1.00      1.00         9
         567       0.00      0.00      0.00         4
         568       0.67      0.50      0.57         4
         569       0.75      0.75      0.75         4
         570       1.00      0.75      0.86         4
         571       1.00      0.00      0.00         4
         572       0.67      1.00      0.80         4
         573       1.00      0.78      0.88         9
         574       1.00      1.00      1.00         9
         575       1.00      0.60      0.75         5
         576       0.75      0.75      0.75         4
         577       0.50      0.25      0.33         4
         578       1.00      0.75      0.86         4
         579       1.00      1.00      1.00         9
         580       1.00      0.00      0.00         4
         581       0.44      0.44      0.44         9
         582       0.75      0.75      0.75         4
         583       1.00      1.00      1.00         4
         584       1.00      1.00      1.00         4
         585       0.50      1.00      0.67         9
         586       0.57      1.00      0.73         4
         587       0.67      1.00      0.80         4
         588       0.60      0.75      0.67         4
         589       0.00      0.00      0.00         4
         590       0.75      0.75      0.75         4
         591       0.50      1.00      0.67         5
         592       0.69      1.00      0.82         9
         593       0.67      0.50      0.57         4
         594       0.57      1.00      0.73         4
         595       0.80      1.00      0.89         4
         596       0.57      0.80      0.67         5
         597       0.33      0.40      0.36         5
         598       1.00      0.25      0.40         4
         599       1.00      0.50      0.67         4
         600       1.00      0.00      0.00         5
         601       0.73      0.89      0.80         9
         602       0.00      0.00      0.00         4
         603       1.00      0.25      0.40         4
         604       1.00      0.50      0.67         4
         605       1.00      0.75      0.86         4
         606       1.00      1.00      1.00         9
         607       1.00      0.00      0.00         4
         608       1.00      0.50      0.67         4
         609       1.00      0.50      0.67         4
         610       1.00      0.14      0.25         7
         611       1.00      0.75      0.86         4
         612       1.00      0.75      0.86         4
         613       0.50      0.25      0.33         4
         614       1.00      0.50      0.67         4
         615       1.00      0.60      0.75         5
         616       1.00      0.00      0.00         4
         617       0.67      0.50      0.57         4
         618       0.43      0.33      0.38         9
         619       0.50      0.50      0.50         4
         620       0.67      0.44      0.53         9
         621       1.00      0.25      0.40         4
         622       0.67      0.40      0.50         5
         623       1.00      1.00      1.00         4
         624       0.83      1.00      0.91         5
         625       1.00      0.75      0.86         4
         626       1.00      0.50      0.67         4
         627       1.00      1.00      1.00         4
         628       0.00      0.00      0.00         9
         629       1.00      1.00      1.00         4
         630       1.00      0.75      0.86         4
         631       1.00      0.75      0.86         4
         632       0.00      0.00      0.00         4
         633       0.89      0.89      0.89         9
         634       0.60      0.75      0.67         4
         635       0.17      0.25      0.20         4
         636       0.71      0.56      0.63         9
         637       0.22      0.50      0.31         4
         638       0.00      0.00      0.00         4
         639       0.57      1.00      0.73         4
         640       0.20      0.25      0.22         4
         641       0.67      0.50      0.57         4
         642       0.67      0.50      0.57         4
         643       0.60      0.60      0.60         5
         644       0.80      1.00      0.89         4
         645       1.00      0.50      0.67         4
         646       0.00      0.00      0.00         4
         647       0.33      0.75      0.46         4
         648       0.90      1.00      0.95         9
         649       0.12      0.25      0.17         4
         650       1.00      1.00      1.00         4
         651       1.00      1.00      1.00         4
         652       0.17      0.11      0.13         9
         653       1.00      0.00      0.00         4
         654       0.89      1.00      0.94         8
         655       0.00      0.00      0.00         9
         656       0.00      0.00      0.00         4
         657       1.00      0.25      0.40         4
         658       0.88      0.78      0.82         9
         659       1.00      0.00      0.00         9
         660       0.15      0.33      0.21         9
         661       0.88      0.78      0.82         9
         662       1.00      0.50      0.67         4
         663       1.00      0.75      0.86         4
         664       0.75      0.75      0.75         4
         665       1.00      0.89      0.94         9
         666       0.80      1.00      0.89         4
         667       0.50      0.60      0.55         5
         668       1.00      0.80      0.89         5
         669       0.43      0.75      0.55         4
         670       1.00      0.80      0.89         5
         671       0.75      0.75      0.75         4
         672       1.00      0.75      0.86         4
         673       1.00      0.50      0.67         4
         674       0.33      0.50      0.40         4
         675       0.60      0.75      0.67         4
         676       0.50      0.75      0.60         4
         677       0.75      1.00      0.86         9
         678       0.33      0.50      0.40         4
         679       0.00      0.00      0.00         4
         680       0.00      0.00      0.00         4
         681       0.00      0.00      0.00         4
         682       0.00      0.00      0.00         4
         683       0.82      1.00      0.90         9
         684       0.47      0.78      0.58         9
         685       0.41      0.78      0.54         9
         686       0.31      1.00      0.47         4
         687       0.43      0.75      0.55         4
         688       0.67      1.00      0.80         4
         689       0.00      0.00      0.00         4
         690       0.83      0.56      0.67         9
         691       0.50      0.44      0.47         9
         692       0.64      0.78      0.70         9
         693       1.00      0.60      0.75         5
         694       0.40      1.00      0.57         4
         695       0.50      0.40      0.44         5
         696       0.50      1.00      0.67         5
         697       0.60      0.75      0.67         4
         698       0.64      0.78      0.70         9
         699       1.00      1.00      1.00         5
         700       0.00      0.00      0.00         4
         701       1.00      1.00      1.00         4
         702       0.80      1.00      0.89         4
         703       0.75      0.67      0.71         9
         704       0.10      0.25      0.14         4
         705       0.88      0.88      0.88         8
         706       0.00      0.00      0.00         4
         707       0.50      0.20      0.29         5
         708       1.00      0.25      0.40         4
         709       1.00      0.78      0.88         9
         710       0.00      0.00      0.00         4
         711       0.50      0.25      0.33         4
         712       0.00      0.00      0.00         4
         713       0.50      0.20      0.29         5
         714       1.00      1.00      1.00         4
         715       0.88      0.78      0.82         9
         716       0.75      0.75      0.75         4
         717       0.00      0.00      0.00         4
         718       1.00      0.40      0.57         5
         719       1.00      0.60      0.75         5
         720       1.00      0.25      0.40         4
         721       1.00      0.75      0.86         4
         722       1.00      1.00      1.00         9
         723       0.50      0.25      0.33         4
         724       0.00      0.00      0.00         9
         725       0.43      0.33      0.38         9
         726       0.50      0.25      0.33         4
         727       0.67      0.80      0.73         5
         728       1.00      0.40      0.57         5
         729       0.00      0.00      0.00         4
         730       0.44      0.67      0.53         6
         731       0.20      0.20      0.20         5
         732       0.69      1.00      0.82         9
         733       0.88      0.78      0.82         9
         734       0.80      0.80      0.80         5
         735       0.60      0.67      0.63         9
         736       1.00      0.80      0.89         5
         737       0.38      0.75      0.50         4
         738       1.00      0.00      0.00         4
         739       0.44      0.80      0.57         5
         740       0.21      0.33      0.26         9
         741       0.00      0.00      0.00         4
         742       0.57      1.00      0.73         4
         743       0.33      1.00      0.50         4
         744       0.73      0.89      0.80         9
         745       0.12      0.50      0.20         4
         746       0.43      0.75      0.55         4
         747       0.25      0.20      0.22         5
         748       0.75      1.00      0.86         9
         749       0.80      1.00      0.89         4
         750       0.38      0.60      0.46         5
         751       0.00      0.00      0.00         4
         752       0.82      1.00      0.90         9
         753       0.90      1.00      0.95         9
         754       1.00      0.78      0.88         9
         755       0.82      1.00      0.90         9
         756       1.00      0.75      0.86         4
         757       0.62      0.89      0.73         9
         758       1.00      0.67      0.80         9
         759       0.75      1.00      0.86         9
         760       1.00      1.00      1.00         4
         761       1.00      0.80      0.89         5
         762       0.50      0.50      0.50         4
         763       1.00      0.60      0.75         5
         764       0.44      0.80      0.57         5
         765       1.00      0.50      0.67         4
         766       1.00      0.75      0.86         4
         767       1.00      1.00      1.00         4
         768       0.50      0.67      0.57         9
         769       1.00      0.00      0.00         4
         770       1.00      0.00      0.00         4
         771       0.90      1.00      0.95         9
         772       1.00      0.75      0.86         4
         773       1.00      0.40      0.57         5
         774       0.89      0.89      0.89         9
         775       0.00      0.00      0.00         4
         776       0.80      1.00      0.89         4
         777       0.57      1.00      0.73         4
         778       0.73      0.89      0.80         9
         779       1.00      0.00      0.00         4
         780       0.50      0.25      0.33         4
         781       0.80      0.89      0.84         9
         782       0.67      0.80      0.73         5
         783       1.00      0.78      0.88         9
         784       0.00      0.00      0.00         4
         785       0.67      0.44      0.53         9
         786       0.80      1.00      0.89         4
         787       0.86      0.67      0.75         9
         788       0.90      1.00      0.95         9
         789       0.67      0.80      0.73         5
         790       0.50      0.75      0.60         4
         791       0.82      1.00      0.90         9
         792       1.00      0.75      0.86         4
         793       0.57      1.00      0.73         4
         794       1.00      1.00      1.00         5
         795       1.00      0.75      0.86         4
         796       0.75      0.60      0.67         5
         797       1.00      0.75      0.86         4
         798       0.40      1.00      0.57         4
         799       0.67      1.00      0.80         4
         800       0.40      0.50      0.44         4
         801       0.09      0.25      0.13         4
         802       0.05      0.25      0.08         4
         803       0.14      0.25      0.18         4
         804       0.89      0.89      0.89         9
         805       0.80      0.89      0.84         9
         806       0.00      0.00      0.00         4
         807       0.50      0.80      0.62         5
         808       0.86      0.67      0.75         9
         809       1.00      0.89      0.94         9
         810       0.00      0.00      0.00         9
         811       1.00      0.78      0.88         9
         812       0.50      0.67      0.57         9
         813       1.00      0.60      0.75         5
         814       0.21      0.80      0.33         5
         815       0.19      1.00      0.31         5
         816       0.40      0.89      0.55         9
         817       0.80      0.89      0.84         9
         818       0.50      0.25      0.33         4
         819       1.00      0.75      0.86         4
         820       0.60      0.67      0.63         9
         821       0.67      0.44      0.53         9
         822       0.64      0.78      0.70         9
         823       0.75      0.75      0.75         4
         824       0.60      0.75      0.67         4
         825       1.00      0.25      0.40         4
         826       0.06      0.25      0.09         4
         827       1.00      1.00      1.00         4
         828       0.40      0.50      0.44         4
         829       0.62      0.89      0.73         9
         830       0.50      0.50      0.50         4
         831       1.00      0.80      0.89         5
         832       1.00      0.00      0.00         4
         833       0.44      1.00      0.62         4
         834       0.29      0.67      0.40         9
         835       1.00      1.00      1.00         5
         836       0.40      0.22      0.29         9
         837       0.40      0.50      0.44         4
         838       0.75      0.75      0.75         4
         839       0.80      1.00      0.89         4
         840       0.00      0.00      0.00         4
         841       1.00      0.75      0.86         4
         842       0.67      1.00      0.80         4
         843       1.00      1.00      1.00         4
         844       0.44      0.80      0.57         5
         845       0.14      0.25      0.18         4
         846       0.67      0.50      0.57         4
         847       0.00      0.00      0.00         4
         848       0.57      0.80      0.67         5
         849       1.00      0.80      0.89         5
         850       1.00      0.11      0.20         9
         851       0.75      0.75      0.75         8
         852       0.25      0.25      0.25         4
         853       0.25      0.22      0.24         9
         854       1.00      1.00      1.00         5
         855       0.60      0.75      0.67         4
         856       1.00      0.75      0.86         4
         857       1.00      0.25      0.40         4
         858       1.00      0.00      0.00         4
         859       0.80      1.00      0.89         4
         860       1.00      1.00      1.00         4
         861       1.00      1.00      1.00         4
         862       0.83      1.00      0.91         5
         863       0.50      0.67      0.57         9
         864       1.00      0.20      0.33         5
         865       1.00      1.00      1.00         4
         866       0.60      0.75      0.67         4
         867       0.50      0.50      0.50         4
         868       0.00      0.00      0.00         5
         869       1.00      1.00      1.00         5
         870       0.60      0.75      0.67         4
         871       0.33      0.50      0.40         4
         872       0.38      0.75      0.50         4
         873       1.00      0.20      0.33         5
         874       0.00      0.00      0.00         4
         875       0.78      0.78      0.78         9
         876       0.50      0.75      0.60         4
         877       0.00      0.00      0.00         4
         878       0.90      1.00      0.95         9
         879       0.50      0.50      0.50         4
         880       0.00      0.00      0.00         4
         881       0.00      0.00      0.00         4
         882       0.00      0.00      0.00         4
         883       0.57      0.89      0.70         9
         884       0.56      1.00      0.71         5
         885       1.00      0.00      0.00         4
         886       0.60      0.75      0.67         4
         887       0.19      1.00      0.32         4
         888       0.75      0.60      0.67         5
         889       0.20      0.25      0.22         4
         890       0.40      1.00      0.57         4
         891       0.33      0.75      0.46         4
         892       0.67      1.00      0.80         4
         893       1.00      0.78      0.88         9

    accuracy                           0.60      4917
   macro avg       0.67      0.58      0.57      4917
weighted avg       0.67      0.60      0.59      4917

