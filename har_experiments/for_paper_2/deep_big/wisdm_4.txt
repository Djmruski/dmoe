CLASS_ORDER: [334, 796, 25, 473, 214, 697, 550, 489, 479, 105, 553, 541, 245, 365, 832, 372, 498, 571, 802, 39, 698, 106, 738, 872, 10, 509, 750, 384, 184, 459, 466, 230, 160, 437, 456, 361, 374, 143, 232, 204, 893, 166, 310, 702, 780, 236, 85, 602, 317, 726, 863, 618, 33, 169, 270, 438, 480, 426, 536, 59, 435, 71, 299, 404, 534, 320, 451, 216, 735, 824, 359, 590, 217, 183, 485, 678, 594, 741, 545, 224, 369, 487, 732, 123, 608, 734, 54, 701, 193, 642, 664, 430, 475, 839, 60, 5, 164, 593, 378, 401, 222, 362, 828, 90, 393, 343, 493, 652, 516, 173, 377, 94, 714, 782, 198, 850, 727, 200, 520, 243, 745, 671, 366, 301, 411, 282, 797, 315, 18, 659, 241, 566, 661, 757, 584, 326, 617, 269, 763, 787, 323, 858, 586, 569, 605, 367, 49, 207, 422, 322, 624, 8, 470, 405, 767, 751, 891, 647, 526, 805, 142, 570, 119, 254, 563, 665, 576, 696, 80, 68, 555, 539, 428, 20, 167, 107, 807, 736, 445, 464, 47, 619, 424, 851, 476, 65, 388, 0, 91, 443, 471, 623, 330, 138, 392, 502, 337, 389, 756, 329, 484, 165, 159, 557, 810, 825, 775, 208, 294, 13, 504, 690, 495, 179, 517, 634, 172, 731, 656, 546, 646, 158, 518, 651, 345, 627, 3, 621, 491, 632, 342, 859, 496, 186, 410, 431, 458, 201, 29, 336, 156, 694, 134, 655, 666, 34, 759, 135, 742, 127, 486, 715, 341, 572, 711, 879, 710, 837, 286, 258, 890, 845, 483, 633, 399, 74, 657, 611, 601, 748, 754, 700, 565, 679, 591, 834, 615, 455, 319, 260, 226, 675, 112, 454, 285, 588, 743, 12, 211, 16, 339, 549, 838, 364, 253, 121, 447, 719, 171, 195, 218, 300, 43, 277, 477, 716, 452, 331, 45, 501, 533, 223, 673, 794, 379, 599, 855, 706, 412, 793, 567, 325, 354, 93, 674, 849, 356, 687, 713, 781, 478, 869, 789, 875, 635, 185, 831, 808, 162, 308, 225, 689, 266, 73, 864, 425, 352, 641, 444, 705, 19, 560, 154, 852, 752, 874, 723, 497, 603, 212, 279, 97, 340, 287, 768, 804, 597, 770, 463, 111, 209, 387, 692, 654, 88, 115, 427, 531, 822, 799, 749, 747, 283, 231, 668, 291, 564, 190, 413, 511, 779, 525, 503, 187, 695, 547, 148, 281, 351, 335, 227, 556, 375, 629, 272, 507, 888, 406, 89, 829, 649, 559, 235, 178, 109, 176, 188, 355, 663, 382, 815, 867, 862, 622, 95, 78, 267, 402, 58, 527, 6, 640, 30, 769, 449, 699, 614, 885, 48, 809, 871, 332, 349, 680, 86, 889, 177, 432, 219, 350, 817, 124, 468, 237, 551, 298, 81, 414, 442, 257, 725, 870, 524, 69, 248, 535, 868, 803, 630, 181, 537, 776, 508, 306, 755, 400, 312, 24, 448, 636, 638, 153, 290, 274, 505, 523, 318, 887, 55, 643, 199, 612, 609, 577, 346, 811, 457, 854, 371, 273, 819, 785, 528, 532, 733, 465, 842, 853, 314, 686, 814, 510, 360, 46, 259, 191, 316, 129, 861, 467, 786, 753, 585, 1, 394, 126, 110, 429, 63, 766, 709, 856, 221, 189, 229, 289, 161, 407, 344, 490, 762, 145, 841, 830, 441, 252, 436, 801, 722, 568, 206, 791, 450, 194, 36, 684, 739, 144, 233, 117, 108, 877, 579, 113, 303, 765, 598, 625, 583, 327, 240, 616, 691, 681, 297, 433, 880, 876, 823, 415, 878, 746, 103, 865, 99, 538, 175, 132, 481, 860, 295, 730, 14, 540, 293, 688, 499, 77, 157, 840, 658, 53, 561, 44, 613, 92, 820, 42, 774, 262, 239, 672, 228, 155, 170, 251, 573, 57, 51, 403, 210, 558, 304, 133, 420, 242, 292, 818, 338, 778, 637, 247, 606, 884, 631, 708, 514, 720, 353, 670, 114, 96, 806, 773, 607, 246, 41, 833, 309, 729, 137, 682, 744, 35, 761, 513, 390, 446, 61, 409, 27, 610, 521, 288, 373, 66, 650, 639, 101, 278, 866, 653, 717, 626, 578, 703, 423, 575, 843, 581, 552, 141, 40, 857, 592, 17, 892, 83, 784, 75, 718, 439, 79, 582, 771, 264, 721, 9, 98, 530, 506, 685, 249, 529, 795, 881, 324, 238, 460, 522, 302, 472, 363, 120, 554, 600, 383, 790, 544, 192, 76, 434, 182, 376, 512, 515, 385, 70, 7, 130, 777, 197, 100, 580, 660, 883, 31, 707, 203, 102, 328, 596, 469, 56, 500, 462, 205, 131, 662, 87, 474, 84, 370, 669, 62, 645, 4, 827, 846, 215, 882, 886, 548, 311, 202, 764, 408, 268, 667, 104, 492, 147, 168, 174, 677, 418, 396, 23, 821, 737, 140, 64, 296, 488, 21, 15, 196, 22, 826, 52, 395, 28, 798, 542, 800, 519, 276, 255, 728, 391, 421, 595, 813, 150, 284, 628, 783, 380, 740, 368, 836, 263, 118, 163, 772, 847, 271, 180, 835, 453, 760, 574, 2, 386, 125, 213, 724, 275, 788, 848, 122, 26, 307, 50, 280, 648, 151, 358, 543, 38, 82, 152, 313, 844, 146, 587, 693, 644, 244, 416, 139, 256, 116, 620, 792, 589, 873, 347, 32, 440, 149, 397, 220, 234, 683, 67, 11, 261, 461, 676, 562, 816, 398, 494, 758, 381, 250, 357, 812, 482, 604, 348, 704, 417, 712, 37, 265, 419, 321, 333, 128, 72, 136, 305]
class_group: [(334, 796, 25, 473, 214, 697, 550, 489, 479, 105, 553, 541, 245, 365, 832, 372, 498, 571, 802, 39, 698, 106, 738, 872, 10, 509, 750, 384, 184, 459, 466, 230, 160, 437), (456, 361, 374, 143, 232, 204, 893, 166, 310, 702, 780, 236, 85, 602, 317, 726, 863, 618, 33, 169), (270, 438, 480, 426, 536, 59, 435, 71, 299, 404, 534, 320, 451, 216, 735, 824, 359, 590, 217, 183), (485, 678, 594, 741, 545, 224, 369, 487, 732, 123, 608, 734, 54, 701, 193, 642, 664, 430, 475, 839), (60, 5, 164, 593, 378, 401, 222, 362, 828, 90, 393, 343, 493, 652, 516, 173, 377, 94, 714, 782), (198, 850, 727, 200, 520, 243, 745, 671, 366, 301, 411, 282, 797, 315, 18, 659, 241, 566, 661, 757), (584, 326, 617, 269, 763, 787, 323, 858, 586, 569, 605, 367, 49, 207, 422, 322, 624, 8, 470, 405), (767, 751, 891, 647, 526, 805, 142, 570, 119, 254, 563, 665, 576, 696, 80, 68, 555, 539, 428, 20), (167, 107, 807, 736, 445, 464, 47, 619, 424, 851, 476, 65, 388, 0, 91, 443, 471, 623, 330, 138), (392, 502, 337, 389, 756, 329, 484, 165, 159, 557, 810, 825, 775, 208, 294, 13, 504, 690, 495, 179), (517, 634, 172, 731, 656, 546, 646, 158, 518, 651, 345, 627, 3, 621, 491, 632, 342, 859, 496, 186), (410, 431, 458, 201, 29, 336, 156, 694, 134, 655, 666, 34, 759, 135, 742, 127, 486, 715, 341, 572), (711, 879, 710, 837, 286, 258, 890, 845, 483, 633, 399, 74, 657, 611, 601, 748, 754, 700, 565, 679), (591, 834, 615, 455, 319, 260, 226, 675, 112, 454, 285, 588, 743, 12, 211, 16, 339, 549, 838, 364), (253, 121, 447, 719, 171, 195, 218, 300, 43, 277, 477, 716, 452, 331, 45, 501, 533, 223, 673, 794), (379, 599, 855, 706, 412, 793, 567, 325, 354, 93, 674, 849, 356, 687, 713, 781, 478, 869, 789, 875), (635, 185, 831, 808, 162, 308, 225, 689, 266, 73, 864, 425, 352, 641, 444, 705, 19, 560, 154, 852), (752, 874, 723, 497, 603, 212, 279, 97, 340, 287, 768, 804, 597, 770, 463, 111, 209, 387, 692, 654), (88, 115, 427, 531, 822, 799, 749, 747, 283, 231, 668, 291, 564, 190, 413, 511, 779, 525, 503, 187), (695, 547, 148, 281, 351, 335, 227, 556, 375, 629, 272, 507, 888, 406, 89, 829, 649, 559, 235, 178), (109, 176, 188, 355, 663, 382, 815, 867, 862, 622, 95, 78, 267, 402, 58, 527, 6, 640, 30, 769), (449, 699, 614, 885, 48, 809, 871, 332, 349, 680, 86, 889, 177, 432, 219, 350, 817, 124, 468, 237), (551, 298, 81, 414, 442, 257, 725, 870, 524, 69, 248, 535, 868, 803, 630, 181, 537, 776, 508, 306), (755, 400, 312, 24, 448, 636, 638, 153, 290, 274, 505, 523, 318, 887, 55, 643, 199, 612, 609, 577), (346, 811, 457, 854, 371, 273, 819, 785, 528, 532, 733, 465, 842, 853, 314, 686, 814, 510, 360, 46), (259, 191, 316, 129, 861, 467, 786, 753, 585, 1, 394, 126, 110, 429, 63, 766, 709, 856, 221, 189), (229, 289, 161, 407, 344, 490, 762, 145, 841, 830, 441, 252, 436, 801, 722, 568, 206, 791, 450, 194), (36, 684, 739, 144, 233, 117, 108, 877, 579, 113, 303, 765, 598, 625, 583, 327, 240, 616, 691, 681), (297, 433, 880, 876, 823, 415, 878, 746, 103, 865, 99, 538, 175, 132, 481, 860, 295, 730, 14, 540), (293, 688, 499, 77, 157, 840, 658, 53, 561, 44, 613, 92, 820, 42, 774, 262, 239, 672, 228, 155), (170, 251, 573, 57, 51, 403, 210, 558, 304, 133, 420, 242, 292, 818, 338, 778, 637, 247, 606, 884), (631, 708, 514, 720, 353, 670, 114, 96, 806, 773, 607, 246, 41, 833, 309, 729, 137, 682, 744, 35), (761, 513, 390, 446, 61, 409, 27, 610, 521, 288, 373, 66, 650, 639, 101, 278, 866, 653, 717, 626), (578, 703, 423, 575, 843, 581, 552, 141, 40, 857, 592, 17, 892, 83, 784, 75, 718, 439, 79, 582), (771, 264, 721, 9, 98, 530, 506, 685, 249, 529, 795, 881, 324, 238, 460, 522, 302, 472, 363, 120), (554, 600, 383, 790, 544, 192, 76, 434, 182, 376, 512, 515, 385, 70, 7, 130, 777, 197, 100, 580), (660, 883, 31, 707, 203, 102, 328, 596, 469, 56, 500, 462, 205, 131, 662, 87, 474, 84, 370, 669), (62, 645, 4, 827, 846, 215, 882, 886, 548, 311, 202, 764, 408, 268, 667, 104, 492, 147, 168, 174), (677, 418, 396, 23, 821, 737, 140, 64, 296, 488, 21, 15, 196, 22, 826, 52, 395, 28, 798, 542), (800, 519, 276, 255, 728, 391, 421, 595, 813, 150, 284, 628, 783, 380, 740, 368, 836, 263, 118, 163), (772, 847, 271, 180, 835, 453, 760, 574, 2, 386, 125, 213, 724, 275, 788, 848, 122, 26, 307, 50), (280, 648, 151, 358, 543, 38, 82, 152, 313, 844, 146, 587, 693, 644, 244, 416, 139, 256, 116, 620), (792, 589, 873, 347, 32, 440, 149, 397, 220, 234, 683, 67, 11, 261, 461, 676, 562, 816, 398, 494), (758, 381, 250, 357, 812, 482, 604, 348, 704, 417, 712, 37, 265, 419, 321, 333, 128, 72, 136, 305)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.9577	step1_train_accuracy: 59.2369
STEP-1	Epoch: 20/50	loss: 1.0448	step1_train_accuracy: 85.5422
STEP-1	Epoch: 30/50	loss: 0.6076	step1_train_accuracy: 94.9799
STEP-1	Epoch: 40/50	loss: 0.3885	step1_train_accuracy: 96.3855
STEP-1	Epoch: 50/50	loss: 0.2693	step1_train_accuracy: 97.3896
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.0631	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.0566	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.0535	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0516	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0501	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0491	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0483	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0476	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0471	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0467	gate_loss: 0.0000	step2_classification_accuracy: 97.0588	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 98.4000	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.8952	step1_train_accuracy: 64.8903
STEP-1	Epoch: 20/50	loss: 0.9731	step1_train_accuracy: 87.7743
STEP-1	Epoch: 30/50	loss: 0.5617	step1_train_accuracy: 92.4765
STEP-1	Epoch: 40/50	loss: 0.3610	step1_train_accuracy: 95.2978
STEP-1	Epoch: 50/50	loss: 0.2510	step1_train_accuracy: 97.1787
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1300	gate_loss: 0.3714	step2_classification_accuracy: 95.9259	step_2_gate_accuracy: 87.7778
STEP-2	Epoch: 40/200	classification_loss: 0.1010	gate_loss: 0.1689	step2_classification_accuracy: 96.4815	step_2_gate_accuracy: 96.2963
STEP-2	Epoch: 60/200	classification_loss: 0.0988	gate_loss: 0.0997	step2_classification_accuracy: 96.6667	step_2_gate_accuracy: 98.1481
STEP-2	Epoch: 80/200	classification_loss: 0.0881	gate_loss: 0.0711	step2_classification_accuracy: 96.4815	step_2_gate_accuracy: 97.9630
STEP-2	Epoch: 100/200	classification_loss: 0.0755	gate_loss: 0.0551	step2_classification_accuracy: 96.6667	step_2_gate_accuracy: 98.3333
STEP-2	Epoch: 120/200	classification_loss: 0.0705	gate_loss: 0.0445	step2_classification_accuracy: 97.5926	step_2_gate_accuracy: 99.2593
STEP-2	Epoch: 140/200	classification_loss: 0.0787	gate_loss: 0.0426	step2_classification_accuracy: 96.2963	step_2_gate_accuracy: 98.1481
STEP-2	Epoch: 160/200	classification_loss: 0.0724	gate_loss: 0.0389	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 98.7037
STEP-2	Epoch: 180/200	classification_loss: 0.0808	gate_loss: 0.0369	step2_classification_accuracy: 96.4815	step_2_gate_accuracy: 98.3333
STEP-2	Epoch: 200/200	classification_loss: 0.0643	gate_loss: 0.0331	step2_classification_accuracy: 97.2222	step_2_gate_accuracy: 98.5185
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 95.2000	gate_accuracy: 96.8000
	Task-1	val_accuracy: 92.5000	gate_accuracy: 90.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 94.1463


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 1.7284	step1_train_accuracy: 72.4852
STEP-1	Epoch: 20/50	loss: 0.7276	step1_train_accuracy: 90.8284
STEP-1	Epoch: 30/50	loss: 0.4017	step1_train_accuracy: 96.7456
STEP-1	Epoch: 40/50	loss: 0.2635	step1_train_accuracy: 98.2249
STEP-1	Epoch: 50/50	loss: 0.1892	step1_train_accuracy: 98.5207
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2272	gate_loss: 0.5882	step2_classification_accuracy: 92.9730	step_2_gate_accuracy: 83.6486
STEP-2	Epoch: 40/200	classification_loss: 0.1418	gate_loss: 0.2500	step2_classification_accuracy: 94.5946	step_2_gate_accuracy: 95.5405
STEP-2	Epoch: 60/200	classification_loss: 0.1346	gate_loss: 0.1336	step2_classification_accuracy: 94.5946	step_2_gate_accuracy: 97.1622
STEP-2	Epoch: 80/200	classification_loss: 0.1228	gate_loss: 0.0859	step2_classification_accuracy: 95.1351	step_2_gate_accuracy: 97.9730
STEP-2	Epoch: 100/200	classification_loss: 0.1173	gate_loss: 0.0679	step2_classification_accuracy: 94.8649	step_2_gate_accuracy: 97.7027
STEP-2	Epoch: 120/200	classification_loss: 0.1131	gate_loss: 0.0571	step2_classification_accuracy: 95.1351	step_2_gate_accuracy: 98.1081
STEP-2	Epoch: 140/200	classification_loss: 0.1100	gate_loss: 0.0512	step2_classification_accuracy: 95.2703	step_2_gate_accuracy: 98.1081
STEP-2	Epoch: 160/200	classification_loss: 0.1039	gate_loss: 0.0462	step2_classification_accuracy: 95.8108	step_2_gate_accuracy: 98.6487
STEP-2	Epoch: 180/200	classification_loss: 0.1027	gate_loss: 0.0427	step2_classification_accuracy: 95.4054	step_2_gate_accuracy: 98.2432
STEP-2	Epoch: 200/200	classification_loss: 0.0977	gate_loss: 0.0399	step2_classification_accuracy: 95.9459	step_2_gate_accuracy: 98.5135
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 92.8000	gate_accuracy: 94.4000
	Task-1	val_accuracy: 91.2500	gate_accuracy: 91.2500
	Task-2	val_accuracy: 88.0952	gate_accuracy: 88.0952
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 91.6955


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 1.6317	step1_train_accuracy: 76.1468
STEP-1	Epoch: 20/50	loss: 0.6192	step1_train_accuracy: 90.3670
STEP-1	Epoch: 30/50	loss: 0.3388	step1_train_accuracy: 96.7890
STEP-1	Epoch: 40/50	loss: 0.2128	step1_train_accuracy: 98.3945
STEP-1	Epoch: 50/50	loss: 0.1502	step1_train_accuracy: 99.0826
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1796	gate_loss: 0.8361	step2_classification_accuracy: 93.2979	step_2_gate_accuracy: 71.2766
STEP-2	Epoch: 40/200	classification_loss: 0.1628	gate_loss: 0.3590	step2_classification_accuracy: 94.4681	step_2_gate_accuracy: 93.0851
STEP-2	Epoch: 60/200	classification_loss: 0.1100	gate_loss: 0.1682	step2_classification_accuracy: 95.8511	step_2_gate_accuracy: 97.6596
STEP-2	Epoch: 80/200	classification_loss: 0.0965	gate_loss: 0.0956	step2_classification_accuracy: 96.2766	step_2_gate_accuracy: 98.5106
STEP-2	Epoch: 100/200	classification_loss: 0.0898	gate_loss: 0.0680	step2_classification_accuracy: 96.0638	step_2_gate_accuracy: 98.7234
STEP-2	Epoch: 120/200	classification_loss: 0.0903	gate_loss: 0.0555	step2_classification_accuracy: 96.3830	step_2_gate_accuracy: 98.8298
STEP-2	Epoch: 140/200	classification_loss: 0.0906	gate_loss: 0.0467	step2_classification_accuracy: 95.9574	step_2_gate_accuracy: 98.8298
STEP-2	Epoch: 160/200	classification_loss: 0.0835	gate_loss: 0.0420	step2_classification_accuracy: 96.1702	step_2_gate_accuracy: 98.9362
STEP-2	Epoch: 180/200	classification_loss: 0.0874	gate_loss: 0.0391	step2_classification_accuracy: 96.0638	step_2_gate_accuracy: 98.9362
STEP-2	Epoch: 200/200	classification_loss: 0.0761	gate_loss: 0.0338	step2_classification_accuracy: 96.4894	step_2_gate_accuracy: 99.0425
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 88.8000	gate_accuracy: 92.8000
	Task-1	val_accuracy: 87.5000	gate_accuracy: 86.2500
	Task-2	val_accuracy: 86.9048	gate_accuracy: 89.2857
	Task-3	val_accuracy: 93.5780	gate_accuracy: 95.4128
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 91.4573


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.2470	step1_train_accuracy: 51.1864
STEP-1	Epoch: 20/50	loss: 1.0687	step1_train_accuracy: 86.4407
STEP-1	Epoch: 30/50	loss: 0.5690	step1_train_accuracy: 96.2712
STEP-1	Epoch: 40/50	loss: 0.3684	step1_train_accuracy: 96.2712
STEP-1	Epoch: 50/50	loss: 0.2667	step1_train_accuracy: 97.6271
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2668	gate_loss: 1.0013	step2_classification_accuracy: 91.4912	step_2_gate_accuracy: 67.1930
STEP-2	Epoch: 40/200	classification_loss: 0.1868	gate_loss: 0.4125	step2_classification_accuracy: 92.1053	step_2_gate_accuracy: 89.2982
STEP-2	Epoch: 60/200	classification_loss: 0.1424	gate_loss: 0.1985	step2_classification_accuracy: 94.3860	step_2_gate_accuracy: 95.0000
STEP-2	Epoch: 80/200	classification_loss: 0.1248	gate_loss: 0.1314	step2_classification_accuracy: 94.6491	step_2_gate_accuracy: 96.0526
STEP-2	Epoch: 100/200	classification_loss: 0.1178	gate_loss: 0.1058	step2_classification_accuracy: 95.0000	step_2_gate_accuracy: 96.3158
STEP-2	Epoch: 120/200	classification_loss: 0.1097	gate_loss: 0.0902	step2_classification_accuracy: 95.5263	step_2_gate_accuracy: 96.9298
STEP-2	Epoch: 140/200	classification_loss: 0.1080	gate_loss: 0.0809	step2_classification_accuracy: 95.0000	step_2_gate_accuracy: 97.1053
STEP-2	Epoch: 160/200	classification_loss: 0.1043	gate_loss: 0.0754	step2_classification_accuracy: 95.8772	step_2_gate_accuracy: 97.1053
STEP-2	Epoch: 180/200	classification_loss: 0.1016	gate_loss: 0.0692	step2_classification_accuracy: 95.5263	step_2_gate_accuracy: 97.0175
STEP-2	Epoch: 200/200	classification_loss: 0.1023	gate_loss: 0.0704	step2_classification_accuracy: 95.2632	step_2_gate_accuracy: 97.1053
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 81.6000	gate_accuracy: 85.6000
	Task-1	val_accuracy: 88.7500	gate_accuracy: 83.7500
	Task-2	val_accuracy: 84.5238	gate_accuracy: 80.9524
	Task-3	val_accuracy: 92.6606	gate_accuracy: 92.6606
	Task-4	val_accuracy: 86.4865	gate_accuracy: 79.7297
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 85.1695


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.1666	step1_train_accuracy: 47.5155
STEP-1	Epoch: 20/50	loss: 0.9253	step1_train_accuracy: 77.3292
STEP-1	Epoch: 30/50	loss: 0.5287	step1_train_accuracy: 84.1615
STEP-1	Epoch: 40/50	loss: 0.4057	step1_train_accuracy: 87.8882
STEP-1	Epoch: 50/50	loss: 0.3075	step1_train_accuracy: 88.8199
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3167	gate_loss: 1.1140	step2_classification_accuracy: 89.4776	step_2_gate_accuracy: 65.8955
STEP-2	Epoch: 40/200	classification_loss: 0.2149	gate_loss: 0.4205	step2_classification_accuracy: 92.8358	step_2_gate_accuracy: 91.1194
STEP-2	Epoch: 60/200	classification_loss: 0.1712	gate_loss: 0.2069	step2_classification_accuracy: 93.5821	step_2_gate_accuracy: 95.2239
STEP-2	Epoch: 80/200	classification_loss: 0.1543	gate_loss: 0.1393	step2_classification_accuracy: 94.2537	step_2_gate_accuracy: 96.6418
STEP-2	Epoch: 100/200	classification_loss: 0.1450	gate_loss: 0.1057	step2_classification_accuracy: 94.4776	step_2_gate_accuracy: 97.3134
STEP-2	Epoch: 120/200	classification_loss: 0.1358	gate_loss: 0.0865	step2_classification_accuracy: 94.3284	step_2_gate_accuracy: 97.5373
STEP-2	Epoch: 140/200	classification_loss: 0.1234	gate_loss: 0.0702	step2_classification_accuracy: 94.8507	step_2_gate_accuracy: 98.2836
STEP-2	Epoch: 160/200	classification_loss: 0.1284	gate_loss: 0.0672	step2_classification_accuracy: 94.7015	step_2_gate_accuracy: 97.6866
STEP-2	Epoch: 180/200	classification_loss: 0.1168	gate_loss: 0.0566	step2_classification_accuracy: 95.0746	step_2_gate_accuracy: 98.0597
STEP-2	Epoch: 200/200	classification_loss: 0.1138	gate_loss: 0.0523	step2_classification_accuracy: 94.9254	step_2_gate_accuracy: 98.4328
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.0000	gate_accuracy: 89.6000
	Task-1	val_accuracy: 81.2500	gate_accuracy: 82.5000
	Task-2	val_accuracy: 82.1429	gate_accuracy: 84.5238
	Task-3	val_accuracy: 91.7431	gate_accuracy: 90.8257
	Task-4	val_accuracy: 78.3784	gate_accuracy: 78.3784
	Task-5	val_accuracy: 69.1358	gate_accuracy: 85.1852
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 85.8951


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 2.0505	step1_train_accuracy: 57.6705
STEP-1	Epoch: 20/50	loss: 0.9898	step1_train_accuracy: 77.2727
STEP-1	Epoch: 30/50	loss: 0.5505	step1_train_accuracy: 93.1818
STEP-1	Epoch: 40/50	loss: 0.3478	step1_train_accuracy: 97.4432
STEP-1	Epoch: 50/50	loss: 0.2376	step1_train_accuracy: 97.4432
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3734	gate_loss: 1.2935	step2_classification_accuracy: 87.3377	step_2_gate_accuracy: 59.6753
STEP-2	Epoch: 40/200	classification_loss: 0.2315	gate_loss: 0.5165	step2_classification_accuracy: 91.8182	step_2_gate_accuracy: 87.2727
STEP-2	Epoch: 60/200	classification_loss: 0.1993	gate_loss: 0.2521	step2_classification_accuracy: 92.4026	step_2_gate_accuracy: 94.2208
STEP-2	Epoch: 80/200	classification_loss: 0.1833	gate_loss: 0.1604	step2_classification_accuracy: 93.2468	step_2_gate_accuracy: 96.1688
STEP-2	Epoch: 100/200	classification_loss: 0.1592	gate_loss: 0.1170	step2_classification_accuracy: 93.6364	step_2_gate_accuracy: 97.0130
STEP-2	Epoch: 120/200	classification_loss: 0.1546	gate_loss: 0.0964	step2_classification_accuracy: 93.5714	step_2_gate_accuracy: 97.1429
STEP-2	Epoch: 140/200	classification_loss: 0.1399	gate_loss: 0.0828	step2_classification_accuracy: 94.0909	step_2_gate_accuracy: 97.4675
STEP-2	Epoch: 160/200	classification_loss: 0.1265	gate_loss: 0.0674	step2_classification_accuracy: 94.6104	step_2_gate_accuracy: 97.9221
STEP-2	Epoch: 180/200	classification_loss: 0.1218	gate_loss: 0.0640	step2_classification_accuracy: 94.9351	step_2_gate_accuracy: 98.3117
STEP-2	Epoch: 200/200	classification_loss: 0.1307	gate_loss: 0.0650	step2_classification_accuracy: 94.2857	step_2_gate_accuracy: 97.7273
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.2000	gate_accuracy: 86.4000
	Task-1	val_accuracy: 86.2500	gate_accuracy: 86.2500
	Task-2	val_accuracy: 78.5714	gate_accuracy: 79.7619
	Task-3	val_accuracy: 86.2385	gate_accuracy: 86.2385
	Task-4	val_accuracy: 85.1351	gate_accuracy: 83.7838
	Task-5	val_accuracy: 69.1358	gate_accuracy: 83.9506
	Task-6	val_accuracy: 86.3636	gate_accuracy: 87.5000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 85.0234


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 2.3600	step1_train_accuracy: 41.2587
STEP-1	Epoch: 20/50	loss: 1.0840	step1_train_accuracy: 77.6224
STEP-1	Epoch: 30/50	loss: 0.6690	step1_train_accuracy: 86.0140
STEP-1	Epoch: 40/50	loss: 0.4790	step1_train_accuracy: 91.2587
STEP-1	Epoch: 50/50	loss: 0.3697	step1_train_accuracy: 92.6573
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3797	gate_loss: 1.4265	step2_classification_accuracy: 87.4713	step_2_gate_accuracy: 55.6897
STEP-2	Epoch: 40/200	classification_loss: 0.2697	gate_loss: 0.5954	step2_classification_accuracy: 90.9195	step_2_gate_accuracy: 85.4023
STEP-2	Epoch: 60/200	classification_loss: 0.2027	gate_loss: 0.2711	step2_classification_accuracy: 92.6437	step_2_gate_accuracy: 94.3103
STEP-2	Epoch: 80/200	classification_loss: 0.1766	gate_loss: 0.1644	step2_classification_accuracy: 93.2184	step_2_gate_accuracy: 96.6667
STEP-2	Epoch: 100/200	classification_loss: 0.1694	gate_loss: 0.1216	step2_classification_accuracy: 93.8506	step_2_gate_accuracy: 97.2414
STEP-2	Epoch: 120/200	classification_loss: 0.1448	gate_loss: 0.0920	step2_classification_accuracy: 94.3678	step_2_gate_accuracy: 97.2414
STEP-2	Epoch: 140/200	classification_loss: 0.1384	gate_loss: 0.0807	step2_classification_accuracy: 94.6552	step_2_gate_accuracy: 97.4138
STEP-2	Epoch: 160/200	classification_loss: 0.1347	gate_loss: 0.0712	step2_classification_accuracy: 94.6552	step_2_gate_accuracy: 97.8736
STEP-2	Epoch: 180/200	classification_loss: 0.1262	gate_loss: 0.0621	step2_classification_accuracy: 94.8851	step_2_gate_accuracy: 97.9885
STEP-2	Epoch: 200/200	classification_loss: 0.1231	gate_loss: 0.0581	step2_classification_accuracy: 94.8851	step_2_gate_accuracy: 98.0460
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.6000	gate_accuracy: 81.6000
	Task-1	val_accuracy: 82.5000	gate_accuracy: 85.0000
	Task-2	val_accuracy: 80.9524	gate_accuracy: 79.7619
	Task-3	val_accuracy: 84.4037	gate_accuracy: 83.4862
	Task-4	val_accuracy: 79.7297	gate_accuracy: 78.3784
	Task-5	val_accuracy: 67.9012	gate_accuracy: 77.7778
	Task-6	val_accuracy: 86.3636	gate_accuracy: 82.9545
	Task-7	val_accuracy: 86.1111	gate_accuracy: 87.5000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.0477


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 1.7648	step1_train_accuracy: 64.9215
STEP-1	Epoch: 20/50	loss: 0.7070	step1_train_accuracy: 92.9319
STEP-1	Epoch: 30/50	loss: 0.3506	step1_train_accuracy: 96.8586
STEP-1	Epoch: 40/50	loss: 0.2197	step1_train_accuracy: 98.4293
STEP-1	Epoch: 50/50	loss: 0.1571	step1_train_accuracy: 98.4293
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3784	gate_loss: 1.5216	step2_classification_accuracy: 87.8351	step_2_gate_accuracy: 51.8041
STEP-2	Epoch: 40/200	classification_loss: 0.2565	gate_loss: 0.6211	step2_classification_accuracy: 91.4433	step_2_gate_accuracy: 84.7423
STEP-2	Epoch: 60/200	classification_loss: 0.1968	gate_loss: 0.2920	step2_classification_accuracy: 93.3505	step_2_gate_accuracy: 94.0206
STEP-2	Epoch: 80/200	classification_loss: 0.1717	gate_loss: 0.1807	step2_classification_accuracy: 94.1753	step_2_gate_accuracy: 96.0309
STEP-2	Epoch: 100/200	classification_loss: 0.1486	gate_loss: 0.1303	step2_classification_accuracy: 94.5876	step_2_gate_accuracy: 96.7526
STEP-2	Epoch: 120/200	classification_loss: 0.1412	gate_loss: 0.1052	step2_classification_accuracy: 95.0000	step_2_gate_accuracy: 96.9072
STEP-2	Epoch: 140/200	classification_loss: 0.1348	gate_loss: 0.0910	step2_classification_accuracy: 94.9485	step_2_gate_accuracy: 97.6804
STEP-2	Epoch: 160/200	classification_loss: 0.1232	gate_loss: 0.0763	step2_classification_accuracy: 95.6701	step_2_gate_accuracy: 98.0928
STEP-2	Epoch: 180/200	classification_loss: 0.1243	gate_loss: 0.0705	step2_classification_accuracy: 95.3608	step_2_gate_accuracy: 97.7320
STEP-2	Epoch: 200/200	classification_loss: 0.1181	gate_loss: 0.0633	step2_classification_accuracy: 95.5670	step_2_gate_accuracy: 98.0928
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.0000	gate_accuracy: 81.6000
	Task-1	val_accuracy: 85.0000	gate_accuracy: 86.2500
	Task-2	val_accuracy: 76.1905	gate_accuracy: 77.3810
	Task-3	val_accuracy: 84.4037	gate_accuracy: 88.9908
	Task-4	val_accuracy: 82.4324	gate_accuracy: 79.7297
	Task-5	val_accuracy: 64.1975	gate_accuracy: 81.4815
	Task-6	val_accuracy: 84.0909	gate_accuracy: 78.4091
	Task-7	val_accuracy: 88.8889	gate_accuracy: 91.6667
	Task-8	val_accuracy: 80.0000	gate_accuracy: 73.6842
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.0545


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.4955	step1_train_accuracy: 37.9195
STEP-1	Epoch: 20/50	loss: 1.0978	step1_train_accuracy: 83.5570
STEP-1	Epoch: 30/50	loss: 0.5330	step1_train_accuracy: 93.9597
STEP-1	Epoch: 40/50	loss: 0.3339	step1_train_accuracy: 96.3087
STEP-1	Epoch: 50/50	loss: 0.2453	step1_train_accuracy: 95.3020
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4684	gate_loss: 1.6514	step2_classification_accuracy: 82.4299	step_2_gate_accuracy: 46.4486
STEP-2	Epoch: 40/200	classification_loss: 0.3477	gate_loss: 0.7211	step2_classification_accuracy: 86.8224	step_2_gate_accuracy: 80.0935
STEP-2	Epoch: 60/200	classification_loss: 0.2631	gate_loss: 0.3751	step2_classification_accuracy: 89.6729	step_2_gate_accuracy: 90.0467
STEP-2	Epoch: 80/200	classification_loss: 0.2315	gate_loss: 0.2497	step2_classification_accuracy: 90.6542	step_2_gate_accuracy: 93.1308
STEP-2	Epoch: 100/200	classification_loss: 0.2041	gate_loss: 0.1870	step2_classification_accuracy: 91.7757	step_2_gate_accuracy: 94.4392
STEP-2	Epoch: 120/200	classification_loss: 0.1931	gate_loss: 0.1529	step2_classification_accuracy: 92.0093	step_2_gate_accuracy: 95.0467
STEP-2	Epoch: 140/200	classification_loss: 0.1694	gate_loss: 0.1295	step2_classification_accuracy: 92.8972	step_2_gate_accuracy: 95.3738
STEP-2	Epoch: 160/200	classification_loss: 0.1530	gate_loss: 0.1077	step2_classification_accuracy: 93.2243	step_2_gate_accuracy: 96.7757
STEP-2	Epoch: 180/200	classification_loss: 0.1437	gate_loss: 0.0961	step2_classification_accuracy: 93.9252	step_2_gate_accuracy: 96.9626
STEP-2	Epoch: 200/200	classification_loss: 0.1282	gate_loss: 0.0808	step2_classification_accuracy: 94.6262	step_2_gate_accuracy: 97.7570
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.6000	gate_accuracy: 83.2000
	Task-1	val_accuracy: 82.5000	gate_accuracy: 81.2500
	Task-2	val_accuracy: 75.0000	gate_accuracy: 76.1905
	Task-3	val_accuracy: 79.8165	gate_accuracy: 80.7339
	Task-4	val_accuracy: 75.6757	gate_accuracy: 74.3243
	Task-5	val_accuracy: 58.0247	gate_accuracy: 77.7778
	Task-6	val_accuracy: 79.5455	gate_accuracy: 76.1364
	Task-7	val_accuracy: 87.5000	gate_accuracy: 83.3333
	Task-8	val_accuracy: 72.6316	gate_accuracy: 69.4737
	Task-9	val_accuracy: 77.3333	gate_accuracy: 77.3333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.1427


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.2318	step1_train_accuracy: 52.7607
STEP-1	Epoch: 20/50	loss: 0.8338	step1_train_accuracy: 92.9448
STEP-1	Epoch: 30/50	loss: 0.3864	step1_train_accuracy: 96.6258
STEP-1	Epoch: 40/50	loss: 0.2362	step1_train_accuracy: 98.4663
STEP-1	Epoch: 50/50	loss: 0.1664	step1_train_accuracy: 98.7730
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4863	gate_loss: 1.6940	step2_classification_accuracy: 82.6923	step_2_gate_accuracy: 49.1880
STEP-2	Epoch: 40/200	classification_loss: 0.3452	gate_loss: 0.7185	step2_classification_accuracy: 87.4359	step_2_gate_accuracy: 80.9829
STEP-2	Epoch: 60/200	classification_loss: 0.2715	gate_loss: 0.3618	step2_classification_accuracy: 90.0000	step_2_gate_accuracy: 90.6838
STEP-2	Epoch: 80/200	classification_loss: 0.2402	gate_loss: 0.2406	step2_classification_accuracy: 90.5983	step_2_gate_accuracy: 93.5043
STEP-2	Epoch: 100/200	classification_loss: 0.2102	gate_loss: 0.1774	step2_classification_accuracy: 91.7094	step_2_gate_accuracy: 95.1709
STEP-2	Epoch: 120/200	classification_loss: 0.2007	gate_loss: 0.1497	step2_classification_accuracy: 92.1368	step_2_gate_accuracy: 95.2564
STEP-2	Epoch: 140/200	classification_loss: 0.1870	gate_loss: 0.1274	step2_classification_accuracy: 92.7350	step_2_gate_accuracy: 96.0256
STEP-2	Epoch: 160/200	classification_loss: 0.1801	gate_loss: 0.1167	step2_classification_accuracy: 92.6068	step_2_gate_accuracy: 95.9829
STEP-2	Epoch: 180/200	classification_loss: 0.1710	gate_loss: 0.1049	step2_classification_accuracy: 93.0769	step_2_gate_accuracy: 96.7521
STEP-2	Epoch: 200/200	classification_loss: 0.1610	gate_loss: 0.0941	step2_classification_accuracy: 93.4615	step_2_gate_accuracy: 96.8803
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.6000	gate_accuracy: 77.6000
	Task-1	val_accuracy: 80.0000	gate_accuracy: 82.5000
	Task-2	val_accuracy: 71.4286	gate_accuracy: 76.1905
	Task-3	val_accuracy: 79.8165	gate_accuracy: 82.5688
	Task-4	val_accuracy: 75.6757	gate_accuracy: 79.7297
	Task-5	val_accuracy: 60.4938	gate_accuracy: 74.0741
	Task-6	val_accuracy: 71.5909	gate_accuracy: 72.7273
	Task-7	val_accuracy: 77.7778	gate_accuracy: 70.8333
	Task-8	val_accuracy: 70.5263	gate_accuracy: 68.4211
	Task-9	val_accuracy: 73.3333	gate_accuracy: 72.0000
	Task-10	val_accuracy: 77.7778	gate_accuracy: 71.6049
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.5187


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.2064	step1_train_accuracy: 58.0745
STEP-1	Epoch: 20/50	loss: 0.8758	step1_train_accuracy: 85.7143
STEP-1	Epoch: 30/50	loss: 0.4348	step1_train_accuracy: 95.6522
STEP-1	Epoch: 40/50	loss: 0.2570	step1_train_accuracy: 95.9627
STEP-1	Epoch: 50/50	loss: 0.1997	step1_train_accuracy: 97.2050
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5103	gate_loss: 1.8585	step2_classification_accuracy: 82.3622	step_2_gate_accuracy: 40.1968
STEP-2	Epoch: 40/200	classification_loss: 0.3950	gate_loss: 0.8302	step2_classification_accuracy: 85.3150	step_2_gate_accuracy: 76.8898
STEP-2	Epoch: 60/200	classification_loss: 0.3137	gate_loss: 0.4257	step2_classification_accuracy: 88.6220	step_2_gate_accuracy: 88.4252
STEP-2	Epoch: 80/200	classification_loss: 0.2734	gate_loss: 0.2810	step2_classification_accuracy: 90.2756	step_2_gate_accuracy: 92.2441
STEP-2	Epoch: 100/200	classification_loss: 0.2444	gate_loss: 0.2185	step2_classification_accuracy: 90.4724	step_2_gate_accuracy: 93.3858
STEP-2	Epoch: 120/200	classification_loss: 0.2307	gate_loss: 0.1853	step2_classification_accuracy: 90.9843	step_2_gate_accuracy: 94.0551
STEP-2	Epoch: 140/200	classification_loss: 0.2078	gate_loss: 0.1569	step2_classification_accuracy: 92.0079	step_2_gate_accuracy: 95.1181
STEP-2	Epoch: 160/200	classification_loss: 0.1996	gate_loss: 0.1405	step2_classification_accuracy: 91.8898	step_2_gate_accuracy: 95.3937
STEP-2	Epoch: 180/200	classification_loss: 0.1830	gate_loss: 0.1239	step2_classification_accuracy: 92.4803	step_2_gate_accuracy: 96.0236
STEP-2	Epoch: 200/200	classification_loss: 0.1661	gate_loss: 0.1084	step2_classification_accuracy: 93.2677	step_2_gate_accuracy: 96.5748
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 68.0000	gate_accuracy: 75.2000
	Task-1	val_accuracy: 75.0000	gate_accuracy: 82.5000
	Task-2	val_accuracy: 73.8095	gate_accuracy: 76.1905
	Task-3	val_accuracy: 81.6514	gate_accuracy: 80.7339
	Task-4	val_accuracy: 78.3784	gate_accuracy: 81.0811
	Task-5	val_accuracy: 54.3210	gate_accuracy: 60.4938
	Task-6	val_accuracy: 77.2727	gate_accuracy: 82.9545
	Task-7	val_accuracy: 77.7778	gate_accuracy: 72.2222
	Task-8	val_accuracy: 73.6842	gate_accuracy: 71.5789
	Task-9	val_accuracy: 77.3333	gate_accuracy: 76.0000
	Task-10	val_accuracy: 81.4815	gate_accuracy: 75.3086
	Task-11	val_accuracy: 72.8395	gate_accuracy: 76.5432
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.9809


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.1907	step1_train_accuracy: 45.5090
STEP-1	Epoch: 20/50	loss: 0.9830	step1_train_accuracy: 84.7305
STEP-1	Epoch: 30/50	loss: 0.5707	step1_train_accuracy: 92.8144
STEP-1	Epoch: 40/50	loss: 0.3965	step1_train_accuracy: 94.3114
STEP-1	Epoch: 50/50	loss: 0.3015	step1_train_accuracy: 95.5090
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5698	gate_loss: 1.8705	step2_classification_accuracy: 80.5839	step_2_gate_accuracy: 39.6715
STEP-2	Epoch: 40/200	classification_loss: 0.4161	gate_loss: 0.8376	step2_classification_accuracy: 85.9489	step_2_gate_accuracy: 76.7153
STEP-2	Epoch: 60/200	classification_loss: 0.3399	gate_loss: 0.4397	step2_classification_accuracy: 87.9927	step_2_gate_accuracy: 87.8102
STEP-2	Epoch: 80/200	classification_loss: 0.2861	gate_loss: 0.2900	step2_classification_accuracy: 90.1095	step_2_gate_accuracy: 92.3723
STEP-2	Epoch: 100/200	classification_loss: 0.2756	gate_loss: 0.2397	step2_classification_accuracy: 90.3285	step_2_gate_accuracy: 92.9927
STEP-2	Epoch: 120/200	classification_loss: 0.2341	gate_loss: 0.1835	step2_classification_accuracy: 91.3869	step_2_gate_accuracy: 93.9051
STEP-2	Epoch: 140/200	classification_loss: 0.2124	gate_loss: 0.1556	step2_classification_accuracy: 91.9343	step_2_gate_accuracy: 94.7810
STEP-2	Epoch: 160/200	classification_loss: 0.2082	gate_loss: 0.1465	step2_classification_accuracy: 92.4088	step_2_gate_accuracy: 95.4380
STEP-2	Epoch: 180/200	classification_loss: 0.1960	gate_loss: 0.1304	step2_classification_accuracy: 92.5182	step_2_gate_accuracy: 95.3285
STEP-2	Epoch: 200/200	classification_loss: 0.1873	gate_loss: 0.1202	step2_classification_accuracy: 92.5912	step_2_gate_accuracy: 96.0219
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.6000	gate_accuracy: 78.4000
	Task-1	val_accuracy: 77.5000	gate_accuracy: 77.5000
	Task-2	val_accuracy: 70.2381	gate_accuracy: 72.6190
	Task-3	val_accuracy: 82.5688	gate_accuracy: 81.6514
	Task-4	val_accuracy: 77.0270	gate_accuracy: 77.0270
	Task-5	val_accuracy: 55.5556	gate_accuracy: 62.9630
	Task-6	val_accuracy: 70.4545	gate_accuracy: 76.1364
	Task-7	val_accuracy: 84.7222	gate_accuracy: 86.1111
	Task-8	val_accuracy: 73.6842	gate_accuracy: 70.5263
	Task-9	val_accuracy: 80.0000	gate_accuracy: 74.6667
	Task-10	val_accuracy: 77.7778	gate_accuracy: 74.0741
	Task-11	val_accuracy: 74.0741	gate_accuracy: 71.6049
	Task-12	val_accuracy: 72.6190	gate_accuracy: 70.2381
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.0221


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 2.2417	step1_train_accuracy: 52.2659
STEP-1	Epoch: 20/50	loss: 0.8722	step1_train_accuracy: 82.7795
STEP-1	Epoch: 30/50	loss: 0.5023	step1_train_accuracy: 92.4471
STEP-1	Epoch: 40/50	loss: 0.3660	step1_train_accuracy: 93.6556
STEP-1	Epoch: 50/50	loss: 0.2799	step1_train_accuracy: 94.8641
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5734	gate_loss: 1.9854	step2_classification_accuracy: 80.8163	step_2_gate_accuracy: 36.2245
STEP-2	Epoch: 40/200	classification_loss: 0.4260	gate_loss: 0.9082	step2_classification_accuracy: 85.5102	step_2_gate_accuracy: 74.2857
STEP-2	Epoch: 60/200	classification_loss: 0.3385	gate_loss: 0.4707	step2_classification_accuracy: 88.1633	step_2_gate_accuracy: 87.3469
STEP-2	Epoch: 80/200	classification_loss: 0.2848	gate_loss: 0.3127	step2_classification_accuracy: 90.0680	step_2_gate_accuracy: 90.9184
STEP-2	Epoch: 100/200	classification_loss: 0.2536	gate_loss: 0.2405	step2_classification_accuracy: 90.8844	step_2_gate_accuracy: 92.9932
STEP-2	Epoch: 120/200	classification_loss: 0.2371	gate_loss: 0.1986	step2_classification_accuracy: 91.5646	step_2_gate_accuracy: 94.2177
STEP-2	Epoch: 140/200	classification_loss: 0.2139	gate_loss: 0.1706	step2_classification_accuracy: 91.8707	step_2_gate_accuracy: 94.4558
STEP-2	Epoch: 160/200	classification_loss: 0.1970	gate_loss: 0.1495	step2_classification_accuracy: 92.6190	step_2_gate_accuracy: 95.3401
STEP-2	Epoch: 180/200	classification_loss: 0.1890	gate_loss: 0.1384	step2_classification_accuracy: 92.7551	step_2_gate_accuracy: 95.4762
STEP-2	Epoch: 200/200	classification_loss: 0.1842	gate_loss: 0.1306	step2_classification_accuracy: 92.8231	step_2_gate_accuracy: 95.5442
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.4000	gate_accuracy: 75.2000
	Task-1	val_accuracy: 76.2500	gate_accuracy: 78.7500
	Task-2	val_accuracy: 73.8095	gate_accuracy: 70.2381
	Task-3	val_accuracy: 79.8165	gate_accuracy: 82.5688
	Task-4	val_accuracy: 71.6216	gate_accuracy: 71.6216
	Task-5	val_accuracy: 55.5556	gate_accuracy: 70.3704
	Task-6	val_accuracy: 70.4545	gate_accuracy: 71.5909
	Task-7	val_accuracy: 72.2222	gate_accuracy: 70.8333
	Task-8	val_accuracy: 75.7895	gate_accuracy: 73.6842
	Task-9	val_accuracy: 69.3333	gate_accuracy: 66.6667
	Task-10	val_accuracy: 69.1358	gate_accuracy: 62.9630
	Task-11	val_accuracy: 77.7778	gate_accuracy: 72.8395
	Task-12	val_accuracy: 65.4762	gate_accuracy: 66.6667
	Task-13	val_accuracy: 79.5181	gate_accuracy: 80.7229
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.8548


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.9399	step1_train_accuracy: 38.8112
STEP-1	Epoch: 20/50	loss: 1.2494	step1_train_accuracy: 76.9231
STEP-1	Epoch: 30/50	loss: 0.5591	step1_train_accuracy: 95.1049
STEP-1	Epoch: 40/50	loss: 0.3822	step1_train_accuracy: 95.8042
STEP-1	Epoch: 50/50	loss: 0.2699	step1_train_accuracy: 95.8042
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6110	gate_loss: 1.9881	step2_classification_accuracy: 80.1911	step_2_gate_accuracy: 35.9873
STEP-2	Epoch: 40/200	classification_loss: 0.4844	gate_loss: 0.8761	step2_classification_accuracy: 83.4076	step_2_gate_accuracy: 75.3822
STEP-2	Epoch: 60/200	classification_loss: 0.3795	gate_loss: 0.4581	step2_classification_accuracy: 86.8790	step_2_gate_accuracy: 87.0382
STEP-2	Epoch: 80/200	classification_loss: 0.3167	gate_loss: 0.3116	step2_classification_accuracy: 88.5350	step_2_gate_accuracy: 91.1147
STEP-2	Epoch: 100/200	classification_loss: 0.2898	gate_loss: 0.2430	step2_classification_accuracy: 89.0127	step_2_gate_accuracy: 92.3885
STEP-2	Epoch: 120/200	classification_loss: 0.2578	gate_loss: 0.1999	step2_classification_accuracy: 90.1274	step_2_gate_accuracy: 93.8535
STEP-2	Epoch: 140/200	classification_loss: 0.2438	gate_loss: 0.1752	step2_classification_accuracy: 90.3822	step_2_gate_accuracy: 94.5860
STEP-2	Epoch: 160/200	classification_loss: 0.2312	gate_loss: 0.1607	step2_classification_accuracy: 90.9873	step_2_gate_accuracy: 94.6815
STEP-2	Epoch: 180/200	classification_loss: 0.2184	gate_loss: 0.1451	step2_classification_accuracy: 91.2739	step_2_gate_accuracy: 95.3185
STEP-2	Epoch: 200/200	classification_loss: 0.2023	gate_loss: 0.1306	step2_classification_accuracy: 91.7516	step_2_gate_accuracy: 95.4140
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.6000	gate_accuracy: 80.8000
	Task-1	val_accuracy: 80.0000	gate_accuracy: 82.5000
	Task-2	val_accuracy: 65.4762	gate_accuracy: 67.8571
	Task-3	val_accuracy: 79.8165	gate_accuracy: 77.0642
	Task-4	val_accuracy: 71.6216	gate_accuracy: 74.3243
	Task-5	val_accuracy: 51.8519	gate_accuracy: 70.3704
	Task-6	val_accuracy: 68.1818	gate_accuracy: 68.1818
	Task-7	val_accuracy: 76.3889	gate_accuracy: 79.1667
	Task-8	val_accuracy: 77.8947	gate_accuracy: 75.7895
	Task-9	val_accuracy: 61.3333	gate_accuracy: 61.3333
	Task-10	val_accuracy: 75.3086	gate_accuracy: 74.0741
	Task-11	val_accuracy: 70.3704	gate_accuracy: 69.1358
	Task-12	val_accuracy: 63.0952	gate_accuracy: 59.5238
	Task-13	val_accuracy: 78.3133	gate_accuracy: 77.1084
	Task-14	val_accuracy: 76.3889	gate_accuracy: 79.1667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.3645


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.1725	step1_train_accuracy: 61.5385
STEP-1	Epoch: 20/50	loss: 0.7214	step1_train_accuracy: 88.6154
STEP-1	Epoch: 30/50	loss: 0.3817	step1_train_accuracy: 94.7692
STEP-1	Epoch: 40/50	loss: 0.2794	step1_train_accuracy: 95.0769
STEP-1	Epoch: 50/50	loss: 0.2096	step1_train_accuracy: 96.3077
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6226	gate_loss: 2.0377	step2_classification_accuracy: 79.2216	step_2_gate_accuracy: 37.4850
STEP-2	Epoch: 40/200	classification_loss: 0.4566	gate_loss: 0.8824	step2_classification_accuracy: 83.8623	step_2_gate_accuracy: 75.0000
STEP-2	Epoch: 60/200	classification_loss: 0.3609	gate_loss: 0.4582	step2_classification_accuracy: 87.4251	step_2_gate_accuracy: 86.9760
STEP-2	Epoch: 80/200	classification_loss: 0.3059	gate_loss: 0.3085	step2_classification_accuracy: 89.4910	step_2_gate_accuracy: 90.9281
STEP-2	Epoch: 100/200	classification_loss: 0.2770	gate_loss: 0.2364	step2_classification_accuracy: 90.0299	step_2_gate_accuracy: 92.8144
STEP-2	Epoch: 120/200	classification_loss: 0.2403	gate_loss: 0.1940	step2_classification_accuracy: 91.2874	step_2_gate_accuracy: 94.2814
STEP-2	Epoch: 140/200	classification_loss: 0.2241	gate_loss: 0.1713	step2_classification_accuracy: 91.9760	step_2_gate_accuracy: 94.7605
STEP-2	Epoch: 160/200	classification_loss: 0.2124	gate_loss: 0.1512	step2_classification_accuracy: 92.0359	step_2_gate_accuracy: 94.7605
STEP-2	Epoch: 180/200	classification_loss: 0.1990	gate_loss: 0.1373	step2_classification_accuracy: 92.4850	step_2_gate_accuracy: 95.5389
STEP-2	Epoch: 200/200	classification_loss: 0.1916	gate_loss: 0.1265	step2_classification_accuracy: 92.7545	step_2_gate_accuracy: 95.9281
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 64.8000	gate_accuracy: 70.4000
	Task-1	val_accuracy: 75.0000	gate_accuracy: 78.7500
	Task-2	val_accuracy: 64.2857	gate_accuracy: 60.7143
	Task-3	val_accuracy: 79.8165	gate_accuracy: 75.2294
	Task-4	val_accuracy: 75.6757	gate_accuracy: 72.9730
	Task-5	val_accuracy: 55.5556	gate_accuracy: 74.0741
	Task-6	val_accuracy: 63.6364	gate_accuracy: 67.0455
	Task-7	val_accuracy: 70.8333	gate_accuracy: 73.6111
	Task-8	val_accuracy: 63.1579	gate_accuracy: 63.1579
	Task-9	val_accuracy: 57.3333	gate_accuracy: 58.6667
	Task-10	val_accuracy: 74.0741	gate_accuracy: 71.6049
	Task-11	val_accuracy: 71.6049	gate_accuracy: 71.6049
	Task-12	val_accuracy: 61.9048	gate_accuracy: 63.0952
	Task-13	val_accuracy: 74.6988	gate_accuracy: 73.4940
	Task-14	val_accuracy: 76.3889	gate_accuracy: 75.0000
	Task-15	val_accuracy: 71.6049	gate_accuracy: 70.3704
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.9634


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.5435	step1_train_accuracy: 51.8900
STEP-1	Epoch: 20/50	loss: 0.8913	step1_train_accuracy: 91.0653
STEP-1	Epoch: 30/50	loss: 0.4830	step1_train_accuracy: 94.5017
STEP-1	Epoch: 40/50	loss: 0.3176	step1_train_accuracy: 96.2199
STEP-1	Epoch: 50/50	loss: 0.2324	step1_train_accuracy: 97.5945
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6070	gate_loss: 2.1761	step2_classification_accuracy: 79.5763	step_2_gate_accuracy: 33.7853
STEP-2	Epoch: 40/200	classification_loss: 0.4714	gate_loss: 0.9287	step2_classification_accuracy: 83.2486	step_2_gate_accuracy: 75.4237
STEP-2	Epoch: 60/200	classification_loss: 0.3612	gate_loss: 0.4785	step2_classification_accuracy: 87.5424	step_2_gate_accuracy: 86.7232
STEP-2	Epoch: 80/200	classification_loss: 0.3212	gate_loss: 0.3345	step2_classification_accuracy: 88.0508	step_2_gate_accuracy: 90.3955
STEP-2	Epoch: 100/200	classification_loss: 0.2980	gate_loss: 0.2666	step2_classification_accuracy: 89.4350	step_2_gate_accuracy: 91.7514
STEP-2	Epoch: 120/200	classification_loss: 0.2658	gate_loss: 0.2173	step2_classification_accuracy: 90.2260	step_2_gate_accuracy: 93.2486
STEP-2	Epoch: 140/200	classification_loss: 0.2454	gate_loss: 0.1871	step2_classification_accuracy: 90.6497	step_2_gate_accuracy: 94.1808
STEP-2	Epoch: 160/200	classification_loss: 0.2351	gate_loss: 0.1692	step2_classification_accuracy: 91.1864	step_2_gate_accuracy: 94.1525
STEP-2	Epoch: 180/200	classification_loss: 0.2204	gate_loss: 0.1541	step2_classification_accuracy: 91.7514	step_2_gate_accuracy: 94.5763
STEP-2	Epoch: 200/200	classification_loss: 0.2159	gate_loss: 0.1483	step2_classification_accuracy: 91.7797	step_2_gate_accuracy: 94.8023
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.4000	gate_accuracy: 72.8000
	Task-1	val_accuracy: 65.0000	gate_accuracy: 75.0000
	Task-2	val_accuracy: 73.8095	gate_accuracy: 71.4286
	Task-3	val_accuracy: 76.1468	gate_accuracy: 79.8165
	Task-4	val_accuracy: 75.6757	gate_accuracy: 75.6757
	Task-5	val_accuracy: 46.9136	gate_accuracy: 60.4938
	Task-6	val_accuracy: 68.1818	gate_accuracy: 75.0000
	Task-7	val_accuracy: 73.6111	gate_accuracy: 75.0000
	Task-8	val_accuracy: 70.5263	gate_accuracy: 68.4211
	Task-9	val_accuracy: 57.3333	gate_accuracy: 57.3333
	Task-10	val_accuracy: 72.8395	gate_accuracy: 66.6667
	Task-11	val_accuracy: 74.0741	gate_accuracy: 75.3086
	Task-12	val_accuracy: 61.9048	gate_accuracy: 58.3333
	Task-13	val_accuracy: 78.3133	gate_accuracy: 74.6988
	Task-14	val_accuracy: 77.7778	gate_accuracy: 81.9444
	Task-15	val_accuracy: 72.8395	gate_accuracy: 74.0741
	Task-16	val_accuracy: 73.9726	gate_accuracy: 72.6027
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.5577


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 2.2482	step1_train_accuracy: 49.7006
STEP-1	Epoch: 20/50	loss: 0.8329	step1_train_accuracy: 79.6407
STEP-1	Epoch: 30/50	loss: 0.4680	step1_train_accuracy: 90.1198
STEP-1	Epoch: 40/50	loss: 0.3185	step1_train_accuracy: 94.3114
STEP-1	Epoch: 50/50	loss: 0.2330	step1_train_accuracy: 95.2096
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6774	gate_loss: 2.2218	step2_classification_accuracy: 77.5401	step_2_gate_accuracy: 31.8984
STEP-2	Epoch: 40/200	classification_loss: 0.5471	gate_loss: 0.9790	step2_classification_accuracy: 81.6043	step_2_gate_accuracy: 72.6203
STEP-2	Epoch: 60/200	classification_loss: 0.4356	gate_loss: 0.5238	step2_classification_accuracy: 85.2406	step_2_gate_accuracy: 84.6257
STEP-2	Epoch: 80/200	classification_loss: 0.3709	gate_loss: 0.3662	step2_classification_accuracy: 87.0856	step_2_gate_accuracy: 88.8503
STEP-2	Epoch: 100/200	classification_loss: 0.3335	gate_loss: 0.2943	step2_classification_accuracy: 88.0214	step_2_gate_accuracy: 90.7754
STEP-2	Epoch: 120/200	classification_loss: 0.3110	gate_loss: 0.2477	step2_classification_accuracy: 88.3422	step_2_gate_accuracy: 91.6845
STEP-2	Epoch: 140/200	classification_loss: 0.2858	gate_loss: 0.2159	step2_classification_accuracy: 89.5722	step_2_gate_accuracy: 92.7808
STEP-2	Epoch: 160/200	classification_loss: 0.2642	gate_loss: 0.1901	step2_classification_accuracy: 90.5348	step_2_gate_accuracy: 93.6631
STEP-2	Epoch: 180/200	classification_loss: 0.2475	gate_loss: 0.1783	step2_classification_accuracy: 90.6952	step_2_gate_accuracy: 94.1711
STEP-2	Epoch: 200/200	classification_loss: 0.2370	gate_loss: 0.1654	step2_classification_accuracy: 91.2032	step_2_gate_accuracy: 94.7594
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.8000	gate_accuracy: 72.8000
	Task-1	val_accuracy: 76.2500	gate_accuracy: 83.7500
	Task-2	val_accuracy: 69.0476	gate_accuracy: 72.6190
	Task-3	val_accuracy: 74.3119	gate_accuracy: 77.9817
	Task-4	val_accuracy: 71.6216	gate_accuracy: 70.2703
	Task-5	val_accuracy: 59.2593	gate_accuracy: 69.1358
	Task-6	val_accuracy: 68.1818	gate_accuracy: 72.7273
	Task-7	val_accuracy: 72.2222	gate_accuracy: 76.3889
	Task-8	val_accuracy: 66.3158	gate_accuracy: 67.3684
	Task-9	val_accuracy: 62.6667	gate_accuracy: 65.3333
	Task-10	val_accuracy: 75.3086	gate_accuracy: 71.6049
	Task-11	val_accuracy: 70.3704	gate_accuracy: 67.9012
	Task-12	val_accuracy: 69.0476	gate_accuracy: 64.2857
	Task-13	val_accuracy: 67.4699	gate_accuracy: 66.2651
	Task-14	val_accuracy: 79.1667	gate_accuracy: 81.9444
	Task-15	val_accuracy: 71.6049	gate_accuracy: 71.6049
	Task-16	val_accuracy: 68.4932	gate_accuracy: 61.6438
	Task-17	val_accuracy: 74.6988	gate_accuracy: 68.6747
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.3346


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 3.2445	step1_train_accuracy: 45.8647
STEP-1	Epoch: 20/50	loss: 1.4279	step1_train_accuracy: 73.6842
STEP-1	Epoch: 30/50	loss: 0.7837	step1_train_accuracy: 88.3459
STEP-1	Epoch: 40/50	loss: 0.5200	step1_train_accuracy: 92.4812
STEP-1	Epoch: 50/50	loss: 0.3854	step1_train_accuracy: 93.2331
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6381	gate_loss: 2.2033	step2_classification_accuracy: 78.2741	step_2_gate_accuracy: 31.3959
STEP-2	Epoch: 40/200	classification_loss: 0.5046	gate_loss: 0.9317	step2_classification_accuracy: 82.7411	step_2_gate_accuracy: 75.1269
STEP-2	Epoch: 60/200	classification_loss: 0.3912	gate_loss: 0.4929	step2_classification_accuracy: 85.9391	step_2_gate_accuracy: 85.8883
STEP-2	Epoch: 80/200	classification_loss: 0.3458	gate_loss: 0.3461	step2_classification_accuracy: 87.6904	step_2_gate_accuracy: 90.0761
STEP-2	Epoch: 100/200	classification_loss: 0.3019	gate_loss: 0.2694	step2_classification_accuracy: 88.8071	step_2_gate_accuracy: 91.6244
STEP-2	Epoch: 120/200	classification_loss: 0.2707	gate_loss: 0.2246	step2_classification_accuracy: 89.6701	step_2_gate_accuracy: 92.8934
STEP-2	Epoch: 140/200	classification_loss: 0.2548	gate_loss: 0.2004	step2_classification_accuracy: 90.5330	step_2_gate_accuracy: 93.3756
STEP-2	Epoch: 160/200	classification_loss: 0.2336	gate_loss: 0.1789	step2_classification_accuracy: 90.8629	step_2_gate_accuracy: 94.0863
STEP-2	Epoch: 180/200	classification_loss: 0.2271	gate_loss: 0.1653	step2_classification_accuracy: 90.9898	step_2_gate_accuracy: 94.4162
STEP-2	Epoch: 200/200	classification_loss: 0.2338	gate_loss: 0.1641	step2_classification_accuracy: 91.4213	step_2_gate_accuracy: 94.5939
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 64.0000	gate_accuracy: 76.0000
	Task-1	val_accuracy: 78.7500	gate_accuracy: 85.0000
	Task-2	val_accuracy: 69.0476	gate_accuracy: 69.0476
	Task-3	val_accuracy: 76.1468	gate_accuracy: 78.8991
	Task-4	val_accuracy: 78.3784	gate_accuracy: 77.0270
	Task-5	val_accuracy: 54.3210	gate_accuracy: 64.1975
	Task-6	val_accuracy: 65.9091	gate_accuracy: 68.1818
	Task-7	val_accuracy: 72.2222	gate_accuracy: 75.0000
	Task-8	val_accuracy: 68.4211	gate_accuracy: 70.5263
	Task-9	val_accuracy: 57.3333	gate_accuracy: 64.0000
	Task-10	val_accuracy: 77.7778	gate_accuracy: 74.0741
	Task-11	val_accuracy: 71.6049	gate_accuracy: 69.1358
	Task-12	val_accuracy: 60.7143	gate_accuracy: 57.1429
	Task-13	val_accuracy: 69.8795	gate_accuracy: 67.4699
	Task-14	val_accuracy: 70.8333	gate_accuracy: 70.8333
	Task-15	val_accuracy: 71.6049	gate_accuracy: 72.8395
	Task-16	val_accuracy: 73.9726	gate_accuracy: 71.2329
	Task-17	val_accuracy: 73.4940	gate_accuracy: 73.4940
	Task-18	val_accuracy: 77.6119	gate_accuracy: 74.6269
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.6625


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.8435	step1_train_accuracy: 56.8027
STEP-1	Epoch: 20/50	loss: 0.9215	step1_train_accuracy: 79.9320
STEP-1	Epoch: 30/50	loss: 0.4684	step1_train_accuracy: 92.5170
STEP-1	Epoch: 40/50	loss: 0.3223	step1_train_accuracy: 92.8571
STEP-1	Epoch: 50/50	loss: 0.2462	step1_train_accuracy: 93.5374
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7040	gate_loss: 2.2906	step2_classification_accuracy: 77.1014	step_2_gate_accuracy: 30.3140
STEP-2	Epoch: 40/200	classification_loss: 0.5659	gate_loss: 0.9830	step2_classification_accuracy: 81.3768	step_2_gate_accuracy: 74.2512
STEP-2	Epoch: 60/200	classification_loss: 0.4534	gate_loss: 0.5292	step2_classification_accuracy: 85.2899	step_2_gate_accuracy: 85.2174
STEP-2	Epoch: 80/200	classification_loss: 0.3705	gate_loss: 0.3637	step2_classification_accuracy: 87.4638	step_2_gate_accuracy: 89.7826
STEP-2	Epoch: 100/200	classification_loss: 0.3412	gate_loss: 0.2982	step2_classification_accuracy: 88.2609	step_2_gate_accuracy: 90.4831
STEP-2	Epoch: 120/200	classification_loss: 0.3313	gate_loss: 0.2628	step2_classification_accuracy: 88.7923	step_2_gate_accuracy: 91.6908
STEP-2	Epoch: 140/200	classification_loss: 0.2790	gate_loss: 0.2096	step2_classification_accuracy: 90.0966	step_2_gate_accuracy: 93.2367
STEP-2	Epoch: 160/200	classification_loss: 0.2676	gate_loss: 0.1937	step2_classification_accuracy: 90.3623	step_2_gate_accuracy: 93.5266
STEP-2	Epoch: 180/200	classification_loss: 0.2504	gate_loss: 0.1785	step2_classification_accuracy: 90.6280	step_2_gate_accuracy: 94.2271
STEP-2	Epoch: 200/200	classification_loss: 0.2349	gate_loss: 0.1606	step2_classification_accuracy: 91.2560	step_2_gate_accuracy: 94.7101
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.8000	gate_accuracy: 70.4000
	Task-1	val_accuracy: 72.5000	gate_accuracy: 72.5000
	Task-2	val_accuracy: 69.0476	gate_accuracy: 70.2381
	Task-3	val_accuracy: 79.8165	gate_accuracy: 77.9817
	Task-4	val_accuracy: 74.3243	gate_accuracy: 74.3243
	Task-5	val_accuracy: 58.0247	gate_accuracy: 69.1358
	Task-6	val_accuracy: 63.6364	gate_accuracy: 69.3182
	Task-7	val_accuracy: 65.2778	gate_accuracy: 69.4444
	Task-8	val_accuracy: 73.6842	gate_accuracy: 71.5789
	Task-9	val_accuracy: 58.6667	gate_accuracy: 56.0000
	Task-10	val_accuracy: 72.8395	gate_accuracy: 71.6049
	Task-11	val_accuracy: 65.4321	gate_accuracy: 66.6667
	Task-12	val_accuracy: 54.7619	gate_accuracy: 46.4286
	Task-13	val_accuracy: 68.6747	gate_accuracy: 72.2892
	Task-14	val_accuracy: 70.8333	gate_accuracy: 75.0000
	Task-15	val_accuracy: 76.5432	gate_accuracy: 71.6049
	Task-16	val_accuracy: 68.4932	gate_accuracy: 65.7534
	Task-17	val_accuracy: 61.4458	gate_accuracy: 57.8313
	Task-18	val_accuracy: 61.1940	gate_accuracy: 67.1642
	Task-19	val_accuracy: 79.4521	gate_accuracy: 75.3425
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.6936


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.7193	step1_train_accuracy: 63.6042
STEP-1	Epoch: 20/50	loss: 0.8663	step1_train_accuracy: 87.2792
STEP-1	Epoch: 30/50	loss: 0.4419	step1_train_accuracy: 97.5265
STEP-1	Epoch: 40/50	loss: 0.3140	step1_train_accuracy: 97.8799
STEP-1	Epoch: 50/50	loss: 0.1759	step1_train_accuracy: 98.9399
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7140	gate_loss: 2.3375	step2_classification_accuracy: 76.9355	step_2_gate_accuracy: 28.9862
STEP-2	Epoch: 40/200	classification_loss: 0.5589	gate_loss: 0.9758	step2_classification_accuracy: 81.4286	step_2_gate_accuracy: 73.5945
STEP-2	Epoch: 60/200	classification_loss: 0.4311	gate_loss: 0.5182	step2_classification_accuracy: 85.8525	step_2_gate_accuracy: 85.4839
STEP-2	Epoch: 80/200	classification_loss: 0.3684	gate_loss: 0.3591	step2_classification_accuracy: 87.5806	step_2_gate_accuracy: 89.0553
STEP-2	Epoch: 100/200	classification_loss: 0.3198	gate_loss: 0.2820	step2_classification_accuracy: 88.9171	step_2_gate_accuracy: 91.6590
STEP-2	Epoch: 120/200	classification_loss: 0.2920	gate_loss: 0.2379	step2_classification_accuracy: 89.6313	step_2_gate_accuracy: 92.6958
STEP-2	Epoch: 140/200	classification_loss: 0.2720	gate_loss: 0.2091	step2_classification_accuracy: 90.2304	step_2_gate_accuracy: 93.5023
STEP-2	Epoch: 160/200	classification_loss: 0.2474	gate_loss: 0.1824	step2_classification_accuracy: 90.7143	step_2_gate_accuracy: 93.9862
STEP-2	Epoch: 180/200	classification_loss: 0.2375	gate_loss: 0.1707	step2_classification_accuracy: 90.9447	step_2_gate_accuracy: 94.7465
STEP-2	Epoch: 200/200	classification_loss: 0.2237	gate_loss: 0.1570	step2_classification_accuracy: 91.5207	step_2_gate_accuracy: 94.6313
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.4000	gate_accuracy: 69.6000
	Task-1	val_accuracy: 73.7500	gate_accuracy: 78.7500
	Task-2	val_accuracy: 69.0476	gate_accuracy: 65.4762
	Task-3	val_accuracy: 69.7248	gate_accuracy: 73.3945
	Task-4	val_accuracy: 74.3243	gate_accuracy: 78.3784
	Task-5	val_accuracy: 53.0864	gate_accuracy: 67.9012
	Task-6	val_accuracy: 70.4545	gate_accuracy: 77.2727
	Task-7	val_accuracy: 65.2778	gate_accuracy: 66.6667
	Task-8	val_accuracy: 71.5789	gate_accuracy: 69.4737
	Task-9	val_accuracy: 57.3333	gate_accuracy: 60.0000
	Task-10	val_accuracy: 66.6667	gate_accuracy: 67.9012
	Task-11	val_accuracy: 70.3704	gate_accuracy: 69.1358
	Task-12	val_accuracy: 60.7143	gate_accuracy: 60.7143
	Task-13	val_accuracy: 71.0843	gate_accuracy: 71.0843
	Task-14	val_accuracy: 75.0000	gate_accuracy: 77.7778
	Task-15	val_accuracy: 70.3704	gate_accuracy: 69.1358
	Task-16	val_accuracy: 63.0137	gate_accuracy: 57.5342
	Task-17	val_accuracy: 62.6506	gate_accuracy: 67.4699
	Task-18	val_accuracy: 67.1642	gate_accuracy: 65.6716
	Task-19	val_accuracy: 78.0822	gate_accuracy: 76.7123
	Task-20	val_accuracy: 81.6901	gate_accuracy: 81.6901
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.0924


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.7121	step1_train_accuracy: 58.2781
STEP-1	Epoch: 20/50	loss: 0.9242	step1_train_accuracy: 81.1258
STEP-1	Epoch: 30/50	loss: 0.5719	step1_train_accuracy: 90.0662
STEP-1	Epoch: 40/50	loss: 0.3687	step1_train_accuracy: 90.3974
STEP-1	Epoch: 50/50	loss: 0.2997	step1_train_accuracy: 93.7086
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7378	gate_loss: 2.3185	step2_classification_accuracy: 75.6828	step_2_gate_accuracy: 31.4317
STEP-2	Epoch: 40/200	classification_loss: 0.5762	gate_loss: 0.9602	step2_classification_accuracy: 81.1894	step_2_gate_accuracy: 72.6652
STEP-2	Epoch: 60/200	classification_loss: 0.4475	gate_loss: 0.5230	step2_classification_accuracy: 84.5154	step_2_gate_accuracy: 85.3965
STEP-2	Epoch: 80/200	classification_loss: 0.3775	gate_loss: 0.3655	step2_classification_accuracy: 87.1586	step_2_gate_accuracy: 89.1410
STEP-2	Epoch: 100/200	classification_loss: 0.3264	gate_loss: 0.2813	step2_classification_accuracy: 88.2599	step_2_gate_accuracy: 91.1454
STEP-2	Epoch: 120/200	classification_loss: 0.2906	gate_loss: 0.2375	step2_classification_accuracy: 89.3392	step_2_gate_accuracy: 92.7093
STEP-2	Epoch: 140/200	classification_loss: 0.2756	gate_loss: 0.2094	step2_classification_accuracy: 89.8678	step_2_gate_accuracy: 93.4141
STEP-2	Epoch: 160/200	classification_loss: 0.2575	gate_loss: 0.1884	step2_classification_accuracy: 90.3524	step_2_gate_accuracy: 93.8546
STEP-2	Epoch: 180/200	classification_loss: 0.2360	gate_loss: 0.1668	step2_classification_accuracy: 91.2115	step_2_gate_accuracy: 94.6696
STEP-2	Epoch: 200/200	classification_loss: 0.2325	gate_loss: 0.1583	step2_classification_accuracy: 91.0573	step_2_gate_accuracy: 94.8899
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.6000	gate_accuracy: 72.8000
	Task-1	val_accuracy: 76.2500	gate_accuracy: 81.2500
	Task-2	val_accuracy: 67.8571	gate_accuracy: 71.4286
	Task-3	val_accuracy: 76.1468	gate_accuracy: 79.8165
	Task-4	val_accuracy: 79.7297	gate_accuracy: 83.7838
	Task-5	val_accuracy: 54.3210	gate_accuracy: 65.4321
	Task-6	val_accuracy: 65.9091	gate_accuracy: 65.9091
	Task-7	val_accuracy: 68.0556	gate_accuracy: 70.8333
	Task-8	val_accuracy: 67.3684	gate_accuracy: 68.4211
	Task-9	val_accuracy: 58.6667	gate_accuracy: 61.3333
	Task-10	val_accuracy: 69.1358	gate_accuracy: 61.7284
	Task-11	val_accuracy: 69.1358	gate_accuracy: 67.9012
	Task-12	val_accuracy: 53.5714	gate_accuracy: 46.4286
	Task-13	val_accuracy: 62.6506	gate_accuracy: 59.0361
	Task-14	val_accuracy: 73.6111	gate_accuracy: 79.1667
	Task-15	val_accuracy: 71.6049	gate_accuracy: 71.6049
	Task-16	val_accuracy: 61.6438	gate_accuracy: 56.1644
	Task-17	val_accuracy: 60.2410	gate_accuracy: 51.8072
	Task-18	val_accuracy: 67.1642	gate_accuracy: 64.1791
	Task-19	val_accuracy: 82.1918	gate_accuracy: 80.8219
	Task-20	val_accuracy: 81.6901	gate_accuracy: 76.0563
	Task-21	val_accuracy: 78.9474	gate_accuracy: 85.5263
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.1925


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.3858	step1_train_accuracy: 44.6541
STEP-1	Epoch: 20/50	loss: 1.0208	step1_train_accuracy: 78.9308
STEP-1	Epoch: 30/50	loss: 0.5363	step1_train_accuracy: 93.3962
STEP-1	Epoch: 40/50	loss: 0.3455	step1_train_accuracy: 97.1698
STEP-1	Epoch: 50/50	loss: 0.2476	step1_train_accuracy: 97.4843
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7682	gate_loss: 2.3504	step2_classification_accuracy: 75.0211	step_2_gate_accuracy: 29.9578
STEP-2	Epoch: 40/200	classification_loss: 0.6026	gate_loss: 0.9915	step2_classification_accuracy: 80.7384	step_2_gate_accuracy: 72.3629
STEP-2	Epoch: 60/200	classification_loss: 0.4639	gate_loss: 0.5444	step2_classification_accuracy: 85.0633	step_2_gate_accuracy: 84.4726
STEP-2	Epoch: 80/200	classification_loss: 0.3757	gate_loss: 0.3750	step2_classification_accuracy: 87.5105	step_2_gate_accuracy: 88.7975
STEP-2	Epoch: 100/200	classification_loss: 0.3214	gate_loss: 0.2887	step2_classification_accuracy: 88.7553	step_2_gate_accuracy: 91.5823
STEP-2	Epoch: 120/200	classification_loss: 0.2938	gate_loss: 0.2438	step2_classification_accuracy: 89.8523	step_2_gate_accuracy: 92.6582
STEP-2	Epoch: 140/200	classification_loss: 0.2884	gate_loss: 0.2203	step2_classification_accuracy: 89.9578	step_2_gate_accuracy: 93.4388
STEP-2	Epoch: 160/200	classification_loss: 0.2581	gate_loss: 0.1909	step2_classification_accuracy: 90.7806	step_2_gate_accuracy: 94.1139
STEP-2	Epoch: 180/200	classification_loss: 0.2495	gate_loss: 0.1734	step2_classification_accuracy: 90.8439	step_2_gate_accuracy: 94.3249
STEP-2	Epoch: 200/200	classification_loss: 0.2306	gate_loss: 0.1548	step2_classification_accuracy: 91.6456	step_2_gate_accuracy: 94.9578
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.4000	gate_accuracy: 72.8000
	Task-1	val_accuracy: 63.7500	gate_accuracy: 73.7500
	Task-2	val_accuracy: 60.7143	gate_accuracy: 64.2857
	Task-3	val_accuracy: 64.2202	gate_accuracy: 69.7248
	Task-4	val_accuracy: 72.9730	gate_accuracy: 66.2162
	Task-5	val_accuracy: 50.6173	gate_accuracy: 58.0247
	Task-6	val_accuracy: 65.9091	gate_accuracy: 59.0909
	Task-7	val_accuracy: 73.6111	gate_accuracy: 66.6667
	Task-8	val_accuracy: 69.4737	gate_accuracy: 72.6316
	Task-9	val_accuracy: 60.0000	gate_accuracy: 65.3333
	Task-10	val_accuracy: 60.4938	gate_accuracy: 59.2593
	Task-11	val_accuracy: 66.6667	gate_accuracy: 65.4321
	Task-12	val_accuracy: 60.7143	gate_accuracy: 58.3333
	Task-13	val_accuracy: 72.2892	gate_accuracy: 65.0602
	Task-14	val_accuracy: 66.6667	gate_accuracy: 68.0556
	Task-15	val_accuracy: 66.6667	gate_accuracy: 65.4321
	Task-16	val_accuracy: 64.3836	gate_accuracy: 63.0137
	Task-17	val_accuracy: 65.0602	gate_accuracy: 56.6265
	Task-18	val_accuracy: 74.6269	gate_accuracy: 73.1343
	Task-19	val_accuracy: 76.7123	gate_accuracy: 75.3425
	Task-20	val_accuracy: 76.0563	gate_accuracy: 71.8310
	Task-21	val_accuracy: 67.1053	gate_accuracy: 72.3684
	Task-22	val_accuracy: 73.7500	gate_accuracy: 58.7500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.2076


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 1.8451	step1_train_accuracy: 66.2437
STEP-1	Epoch: 20/50	loss: 0.6577	step1_train_accuracy: 84.2640
STEP-1	Epoch: 30/50	loss: 0.3784	step1_train_accuracy: 92.8934
STEP-1	Epoch: 40/50	loss: 0.2658	step1_train_accuracy: 94.9239
STEP-1	Epoch: 50/50	loss: 0.2035	step1_train_accuracy: 97.7157
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8159	gate_loss: 2.3823	step2_classification_accuracy: 73.8866	step_2_gate_accuracy: 28.6437
STEP-2	Epoch: 40/200	classification_loss: 0.6453	gate_loss: 1.0401	step2_classification_accuracy: 79.0688	step_2_gate_accuracy: 71.8826
STEP-2	Epoch: 60/200	classification_loss: 0.5017	gate_loss: 0.5721	step2_classification_accuracy: 83.9879	step_2_gate_accuracy: 83.3401
STEP-2	Epoch: 80/200	classification_loss: 0.4191	gate_loss: 0.4020	step2_classification_accuracy: 85.7287	step_2_gate_accuracy: 87.9555
STEP-2	Epoch: 100/200	classification_loss: 0.3741	gate_loss: 0.3160	step2_classification_accuracy: 87.5911	step_2_gate_accuracy: 89.9595
STEP-2	Epoch: 120/200	classification_loss: 0.3292	gate_loss: 0.2641	step2_classification_accuracy: 88.4818	step_2_gate_accuracy: 91.5587
STEP-2	Epoch: 140/200	classification_loss: 0.3068	gate_loss: 0.2316	step2_classification_accuracy: 89.5547	step_2_gate_accuracy: 92.5304
STEP-2	Epoch: 160/200	classification_loss: 0.2932	gate_loss: 0.2115	step2_classification_accuracy: 89.9190	step_2_gate_accuracy: 93.1579
STEP-2	Epoch: 180/200	classification_loss: 0.2697	gate_loss: 0.1901	step2_classification_accuracy: 90.6073	step_2_gate_accuracy: 93.6032
STEP-2	Epoch: 200/200	classification_loss: 0.2583	gate_loss: 0.1752	step2_classification_accuracy: 90.9919	step_2_gate_accuracy: 94.2105
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.6000	gate_accuracy: 67.2000
	Task-1	val_accuracy: 76.2500	gate_accuracy: 81.2500
	Task-2	val_accuracy: 64.2857	gate_accuracy: 64.2857
	Task-3	val_accuracy: 69.7248	gate_accuracy: 72.4771
	Task-4	val_accuracy: 67.5676	gate_accuracy: 71.6216
	Task-5	val_accuracy: 54.3210	gate_accuracy: 64.1975
	Task-6	val_accuracy: 57.9545	gate_accuracy: 71.5909
	Task-7	val_accuracy: 63.8889	gate_accuracy: 62.5000
	Task-8	val_accuracy: 65.2632	gate_accuracy: 65.2632
	Task-9	val_accuracy: 68.0000	gate_accuracy: 72.0000
	Task-10	val_accuracy: 64.1975	gate_accuracy: 61.7284
	Task-11	val_accuracy: 72.8395	gate_accuracy: 69.1358
	Task-12	val_accuracy: 59.5238	gate_accuracy: 55.9524
	Task-13	val_accuracy: 69.8795	gate_accuracy: 72.2892
	Task-14	val_accuracy: 72.2222	gate_accuracy: 72.2222
	Task-15	val_accuracy: 70.3704	gate_accuracy: 65.4321
	Task-16	val_accuracy: 65.7534	gate_accuracy: 61.6438
	Task-17	val_accuracy: 69.8795	gate_accuracy: 71.0843
	Task-18	val_accuracy: 59.7015	gate_accuracy: 62.6866
	Task-19	val_accuracy: 76.7123	gate_accuracy: 75.3425
	Task-20	val_accuracy: 83.0986	gate_accuracy: 81.6901
	Task-21	val_accuracy: 75.0000	gate_accuracy: 78.9474
	Task-22	val_accuracy: 61.2500	gate_accuracy: 58.7500
	Task-23	val_accuracy: 65.6566	gate_accuracy: 58.5859
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.0926


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.9062	step1_train_accuracy: 44.4444
STEP-1	Epoch: 20/50	loss: 1.0121	step1_train_accuracy: 84.0278
STEP-1	Epoch: 30/50	loss: 0.5228	step1_train_accuracy: 92.3611
STEP-1	Epoch: 40/50	loss: 0.3696	step1_train_accuracy: 93.4028
STEP-1	Epoch: 50/50	loss: 0.2766	step1_train_accuracy: 96.5278
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8124	gate_loss: 2.3994	step2_classification_accuracy: 74.1634	step_2_gate_accuracy: 29.9027
STEP-2	Epoch: 40/200	classification_loss: 0.6391	gate_loss: 1.0111	step2_classification_accuracy: 79.4163	step_2_gate_accuracy: 72.3346
STEP-2	Epoch: 60/200	classification_loss: 0.5064	gate_loss: 0.5752	step2_classification_accuracy: 83.6965	step_2_gate_accuracy: 83.4241
STEP-2	Epoch: 80/200	classification_loss: 0.4302	gate_loss: 0.4108	step2_classification_accuracy: 86.1479	step_2_gate_accuracy: 87.9961
STEP-2	Epoch: 100/200	classification_loss: 0.3736	gate_loss: 0.3229	step2_classification_accuracy: 87.5292	step_2_gate_accuracy: 90.1167
STEP-2	Epoch: 120/200	classification_loss: 0.3423	gate_loss: 0.2728	step2_classification_accuracy: 88.7354	step_2_gate_accuracy: 91.3813
STEP-2	Epoch: 140/200	classification_loss: 0.3135	gate_loss: 0.2351	step2_classification_accuracy: 89.2607	step_2_gate_accuracy: 92.4708
STEP-2	Epoch: 160/200	classification_loss: 0.2857	gate_loss: 0.2076	step2_classification_accuracy: 90.0778	step_2_gate_accuracy: 93.1907
STEP-2	Epoch: 180/200	classification_loss: 0.2723	gate_loss: 0.1895	step2_classification_accuracy: 90.4086	step_2_gate_accuracy: 94.0078
STEP-2	Epoch: 200/200	classification_loss: 0.2642	gate_loss: 0.1777	step2_classification_accuracy: 90.4086	step_2_gate_accuracy: 94.1440
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.0000	gate_accuracy: 68.0000
	Task-1	val_accuracy: 65.0000	gate_accuracy: 71.2500
	Task-2	val_accuracy: 70.2381	gate_accuracy: 72.6190
	Task-3	val_accuracy: 75.2294	gate_accuracy: 77.9817
	Task-4	val_accuracy: 74.3243	gate_accuracy: 77.0270
	Task-5	val_accuracy: 53.0864	gate_accuracy: 71.6049
	Task-6	val_accuracy: 57.9545	gate_accuracy: 65.9091
	Task-7	val_accuracy: 55.5556	gate_accuracy: 58.3333
	Task-8	val_accuracy: 63.1579	gate_accuracy: 61.0526
	Task-9	val_accuracy: 52.0000	gate_accuracy: 53.3333
	Task-10	val_accuracy: 61.7284	gate_accuracy: 58.0247
	Task-11	val_accuracy: 74.0741	gate_accuracy: 70.3704
	Task-12	val_accuracy: 63.0952	gate_accuracy: 57.1429
	Task-13	val_accuracy: 65.0602	gate_accuracy: 65.0602
	Task-14	val_accuracy: 63.8889	gate_accuracy: 65.2778
	Task-15	val_accuracy: 67.9012	gate_accuracy: 61.7284
	Task-16	val_accuracy: 58.9041	gate_accuracy: 50.6849
	Task-17	val_accuracy: 54.2169	gate_accuracy: 53.0120
	Task-18	val_accuracy: 62.6866	gate_accuracy: 62.6866
	Task-19	val_accuracy: 80.8219	gate_accuracy: 75.3425
	Task-20	val_accuracy: 78.8732	gate_accuracy: 73.2394
	Task-21	val_accuracy: 64.4737	gate_accuracy: 67.1053
	Task-22	val_accuracy: 68.7500	gate_accuracy: 65.0000
	Task-23	val_accuracy: 65.6566	gate_accuracy: 65.6566
	Task-24	val_accuracy: 68.0556	gate_accuracy: 68.0556
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.6144


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 1.9811	step1_train_accuracy: 61.5183
STEP-1	Epoch: 20/50	loss: 0.7059	step1_train_accuracy: 83.5079
STEP-1	Epoch: 30/50	loss: 0.4216	step1_train_accuracy: 90.8377
STEP-1	Epoch: 40/50	loss: 0.3038	step1_train_accuracy: 93.4555
STEP-1	Epoch: 50/50	loss: 0.2339	step1_train_accuracy: 96.0733
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8415	gate_loss: 2.5003	step2_classification_accuracy: 73.3895	step_2_gate_accuracy: 28.5955
STEP-2	Epoch: 40/200	classification_loss: 0.6530	gate_loss: 1.0620	step2_classification_accuracy: 79.1199	step_2_gate_accuracy: 71.0487
STEP-2	Epoch: 60/200	classification_loss: 0.5096	gate_loss: 0.5777	step2_classification_accuracy: 83.1273	step_2_gate_accuracy: 83.7079
STEP-2	Epoch: 80/200	classification_loss: 0.4375	gate_loss: 0.4105	step2_classification_accuracy: 85.7865	step_2_gate_accuracy: 88.1648
STEP-2	Epoch: 100/200	classification_loss: 0.3940	gate_loss: 0.3293	step2_classification_accuracy: 86.7603	step_2_gate_accuracy: 89.8127
STEP-2	Epoch: 120/200	classification_loss: 0.3605	gate_loss: 0.2837	step2_classification_accuracy: 87.9963	step_2_gate_accuracy: 91.1236
STEP-2	Epoch: 140/200	classification_loss: 0.3362	gate_loss: 0.2533	step2_classification_accuracy: 88.5393	step_2_gate_accuracy: 91.8352
STEP-2	Epoch: 160/200	classification_loss: 0.3175	gate_loss: 0.2257	step2_classification_accuracy: 89.0824	step_2_gate_accuracy: 92.8464
STEP-2	Epoch: 180/200	classification_loss: 0.2939	gate_loss: 0.2044	step2_classification_accuracy: 89.7378	step_2_gate_accuracy: 93.2584
STEP-2	Epoch: 200/200	classification_loss: 0.2785	gate_loss: 0.1879	step2_classification_accuracy: 90.2809	step_2_gate_accuracy: 93.6142
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.0000	gate_accuracy: 69.6000
	Task-1	val_accuracy: 66.2500	gate_accuracy: 72.5000
	Task-2	val_accuracy: 69.0476	gate_accuracy: 72.6190
	Task-3	val_accuracy: 67.8899	gate_accuracy: 72.4771
	Task-4	val_accuracy: 72.9730	gate_accuracy: 77.0270
	Task-5	val_accuracy: 56.7901	gate_accuracy: 66.6667
	Task-6	val_accuracy: 56.8182	gate_accuracy: 63.6364
	Task-7	val_accuracy: 65.2778	gate_accuracy: 68.0556
	Task-8	val_accuracy: 63.1579	gate_accuracy: 62.1053
	Task-9	val_accuracy: 54.6667	gate_accuracy: 54.6667
	Task-10	val_accuracy: 74.0741	gate_accuracy: 71.6049
	Task-11	val_accuracy: 67.9012	gate_accuracy: 69.1358
	Task-12	val_accuracy: 53.5714	gate_accuracy: 50.0000
	Task-13	val_accuracy: 65.0602	gate_accuracy: 65.0602
	Task-14	val_accuracy: 65.2778	gate_accuracy: 69.4444
	Task-15	val_accuracy: 66.6667	gate_accuracy: 62.9630
	Task-16	val_accuracy: 63.0137	gate_accuracy: 54.7945
	Task-17	val_accuracy: 62.6506	gate_accuracy: 60.2410
	Task-18	val_accuracy: 62.6866	gate_accuracy: 62.6866
	Task-19	val_accuracy: 78.0822	gate_accuracy: 78.0822
	Task-20	val_accuracy: 70.4225	gate_accuracy: 71.8310
	Task-21	val_accuracy: 73.6842	gate_accuracy: 73.6842
	Task-22	val_accuracy: 65.0000	gate_accuracy: 60.0000
	Task-23	val_accuracy: 67.6768	gate_accuracy: 63.6364
	Task-24	val_accuracy: 70.8333	gate_accuracy: 63.8889
	Task-25	val_accuracy: 75.7895	gate_accuracy: 67.3684
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.3417


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 2.4327	step1_train_accuracy: 54.2373
STEP-1	Epoch: 20/50	loss: 1.0943	step1_train_accuracy: 73.7288
STEP-1	Epoch: 30/50	loss: 0.5885	step1_train_accuracy: 87.5706
STEP-1	Epoch: 40/50	loss: 0.4013	step1_train_accuracy: 92.3729
STEP-1	Epoch: 50/50	loss: 0.3037	step1_train_accuracy: 92.6554
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8706	gate_loss: 2.4594	step2_classification_accuracy: 72.4007	step_2_gate_accuracy: 29.0433
STEP-2	Epoch: 40/200	classification_loss: 0.6949	gate_loss: 1.0535	step2_classification_accuracy: 78.4657	step_2_gate_accuracy: 71.0469
STEP-2	Epoch: 60/200	classification_loss: 0.5555	gate_loss: 0.6128	step2_classification_accuracy: 82.0217	step_2_gate_accuracy: 82.0397
STEP-2	Epoch: 80/200	classification_loss: 0.4564	gate_loss: 0.4381	step2_classification_accuracy: 85.0722	step_2_gate_accuracy: 86.8231
STEP-2	Epoch: 100/200	classification_loss: 0.3951	gate_loss: 0.3447	step2_classification_accuracy: 87.0217	step_2_gate_accuracy: 89.6931
STEP-2	Epoch: 120/200	classification_loss: 0.3638	gate_loss: 0.2921	step2_classification_accuracy: 87.7798	step_2_gate_accuracy: 90.4513
STEP-2	Epoch: 140/200	classification_loss: 0.3196	gate_loss: 0.2465	step2_classification_accuracy: 88.9531	step_2_gate_accuracy: 92.3466
STEP-2	Epoch: 160/200	classification_loss: 0.3031	gate_loss: 0.2227	step2_classification_accuracy: 89.5668	step_2_gate_accuracy: 92.9783
STEP-2	Epoch: 180/200	classification_loss: 0.2947	gate_loss: 0.2127	step2_classification_accuracy: 89.8736	step_2_gate_accuracy: 93.6282
STEP-2	Epoch: 200/200	classification_loss: 0.2688	gate_loss: 0.1869	step2_classification_accuracy: 90.6318	step_2_gate_accuracy: 94.2058
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.8000	gate_accuracy: 75.2000
	Task-1	val_accuracy: 62.5000	gate_accuracy: 75.0000
	Task-2	val_accuracy: 59.5238	gate_accuracy: 64.2857
	Task-3	val_accuracy: 62.3853	gate_accuracy: 67.8899
	Task-4	val_accuracy: 64.8649	gate_accuracy: 66.2162
	Task-5	val_accuracy: 48.1481	gate_accuracy: 59.2593
	Task-6	val_accuracy: 61.3636	gate_accuracy: 63.6364
	Task-7	val_accuracy: 70.8333	gate_accuracy: 73.6111
	Task-8	val_accuracy: 68.4211	gate_accuracy: 67.3684
	Task-9	val_accuracy: 45.3333	gate_accuracy: 48.0000
	Task-10	val_accuracy: 60.4938	gate_accuracy: 55.5556
	Task-11	val_accuracy: 65.4321	gate_accuracy: 60.4938
	Task-12	val_accuracy: 52.3810	gate_accuracy: 48.8095
	Task-13	val_accuracy: 69.8795	gate_accuracy: 68.6747
	Task-14	val_accuracy: 68.0556	gate_accuracy: 70.8333
	Task-15	val_accuracy: 71.6049	gate_accuracy: 65.4321
	Task-16	val_accuracy: 57.5342	gate_accuracy: 54.7945
	Task-17	val_accuracy: 60.2410	gate_accuracy: 51.8072
	Task-18	val_accuracy: 65.6716	gate_accuracy: 65.6716
	Task-19	val_accuracy: 75.3425	gate_accuracy: 68.4932
	Task-20	val_accuracy: 73.2394	gate_accuracy: 70.4225
	Task-21	val_accuracy: 72.3684	gate_accuracy: 80.2632
	Task-22	val_accuracy: 61.2500	gate_accuracy: 53.7500
	Task-23	val_accuracy: 66.6667	gate_accuracy: 68.6869
	Task-24	val_accuracy: 69.4444	gate_accuracy: 70.8333
	Task-25	val_accuracy: 62.1053	gate_accuracy: 63.1579
	Task-26	val_accuracy: 70.7865	gate_accuracy: 70.7865
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.9576


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.5721	step1_train_accuracy: 45.5975
STEP-1	Epoch: 20/50	loss: 1.1287	step1_train_accuracy: 75.1572
STEP-1	Epoch: 30/50	loss: 0.6577	step1_train_accuracy: 92.4528
STEP-1	Epoch: 40/50	loss: 0.4399	step1_train_accuracy: 93.3962
STEP-1	Epoch: 50/50	loss: 0.3217	step1_train_accuracy: 95.2830
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8891	gate_loss: 2.4553	step2_classification_accuracy: 72.1603	step_2_gate_accuracy: 28.9199
STEP-2	Epoch: 40/200	classification_loss: 0.7055	gate_loss: 1.0482	step2_classification_accuracy: 77.6307	step_2_gate_accuracy: 70.5052
STEP-2	Epoch: 60/200	classification_loss: 0.5560	gate_loss: 0.6092	step2_classification_accuracy: 82.1603	step_2_gate_accuracy: 82.5784
STEP-2	Epoch: 80/200	classification_loss: 0.4593	gate_loss: 0.4356	step2_classification_accuracy: 85.6098	step_2_gate_accuracy: 86.7596
STEP-2	Epoch: 100/200	classification_loss: 0.4069	gate_loss: 0.3472	step2_classification_accuracy: 86.6551	step_2_gate_accuracy: 89.2160
STEP-2	Epoch: 120/200	classification_loss: 0.3663	gate_loss: 0.2915	step2_classification_accuracy: 87.9443	step_2_gate_accuracy: 90.5052
STEP-2	Epoch: 140/200	classification_loss: 0.3278	gate_loss: 0.2494	step2_classification_accuracy: 89.0070	step_2_gate_accuracy: 92.1603
STEP-2	Epoch: 160/200	classification_loss: 0.3011	gate_loss: 0.2199	step2_classification_accuracy: 89.6516	step_2_gate_accuracy: 92.7700
STEP-2	Epoch: 180/200	classification_loss: 0.2861	gate_loss: 0.2011	step2_classification_accuracy: 90.1568	step_2_gate_accuracy: 93.5889
STEP-2	Epoch: 200/200	classification_loss: 0.2661	gate_loss: 0.1812	step2_classification_accuracy: 91.0801	step_2_gate_accuracy: 94.2334
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.6000	gate_accuracy: 71.2000
	Task-1	val_accuracy: 60.0000	gate_accuracy: 66.2500
	Task-2	val_accuracy: 67.8571	gate_accuracy: 66.6667
	Task-3	val_accuracy: 66.0550	gate_accuracy: 69.7248
	Task-4	val_accuracy: 72.9730	gate_accuracy: 66.2162
	Task-5	val_accuracy: 50.6173	gate_accuracy: 65.4321
	Task-6	val_accuracy: 54.5455	gate_accuracy: 62.5000
	Task-7	val_accuracy: 63.8889	gate_accuracy: 72.2222
	Task-8	val_accuracy: 64.2105	gate_accuracy: 61.0526
	Task-9	val_accuracy: 52.0000	gate_accuracy: 54.6667
	Task-10	val_accuracy: 70.3704	gate_accuracy: 67.9012
	Task-11	val_accuracy: 67.9012	gate_accuracy: 60.4938
	Task-12	val_accuracy: 61.9048	gate_accuracy: 54.7619
	Task-13	val_accuracy: 65.0602	gate_accuracy: 66.2651
	Task-14	val_accuracy: 65.2778	gate_accuracy: 63.8889
	Task-15	val_accuracy: 66.6667	gate_accuracy: 65.4321
	Task-16	val_accuracy: 45.2055	gate_accuracy: 46.5753
	Task-17	val_accuracy: 66.2651	gate_accuracy: 55.4217
	Task-18	val_accuracy: 62.6866	gate_accuracy: 59.7015
	Task-19	val_accuracy: 73.9726	gate_accuracy: 71.2329
	Task-20	val_accuracy: 71.8310	gate_accuracy: 70.4225
	Task-21	val_accuracy: 71.0526	gate_accuracy: 73.6842
	Task-22	val_accuracy: 73.7500	gate_accuracy: 60.0000
	Task-23	val_accuracy: 64.6465	gate_accuracy: 65.6566
	Task-24	val_accuracy: 70.8333	gate_accuracy: 68.0556
	Task-25	val_accuracy: 72.6316	gate_accuracy: 72.6316
	Task-26	val_accuracy: 80.8989	gate_accuracy: 76.4045
	Task-27	val_accuracy: 70.0000	gate_accuracy: 68.7500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.3465


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 2.9564	step1_train_accuracy: 46.4151
STEP-1	Epoch: 20/50	loss: 1.0258	step1_train_accuracy: 89.4340
STEP-1	Epoch: 30/50	loss: 0.4951	step1_train_accuracy: 96.2264
STEP-1	Epoch: 40/50	loss: 0.3035	step1_train_accuracy: 98.4906
STEP-1	Epoch: 50/50	loss: 0.2127	step1_train_accuracy: 99.2453
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8955	gate_loss: 2.5985	step2_classification_accuracy: 73.0135	step_2_gate_accuracy: 25.5556
STEP-2	Epoch: 40/200	classification_loss: 0.7267	gate_loss: 1.1048	step2_classification_accuracy: 78.0640	step_2_gate_accuracy: 69.2929
STEP-2	Epoch: 60/200	classification_loss: 0.5625	gate_loss: 0.6261	step2_classification_accuracy: 82.5084	step_2_gate_accuracy: 82.0370
STEP-2	Epoch: 80/200	classification_loss: 0.4794	gate_loss: 0.4551	step2_classification_accuracy: 84.7811	step_2_gate_accuracy: 86.4478
STEP-2	Epoch: 100/200	classification_loss: 0.4149	gate_loss: 0.3580	step2_classification_accuracy: 86.5657	step_2_gate_accuracy: 89.2761
STEP-2	Epoch: 120/200	classification_loss: 0.3703	gate_loss: 0.3027	step2_classification_accuracy: 87.8451	step_2_gate_accuracy: 90.8249
STEP-2	Epoch: 140/200	classification_loss: 0.3381	gate_loss: 0.2634	step2_classification_accuracy: 88.7374	step_2_gate_accuracy: 91.7340
STEP-2	Epoch: 160/200	classification_loss: 0.3167	gate_loss: 0.2353	step2_classification_accuracy: 89.3266	step_2_gate_accuracy: 92.6431
STEP-2	Epoch: 180/200	classification_loss: 0.2961	gate_loss: 0.2146	step2_classification_accuracy: 89.7980	step_2_gate_accuracy: 92.9798
STEP-2	Epoch: 200/200	classification_loss: 0.2800	gate_loss: 0.1964	step2_classification_accuracy: 90.0842	step_2_gate_accuracy: 93.6532
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.4000	gate_accuracy: 68.0000
	Task-1	val_accuracy: 61.2500	gate_accuracy: 68.7500
	Task-2	val_accuracy: 63.0952	gate_accuracy: 65.4762
	Task-3	val_accuracy: 66.0550	gate_accuracy: 74.3119
	Task-4	val_accuracy: 78.3784	gate_accuracy: 78.3784
	Task-5	val_accuracy: 48.1481	gate_accuracy: 60.4938
	Task-6	val_accuracy: 59.0909	gate_accuracy: 59.0909
	Task-7	val_accuracy: 65.2778	gate_accuracy: 69.4444
	Task-8	val_accuracy: 58.9474	gate_accuracy: 58.9474
	Task-9	val_accuracy: 56.0000	gate_accuracy: 60.0000
	Task-10	val_accuracy: 65.4321	gate_accuracy: 65.4321
	Task-11	val_accuracy: 66.6667	gate_accuracy: 64.1975
	Task-12	val_accuracy: 55.9524	gate_accuracy: 53.5714
	Task-13	val_accuracy: 62.6506	gate_accuracy: 69.8795
	Task-14	val_accuracy: 61.1111	gate_accuracy: 63.8889
	Task-15	val_accuracy: 67.9012	gate_accuracy: 70.3704
	Task-16	val_accuracy: 53.4247	gate_accuracy: 50.6849
	Task-17	val_accuracy: 54.2169	gate_accuracy: 48.1928
	Task-18	val_accuracy: 62.6866	gate_accuracy: 67.1642
	Task-19	val_accuracy: 73.9726	gate_accuracy: 72.6027
	Task-20	val_accuracy: 67.6056	gate_accuracy: 61.9718
	Task-21	val_accuracy: 72.3684	gate_accuracy: 73.6842
	Task-22	val_accuracy: 62.5000	gate_accuracy: 61.2500
	Task-23	val_accuracy: 62.6263	gate_accuracy: 65.6566
	Task-24	val_accuracy: 66.6667	gate_accuracy: 55.5556
	Task-25	val_accuracy: 62.1053	gate_accuracy: 65.2632
	Task-26	val_accuracy: 78.6517	gate_accuracy: 68.5393
	Task-27	val_accuracy: 73.7500	gate_accuracy: 70.0000
	Task-28	val_accuracy: 69.6970	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.8388


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.3660	step1_train_accuracy: 53.4819
STEP-1	Epoch: 20/50	loss: 0.9965	step1_train_accuracy: 78.8301
STEP-1	Epoch: 30/50	loss: 0.5988	step1_train_accuracy: 89.4150
STEP-1	Epoch: 40/50	loss: 0.4093	step1_train_accuracy: 91.3649
STEP-1	Epoch: 50/50	loss: 0.2956	step1_train_accuracy: 93.8719
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9149	gate_loss: 2.5392	step2_classification_accuracy: 71.3355	step_2_gate_accuracy: 27.8502
STEP-2	Epoch: 40/200	classification_loss: 0.7267	gate_loss: 1.0675	step2_classification_accuracy: 77.1987	step_2_gate_accuracy: 69.3974
STEP-2	Epoch: 60/200	classification_loss: 0.5719	gate_loss: 0.6202	step2_classification_accuracy: 82.2150	step_2_gate_accuracy: 81.5635
STEP-2	Epoch: 80/200	classification_loss: 0.4902	gate_loss: 0.4528	step2_classification_accuracy: 84.5928	step_2_gate_accuracy: 86.0423
STEP-2	Epoch: 100/200	classification_loss: 0.4331	gate_loss: 0.3672	step2_classification_accuracy: 86.2215	step_2_gate_accuracy: 88.6319
STEP-2	Epoch: 120/200	classification_loss: 0.3893	gate_loss: 0.3059	step2_classification_accuracy: 87.3290	step_2_gate_accuracy: 90.2117
STEP-2	Epoch: 140/200	classification_loss: 0.3475	gate_loss: 0.2656	step2_classification_accuracy: 88.4528	step_2_gate_accuracy: 91.5147
STEP-2	Epoch: 160/200	classification_loss: 0.3329	gate_loss: 0.2434	step2_classification_accuracy: 88.9088	step_2_gate_accuracy: 91.9381
STEP-2	Epoch: 180/200	classification_loss: 0.3109	gate_loss: 0.2238	step2_classification_accuracy: 89.7394	step_2_gate_accuracy: 92.8827
STEP-2	Epoch: 200/200	classification_loss: 0.2916	gate_loss: 0.2003	step2_classification_accuracy: 89.9023	step_2_gate_accuracy: 93.4528
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.8000	gate_accuracy: 70.4000
	Task-1	val_accuracy: 68.7500	gate_accuracy: 75.0000
	Task-2	val_accuracy: 54.7619	gate_accuracy: 61.9048
	Task-3	val_accuracy: 61.4679	gate_accuracy: 69.7248
	Task-4	val_accuracy: 66.2162	gate_accuracy: 71.6216
	Task-5	val_accuracy: 44.4444	gate_accuracy: 51.8519
	Task-6	val_accuracy: 56.8182	gate_accuracy: 56.8182
	Task-7	val_accuracy: 65.2778	gate_accuracy: 69.4444
	Task-8	val_accuracy: 63.1579	gate_accuracy: 60.0000
	Task-9	val_accuracy: 58.6667	gate_accuracy: 61.3333
	Task-10	val_accuracy: 60.4938	gate_accuracy: 56.7901
	Task-11	val_accuracy: 65.4321	gate_accuracy: 65.4321
	Task-12	val_accuracy: 57.1429	gate_accuracy: 52.3810
	Task-13	val_accuracy: 61.4458	gate_accuracy: 55.4217
	Task-14	val_accuracy: 66.6667	gate_accuracy: 68.0556
	Task-15	val_accuracy: 74.0741	gate_accuracy: 76.5432
	Task-16	val_accuracy: 60.2740	gate_accuracy: 57.5342
	Task-17	val_accuracy: 59.0361	gate_accuracy: 57.8313
	Task-18	val_accuracy: 65.6716	gate_accuracy: 62.6866
	Task-19	val_accuracy: 76.7123	gate_accuracy: 76.7123
	Task-20	val_accuracy: 76.0563	gate_accuracy: 70.4225
	Task-21	val_accuracy: 72.3684	gate_accuracy: 72.3684
	Task-22	val_accuracy: 71.2500	gate_accuracy: 70.0000
	Task-23	val_accuracy: 65.6566	gate_accuracy: 63.6364
	Task-24	val_accuracy: 69.4444	gate_accuracy: 59.7222
	Task-25	val_accuracy: 67.3684	gate_accuracy: 67.3684
	Task-26	val_accuracy: 71.9101	gate_accuracy: 73.0337
	Task-27	val_accuracy: 73.7500	gate_accuracy: 73.7500
	Task-28	val_accuracy: 63.6364	gate_accuracy: 60.6061
	Task-29	val_accuracy: 57.7778	gate_accuracy: 56.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.8649


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 2.1203	step1_train_accuracy: 56.9948
STEP-1	Epoch: 20/50	loss: 0.8486	step1_train_accuracy: 82.9016
STEP-1	Epoch: 30/50	loss: 0.4350	step1_train_accuracy: 92.4870
STEP-1	Epoch: 40/50	loss: 0.2817	step1_train_accuracy: 96.8912
STEP-1	Epoch: 50/50	loss: 0.1884	step1_train_accuracy: 98.4456
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9494	gate_loss: 2.6182	step2_classification_accuracy: 70.2050	step_2_gate_accuracy: 25.6940
STEP-2	Epoch: 40/200	classification_loss: 0.7570	gate_loss: 1.1052	step2_classification_accuracy: 76.1514	step_2_gate_accuracy: 68.5647
STEP-2	Epoch: 60/200	classification_loss: 0.5996	gate_loss: 0.6469	step2_classification_accuracy: 80.9306	step_2_gate_accuracy: 80.8675
STEP-2	Epoch: 80/200	classification_loss: 0.5156	gate_loss: 0.4759	step2_classification_accuracy: 83.7855	step_2_gate_accuracy: 85.1420
STEP-2	Epoch: 100/200	classification_loss: 0.4423	gate_loss: 0.3785	step2_classification_accuracy: 85.3312	step_2_gate_accuracy: 87.8076
STEP-2	Epoch: 120/200	classification_loss: 0.3896	gate_loss: 0.3155	step2_classification_accuracy: 86.8139	step_2_gate_accuracy: 89.8738
STEP-2	Epoch: 140/200	classification_loss: 0.3625	gate_loss: 0.2817	step2_classification_accuracy: 87.6025	step_2_gate_accuracy: 90.9779
STEP-2	Epoch: 160/200	classification_loss: 0.3266	gate_loss: 0.2450	step2_classification_accuracy: 89.0379	step_2_gate_accuracy: 92.2555
STEP-2	Epoch: 180/200	classification_loss: 0.3057	gate_loss: 0.2218	step2_classification_accuracy: 88.6435	step_2_gate_accuracy: 92.7129
STEP-2	Epoch: 200/200	classification_loss: 0.2875	gate_loss: 0.2072	step2_classification_accuracy: 89.8107	step_2_gate_accuracy: 93.3754
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.6000	gate_accuracy: 68.8000
	Task-1	val_accuracy: 65.0000	gate_accuracy: 71.2500
	Task-2	val_accuracy: 54.7619	gate_accuracy: 60.7143
	Task-3	val_accuracy: 58.7156	gate_accuracy: 65.1376
	Task-4	val_accuracy: 67.5676	gate_accuracy: 68.9189
	Task-5	val_accuracy: 45.6790	gate_accuracy: 58.0247
	Task-6	val_accuracy: 61.3636	gate_accuracy: 63.6364
	Task-7	val_accuracy: 52.7778	gate_accuracy: 56.9444
	Task-8	val_accuracy: 56.8421	gate_accuracy: 54.7368
	Task-9	val_accuracy: 53.3333	gate_accuracy: 54.6667
	Task-10	val_accuracy: 62.9630	gate_accuracy: 60.4938
	Task-11	val_accuracy: 66.6667	gate_accuracy: 69.1358
	Task-12	val_accuracy: 51.1905	gate_accuracy: 42.8571
	Task-13	val_accuracy: 60.2410	gate_accuracy: 61.4458
	Task-14	val_accuracy: 62.5000	gate_accuracy: 63.8889
	Task-15	val_accuracy: 61.7284	gate_accuracy: 58.0247
	Task-16	val_accuracy: 56.1644	gate_accuracy: 56.1644
	Task-17	val_accuracy: 62.6506	gate_accuracy: 60.2410
	Task-18	val_accuracy: 59.7015	gate_accuracy: 61.1940
	Task-19	val_accuracy: 67.1233	gate_accuracy: 64.3836
	Task-20	val_accuracy: 69.0141	gate_accuracy: 64.7887
	Task-21	val_accuracy: 69.7368	gate_accuracy: 75.0000
	Task-22	val_accuracy: 57.5000	gate_accuracy: 56.2500
	Task-23	val_accuracy: 68.6869	gate_accuracy: 71.7172
	Task-24	val_accuracy: 68.0556	gate_accuracy: 54.1667
	Task-25	val_accuracy: 66.3158	gate_accuracy: 63.1579
	Task-26	val_accuracy: 78.6517	gate_accuracy: 78.6517
	Task-27	val_accuracy: 75.0000	gate_accuracy: 77.5000
	Task-28	val_accuracy: 57.5758	gate_accuracy: 51.5152
	Task-29	val_accuracy: 63.3333	gate_accuracy: 65.5556
	Task-30	val_accuracy: 64.5833	gate_accuracy: 55.2083
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.6408


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 2.4549	step1_train_accuracy: 52.6012
STEP-1	Epoch: 20/50	loss: 1.0411	step1_train_accuracy: 80.6358
STEP-1	Epoch: 30/50	loss: 0.5645	step1_train_accuracy: 90.7514
STEP-1	Epoch: 40/50	loss: 0.3762	step1_train_accuracy: 95.3757
STEP-1	Epoch: 50/50	loss: 0.2618	step1_train_accuracy: 98.5549
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9486	gate_loss: 2.5630	step2_classification_accuracy: 70.9633	step_2_gate_accuracy: 28.8226
STEP-2	Epoch: 40/200	classification_loss: 0.7503	gate_loss: 1.0914	step2_classification_accuracy: 76.1774	step_2_gate_accuracy: 68.8991
STEP-2	Epoch: 60/200	classification_loss: 0.5992	gate_loss: 0.6429	step2_classification_accuracy: 80.6269	step_2_gate_accuracy: 80.3517
STEP-2	Epoch: 80/200	classification_loss: 0.4987	gate_loss: 0.4675	step2_classification_accuracy: 83.8379	step_2_gate_accuracy: 85.7187
STEP-2	Epoch: 100/200	classification_loss: 0.4384	gate_loss: 0.3726	step2_classification_accuracy: 85.6575	step_2_gate_accuracy: 88.2569
STEP-2	Epoch: 120/200	classification_loss: 0.3975	gate_loss: 0.3206	step2_classification_accuracy: 86.8043	step_2_gate_accuracy: 89.6942
STEP-2	Epoch: 140/200	classification_loss: 0.3633	gate_loss: 0.2778	step2_classification_accuracy: 87.8440	step_2_gate_accuracy: 91.1774
STEP-2	Epoch: 160/200	classification_loss: 0.3335	gate_loss: 0.2477	step2_classification_accuracy: 88.7462	step_2_gate_accuracy: 92.0948
STEP-2	Epoch: 180/200	classification_loss: 0.3244	gate_loss: 0.2334	step2_classification_accuracy: 88.7309	step_2_gate_accuracy: 92.2936
STEP-2	Epoch: 200/200	classification_loss: 0.3045	gate_loss: 0.2117	step2_classification_accuracy: 89.4954	step_2_gate_accuracy: 93.1498
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.4000	gate_accuracy: 70.4000
	Task-1	val_accuracy: 67.5000	gate_accuracy: 78.7500
	Task-2	val_accuracy: 57.1429	gate_accuracy: 63.0952
	Task-3	val_accuracy: 68.8073	gate_accuracy: 68.8073
	Task-4	val_accuracy: 71.6216	gate_accuracy: 77.0270
	Task-5	val_accuracy: 49.3827	gate_accuracy: 53.0864
	Task-6	val_accuracy: 52.2727	gate_accuracy: 54.5455
	Task-7	val_accuracy: 63.8889	gate_accuracy: 62.5000
	Task-8	val_accuracy: 60.0000	gate_accuracy: 57.8947
	Task-9	val_accuracy: 56.0000	gate_accuracy: 57.3333
	Task-10	val_accuracy: 62.9630	gate_accuracy: 58.0247
	Task-11	val_accuracy: 65.4321	gate_accuracy: 71.6049
	Task-12	val_accuracy: 52.3810	gate_accuracy: 46.4286
	Task-13	val_accuracy: 59.0361	gate_accuracy: 62.6506
	Task-14	val_accuracy: 68.0556	gate_accuracy: 69.4444
	Task-15	val_accuracy: 69.1358	gate_accuracy: 65.4321
	Task-16	val_accuracy: 61.6438	gate_accuracy: 58.9041
	Task-17	val_accuracy: 63.8554	gate_accuracy: 67.4699
	Task-18	val_accuracy: 61.1940	gate_accuracy: 61.1940
	Task-19	val_accuracy: 72.6027	gate_accuracy: 65.7534
	Task-20	val_accuracy: 70.4225	gate_accuracy: 66.1972
	Task-21	val_accuracy: 73.6842	gate_accuracy: 73.6842
	Task-22	val_accuracy: 57.5000	gate_accuracy: 57.5000
	Task-23	val_accuracy: 68.6869	gate_accuracy: 66.6667
	Task-24	val_accuracy: 59.7222	gate_accuracy: 56.9444
	Task-25	val_accuracy: 62.1053	gate_accuracy: 54.7368
	Task-26	val_accuracy: 80.8989	gate_accuracy: 76.4045
	Task-27	val_accuracy: 68.7500	gate_accuracy: 67.5000
	Task-28	val_accuracy: 71.2121	gate_accuracy: 68.1818
	Task-29	val_accuracy: 51.1111	gate_accuracy: 55.5556
	Task-30	val_accuracy: 69.7917	gate_accuracy: 66.6667
	Task-31	val_accuracy: 59.3023	gate_accuracy: 52.3256
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.5475


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 2.2610	step1_train_accuracy: 59.6206
STEP-1	Epoch: 20/50	loss: 0.8899	step1_train_accuracy: 79.6748
STEP-1	Epoch: 30/50	loss: 0.4336	step1_train_accuracy: 93.7669
STEP-1	Epoch: 40/50	loss: 0.2608	step1_train_accuracy: 98.3740
STEP-1	Epoch: 50/50	loss: 0.1780	step1_train_accuracy: 99.1870
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9935	gate_loss: 2.5759	step2_classification_accuracy: 69.1840	step_2_gate_accuracy: 27.5816
STEP-2	Epoch: 40/200	classification_loss: 0.8035	gate_loss: 1.1043	step2_classification_accuracy: 75.5193	step_2_gate_accuracy: 68.3828
STEP-2	Epoch: 60/200	classification_loss: 0.6374	gate_loss: 0.6648	step2_classification_accuracy: 80.4896	step_2_gate_accuracy: 80.2374
STEP-2	Epoch: 80/200	classification_loss: 0.5369	gate_loss: 0.4917	step2_classification_accuracy: 82.8338	step_2_gate_accuracy: 84.9852
STEP-2	Epoch: 100/200	classification_loss: 0.4591	gate_loss: 0.3902	step2_classification_accuracy: 85.4451	step_2_gate_accuracy: 87.9970
STEP-2	Epoch: 120/200	classification_loss: 0.4155	gate_loss: 0.3310	step2_classification_accuracy: 86.9288	step_2_gate_accuracy: 89.8665
STEP-2	Epoch: 140/200	classification_loss: 0.3769	gate_loss: 0.2899	step2_classification_accuracy: 87.9228	step_2_gate_accuracy: 90.8754
STEP-2	Epoch: 160/200	classification_loss: 0.3459	gate_loss: 0.2563	step2_classification_accuracy: 88.8131	step_2_gate_accuracy: 91.7804
STEP-2	Epoch: 180/200	classification_loss: 0.3306	gate_loss: 0.2382	step2_classification_accuracy: 89.0504	step_2_gate_accuracy: 92.4926
STEP-2	Epoch: 200/200	classification_loss: 0.3021	gate_loss: 0.2095	step2_classification_accuracy: 89.7478	step_2_gate_accuracy: 93.6053
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.6000	gate_accuracy: 68.8000
	Task-1	val_accuracy: 71.2500	gate_accuracy: 73.7500
	Task-2	val_accuracy: 58.3333	gate_accuracy: 61.9048
	Task-3	val_accuracy: 56.8807	gate_accuracy: 55.0459
	Task-4	val_accuracy: 70.2703	gate_accuracy: 70.2703
	Task-5	val_accuracy: 43.2099	gate_accuracy: 56.7901
	Task-6	val_accuracy: 50.0000	gate_accuracy: 60.2273
	Task-7	val_accuracy: 69.4444	gate_accuracy: 72.2222
	Task-8	val_accuracy: 64.2105	gate_accuracy: 58.9474
	Task-9	val_accuracy: 49.3333	gate_accuracy: 49.3333
	Task-10	val_accuracy: 61.7284	gate_accuracy: 64.1975
	Task-11	val_accuracy: 62.9630	gate_accuracy: 59.2593
	Task-12	val_accuracy: 51.1905	gate_accuracy: 46.4286
	Task-13	val_accuracy: 66.2651	gate_accuracy: 68.6747
	Task-14	val_accuracy: 68.0556	gate_accuracy: 73.6111
	Task-15	val_accuracy: 62.9630	gate_accuracy: 58.0247
	Task-16	val_accuracy: 52.0548	gate_accuracy: 52.0548
	Task-17	val_accuracy: 61.4458	gate_accuracy: 56.6265
	Task-18	val_accuracy: 59.7015	gate_accuracy: 62.6866
	Task-19	val_accuracy: 73.9726	gate_accuracy: 68.4932
	Task-20	val_accuracy: 77.4648	gate_accuracy: 76.0563
	Task-21	val_accuracy: 73.6842	gate_accuracy: 75.0000
	Task-22	val_accuracy: 65.0000	gate_accuracy: 66.2500
	Task-23	val_accuracy: 65.6566	gate_accuracy: 68.6869
	Task-24	val_accuracy: 63.8889	gate_accuracy: 56.9444
	Task-25	val_accuracy: 64.2105	gate_accuracy: 62.1053
	Task-26	val_accuracy: 73.0337	gate_accuracy: 64.0449
	Task-27	val_accuracy: 75.0000	gate_accuracy: 70.0000
	Task-28	val_accuracy: 66.6667	gate_accuracy: 62.1212
	Task-29	val_accuracy: 61.1111	gate_accuracy: 60.0000
	Task-30	val_accuracy: 60.4167	gate_accuracy: 57.2917
	Task-31	val_accuracy: 54.6512	gate_accuracy: 47.6744
	Task-32	val_accuracy: 58.6957	gate_accuracy: 55.4348
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.2230


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.2984	step1_train_accuracy: 63.1854
STEP-1	Epoch: 20/50	loss: 0.8540	step1_train_accuracy: 85.9008
STEP-1	Epoch: 30/50	loss: 0.4031	step1_train_accuracy: 94.7781
STEP-1	Epoch: 40/50	loss: 0.2573	step1_train_accuracy: 97.6501
STEP-1	Epoch: 50/50	loss: 0.1766	step1_train_accuracy: 98.4334
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9640	gate_loss: 2.6747	step2_classification_accuracy: 70.5476	step_2_gate_accuracy: 25.5187
STEP-2	Epoch: 40/200	classification_loss: 0.7692	gate_loss: 1.1539	step2_classification_accuracy: 76.2824	step_2_gate_accuracy: 67.7378
STEP-2	Epoch: 60/200	classification_loss: 0.6034	gate_loss: 0.6743	step2_classification_accuracy: 81.2536	step_2_gate_accuracy: 79.8559
STEP-2	Epoch: 80/200	classification_loss: 0.5014	gate_loss: 0.4914	step2_classification_accuracy: 83.9769	step_2_gate_accuracy: 84.5677
STEP-2	Epoch: 100/200	classification_loss: 0.4416	gate_loss: 0.3902	step2_classification_accuracy: 86.1095	step_2_gate_accuracy: 87.6081
STEP-2	Epoch: 120/200	classification_loss: 0.4049	gate_loss: 0.3354	step2_classification_accuracy: 87.1902	step_2_gate_accuracy: 89.5677
STEP-2	Epoch: 140/200	classification_loss: 0.3600	gate_loss: 0.2859	step2_classification_accuracy: 88.2421	step_2_gate_accuracy: 91.0519
STEP-2	Epoch: 160/200	classification_loss: 0.3305	gate_loss: 0.2526	step2_classification_accuracy: 89.1066	step_2_gate_accuracy: 92.0605
STEP-2	Epoch: 180/200	classification_loss: 0.3175	gate_loss: 0.2345	step2_classification_accuracy: 89.3372	step_2_gate_accuracy: 92.8098
STEP-2	Epoch: 200/200	classification_loss: 0.3003	gate_loss: 0.2168	step2_classification_accuracy: 90.0576	step_2_gate_accuracy: 93.1268
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.8000	gate_accuracy: 65.6000
	Task-1	val_accuracy: 61.2500	gate_accuracy: 68.7500
	Task-2	val_accuracy: 64.2857	gate_accuracy: 69.0476
	Task-3	val_accuracy: 64.2202	gate_accuracy: 67.8899
	Task-4	val_accuracy: 71.6216	gate_accuracy: 75.6757
	Task-5	val_accuracy: 46.9136	gate_accuracy: 53.0864
	Task-6	val_accuracy: 57.9545	gate_accuracy: 60.2273
	Task-7	val_accuracy: 61.1111	gate_accuracy: 63.8889
	Task-8	val_accuracy: 55.7895	gate_accuracy: 53.6842
	Task-9	val_accuracy: 58.6667	gate_accuracy: 56.0000
	Task-10	val_accuracy: 66.6667	gate_accuracy: 64.1975
	Task-11	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-12	val_accuracy: 52.3810	gate_accuracy: 50.0000
	Task-13	val_accuracy: 62.6506	gate_accuracy: 65.0602
	Task-14	val_accuracy: 65.2778	gate_accuracy: 66.6667
	Task-15	val_accuracy: 66.6667	gate_accuracy: 58.0247
	Task-16	val_accuracy: 54.7945	gate_accuracy: 56.1644
	Task-17	val_accuracy: 57.8313	gate_accuracy: 51.8072
	Task-18	val_accuracy: 62.6866	gate_accuracy: 64.1791
	Task-19	val_accuracy: 72.6027	gate_accuracy: 69.8630
	Task-20	val_accuracy: 74.6479	gate_accuracy: 70.4225
	Task-21	val_accuracy: 67.1053	gate_accuracy: 69.7368
	Task-22	val_accuracy: 62.5000	gate_accuracy: 61.2500
	Task-23	val_accuracy: 67.6768	gate_accuracy: 66.6667
	Task-24	val_accuracy: 69.4444	gate_accuracy: 68.0556
	Task-25	val_accuracy: 61.0526	gate_accuracy: 55.7895
	Task-26	val_accuracy: 83.1461	gate_accuracy: 71.9101
	Task-27	val_accuracy: 71.2500	gate_accuracy: 70.0000
	Task-28	val_accuracy: 69.6970	gate_accuracy: 66.6667
	Task-29	val_accuracy: 57.7778	gate_accuracy: 61.1111
	Task-30	val_accuracy: 60.4167	gate_accuracy: 53.1250
	Task-31	val_accuracy: 52.3256	gate_accuracy: 46.5116
	Task-32	val_accuracy: 61.9565	gate_accuracy: 60.8696
	Task-33	val_accuracy: 81.2500	gate_accuracy: 76.0417
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.9695


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 2.7996	step1_train_accuracy: 48.5207
STEP-1	Epoch: 20/50	loss: 1.0760	step1_train_accuracy: 75.4438
STEP-1	Epoch: 30/50	loss: 0.6007	step1_train_accuracy: 90.5325
STEP-1	Epoch: 40/50	loss: 0.4080	step1_train_accuracy: 92.8994
STEP-1	Epoch: 50/50	loss: 0.3129	step1_train_accuracy: 92.3077
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0090	gate_loss: 2.5449	step2_classification_accuracy: 69.4818	step_2_gate_accuracy: 30.1401
STEP-2	Epoch: 40/200	classification_loss: 0.8006	gate_loss: 1.1055	step2_classification_accuracy: 75.1541	step_2_gate_accuracy: 68.0532
STEP-2	Epoch: 60/200	classification_loss: 0.6316	gate_loss: 0.6806	step2_classification_accuracy: 80.0980	step_2_gate_accuracy: 79.9440
STEP-2	Epoch: 80/200	classification_loss: 0.5432	gate_loss: 0.5149	step2_classification_accuracy: 83.2633	step_2_gate_accuracy: 84.1457
STEP-2	Epoch: 100/200	classification_loss: 0.4679	gate_loss: 0.4156	step2_classification_accuracy: 84.9440	step_2_gate_accuracy: 86.9188
STEP-2	Epoch: 120/200	classification_loss: 0.4244	gate_loss: 0.3492	step2_classification_accuracy: 86.4006	step_2_gate_accuracy: 89.2297
STEP-2	Epoch: 140/200	classification_loss: 0.3948	gate_loss: 0.3143	step2_classification_accuracy: 87.3810	step_2_gate_accuracy: 90.1821
STEP-2	Epoch: 160/200	classification_loss: 0.3610	gate_loss: 0.2792	step2_classification_accuracy: 88.3754	step_2_gate_accuracy: 91.3726
STEP-2	Epoch: 180/200	classification_loss: 0.3522	gate_loss: 0.2606	step2_classification_accuracy: 88.5434	step_2_gate_accuracy: 91.6387
STEP-2	Epoch: 200/200	classification_loss: 0.3204	gate_loss: 0.2332	step2_classification_accuracy: 89.2717	step_2_gate_accuracy: 92.4230
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.6000	gate_accuracy: 73.6000
	Task-1	val_accuracy: 57.5000	gate_accuracy: 71.2500
	Task-2	val_accuracy: 63.0952	gate_accuracy: 63.0952
	Task-3	val_accuracy: 55.9633	gate_accuracy: 66.0550
	Task-4	val_accuracy: 64.8649	gate_accuracy: 70.2703
	Task-5	val_accuracy: 48.1481	gate_accuracy: 55.5556
	Task-6	val_accuracy: 54.5455	gate_accuracy: 61.3636
	Task-7	val_accuracy: 61.1111	gate_accuracy: 59.7222
	Task-8	val_accuracy: 63.1579	gate_accuracy: 63.1579
	Task-9	val_accuracy: 42.6667	gate_accuracy: 41.3333
	Task-10	val_accuracy: 59.2593	gate_accuracy: 58.0247
	Task-11	val_accuracy: 62.9630	gate_accuracy: 60.4938
	Task-12	val_accuracy: 57.1429	gate_accuracy: 44.0476
	Task-13	val_accuracy: 67.4699	gate_accuracy: 71.0843
	Task-14	val_accuracy: 68.0556	gate_accuracy: 75.0000
	Task-15	val_accuracy: 62.9630	gate_accuracy: 62.9630
	Task-16	val_accuracy: 52.0548	gate_accuracy: 52.0548
	Task-17	val_accuracy: 62.6506	gate_accuracy: 60.2410
	Task-18	val_accuracy: 65.6716	gate_accuracy: 64.1791
	Task-19	val_accuracy: 75.3425	gate_accuracy: 73.9726
	Task-20	val_accuracy: 74.6479	gate_accuracy: 67.6056
	Task-21	val_accuracy: 71.0526	gate_accuracy: 75.0000
	Task-22	val_accuracy: 72.5000	gate_accuracy: 70.0000
	Task-23	val_accuracy: 60.6061	gate_accuracy: 63.6364
	Task-24	val_accuracy: 66.6667	gate_accuracy: 63.8889
	Task-25	val_accuracy: 67.3684	gate_accuracy: 67.3684
	Task-26	val_accuracy: 73.0337	gate_accuracy: 60.6742
	Task-27	val_accuracy: 65.0000	gate_accuracy: 65.0000
	Task-28	val_accuracy: 59.0909	gate_accuracy: 53.0303
	Task-29	val_accuracy: 55.5556	gate_accuracy: 56.6667
	Task-30	val_accuracy: 63.5417	gate_accuracy: 58.3333
	Task-31	val_accuracy: 58.1395	gate_accuracy: 50.0000
	Task-32	val_accuracy: 67.3913	gate_accuracy: 59.7826
	Task-33	val_accuracy: 73.9583	gate_accuracy: 68.7500
	Task-34	val_accuracy: 67.8571	gate_accuracy: 64.2857
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.7685


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.8227	step1_train_accuracy: 42.1538
STEP-1	Epoch: 20/50	loss: 0.9837	step1_train_accuracy: 84.9231
STEP-1	Epoch: 30/50	loss: 0.4938	step1_train_accuracy: 92.3077
STEP-1	Epoch: 40/50	loss: 0.3142	step1_train_accuracy: 96.6154
STEP-1	Epoch: 50/50	loss: 0.2375	step1_train_accuracy: 97.8462
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9985	gate_loss: 2.6131	step2_classification_accuracy: 69.2643	step_2_gate_accuracy: 27.7112
STEP-2	Epoch: 40/200	classification_loss: 0.7919	gate_loss: 1.1243	step2_classification_accuracy: 76.1853	step_2_gate_accuracy: 67.8883
STEP-2	Epoch: 60/200	classification_loss: 0.6256	gate_loss: 0.6770	step2_classification_accuracy: 80.6948	step_2_gate_accuracy: 79.7956
STEP-2	Epoch: 80/200	classification_loss: 0.5163	gate_loss: 0.4944	step2_classification_accuracy: 83.4605	step_2_gate_accuracy: 85.0136
STEP-2	Epoch: 100/200	classification_loss: 0.4444	gate_loss: 0.3895	step2_classification_accuracy: 85.8174	step_2_gate_accuracy: 87.6567
STEP-2	Epoch: 120/200	classification_loss: 0.3969	gate_loss: 0.3272	step2_classification_accuracy: 87.0163	step_2_gate_accuracy: 90.0273
STEP-2	Epoch: 140/200	classification_loss: 0.3639	gate_loss: 0.2889	step2_classification_accuracy: 88.0790	step_2_gate_accuracy: 91.2670
STEP-2	Epoch: 160/200	classification_loss: 0.3383	gate_loss: 0.2558	step2_classification_accuracy: 88.8692	step_2_gate_accuracy: 92.2616
STEP-2	Epoch: 180/200	classification_loss: 0.3212	gate_loss: 0.2363	step2_classification_accuracy: 89.4414	step_2_gate_accuracy: 92.6975
STEP-2	Epoch: 200/200	classification_loss: 0.3055	gate_loss: 0.2176	step2_classification_accuracy: 89.7139	step_2_gate_accuracy: 93.0518
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.8000	gate_accuracy: 68.0000
	Task-1	val_accuracy: 68.7500	gate_accuracy: 75.0000
	Task-2	val_accuracy: 60.7143	gate_accuracy: 65.4762
	Task-3	val_accuracy: 62.3853	gate_accuracy: 66.9725
	Task-4	val_accuracy: 68.9189	gate_accuracy: 68.9189
	Task-5	val_accuracy: 50.6173	gate_accuracy: 59.2593
	Task-6	val_accuracy: 63.6364	gate_accuracy: 68.1818
	Task-7	val_accuracy: 55.5556	gate_accuracy: 59.7222
	Task-8	val_accuracy: 54.7368	gate_accuracy: 48.4211
	Task-9	val_accuracy: 54.6667	gate_accuracy: 52.0000
	Task-10	val_accuracy: 61.7284	gate_accuracy: 59.2593
	Task-11	val_accuracy: 64.1975	gate_accuracy: 60.4938
	Task-12	val_accuracy: 51.1905	gate_accuracy: 44.0476
	Task-13	val_accuracy: 60.2410	gate_accuracy: 56.6265
	Task-14	val_accuracy: 61.1111	gate_accuracy: 63.8889
	Task-15	val_accuracy: 66.6667	gate_accuracy: 67.9012
	Task-16	val_accuracy: 56.1644	gate_accuracy: 52.0548
	Task-17	val_accuracy: 61.4458	gate_accuracy: 59.0361
	Task-18	val_accuracy: 58.2090	gate_accuracy: 58.2090
	Task-19	val_accuracy: 78.0822	gate_accuracy: 75.3425
	Task-20	val_accuracy: 71.8310	gate_accuracy: 66.1972
	Task-21	val_accuracy: 78.9474	gate_accuracy: 80.2632
	Task-22	val_accuracy: 71.2500	gate_accuracy: 66.2500
	Task-23	val_accuracy: 62.6263	gate_accuracy: 64.6465
	Task-24	val_accuracy: 70.8333	gate_accuracy: 70.8333
	Task-25	val_accuracy: 67.3684	gate_accuracy: 65.2632
	Task-26	val_accuracy: 70.7865	gate_accuracy: 64.0449
	Task-27	val_accuracy: 65.0000	gate_accuracy: 62.5000
	Task-28	val_accuracy: 63.6364	gate_accuracy: 54.5455
	Task-29	val_accuracy: 62.2222	gate_accuracy: 66.6667
	Task-30	val_accuracy: 67.7083	gate_accuracy: 63.5417
	Task-31	val_accuracy: 51.1628	gate_accuracy: 43.0233
	Task-32	val_accuracy: 59.7826	gate_accuracy: 55.4348
	Task-33	val_accuracy: 79.1667	gate_accuracy: 79.1667
	Task-34	val_accuracy: 71.4286	gate_accuracy: 65.4762
	Task-35	val_accuracy: 70.3704	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.9728


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 3.8888	step1_train_accuracy: 27.0073
STEP-1	Epoch: 20/50	loss: 1.2506	step1_train_accuracy: 79.5620
STEP-1	Epoch: 30/50	loss: 0.6656	step1_train_accuracy: 93.7956
STEP-1	Epoch: 40/50	loss: 0.4363	step1_train_accuracy: 96.3504
STEP-1	Epoch: 50/50	loss: 0.2983	step1_train_accuracy: 97.8102
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0457	gate_loss: 2.6001	step2_classification_accuracy: 67.8647	step_2_gate_accuracy: 27.7321
STEP-2	Epoch: 40/200	classification_loss: 0.8241	gate_loss: 1.1065	step2_classification_accuracy: 75.3050	step_2_gate_accuracy: 68.8064
STEP-2	Epoch: 60/200	classification_loss: 0.6616	gate_loss: 0.6934	step2_classification_accuracy: 80.2918	step_2_gate_accuracy: 79.0981
STEP-2	Epoch: 80/200	classification_loss: 0.5557	gate_loss: 0.5212	step2_classification_accuracy: 83.0371	step_2_gate_accuracy: 84.0981
STEP-2	Epoch: 100/200	classification_loss: 0.4861	gate_loss: 0.4216	step2_classification_accuracy: 84.7347	step_2_gate_accuracy: 86.7772
STEP-2	Epoch: 120/200	classification_loss: 0.4271	gate_loss: 0.3551	step2_classification_accuracy: 86.4058	step_2_gate_accuracy: 89.2308
STEP-2	Epoch: 140/200	classification_loss: 0.3883	gate_loss: 0.3030	step2_classification_accuracy: 87.4271	step_2_gate_accuracy: 90.6631
STEP-2	Epoch: 160/200	classification_loss: 0.3518	gate_loss: 0.2681	step2_classification_accuracy: 88.5411	step_2_gate_accuracy: 91.5650
STEP-2	Epoch: 180/200	classification_loss: 0.3410	gate_loss: 0.2528	step2_classification_accuracy: 89.0186	step_2_gate_accuracy: 92.1353
STEP-2	Epoch: 200/200	classification_loss: 0.3251	gate_loss: 0.2336	step2_classification_accuracy: 89.0451	step_2_gate_accuracy: 92.4801
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.0000	gate_accuracy: 67.2000
	Task-1	val_accuracy: 65.0000	gate_accuracy: 72.5000
	Task-2	val_accuracy: 55.9524	gate_accuracy: 58.3333
	Task-3	val_accuracy: 65.1376	gate_accuracy: 69.7248
	Task-4	val_accuracy: 68.9189	gate_accuracy: 71.6216
	Task-5	val_accuracy: 49.3827	gate_accuracy: 59.2593
	Task-6	val_accuracy: 52.2727	gate_accuracy: 54.5455
	Task-7	val_accuracy: 54.1667	gate_accuracy: 56.9444
	Task-8	val_accuracy: 55.7895	gate_accuracy: 52.6316
	Task-9	val_accuracy: 50.6667	gate_accuracy: 53.3333
	Task-10	val_accuracy: 62.9630	gate_accuracy: 58.0247
	Task-11	val_accuracy: 64.1975	gate_accuracy: 66.6667
	Task-12	val_accuracy: 58.3333	gate_accuracy: 53.5714
	Task-13	val_accuracy: 67.4699	gate_accuracy: 68.6747
	Task-14	val_accuracy: 61.1111	gate_accuracy: 69.4444
	Task-15	val_accuracy: 59.2593	gate_accuracy: 58.0247
	Task-16	val_accuracy: 49.3151	gate_accuracy: 47.9452
	Task-17	val_accuracy: 61.4458	gate_accuracy: 59.0361
	Task-18	val_accuracy: 64.1791	gate_accuracy: 64.1791
	Task-19	val_accuracy: 76.7123	gate_accuracy: 75.3425
	Task-20	val_accuracy: 76.0563	gate_accuracy: 67.6056
	Task-21	val_accuracy: 65.7895	gate_accuracy: 61.8421
	Task-22	val_accuracy: 62.5000	gate_accuracy: 57.5000
	Task-23	val_accuracy: 61.6162	gate_accuracy: 58.5859
	Task-24	val_accuracy: 65.2778	gate_accuracy: 61.1111
	Task-25	val_accuracy: 60.0000	gate_accuracy: 60.0000
	Task-26	val_accuracy: 65.1685	gate_accuracy: 64.0449
	Task-27	val_accuracy: 65.0000	gate_accuracy: 65.0000
	Task-28	val_accuracy: 63.6364	gate_accuracy: 54.5455
	Task-29	val_accuracy: 62.2222	gate_accuracy: 64.4444
	Task-30	val_accuracy: 64.5833	gate_accuracy: 58.3333
	Task-31	val_accuracy: 51.1628	gate_accuracy: 47.6744
	Task-32	val_accuracy: 54.3478	gate_accuracy: 51.0870
	Task-33	val_accuracy: 80.2083	gate_accuracy: 76.0417
	Task-34	val_accuracy: 76.1905	gate_accuracy: 76.1905
	Task-35	val_accuracy: 67.9012	gate_accuracy: 62.9630
	Task-36	val_accuracy: 60.2941	gate_accuracy: 63.2353
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.8754


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.9971	step1_train_accuracy: 39.7436
STEP-1	Epoch: 20/50	loss: 1.1604	step1_train_accuracy: 76.6026
STEP-1	Epoch: 30/50	loss: 0.5866	step1_train_accuracy: 90.0641
STEP-1	Epoch: 40/50	loss: 0.3824	step1_train_accuracy: 93.5897
STEP-1	Epoch: 50/50	loss: 0.2739	step1_train_accuracy: 95.5128
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0707	gate_loss: 2.7084	step2_classification_accuracy: 68.3592	step_2_gate_accuracy: 26.2920
STEP-2	Epoch: 40/200	classification_loss: 0.8594	gate_loss: 1.1745	step2_classification_accuracy: 74.8191	step_2_gate_accuracy: 66.4341
STEP-2	Epoch: 60/200	classification_loss: 0.6798	gate_loss: 0.7296	step2_classification_accuracy: 79.4057	step_2_gate_accuracy: 77.5323
STEP-2	Epoch: 80/200	classification_loss: 0.5562	gate_loss: 0.5351	step2_classification_accuracy: 82.8036	step_2_gate_accuracy: 83.6434
STEP-2	Epoch: 100/200	classification_loss: 0.4766	gate_loss: 0.4240	step2_classification_accuracy: 85.1680	step_2_gate_accuracy: 86.8992
STEP-2	Epoch: 120/200	classification_loss: 0.4362	gate_loss: 0.3642	step2_classification_accuracy: 86.2791	step_2_gate_accuracy: 88.2558
STEP-2	Epoch: 140/200	classification_loss: 0.3926	gate_loss: 0.3136	step2_classification_accuracy: 87.3514	step_2_gate_accuracy: 90.1938
STEP-2	Epoch: 160/200	classification_loss: 0.3647	gate_loss: 0.2827	step2_classification_accuracy: 88.1395	step_2_gate_accuracy: 90.7752
STEP-2	Epoch: 180/200	classification_loss: 0.3378	gate_loss: 0.2545	step2_classification_accuracy: 89.0827	step_2_gate_accuracy: 92.0930
STEP-2	Epoch: 200/200	classification_loss: 0.3122	gate_loss: 0.2346	step2_classification_accuracy: 89.4057	step_2_gate_accuracy: 92.6357
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.2000	gate_accuracy: 68.8000
	Task-1	val_accuracy: 57.5000	gate_accuracy: 72.5000
	Task-2	val_accuracy: 50.0000	gate_accuracy: 58.3333
	Task-3	val_accuracy: 52.2936	gate_accuracy: 56.8807
	Task-4	val_accuracy: 67.5676	gate_accuracy: 67.5676
	Task-5	val_accuracy: 41.9753	gate_accuracy: 46.9136
	Task-6	val_accuracy: 50.0000	gate_accuracy: 56.8182
	Task-7	val_accuracy: 55.5556	gate_accuracy: 56.9444
	Task-8	val_accuracy: 58.9474	gate_accuracy: 54.7368
	Task-9	val_accuracy: 49.3333	gate_accuracy: 54.6667
	Task-10	val_accuracy: 70.3704	gate_accuracy: 65.4321
	Task-11	val_accuracy: 67.9012	gate_accuracy: 70.3704
	Task-12	val_accuracy: 50.0000	gate_accuracy: 51.1905
	Task-13	val_accuracy: 63.8554	gate_accuracy: 62.6506
	Task-14	val_accuracy: 65.2778	gate_accuracy: 66.6667
	Task-15	val_accuracy: 61.7284	gate_accuracy: 59.2593
	Task-16	val_accuracy: 43.8356	gate_accuracy: 41.0959
	Task-17	val_accuracy: 57.8313	gate_accuracy: 56.6265
	Task-18	val_accuracy: 58.2090	gate_accuracy: 59.7015
	Task-19	val_accuracy: 71.2329	gate_accuracy: 69.8630
	Task-20	val_accuracy: 76.0563	gate_accuracy: 70.4225
	Task-21	val_accuracy: 77.6316	gate_accuracy: 81.5789
	Task-22	val_accuracy: 62.5000	gate_accuracy: 58.7500
	Task-23	val_accuracy: 60.6061	gate_accuracy: 54.5455
	Task-24	val_accuracy: 59.7222	gate_accuracy: 52.7778
	Task-25	val_accuracy: 60.0000	gate_accuracy: 64.2105
	Task-26	val_accuracy: 78.6517	gate_accuracy: 70.7865
	Task-27	val_accuracy: 72.5000	gate_accuracy: 73.7500
	Task-28	val_accuracy: 63.6364	gate_accuracy: 53.0303
	Task-29	val_accuracy: 58.8889	gate_accuracy: 62.2222
	Task-30	val_accuracy: 67.7083	gate_accuracy: 58.3333
	Task-31	val_accuracy: 60.4651	gate_accuracy: 55.8140
	Task-32	val_accuracy: 52.1739	gate_accuracy: 48.9130
	Task-33	val_accuracy: 75.0000	gate_accuracy: 69.7917
	Task-34	val_accuracy: 73.8095	gate_accuracy: 75.0000
	Task-35	val_accuracy: 69.1358	gate_accuracy: 60.4938
	Task-36	val_accuracy: 83.8235	gate_accuracy: 82.3529
	Task-37	val_accuracy: 56.4103	gate_accuracy: 52.5641
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.5823


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 2.2653	step1_train_accuracy: 52.1490
STEP-1	Epoch: 20/50	loss: 0.8061	step1_train_accuracy: 87.3925
STEP-1	Epoch: 30/50	loss: 0.4418	step1_train_accuracy: 95.4155
STEP-1	Epoch: 40/50	loss: 0.2748	step1_train_accuracy: 98.8539
STEP-1	Epoch: 50/50	loss: 0.1829	step1_train_accuracy: 99.1404
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0775	gate_loss: 2.7085	step2_classification_accuracy: 67.8212	step_2_gate_accuracy: 25.1385
STEP-2	Epoch: 40/200	classification_loss: 0.8735	gate_loss: 1.1772	step2_classification_accuracy: 74.7355	step_2_gate_accuracy: 66.4358
STEP-2	Epoch: 60/200	classification_loss: 0.6974	gate_loss: 0.7215	step2_classification_accuracy: 79.4207	step_2_gate_accuracy: 77.9345
STEP-2	Epoch: 80/200	classification_loss: 0.5766	gate_loss: 0.5427	step2_classification_accuracy: 82.4559	step_2_gate_accuracy: 83.0227
STEP-2	Epoch: 100/200	classification_loss: 0.4961	gate_loss: 0.4309	step2_classification_accuracy: 84.6599	step_2_gate_accuracy: 86.5113
STEP-2	Epoch: 120/200	classification_loss: 0.4439	gate_loss: 0.3628	step2_classification_accuracy: 86.0202	step_2_gate_accuracy: 88.8539
STEP-2	Epoch: 140/200	classification_loss: 0.4007	gate_loss: 0.3175	step2_classification_accuracy: 87.1914	step_2_gate_accuracy: 89.8237
STEP-2	Epoch: 160/200	classification_loss: 0.3614	gate_loss: 0.2771	step2_classification_accuracy: 88.2997	step_2_gate_accuracy: 91.4106
STEP-2	Epoch: 180/200	classification_loss: 0.3472	gate_loss: 0.2557	step2_classification_accuracy: 88.8665	step_2_gate_accuracy: 91.9899
STEP-2	Epoch: 200/200	classification_loss: 0.3119	gate_loss: 0.2269	step2_classification_accuracy: 89.8489	step_2_gate_accuracy: 92.8715
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.0000	gate_accuracy: 65.6000
	Task-1	val_accuracy: 58.7500	gate_accuracy: 67.5000
	Task-2	val_accuracy: 52.3810	gate_accuracy: 57.1429
	Task-3	val_accuracy: 58.7156	gate_accuracy: 65.1376
	Task-4	val_accuracy: 63.5135	gate_accuracy: 66.2162
	Task-5	val_accuracy: 44.4444	gate_accuracy: 51.8519
	Task-6	val_accuracy: 50.0000	gate_accuracy: 54.5455
	Task-7	val_accuracy: 61.1111	gate_accuracy: 68.0556
	Task-8	val_accuracy: 66.3158	gate_accuracy: 63.1579
	Task-9	val_accuracy: 37.3333	gate_accuracy: 42.6667
	Task-10	val_accuracy: 64.1975	gate_accuracy: 60.4938
	Task-11	val_accuracy: 66.6667	gate_accuracy: 65.4321
	Task-12	val_accuracy: 55.9524	gate_accuracy: 50.0000
	Task-13	val_accuracy: 67.4699	gate_accuracy: 68.6747
	Task-14	val_accuracy: 66.6667	gate_accuracy: 65.2778
	Task-15	val_accuracy: 61.7284	gate_accuracy: 64.1975
	Task-16	val_accuracy: 46.5753	gate_accuracy: 43.8356
	Task-17	val_accuracy: 56.6265	gate_accuracy: 57.8313
	Task-18	val_accuracy: 65.6716	gate_accuracy: 65.6716
	Task-19	val_accuracy: 72.6027	gate_accuracy: 61.6438
	Task-20	val_accuracy: 66.1972	gate_accuracy: 63.3803
	Task-21	val_accuracy: 72.3684	gate_accuracy: 73.6842
	Task-22	val_accuracy: 71.2500	gate_accuracy: 66.2500
	Task-23	val_accuracy: 64.6465	gate_accuracy: 66.6667
	Task-24	val_accuracy: 62.5000	gate_accuracy: 54.1667
	Task-25	val_accuracy: 48.4211	gate_accuracy: 42.1053
	Task-26	val_accuracy: 73.0337	gate_accuracy: 68.5393
	Task-27	val_accuracy: 72.5000	gate_accuracy: 68.7500
	Task-28	val_accuracy: 69.6970	gate_accuracy: 63.6364
	Task-29	val_accuracy: 56.6667	gate_accuracy: 57.7778
	Task-30	val_accuracy: 67.7083	gate_accuracy: 63.5417
	Task-31	val_accuracy: 53.4884	gate_accuracy: 50.0000
	Task-32	val_accuracy: 58.6957	gate_accuracy: 55.4348
	Task-33	val_accuracy: 76.0417	gate_accuracy: 65.6250
	Task-34	val_accuracy: 67.8571	gate_accuracy: 66.6667
	Task-35	val_accuracy: 72.8395	gate_accuracy: 65.4321
	Task-36	val_accuracy: 79.4118	gate_accuracy: 77.9412
	Task-37	val_accuracy: 61.5385	gate_accuracy: 55.1282
	Task-38	val_accuracy: 77.0115	gate_accuracy: 74.7126
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.6261


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 3.0508	step1_train_accuracy: 47.4684
STEP-1	Epoch: 20/50	loss: 1.1274	step1_train_accuracy: 76.8987
STEP-1	Epoch: 30/50	loss: 0.5397	step1_train_accuracy: 92.7215
STEP-1	Epoch: 40/50	loss: 0.3718	step1_train_accuracy: 94.9367
STEP-1	Epoch: 50/50	loss: 0.2875	step1_train_accuracy: 94.9367
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1191	gate_loss: 2.6971	step2_classification_accuracy: 66.1794	step_2_gate_accuracy: 26.4865
STEP-2	Epoch: 40/200	classification_loss: 0.9088	gate_loss: 1.2133	step2_classification_accuracy: 72.8501	step_2_gate_accuracy: 65.6880
STEP-2	Epoch: 60/200	classification_loss: 0.7257	gate_loss: 0.7665	step2_classification_accuracy: 78.3784	step_2_gate_accuracy: 77.0762
STEP-2	Epoch: 80/200	classification_loss: 0.6069	gate_loss: 0.5765	step2_classification_accuracy: 81.4742	step_2_gate_accuracy: 82.2113
STEP-2	Epoch: 100/200	classification_loss: 0.5342	gate_loss: 0.4649	step2_classification_accuracy: 83.9312	step_2_gate_accuracy: 85.3808
STEP-2	Epoch: 120/200	classification_loss: 0.4737	gate_loss: 0.3940	step2_classification_accuracy: 85.0983	step_2_gate_accuracy: 87.8747
STEP-2	Epoch: 140/200	classification_loss: 0.4416	gate_loss: 0.3470	step2_classification_accuracy: 86.5971	step_2_gate_accuracy: 89.1155
STEP-2	Epoch: 160/200	classification_loss: 0.4083	gate_loss: 0.3121	step2_classification_accuracy: 87.5921	step_2_gate_accuracy: 90.4054
STEP-2	Epoch: 180/200	classification_loss: 0.3775	gate_loss: 0.2809	step2_classification_accuracy: 88.1204	step_2_gate_accuracy: 91.1671
STEP-2	Epoch: 200/200	classification_loss: 0.3669	gate_loss: 0.2701	step2_classification_accuracy: 88.2432	step_2_gate_accuracy: 91.6093
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.2000	gate_accuracy: 68.0000
	Task-1	val_accuracy: 52.5000	gate_accuracy: 62.5000
	Task-2	val_accuracy: 54.7619	gate_accuracy: 59.5238
	Task-3	val_accuracy: 62.3853	gate_accuracy: 62.3853
	Task-4	val_accuracy: 59.4595	gate_accuracy: 59.4595
	Task-5	val_accuracy: 48.1481	gate_accuracy: 55.5556
	Task-6	val_accuracy: 51.1364	gate_accuracy: 52.2727
	Task-7	val_accuracy: 59.7222	gate_accuracy: 65.2778
	Task-8	val_accuracy: 56.8421	gate_accuracy: 54.7368
	Task-9	val_accuracy: 41.3333	gate_accuracy: 44.0000
	Task-10	val_accuracy: 58.0247	gate_accuracy: 45.6790
	Task-11	val_accuracy: 67.9012	gate_accuracy: 66.6667
	Task-12	val_accuracy: 50.0000	gate_accuracy: 47.6190
	Task-13	val_accuracy: 62.6506	gate_accuracy: 65.0602
	Task-14	val_accuracy: 59.7222	gate_accuracy: 63.8889
	Task-15	val_accuracy: 53.0864	gate_accuracy: 49.3827
	Task-16	val_accuracy: 52.0548	gate_accuracy: 50.6849
	Task-17	val_accuracy: 60.2410	gate_accuracy: 46.9880
	Task-18	val_accuracy: 59.7015	gate_accuracy: 61.1940
	Task-19	val_accuracy: 76.7123	gate_accuracy: 68.4932
	Task-20	val_accuracy: 67.6056	gate_accuracy: 57.7465
	Task-21	val_accuracy: 63.1579	gate_accuracy: 63.1579
	Task-22	val_accuracy: 62.5000	gate_accuracy: 58.7500
	Task-23	val_accuracy: 64.6465	gate_accuracy: 62.6263
	Task-24	val_accuracy: 68.0556	gate_accuracy: 61.1111
	Task-25	val_accuracy: 51.5789	gate_accuracy: 45.2632
	Task-26	val_accuracy: 78.6517	gate_accuracy: 69.6629
	Task-27	val_accuracy: 62.5000	gate_accuracy: 56.2500
	Task-28	val_accuracy: 71.2121	gate_accuracy: 62.1212
	Task-29	val_accuracy: 60.0000	gate_accuracy: 58.8889
	Task-30	val_accuracy: 66.6667	gate_accuracy: 60.4167
	Task-31	val_accuracy: 54.6512	gate_accuracy: 50.0000
	Task-32	val_accuracy: 55.4348	gate_accuracy: 46.7391
	Task-33	val_accuracy: 80.2083	gate_accuracy: 83.3333
	Task-34	val_accuracy: 57.1429	gate_accuracy: 58.3333
	Task-35	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-36	val_accuracy: 73.5294	gate_accuracy: 72.0588
	Task-37	val_accuracy: 61.5385	gate_accuracy: 56.4103
	Task-38	val_accuracy: 74.7126	gate_accuracy: 73.5632
	Task-39	val_accuracy: 78.4810	gate_accuracy: 72.1519
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.7715


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 3.4800	step1_train_accuracy: 45.4874
STEP-1	Epoch: 20/50	loss: 1.0901	step1_train_accuracy: 80.5054
STEP-1	Epoch: 30/50	loss: 0.5595	step1_train_accuracy: 91.3357
STEP-1	Epoch: 40/50	loss: 0.3959	step1_train_accuracy: 92.4188
STEP-1	Epoch: 50/50	loss: 0.3060	step1_train_accuracy: 94.5848
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1322	gate_loss: 2.6469	step2_classification_accuracy: 67.2422	step_2_gate_accuracy: 27.9257
STEP-2	Epoch: 40/200	classification_loss: 0.8998	gate_loss: 1.1842	step2_classification_accuracy: 73.9448	step_2_gate_accuracy: 66.5827
STEP-2	Epoch: 60/200	classification_loss: 0.7008	gate_loss: 0.7408	step2_classification_accuracy: 78.9568	step_2_gate_accuracy: 77.6379
STEP-2	Epoch: 80/200	classification_loss: 0.5869	gate_loss: 0.5493	step2_classification_accuracy: 81.8825	step_2_gate_accuracy: 83.5611
STEP-2	Epoch: 100/200	classification_loss: 0.5129	gate_loss: 0.4448	step2_classification_accuracy: 84.1847	step_2_gate_accuracy: 85.9233
STEP-2	Epoch: 120/200	classification_loss: 0.4457	gate_loss: 0.3691	step2_classification_accuracy: 86.0911	step_2_gate_accuracy: 88.5012
STEP-2	Epoch: 140/200	classification_loss: 0.4117	gate_loss: 0.3252	step2_classification_accuracy: 87.2902	step_2_gate_accuracy: 89.9400
STEP-2	Epoch: 160/200	classification_loss: 0.3805	gate_loss: 0.2875	step2_classification_accuracy: 87.9137	step_2_gate_accuracy: 90.8513
STEP-2	Epoch: 180/200	classification_loss: 0.3553	gate_loss: 0.2661	step2_classification_accuracy: 88.6211	step_2_gate_accuracy: 91.5228
STEP-2	Epoch: 200/200	classification_loss: 0.3405	gate_loss: 0.2480	step2_classification_accuracy: 89.1127	step_2_gate_accuracy: 92.3501
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 48.8000	gate_accuracy: 64.8000
	Task-1	val_accuracy: 50.0000	gate_accuracy: 60.0000
	Task-2	val_accuracy: 50.0000	gate_accuracy: 59.5238
	Task-3	val_accuracy: 57.7982	gate_accuracy: 63.3028
	Task-4	val_accuracy: 79.7297	gate_accuracy: 79.7297
	Task-5	val_accuracy: 43.2099	gate_accuracy: 50.6173
	Task-6	val_accuracy: 55.6818	gate_accuracy: 60.2273
	Task-7	val_accuracy: 58.3333	gate_accuracy: 58.3333
	Task-8	val_accuracy: 51.5789	gate_accuracy: 48.4211
	Task-9	val_accuracy: 56.0000	gate_accuracy: 58.6667
	Task-10	val_accuracy: 55.5556	gate_accuracy: 54.3210
	Task-11	val_accuracy: 64.1975	gate_accuracy: 64.1975
	Task-12	val_accuracy: 45.2381	gate_accuracy: 36.9048
	Task-13	val_accuracy: 65.0602	gate_accuracy: 65.0602
	Task-14	val_accuracy: 58.3333	gate_accuracy: 59.7222
	Task-15	val_accuracy: 61.7284	gate_accuracy: 56.7901
	Task-16	val_accuracy: 54.7945	gate_accuracy: 53.4247
	Task-17	val_accuracy: 59.0361	gate_accuracy: 65.0602
	Task-18	val_accuracy: 61.1940	gate_accuracy: 59.7015
	Task-19	val_accuracy: 78.0822	gate_accuracy: 75.3425
	Task-20	val_accuracy: 80.2817	gate_accuracy: 74.6479
	Task-21	val_accuracy: 71.0526	gate_accuracy: 67.1053
	Task-22	val_accuracy: 62.5000	gate_accuracy: 53.7500
	Task-23	val_accuracy: 65.6566	gate_accuracy: 63.6364
	Task-24	val_accuracy: 59.7222	gate_accuracy: 56.9444
	Task-25	val_accuracy: 62.1053	gate_accuracy: 58.9474
	Task-26	val_accuracy: 77.5281	gate_accuracy: 70.7865
	Task-27	val_accuracy: 62.5000	gate_accuracy: 63.7500
	Task-28	val_accuracy: 62.1212	gate_accuracy: 50.0000
	Task-29	val_accuracy: 58.8889	gate_accuracy: 58.8889
	Task-30	val_accuracy: 58.3333	gate_accuracy: 59.3750
	Task-31	val_accuracy: 52.3256	gate_accuracy: 48.8372
	Task-32	val_accuracy: 55.4348	gate_accuracy: 51.0870
	Task-33	val_accuracy: 76.0417	gate_accuracy: 70.8333
	Task-34	val_accuracy: 71.4286	gate_accuracy: 71.4286
	Task-35	val_accuracy: 69.1358	gate_accuracy: 61.7284
	Task-36	val_accuracy: 64.7059	gate_accuracy: 63.2353
	Task-37	val_accuracy: 53.8462	gate_accuracy: 52.5641
	Task-38	val_accuracy: 75.8621	gate_accuracy: 70.1149
	Task-39	val_accuracy: 74.6835	gate_accuracy: 70.8861
	Task-40	val_accuracy: 68.1159	gate_accuracy: 71.0145
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.0309


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 2.4851	step1_train_accuracy: 48.6154
STEP-1	Epoch: 20/50	loss: 0.8480	step1_train_accuracy: 84.6154
STEP-1	Epoch: 30/50	loss: 0.4940	step1_train_accuracy: 93.8462
STEP-1	Epoch: 40/50	loss: 0.3291	step1_train_accuracy: 96.6154
STEP-1	Epoch: 50/50	loss: 0.2436	step1_train_accuracy: 96.6154
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1491	gate_loss: 2.7003	step2_classification_accuracy: 65.9836	step_2_gate_accuracy: 26.5574
STEP-2	Epoch: 40/200	classification_loss: 0.9285	gate_loss: 1.2117	step2_classification_accuracy: 72.8337	step_2_gate_accuracy: 64.9883
STEP-2	Epoch: 60/200	classification_loss: 0.7324	gate_loss: 0.7724	step2_classification_accuracy: 78.1967	step_2_gate_accuracy: 76.1241
STEP-2	Epoch: 80/200	classification_loss: 0.6181	gate_loss: 0.5817	step2_classification_accuracy: 81.8618	step_2_gate_accuracy: 81.8384
STEP-2	Epoch: 100/200	classification_loss: 0.5358	gate_loss: 0.4690	step2_classification_accuracy: 83.9344	step_2_gate_accuracy: 85.3747
STEP-2	Epoch: 120/200	classification_loss: 0.4701	gate_loss: 0.3891	step2_classification_accuracy: 85.9133	step_2_gate_accuracy: 88.0328
STEP-2	Epoch: 140/200	classification_loss: 0.4383	gate_loss: 0.3471	step2_classification_accuracy: 86.5808	step_2_gate_accuracy: 88.9461
STEP-2	Epoch: 160/200	classification_loss: 0.4031	gate_loss: 0.3055	step2_classification_accuracy: 87.4473	step_2_gate_accuracy: 90.2810
STEP-2	Epoch: 180/200	classification_loss: 0.3791	gate_loss: 0.2803	step2_classification_accuracy: 88.1382	step_2_gate_accuracy: 91.3232
STEP-2	Epoch: 200/200	classification_loss: 0.3543	gate_loss: 0.2532	step2_classification_accuracy: 88.8525	step_2_gate_accuracy: 92.1077
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.4000	gate_accuracy: 63.2000
	Task-1	val_accuracy: 55.0000	gate_accuracy: 62.5000
	Task-2	val_accuracy: 45.2381	gate_accuracy: 47.6190
	Task-3	val_accuracy: 61.4679	gate_accuracy: 65.1376
	Task-4	val_accuracy: 70.2703	gate_accuracy: 72.9730
	Task-5	val_accuracy: 44.4444	gate_accuracy: 54.3210
	Task-6	val_accuracy: 47.7273	gate_accuracy: 57.9545
	Task-7	val_accuracy: 55.5556	gate_accuracy: 59.7222
	Task-8	val_accuracy: 57.8947	gate_accuracy: 55.7895
	Task-9	val_accuracy: 61.3333	gate_accuracy: 64.0000
	Task-10	val_accuracy: 53.0864	gate_accuracy: 56.7901
	Task-11	val_accuracy: 64.1975	gate_accuracy: 60.4938
	Task-12	val_accuracy: 52.3810	gate_accuracy: 48.8095
	Task-13	val_accuracy: 62.6506	gate_accuracy: 57.8313
	Task-14	val_accuracy: 55.5556	gate_accuracy: 52.7778
	Task-15	val_accuracy: 49.3827	gate_accuracy: 46.9136
	Task-16	val_accuracy: 46.5753	gate_accuracy: 46.5753
	Task-17	val_accuracy: 55.4217	gate_accuracy: 46.9880
	Task-18	val_accuracy: 61.1940	gate_accuracy: 59.7015
	Task-19	val_accuracy: 72.6027	gate_accuracy: 67.1233
	Task-20	val_accuracy: 70.4225	gate_accuracy: 66.1972
	Task-21	val_accuracy: 68.4211	gate_accuracy: 61.8421
	Task-22	val_accuracy: 62.5000	gate_accuracy: 58.7500
	Task-23	val_accuracy: 63.6364	gate_accuracy: 59.5960
	Task-24	val_accuracy: 66.6667	gate_accuracy: 52.7778
	Task-25	val_accuracy: 51.5789	gate_accuracy: 46.3158
	Task-26	val_accuracy: 59.5506	gate_accuracy: 56.1798
	Task-27	val_accuracy: 65.0000	gate_accuracy: 58.7500
	Task-28	val_accuracy: 57.5758	gate_accuracy: 48.4848
	Task-29	val_accuracy: 55.5556	gate_accuracy: 60.0000
	Task-30	val_accuracy: 65.6250	gate_accuracy: 58.3333
	Task-31	val_accuracy: 61.6279	gate_accuracy: 52.3256
	Task-32	val_accuracy: 57.6087	gate_accuracy: 53.2609
	Task-33	val_accuracy: 78.1250	gate_accuracy: 69.7917
	Task-34	val_accuracy: 70.2381	gate_accuracy: 66.6667
	Task-35	val_accuracy: 72.8395	gate_accuracy: 71.6049
	Task-36	val_accuracy: 66.1765	gate_accuracy: 64.7059
	Task-37	val_accuracy: 69.2308	gate_accuracy: 57.6923
	Task-38	val_accuracy: 74.7126	gate_accuracy: 70.1149
	Task-39	val_accuracy: 67.0886	gate_accuracy: 60.7595
	Task-40	val_accuracy: 63.7681	gate_accuracy: 69.5652
	Task-41	val_accuracy: 58.0247	gate_accuracy: 54.3210
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 58.7169


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 2.3497	step1_train_accuracy: 50.7042
STEP-1	Epoch: 20/50	loss: 0.9287	step1_train_accuracy: 80.2817
STEP-1	Epoch: 30/50	loss: 0.5027	step1_train_accuracy: 89.5775
STEP-1	Epoch: 40/50	loss: 0.3264	step1_train_accuracy: 96.0563
STEP-1	Epoch: 50/50	loss: 0.2290	step1_train_accuracy: 96.0563
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1495	gate_loss: 2.6681	step2_classification_accuracy: 65.8696	step_2_gate_accuracy: 27.6201
STEP-2	Epoch: 40/200	classification_loss: 0.9160	gate_loss: 1.1853	step2_classification_accuracy: 73.0892	step_2_gate_accuracy: 65.6064
STEP-2	Epoch: 60/200	classification_loss: 0.7454	gate_loss: 0.7764	step2_classification_accuracy: 77.7231	step_2_gate_accuracy: 76.2471
STEP-2	Epoch: 80/200	classification_loss: 0.6262	gate_loss: 0.5896	step2_classification_accuracy: 81.1670	step_2_gate_accuracy: 81.2700
STEP-2	Epoch: 100/200	classification_loss: 0.5388	gate_loss: 0.4767	step2_classification_accuracy: 84.1419	step_2_gate_accuracy: 84.9428
STEP-2	Epoch: 120/200	classification_loss: 0.4711	gate_loss: 0.3974	step2_classification_accuracy: 85.4920	step_2_gate_accuracy: 87.4714
STEP-2	Epoch: 140/200	classification_loss: 0.4380	gate_loss: 0.3544	step2_classification_accuracy: 86.4188	step_2_gate_accuracy: 88.4211
STEP-2	Epoch: 160/200	classification_loss: 0.3867	gate_loss: 0.3034	step2_classification_accuracy: 88.1121	step_2_gate_accuracy: 90.3204
STEP-2	Epoch: 180/200	classification_loss: 0.3657	gate_loss: 0.2814	step2_classification_accuracy: 88.1808	step_2_gate_accuracy: 91.0641
STEP-2	Epoch: 200/200	classification_loss: 0.3391	gate_loss: 0.2499	step2_classification_accuracy: 89.1762	step_2_gate_accuracy: 92.0137
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.2000	gate_accuracy: 64.8000
	Task-1	val_accuracy: 63.7500	gate_accuracy: 73.7500
	Task-2	val_accuracy: 50.0000	gate_accuracy: 60.7143
	Task-3	val_accuracy: 67.8899	gate_accuracy: 68.8073
	Task-4	val_accuracy: 66.2162	gate_accuracy: 68.9189
	Task-5	val_accuracy: 46.9136	gate_accuracy: 54.3210
	Task-6	val_accuracy: 57.9545	gate_accuracy: 60.2273
	Task-7	val_accuracy: 62.5000	gate_accuracy: 62.5000
	Task-8	val_accuracy: 58.9474	gate_accuracy: 53.6842
	Task-9	val_accuracy: 49.3333	gate_accuracy: 54.6667
	Task-10	val_accuracy: 60.4938	gate_accuracy: 60.4938
	Task-11	val_accuracy: 61.7284	gate_accuracy: 60.4938
	Task-12	val_accuracy: 52.3810	gate_accuracy: 47.6190
	Task-13	val_accuracy: 63.8554	gate_accuracy: 63.8554
	Task-14	val_accuracy: 61.1111	gate_accuracy: 63.8889
	Task-15	val_accuracy: 60.4938	gate_accuracy: 55.5556
	Task-16	val_accuracy: 53.4247	gate_accuracy: 49.3151
	Task-17	val_accuracy: 59.0361	gate_accuracy: 51.8072
	Task-18	val_accuracy: 65.6716	gate_accuracy: 65.6716
	Task-19	val_accuracy: 68.4932	gate_accuracy: 63.0137
	Task-20	val_accuracy: 66.1972	gate_accuracy: 59.1549
	Task-21	val_accuracy: 71.0526	gate_accuracy: 63.1579
	Task-22	val_accuracy: 60.0000	gate_accuracy: 57.5000
	Task-23	val_accuracy: 55.5556	gate_accuracy: 44.4444
	Task-24	val_accuracy: 66.6667	gate_accuracy: 65.2778
	Task-25	val_accuracy: 55.7895	gate_accuracy: 50.5263
	Task-26	val_accuracy: 71.9101	gate_accuracy: 67.4157
	Task-27	val_accuracy: 61.2500	gate_accuracy: 60.0000
	Task-28	val_accuracy: 71.2121	gate_accuracy: 62.1212
	Task-29	val_accuracy: 65.5556	gate_accuracy: 67.7778
	Task-30	val_accuracy: 64.5833	gate_accuracy: 61.4583
	Task-31	val_accuracy: 60.4651	gate_accuracy: 56.9767
	Task-32	val_accuracy: 54.3478	gate_accuracy: 51.0870
	Task-33	val_accuracy: 71.8750	gate_accuracy: 59.3750
	Task-34	val_accuracy: 72.6190	gate_accuracy: 72.6190
	Task-35	val_accuracy: 74.0741	gate_accuracy: 67.9012
	Task-36	val_accuracy: 67.6471	gate_accuracy: 66.1765
	Task-37	val_accuracy: 62.8205	gate_accuracy: 61.5385
	Task-38	val_accuracy: 80.4598	gate_accuracy: 74.7126
	Task-39	val_accuracy: 63.2911	gate_accuracy: 51.8987
	Task-40	val_accuracy: 66.6667	gate_accuracy: 72.4638
	Task-41	val_accuracy: 55.5556	gate_accuracy: 50.6173
	Task-42	val_accuracy: 61.7978	gate_accuracy: 68.5393
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.7574


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 3.3090	step1_train_accuracy: 30.9963
STEP-1	Epoch: 20/50	loss: 1.0607	step1_train_accuracy: 86.3469
STEP-1	Epoch: 30/50	loss: 0.6147	step1_train_accuracy: 91.1439
STEP-1	Epoch: 40/50	loss: 0.4403	step1_train_accuracy: 95.5720
STEP-1	Epoch: 50/50	loss: 0.3275	step1_train_accuracy: 96.3100
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10, 874: 10, 875: 10, 876: 10, 877: 10, 878: 10, 879: 10, 880: 10, 881: 10, 882: 10, 883: 10, 884: 10, 885: 10, 886: 10, 887: 10, 888: 10, 889: 10, 890: 10, 891: 10, 892: 10, 893: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1462	gate_loss: 2.6893	step2_classification_accuracy: 65.7159	step_2_gate_accuracy: 27.5056
STEP-2	Epoch: 40/200	classification_loss: 0.9331	gate_loss: 1.1972	step2_classification_accuracy: 72.7405	step_2_gate_accuracy: 65.5145
STEP-2	Epoch: 60/200	classification_loss: 0.7581	gate_loss: 0.7873	step2_classification_accuracy: 77.8188	step_2_gate_accuracy: 75.6711
STEP-2	Epoch: 80/200	classification_loss: 0.6221	gate_loss: 0.5956	step2_classification_accuracy: 81.2864	step_2_gate_accuracy: 81.7002
STEP-2	Epoch: 100/200	classification_loss: 0.5571	gate_loss: 0.4913	step2_classification_accuracy: 83.3221	step_2_gate_accuracy: 84.4183
STEP-2	Epoch: 120/200	classification_loss: 0.4873	gate_loss: 0.4082	step2_classification_accuracy: 84.8434	step_2_gate_accuracy: 87.1812
STEP-2	Epoch: 140/200	classification_loss: 0.4510	gate_loss: 0.3617	step2_classification_accuracy: 86.0515	step_2_gate_accuracy: 88.2662
STEP-2	Epoch: 160/200	classification_loss: 0.4215	gate_loss: 0.3255	step2_classification_accuracy: 86.8792	step_2_gate_accuracy: 89.7204
STEP-2	Epoch: 180/200	classification_loss: 0.3895	gate_loss: 0.2926	step2_classification_accuracy: 87.9418	step_2_gate_accuracy: 90.7494
STEP-2	Epoch: 200/200	classification_loss: 0.3689	gate_loss: 0.2742	step2_classification_accuracy: 88.0201	step_2_gate_accuracy: 91.2975
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.2000	gate_accuracy: 58.4000
	Task-1	val_accuracy: 60.0000	gate_accuracy: 63.7500
	Task-2	val_accuracy: 40.4762	gate_accuracy: 47.6190
	Task-3	val_accuracy: 65.1376	gate_accuracy: 69.7248
	Task-4	val_accuracy: 70.2703	gate_accuracy: 71.6216
	Task-5	val_accuracy: 40.7407	gate_accuracy: 50.6173
	Task-6	val_accuracy: 50.0000	gate_accuracy: 57.9545
	Task-7	val_accuracy: 47.2222	gate_accuracy: 51.3889
	Task-8	val_accuracy: 60.0000	gate_accuracy: 56.8421
	Task-9	val_accuracy: 37.3333	gate_accuracy: 41.3333
	Task-10	val_accuracy: 64.1975	gate_accuracy: 61.7284
	Task-11	val_accuracy: 58.0247	gate_accuracy: 56.7901
	Task-12	val_accuracy: 50.0000	gate_accuracy: 44.0476
	Task-13	val_accuracy: 56.6265	gate_accuracy: 54.2169
	Task-14	val_accuracy: 58.3333	gate_accuracy: 55.5556
	Task-15	val_accuracy: 60.4938	gate_accuracy: 61.7284
	Task-16	val_accuracy: 49.3151	gate_accuracy: 47.9452
	Task-17	val_accuracy: 61.4458	gate_accuracy: 53.0120
	Task-18	val_accuracy: 62.6866	gate_accuracy: 61.1940
	Task-19	val_accuracy: 68.4932	gate_accuracy: 68.4932
	Task-20	val_accuracy: 78.8732	gate_accuracy: 73.2394
	Task-21	val_accuracy: 72.3684	gate_accuracy: 73.6842
	Task-22	val_accuracy: 58.7500	gate_accuracy: 58.7500
	Task-23	val_accuracy: 51.5152	gate_accuracy: 42.4242
	Task-24	val_accuracy: 59.7222	gate_accuracy: 54.1667
	Task-25	val_accuracy: 50.5263	gate_accuracy: 48.4211
	Task-26	val_accuracy: 73.0337	gate_accuracy: 70.7865
	Task-27	val_accuracy: 71.2500	gate_accuracy: 71.2500
	Task-28	val_accuracy: 63.6364	gate_accuracy: 62.1212
	Task-29	val_accuracy: 58.8889	gate_accuracy: 61.1111
	Task-30	val_accuracy: 56.2500	gate_accuracy: 52.0833
	Task-31	val_accuracy: 52.3256	gate_accuracy: 51.1628
	Task-32	val_accuracy: 56.5217	gate_accuracy: 48.9130
	Task-33	val_accuracy: 76.0417	gate_accuracy: 73.9583
	Task-34	val_accuracy: 76.1905	gate_accuracy: 78.5714
	Task-35	val_accuracy: 77.7778	gate_accuracy: 75.3086
	Task-36	val_accuracy: 72.0588	gate_accuracy: 70.5882
	Task-37	val_accuracy: 65.3846	gate_accuracy: 66.6667
	Task-38	val_accuracy: 73.5632	gate_accuracy: 65.5172
	Task-39	val_accuracy: 78.4810	gate_accuracy: 72.1519
	Task-40	val_accuracy: 68.1159	gate_accuracy: 73.9130
	Task-41	val_accuracy: 55.5556	gate_accuracy: 51.8519
	Task-42	val_accuracy: 65.1685	gate_accuracy: 68.5393
	Task-43	val_accuracy: 52.9412	gate_accuracy: 44.1176
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.9505


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 06:32:55	Wall time: 05:45:13
CPU time: 23575.869027473	Wall time: 20713.524201631546
FAA: 73.67665568148642
FF: 26.834202182334646

TRAINER.METRIC.ACCURACY
0: [98.4]
1: [95.19999999999999, 92.5]
2: [92.80000000000001, 91.25, 88.09523809523809]
3: [88.8, 87.5, 86.90476190476191, 93.57798165137615]
4: [81.6, 88.75, 84.52380952380952, 92.66055045871559, 86.48648648648648]
5: [84.0, 81.25, 82.14285714285714, 91.74311926605505, 78.37837837837837, 69.1358024691358]
6: [79.2, 86.25, 78.57142857142857, 86.23853211009175, 85.13513513513513, 69.1358024691358, 86.36363636363636]
7: [73.6, 82.5, 80.95238095238095, 84.40366972477065, 79.72972972972973, 67.90123456790124, 86.36363636363636, 86.11111111111111]
8: [76.0, 85.0, 76.19047619047619, 84.40366972477065, 82.43243243243244, 64.19753086419753, 84.0909090909091, 88.88888888888889, 80.0]
9: [77.60000000000001, 82.5, 75.0, 79.81651376146789, 75.67567567567568, 58.0246913580247, 79.54545454545455, 87.5, 72.63157894736842, 77.33333333333333]
10: [69.6, 80.0, 71.42857142857143, 79.81651376146789, 75.67567567567568, 60.49382716049383, 71.5909090909091, 77.77777777777779, 70.52631578947368, 73.33333333333333, 77.77777777777779]
11: [68.0, 75.0, 73.80952380952381, 81.65137614678899, 78.37837837837837, 54.32098765432099, 77.27272727272727, 77.77777777777779, 73.68421052631578, 77.33333333333333, 81.48148148148148, 72.8395061728395]
12: [73.6, 77.5, 70.23809523809523, 82.56880733944955, 77.02702702702703, 55.55555555555556, 70.45454545454545, 84.72222222222221, 73.68421052631578, 80.0, 77.77777777777779, 74.07407407407408, 72.61904761904762]
13: [70.39999999999999, 76.25, 73.80952380952381, 79.81651376146789, 71.62162162162163, 55.55555555555556, 70.45454545454545, 72.22222222222221, 75.78947368421053, 69.33333333333334, 69.1358024691358, 77.77777777777779, 65.47619047619048, 79.51807228915662]
14: [73.6, 80.0, 65.47619047619048, 79.81651376146789, 71.62162162162163, 51.85185185185185, 68.18181818181817, 76.38888888888889, 77.89473684210526, 61.33333333333333, 75.30864197530865, 70.37037037037037, 63.095238095238095, 78.3132530120482, 76.38888888888889]
15: [64.8, 75.0, 64.28571428571429, 79.81651376146789, 75.67567567567568, 55.55555555555556, 63.63636363636363, 70.83333333333334, 63.1578947368421, 57.333333333333336, 74.07407407407408, 71.60493827160494, 61.904761904761905, 74.69879518072288, 76.38888888888889, 71.60493827160494]
16: [66.4, 65.0, 73.80952380952381, 76.14678899082568, 75.67567567567568, 46.913580246913575, 68.18181818181817, 73.61111111111111, 70.52631578947368, 57.333333333333336, 72.8395061728395, 74.07407407407408, 61.904761904761905, 78.3132530120482, 77.77777777777779, 72.8395061728395, 73.97260273972603]
17: [60.8, 76.25, 69.04761904761905, 74.31192660550458, 71.62162162162163, 59.25925925925925, 68.18181818181817, 72.22222222222221, 66.3157894736842, 62.66666666666667, 75.30864197530865, 70.37037037037037, 69.04761904761905, 67.46987951807229, 79.16666666666666, 71.60493827160494, 68.4931506849315, 74.69879518072288]
18: [64.0, 78.75, 69.04761904761905, 76.14678899082568, 78.37837837837837, 54.32098765432099, 65.9090909090909, 72.22222222222221, 68.42105263157895, 57.333333333333336, 77.77777777777779, 71.60493827160494, 60.71428571428571, 69.87951807228916, 70.83333333333334, 71.60493827160494, 73.97260273972603, 73.49397590361446, 77.61194029850746]
19: [60.8, 72.5, 69.04761904761905, 79.81651376146789, 74.32432432432432, 58.0246913580247, 63.63636363636363, 65.27777777777779, 73.68421052631578, 58.666666666666664, 72.8395061728395, 65.4320987654321, 54.761904761904766, 68.67469879518072, 70.83333333333334, 76.5432098765432, 68.4931506849315, 61.44578313253012, 61.19402985074627, 79.45205479452055]
20: [62.4, 73.75, 69.04761904761905, 69.72477064220183, 74.32432432432432, 53.086419753086425, 70.45454545454545, 65.27777777777779, 71.57894736842105, 57.333333333333336, 66.66666666666666, 70.37037037037037, 60.71428571428571, 71.08433734939759, 75.0, 70.37037037037037, 63.013698630136986, 62.65060240963856, 67.16417910447761, 78.08219178082192, 81.69014084507043]
21: [61.6, 76.25, 67.85714285714286, 76.14678899082568, 79.72972972972973, 54.32098765432099, 65.9090909090909, 68.05555555555556, 67.36842105263158, 58.666666666666664, 69.1358024691358, 69.1358024691358, 53.57142857142857, 62.65060240963856, 73.61111111111111, 71.60493827160494, 61.64383561643836, 60.24096385542169, 67.16417910447761, 82.1917808219178, 81.69014084507043, 78.94736842105263]
22: [62.4, 63.74999999999999, 60.71428571428571, 64.22018348623854, 72.97297297297297, 50.617283950617285, 65.9090909090909, 73.61111111111111, 69.47368421052632, 60.0, 60.49382716049383, 66.66666666666666, 60.71428571428571, 72.28915662650603, 66.66666666666666, 66.66666666666666, 64.38356164383562, 65.06024096385542, 74.6268656716418, 76.71232876712328, 76.05633802816901, 67.10526315789474, 73.75]
23: [57.599999999999994, 76.25, 64.28571428571429, 69.72477064220183, 67.56756756756756, 54.32098765432099, 57.95454545454546, 63.888888888888886, 65.26315789473685, 68.0, 64.19753086419753, 72.8395061728395, 59.523809523809526, 69.87951807228916, 72.22222222222221, 70.37037037037037, 65.75342465753424, 69.87951807228916, 59.70149253731343, 76.71232876712328, 83.09859154929578, 75.0, 61.25000000000001, 65.65656565656566]
24: [56.00000000000001, 65.0, 70.23809523809523, 75.22935779816514, 74.32432432432432, 53.086419753086425, 57.95454545454546, 55.55555555555556, 63.1578947368421, 52.0, 61.72839506172839, 74.07407407407408, 63.095238095238095, 65.06024096385542, 63.888888888888886, 67.90123456790124, 58.9041095890411, 54.21686746987952, 62.68656716417911, 80.82191780821918, 78.87323943661971, 64.47368421052632, 68.75, 65.65656565656566, 68.05555555555556]
25: [56.00000000000001, 66.25, 69.04761904761905, 67.88990825688074, 72.97297297297297, 56.79012345679012, 56.81818181818182, 65.27777777777779, 63.1578947368421, 54.666666666666664, 74.07407407407408, 67.90123456790124, 53.57142857142857, 65.06024096385542, 65.27777777777779, 66.66666666666666, 63.013698630136986, 62.65060240963856, 62.68656716417911, 78.08219178082192, 70.4225352112676, 73.68421052631578, 65.0, 67.67676767676768, 70.83333333333334, 75.78947368421053]
26: [60.8, 62.5, 59.523809523809526, 62.38532110091744, 64.86486486486487, 48.148148148148145, 61.36363636363637, 70.83333333333334, 68.42105263157895, 45.33333333333333, 60.49382716049383, 65.4320987654321, 52.38095238095239, 69.87951807228916, 68.05555555555556, 71.60493827160494, 57.534246575342465, 60.24096385542169, 65.67164179104478, 75.34246575342466, 73.23943661971832, 72.36842105263158, 61.25000000000001, 66.66666666666666, 69.44444444444444, 62.10526315789474, 70.78651685393258]
27: [61.6, 60.0, 67.85714285714286, 66.05504587155964, 72.97297297297297, 50.617283950617285, 54.54545454545454, 63.888888888888886, 64.21052631578948, 52.0, 70.37037037037037, 67.90123456790124, 61.904761904761905, 65.06024096385542, 65.27777777777779, 66.66666666666666, 45.20547945205479, 66.26506024096386, 62.68656716417911, 73.97260273972603, 71.83098591549296, 71.05263157894737, 73.75, 64.64646464646465, 70.83333333333334, 72.63157894736842, 80.89887640449437, 70.0]
28: [58.4, 61.25000000000001, 63.095238095238095, 66.05504587155964, 78.37837837837837, 48.148148148148145, 59.09090909090909, 65.27777777777779, 58.94736842105262, 56.00000000000001, 65.4320987654321, 66.66666666666666, 55.952380952380956, 62.65060240963856, 61.111111111111114, 67.90123456790124, 53.42465753424658, 54.21686746987952, 62.68656716417911, 73.97260273972603, 67.6056338028169, 72.36842105263158, 62.5, 62.62626262626263, 66.66666666666666, 62.10526315789474, 78.65168539325843, 73.75, 69.6969696969697]
29: [60.8, 68.75, 54.761904761904766, 61.46788990825688, 66.21621621621621, 44.44444444444444, 56.81818181818182, 65.27777777777779, 63.1578947368421, 58.666666666666664, 60.49382716049383, 65.4320987654321, 57.14285714285714, 61.44578313253012, 66.66666666666666, 74.07407407407408, 60.273972602739725, 59.036144578313255, 65.67164179104478, 76.71232876712328, 76.05633802816901, 72.36842105263158, 71.25, 65.65656565656566, 69.44444444444444, 67.36842105263158, 71.91011235955057, 73.75, 63.63636363636363, 57.77777777777777]
30: [57.599999999999994, 65.0, 54.761904761904766, 58.71559633027523, 67.56756756756756, 45.67901234567901, 61.36363636363637, 52.77777777777778, 56.84210526315789, 53.333333333333336, 62.96296296296296, 66.66666666666666, 51.19047619047619, 60.24096385542169, 62.5, 61.72839506172839, 56.16438356164384, 62.65060240963856, 59.70149253731343, 67.12328767123287, 69.01408450704226, 69.73684210526315, 57.49999999999999, 68.68686868686868, 68.05555555555556, 66.3157894736842, 78.65168539325843, 75.0, 57.57575757575758, 63.33333333333333, 64.58333333333334]
31: [58.4, 67.5, 57.14285714285714, 68.80733944954129, 71.62162162162163, 49.382716049382715, 52.27272727272727, 63.888888888888886, 60.0, 56.00000000000001, 62.96296296296296, 65.4320987654321, 52.38095238095239, 59.036144578313255, 68.05555555555556, 69.1358024691358, 61.64383561643836, 63.85542168674698, 61.19402985074627, 72.6027397260274, 70.4225352112676, 73.68421052631578, 57.49999999999999, 68.68686868686868, 59.72222222222222, 62.10526315789474, 80.89887640449437, 68.75, 71.21212121212122, 51.11111111111111, 69.79166666666666, 59.30232558139535]
32: [57.599999999999994, 71.25, 58.333333333333336, 56.88073394495413, 70.27027027027027, 43.20987654320987, 50.0, 69.44444444444444, 64.21052631578948, 49.333333333333336, 61.72839506172839, 62.96296296296296, 51.19047619047619, 66.26506024096386, 68.05555555555556, 62.96296296296296, 52.054794520547944, 61.44578313253012, 59.70149253731343, 73.97260273972603, 77.46478873239437, 73.68421052631578, 65.0, 65.65656565656566, 63.888888888888886, 64.21052631578948, 73.03370786516854, 75.0, 66.66666666666666, 61.111111111111114, 60.416666666666664, 54.65116279069767, 58.69565217391305]
33: [52.800000000000004, 61.25000000000001, 64.28571428571429, 64.22018348623854, 71.62162162162163, 46.913580246913575, 57.95454545454546, 61.111111111111114, 55.78947368421052, 58.666666666666664, 66.66666666666666, 66.66666666666666, 52.38095238095239, 62.65060240963856, 65.27777777777779, 66.66666666666666, 54.794520547945204, 57.831325301204814, 62.68656716417911, 72.6027397260274, 74.64788732394366, 67.10526315789474, 62.5, 67.67676767676768, 69.44444444444444, 61.05263157894737, 83.14606741573034, 71.25, 69.6969696969697, 57.77777777777777, 60.416666666666664, 52.32558139534884, 61.95652173913043, 81.25]
34: [61.6, 57.49999999999999, 63.095238095238095, 55.96330275229357, 64.86486486486487, 48.148148148148145, 54.54545454545454, 61.111111111111114, 63.1578947368421, 42.66666666666667, 59.25925925925925, 62.96296296296296, 57.14285714285714, 67.46987951807229, 68.05555555555556, 62.96296296296296, 52.054794520547944, 62.65060240963856, 65.67164179104478, 75.34246575342466, 74.64788732394366, 71.05263157894737, 72.5, 60.60606060606061, 66.66666666666666, 67.36842105263158, 73.03370786516854, 65.0, 59.09090909090909, 55.55555555555556, 63.541666666666664, 58.139534883720934, 67.3913043478261, 73.95833333333334, 67.85714285714286]
35: [56.8, 68.75, 60.71428571428571, 62.38532110091744, 68.91891891891892, 50.617283950617285, 63.63636363636363, 55.55555555555556, 54.736842105263165, 54.666666666666664, 61.72839506172839, 64.19753086419753, 51.19047619047619, 60.24096385542169, 61.111111111111114, 66.66666666666666, 56.16438356164384, 61.44578313253012, 58.2089552238806, 78.08219178082192, 71.83098591549296, 78.94736842105263, 71.25, 62.62626262626263, 70.83333333333334, 67.36842105263158, 70.78651685393258, 65.0, 63.63636363636363, 62.22222222222222, 67.70833333333334, 51.162790697674424, 59.78260869565217, 79.16666666666666, 71.42857142857143, 70.37037037037037]
36: [56.00000000000001, 65.0, 55.952380952380956, 65.13761467889908, 68.91891891891892, 49.382716049382715, 52.27272727272727, 54.166666666666664, 55.78947368421052, 50.66666666666667, 62.96296296296296, 64.19753086419753, 58.333333333333336, 67.46987951807229, 61.111111111111114, 59.25925925925925, 49.31506849315068, 61.44578313253012, 64.17910447761194, 76.71232876712328, 76.05633802816901, 65.78947368421053, 62.5, 61.61616161616161, 65.27777777777779, 60.0, 65.1685393258427, 65.0, 63.63636363636363, 62.22222222222222, 64.58333333333334, 51.162790697674424, 54.347826086956516, 80.20833333333334, 76.19047619047619, 67.90123456790124, 60.29411764705882]
37: [59.199999999999996, 57.49999999999999, 50.0, 52.293577981651374, 67.56756756756756, 41.9753086419753, 50.0, 55.55555555555556, 58.94736842105262, 49.333333333333336, 70.37037037037037, 67.90123456790124, 50.0, 63.85542168674698, 65.27777777777779, 61.72839506172839, 43.83561643835616, 57.831325301204814, 58.2089552238806, 71.23287671232876, 76.05633802816901, 77.63157894736842, 62.5, 60.60606060606061, 59.72222222222222, 60.0, 78.65168539325843, 72.5, 63.63636363636363, 58.88888888888889, 67.70833333333334, 60.46511627906976, 52.17391304347826, 75.0, 73.80952380952381, 69.1358024691358, 83.82352941176471, 56.41025641025641]
38: [56.00000000000001, 58.75, 52.38095238095239, 58.71559633027523, 63.51351351351351, 44.44444444444444, 50.0, 61.111111111111114, 66.3157894736842, 37.333333333333336, 64.19753086419753, 66.66666666666666, 55.952380952380956, 67.46987951807229, 66.66666666666666, 61.72839506172839, 46.57534246575342, 56.62650602409639, 65.67164179104478, 72.6027397260274, 66.19718309859155, 72.36842105263158, 71.25, 64.64646464646465, 62.5, 48.421052631578945, 73.03370786516854, 72.5, 69.6969696969697, 56.666666666666664, 67.70833333333334, 53.48837209302325, 58.69565217391305, 76.04166666666666, 67.85714285714286, 72.8395061728395, 79.41176470588235, 61.53846153846154, 77.01149425287356]
39: [55.2, 52.5, 54.761904761904766, 62.38532110091744, 59.45945945945946, 48.148148148148145, 51.13636363636363, 59.72222222222222, 56.84210526315789, 41.333333333333336, 58.0246913580247, 67.90123456790124, 50.0, 62.65060240963856, 59.72222222222222, 53.086419753086425, 52.054794520547944, 60.24096385542169, 59.70149253731343, 76.71232876712328, 67.6056338028169, 63.1578947368421, 62.5, 64.64646464646465, 68.05555555555556, 51.578947368421055, 78.65168539325843, 62.5, 71.21212121212122, 60.0, 66.66666666666666, 54.65116279069767, 55.434782608695656, 80.20833333333334, 57.14285714285714, 72.8395061728395, 73.52941176470588, 61.53846153846154, 74.71264367816092, 78.48101265822784]
40: [48.8, 50.0, 50.0, 57.798165137614674, 79.72972972972973, 43.20987654320987, 55.68181818181818, 58.333333333333336, 51.578947368421055, 56.00000000000001, 55.55555555555556, 64.19753086419753, 45.23809523809524, 65.06024096385542, 58.333333333333336, 61.72839506172839, 54.794520547945204, 59.036144578313255, 61.19402985074627, 78.08219178082192, 80.28169014084507, 71.05263157894737, 62.5, 65.65656565656566, 59.72222222222222, 62.10526315789474, 77.52808988764045, 62.5, 62.121212121212125, 58.88888888888889, 58.333333333333336, 52.32558139534884, 55.434782608695656, 76.04166666666666, 71.42857142857143, 69.1358024691358, 64.70588235294117, 53.84615384615385, 75.86206896551724, 74.68354430379746, 68.11594202898551]
41: [50.4, 55.00000000000001, 45.23809523809524, 61.46788990825688, 70.27027027027027, 44.44444444444444, 47.72727272727273, 55.55555555555556, 57.89473684210527, 61.33333333333333, 53.086419753086425, 64.19753086419753, 52.38095238095239, 62.65060240963856, 55.55555555555556, 49.382716049382715, 46.57534246575342, 55.42168674698795, 61.19402985074627, 72.6027397260274, 70.4225352112676, 68.42105263157895, 62.5, 63.63636363636363, 66.66666666666666, 51.578947368421055, 59.55056179775281, 65.0, 57.57575757575758, 55.55555555555556, 65.625, 61.627906976744185, 57.608695652173914, 78.125, 70.23809523809523, 72.8395061728395, 66.17647058823529, 69.23076923076923, 74.71264367816092, 67.08860759493672, 63.76811594202898, 58.0246913580247]
42: [51.2, 63.74999999999999, 50.0, 67.88990825688074, 66.21621621621621, 46.913580246913575, 57.95454545454546, 62.5, 58.94736842105262, 49.333333333333336, 60.49382716049383, 61.72839506172839, 52.38095238095239, 63.85542168674698, 61.111111111111114, 60.49382716049383, 53.42465753424658, 59.036144578313255, 65.67164179104478, 68.4931506849315, 66.19718309859155, 71.05263157894737, 60.0, 55.55555555555556, 66.66666666666666, 55.78947368421052, 71.91011235955057, 61.25000000000001, 71.21212121212122, 65.55555555555556, 64.58333333333334, 60.46511627906976, 54.347826086956516, 71.875, 72.61904761904762, 74.07407407407408, 67.64705882352942, 62.82051282051282, 80.45977011494253, 63.29113924050633, 66.66666666666666, 55.55555555555556, 61.79775280898876]
43: [51.2, 60.0, 40.476190476190474, 65.13761467889908, 70.27027027027027, 40.74074074074074, 50.0, 47.22222222222222, 60.0, 37.333333333333336, 64.19753086419753, 58.0246913580247, 50.0, 56.62650602409639, 58.333333333333336, 60.49382716049383, 49.31506849315068, 61.44578313253012, 62.68656716417911, 68.4931506849315, 78.87323943661971, 72.36842105263158, 58.75, 51.515151515151516, 59.72222222222222, 50.526315789473685, 73.03370786516854, 71.25, 63.63636363636363, 58.88888888888889, 56.25, 52.32558139534884, 56.52173913043478, 76.04166666666666, 76.19047619047619, 77.77777777777779, 72.05882352941177, 65.38461538461539, 73.5632183908046, 78.48101265822784, 68.11594202898551, 55.55555555555556, 65.1685393258427, 52.94117647058824]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 52.3256
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 63.8889
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 62.8319
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 56.3380
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 70.5882
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 42.2018
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 51.2821
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 47.4747
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 60.3175
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 39.2157
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 66.6667
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 53.6364
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 54.4643
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 52.6786
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 61.6162
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 57.2727
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 54.0000
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 54.4643
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 63.4409
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 72.2772
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 79.5918
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 70.1923
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 61.1111
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 52.3077
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 60.0000
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 59.5238
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 73.7288
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 60.1852
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 66.3043
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 57.5000
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 61.7188
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 47.4138
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 63.1148
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 79.5276
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 70.1754
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 67.2727
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 59.3750
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 69.8113
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 69.2308
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 63.5514
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 58.3333
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 60.0000
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 63.8655
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 57.8947

====================

f1_score(micro): 60.52471018913972
f1_score(macro): 57.411552985173564
Classification report:
              precision    recall  f1-score   support

           0       1.00      1.00      1.00         4
           1       1.00      0.75      0.86         4
           2       1.00      1.00      1.00         4
           3       1.00      0.75      0.86         4
           4       1.00      0.00      0.00         4
           5       1.00      0.40      0.57         5
           6       1.00      0.80      0.89         5
           7       0.20      0.11      0.14         9
           8       1.00      0.25      0.40         4
           9       1.00      0.25      0.40         4
          10       0.70      0.78      0.74         9
          11       0.50      0.20      0.29         5
          12       0.90      1.00      0.95         9
          13       1.00      0.60      0.75         5
          14       0.75      0.75      0.75         4
          15       1.00      0.25      0.40         4
          16       1.00      0.60      0.75         5
          17       0.60      0.33      0.43         9
          18       1.00      1.00      1.00         4
          19       1.00      1.00      1.00         4
          20       1.00      1.00      1.00         4
          21       1.00      1.00      1.00         5
          22       0.67      0.22      0.33         9
          23       0.67      1.00      0.80         4
          24       1.00      0.00      0.00         4
          25       1.00      0.00      0.00         4
          26       0.50      0.75      0.60         4
          27       1.00      0.50      0.67         4
          28       1.00      1.00      1.00         4
          29       0.00      0.00      0.00         4
          30       1.00      0.50      0.67         4
          31       1.00      0.25      0.40         4
          32       1.00      0.00      0.00         4
          33       0.29      0.22      0.25         9
          34       0.00      0.00      0.00         4
          35       1.00      0.00      0.00         4
          36       0.75      0.75      0.75         4
          37       0.86      0.67      0.75         9
          38       0.57      0.44      0.50         9
          39       0.50      0.50      0.50         4
          40       0.75      0.67      0.71         9
          41       1.00      0.80      0.89         5
          42       0.00      0.00      0.00         4
          43       0.80      0.44      0.57         9
          44       1.00      1.00      1.00         4
          45       1.00      1.00      1.00         4
          46       1.00      0.50      0.67         4
          47       0.80      0.80      0.80         5
          48       0.71      1.00      0.83         5
          49       0.67      0.50      0.57         4
          50       0.64      0.78      0.70         9
          51       0.67      1.00      0.80         4
          52       0.80      1.00      0.89         4
          53       1.00      1.00      1.00         4
          54       0.80      0.89      0.84         9
          55       0.62      0.89      0.73         9
          56       0.33      0.25      0.29         4
          57       1.00      0.75      0.86         4
          58       0.88      0.78      0.82         9
          59       0.00      0.00      0.00         4
          60       0.67      0.50      0.57         4
          61       0.57      1.00      0.73         4
          62       1.00      0.75      0.86         4
          63       0.50      0.60      0.55         5
          64       0.25      0.25      0.25         4
          65       1.00      0.50      0.67         4
          66       0.40      0.40      0.40         5
          67       0.62      1.00      0.77         5
          68       0.71      0.56      0.63         9
          69       1.00      0.75      0.86         4
          70       1.00      0.00      0.00         4
          71       0.83      0.56      0.67         9
          72       0.75      0.67      0.71         9
          73       0.60      0.75      0.67         4
          74       0.57      0.44      0.50         9
          75       0.75      1.00      0.86         9
          76       0.73      0.89      0.80         9
          77       0.40      0.44      0.42         9
          78       0.50      0.75      0.60         4
          79       0.50      0.22      0.31         9
          80       1.00      0.75      0.86         4
          81       0.78      0.78      0.78         9
          82       1.00      0.25      0.40         4
          83       0.60      1.00      0.75         9
          84       0.00      0.00      0.00         9
          85       0.50      0.22      0.31         9
          86       0.25      0.25      0.25         4
          87       0.67      0.80      0.73         5
          88       0.55      0.67      0.60         9
          89       0.55      0.67      0.60         9
          90       1.00      0.60      0.75         5
          91       0.40      0.50      0.44         4
          92       0.33      0.50      0.40         4
          93       0.67      0.44      0.53         9
          94       0.00      0.00      0.00         4
          95       0.83      1.00      0.91         5
          96       0.00      0.00      0.00         4
          97       1.00      1.00      1.00         5
          98       0.60      0.75      0.67         4
          99       1.00      0.75      0.86         4
         100       1.00      1.00      1.00         9
         101       0.07      0.25      0.11         4
         102       0.75      0.75      0.75         4
         103       1.00      1.00      1.00         9
         104       0.67      0.40      0.50         5
         105       1.00      0.60      0.75         5
         106       0.71      1.00      0.83         5
         107       1.00      0.60      0.75         5
         108       1.00      1.00      1.00         4
         109       0.67      0.50      0.57         4
         110       0.67      0.50      0.57         4
         111       0.80      0.89      0.84         9
         112       0.50      0.20      0.29         5
         113       1.00      1.00      1.00         4
         114       1.00      0.75      0.86         4
         115       0.80      1.00      0.89         4
         116       1.00      0.25      0.40         4
         117       0.33      0.25      0.29         4
         118       0.27      0.75      0.40         4
         119       1.00      0.67      0.80         9
         120       0.33      0.20      0.25         5
         121       0.33      0.25      0.29         4
         122       0.67      0.22      0.33         9
         123       0.75      0.75      0.75         4
         124       0.00      0.00      0.00         4
         125       0.75      0.75      0.75         4
         126       0.67      0.80      0.73         5
         127       0.67      0.40      0.50         5
         128       1.00      1.00      1.00         5
         129       1.00      0.00      0.00         9
         130       0.25      0.22      0.24         9
         131       1.00      0.75      0.86         4
         132       0.20      0.22      0.21         9
         133       1.00      0.00      0.00         4
         134       1.00      0.00      0.00         4
         135       0.50      0.75      0.60         4
         136       0.60      0.75      0.67         4
         137       0.80      1.00      0.89         4
         138       0.50      0.20      0.29         5
         139       0.50      0.11      0.18         9
         140       0.55      0.67      0.60         9
         141       1.00      0.00      0.00         4
         142       1.00      0.67      0.80         9
         143       0.67      0.50      0.57         4
         144       0.06      0.11      0.08         9
         145       0.50      0.20      0.29         5
         146       1.00      0.33      0.50         9
         147       0.75      0.75      0.75         4
         148       0.50      0.25      0.33         4
         149       0.78      0.78      0.78         9
         150       1.00      1.00      1.00         4
         151       0.89      0.89      0.89         9
         152       0.38      0.75      0.50         4
         153       0.75      0.75      0.75         4
         154       0.00      0.00      0.00         4
         155       0.80      0.80      0.80         5
         156       0.70      0.78      0.74         9
         157       0.67      0.50      0.57         4
         158       0.67      0.50      0.57         4
         159       0.00      0.00      0.00         4
         160       0.71      0.56      0.63         9
         161       1.00      1.00      1.00         4
         162       0.40      0.40      0.40         5
         163       0.67      0.50      0.57         4
         164       0.00      0.00      0.00         5
         165       0.75      0.75      0.75         4
         166       0.50      0.25      0.33         4
         167       0.50      0.50      0.50         4
         168       1.00      0.75      0.86         4
         169       1.00      1.00      1.00         4
         170       0.17      0.25      0.20         4
         171       0.50      0.11      0.18         9
         172       1.00      0.00      0.00         4
         173       1.00      0.80      0.89         5
         174       1.00      0.33      0.50         9
         175       0.75      0.75      0.75         4
         176       0.00      0.00      0.00         4
         177       0.38      0.56      0.45         9
         178       0.54      0.78      0.64         9
         179       0.67      0.50      0.57         4
         180       0.89      0.89      0.89         9
         181       0.33      0.14      0.20         7
         182       0.50      0.17      0.25         6
         183       1.00      0.40      0.57         5
         184       0.00      0.00      0.00         4
         185       0.00      0.00      0.00         4
         186       0.73      0.89      0.80         9
         187       1.00      0.50      0.67         4
         188       0.90      1.00      0.95         9
         189       1.00      1.00      1.00         9
         190       0.70      0.78      0.74         9
         191       1.00      0.75      0.86         4
         192       0.57      1.00      0.73         4
         193       0.67      0.50      0.57         4
         194       0.54      0.78      0.64         9
         195       0.60      0.60      0.60         5
         196       0.75      0.33      0.46         9
         197       0.70      0.78      0.74         9
         198       1.00      0.00      0.00         4
         199       0.50      0.25      0.33         4
         200       1.00      0.25      0.40         4
         201       0.00      0.00      0.00         4
         202       0.17      0.25      0.20         4
         203       0.00      0.00      0.00         4
         204       0.14      0.25      0.18         4
         205       0.67      0.50      0.57         4
         206       0.80      1.00      0.89         4
         207       0.00      0.00      0.00         4
         208       0.62      0.56      0.59         9
         209       0.00      0.00      0.00         4
         210       0.67      0.50      0.57         4
         211       0.50      0.20      0.29         5
         212       0.50      0.25      0.33         4
         213       0.33      0.25      0.29         4
         214       0.75      0.75      0.75         4
         215       1.00      0.75      0.86         4
         216       0.88      0.78      0.82         9
         217       0.57      1.00      0.73         4
         218       0.00      0.00      0.00         9
         219       1.00      1.00      1.00         5
         220       0.50      0.80      0.62         5
         221       1.00      0.00      0.00         4
         222       0.29      0.50      0.36         4
         223       0.50      0.75      0.60         4
         224       1.00      0.80      0.89         5
         225       0.57      0.89      0.70         9
         226       1.00      1.00      1.00         5
         227       0.71      0.56      0.63         9
         228       0.40      0.40      0.40         5
         229       0.80      1.00      0.89         4
         230       0.86      0.67      0.75         9
         231       0.00      0.00      0.00         4
         232       0.83      1.00      0.91         5
         233       0.80      1.00      0.89         4
         234       1.00      0.00      0.00         4
         235       0.83      0.56      0.67         9
         236       1.00      0.00      0.00         4
         237       0.50      0.20      0.29         5
         238       0.80      1.00      0.89         4
         239       0.80      1.00      0.89         4
         240       1.00      0.75      0.86         4
         241       0.67      0.50      0.57         4
         242       0.50      0.25      0.33         4
         243       0.00      0.00      0.00         9
         244       0.67      0.50      0.57         4
         245       0.75      1.00      0.86         9
         246       0.00      0.00      0.00         4
         247       0.75      0.75      0.75         4
         248       1.00      0.80      0.89         5
         249       1.00      1.00      1.00         4
         250       1.00      0.78      0.88         9
         251       0.80      1.00      0.89         4
         252       1.00      0.67      0.80         9
         253       1.00      0.00      0.00         7
         254       1.00      1.00      1.00         4
         255       0.80      1.00      0.89         4
         256       0.08      0.11      0.10         9
         257       1.00      0.78      0.88         9
         258       1.00      1.00      1.00         4
         259       0.12      0.25      0.17         4
         260       0.83      1.00      0.91         5
         261       0.75      0.60      0.67         5
         262       0.00      0.00      0.00         4
         263       0.80      1.00      0.89         4
         264       0.88      0.88      0.88         8
         265       0.89      0.89      0.89         9
         266       0.00      0.00      0.00         9
         267       0.00      0.00      0.00         9
         268       0.43      0.75      0.55         4
         269       0.00      0.00      0.00         4
         270       1.00      0.00      0.00         4
         271       0.67      1.00      0.80         4
         272       0.67      0.40      0.50         5
         273       0.80      1.00      0.89         4
         274       1.00      1.00      1.00         5
         275       1.00      0.56      0.71         9
         276       0.50      0.20      0.29         5
         277       0.09      0.25      0.13         4
         278       0.78      0.78      0.78         9
         279       0.20      0.25      0.22         4
         280       0.75      0.75      0.75         4
         281       1.00      1.00      1.00         4
         282       0.00      0.00      0.00         4
         283       0.50      0.25      0.33         4
         284       0.75      0.75      0.75         4
         285       0.62      0.56      0.59         9
         286       0.67      0.44      0.53         9
         287       1.00      0.00      0.00         4
         288       0.00      0.00      0.00         4
         289       0.00      0.00      0.00         4
         290       1.00      0.78      0.88         9
         291       0.80      1.00      0.89         4
         292       0.70      0.78      0.74         9
         293       0.33      0.25      0.29         4
         294       0.75      0.60      0.67         5
         295       1.00      0.50      0.67         4
         296       1.00      0.80      0.89         5
         297       1.00      1.00      1.00         4
         298       0.00      0.00      0.00         4
         299       0.62      0.89      0.73         9
         300       1.00      0.60      0.75         5
         301       0.80      0.80      0.80         5
         302       1.00      0.89      0.94         9
         303       0.33      0.50      0.40         4
         304       1.00      0.25      0.40         4
         305       0.75      0.75      0.75         4
         306       1.00      0.00      0.00         4
         307       0.33      0.50      0.40         4
         308       0.80      0.89      0.84         9
         309       0.50      0.25      0.33         4
         310       0.25      0.25      0.25         4
         311       1.00      0.75      0.86         4
         312       0.50      0.25      0.33         4
         313       0.60      0.75      0.67         4
         314       0.57      1.00      0.73         4
         315       0.33      0.25      0.29         4
         316       0.00      0.00      0.00         4
         317       0.00      0.00      0.00         9
         318       0.09      0.25      0.13         4
         319       0.80      0.89      0.84         9
         320       0.75      0.75      0.75         4
         321       0.50      0.50      0.50         4
         322       0.40      0.40      0.40         5
         323       1.00      0.67      0.80         9
         324       0.57      1.00      0.73         4
         325       1.00      0.80      0.89         5
         326       0.33      0.25      0.29         4
         327       0.47      0.89      0.62         9
         328       0.44      0.57      0.50         7
         329       1.00      0.75      0.86         4
         330       1.00      0.00      0.00         4
         331       0.50      0.75      0.60         4
         332       1.00      0.56      0.71         9
         333       0.50      1.00      0.67         4
         334       0.40      0.44      0.42         9
         335       0.50      0.25      0.33         4
         336       1.00      1.00      1.00         4
         337       0.00      0.00      0.00         4
         338       0.14      0.50      0.22         4
         339       0.00      0.00      0.00         4
         340       1.00      0.75      0.86         4
         341       0.50      0.67      0.57         9
         342       0.40      0.40      0.40         5
         343       1.00      0.75      0.86         4
         344       0.75      0.60      0.67         5
         345       0.80      1.00      0.89         4
         346       1.00      0.40      0.57         5
         347       0.45      1.00      0.62         5
         348       0.62      1.00      0.77         5
         349       1.00      0.00      0.00         9
         350       0.60      0.75      0.67         4
         351       0.15      0.50      0.24         4
         352       1.00      0.75      0.86         4
         353       0.50      0.50      0.50         4
         354       0.60      0.75      0.67         4
         355       0.67      1.00      0.80         4
         356       0.60      0.75      0.67         4
         357       0.88      0.88      0.88         8
         358       0.56      0.56      0.56         9
         359       0.06      0.25      0.09         4
         360       1.00      0.75      0.86         4
         361       1.00      1.00      1.00         5
         362       1.00      0.56      0.71         9
         363       1.00      1.00      1.00         4
         364       0.86      0.67      0.75         9
         365       0.00      0.00      0.00         4
         366       1.00      0.25      0.40         4
         367       0.50      0.67      0.57         9
         368       0.00      0.00      0.00         5
         369       1.00      0.00      0.00         4
         370       1.00      0.00      0.00         4
         371       0.38      0.33      0.35         9
         372       0.20      0.40      0.27         5
         373       0.50      0.75      0.60         4
         374       1.00      0.75      0.86         4
         375       0.00      0.00      0.00         4
         376       0.40      0.50      0.44         4
         377       1.00      0.25      0.40         4
         378       0.80      1.00      0.89         4
         379       0.60      0.60      0.60         5
         380       1.00      0.80      0.89         5
         381       0.71      1.00      0.83         5
         382       1.00      1.00      1.00         4
         383       0.40      0.50      0.44         4
         384       1.00      0.25      0.40         4
         385       0.70      0.78      0.74         9
         386       1.00      1.00      1.00         4
         387       0.90      1.00      0.95         9
         388       0.00      0.00      0.00         4
         389       0.00      0.00      0.00         4
         390       0.75      0.75      0.75         4
         391       0.60      0.75      0.67         4
         392       1.00      0.75      0.86         4
         393       1.00      0.25      0.40         4
         394       0.62      1.00      0.77         5
         395       0.60      0.75      0.67         4
         396       0.75      0.75      0.75         4
         397       1.00      1.00      1.00         4
         398       1.00      0.75      0.86         4
         399       1.00      0.50      0.67         4
         400       0.80      1.00      0.89         4
         401       1.00      0.00      0.00         4
         402       1.00      0.50      0.67         4
         403       0.82      1.00      0.90         9
         404       0.75      1.00      0.86         9
         405       0.29      0.50      0.36         4
         406       0.78      0.78      0.78         9
         407       0.50      0.25      0.33         4
         408       0.90      1.00      0.95         9
         409       1.00      1.00      1.00         4
         410       0.75      0.75      0.75         4
         411       0.00      0.00      0.00         4
         412       0.33      0.25      0.29         4
         413       1.00      0.50      0.67         4
         414       0.00      0.00      0.00         4
         415       0.75      0.75      0.75         4
         416       0.50      0.78      0.61         9
         417       1.00      1.00      1.00         4
         418       1.00      1.00      1.00         4
         419       0.75      0.75      0.75         4
         420       0.20      0.25      0.22         4
         421       0.75      0.75      0.75         4
         422       0.80      0.80      0.80         5
         423       0.44      1.00      0.62         4
         424       0.71      1.00      0.83         5
         425       1.00      1.00      1.00         4
         426       0.75      0.75      0.75         4
         427       0.83      1.00      0.91         5
         428       0.82      1.00      0.90         9
         429       0.20      0.25      0.22         4
         430       1.00      1.00      1.00         4
         431       0.88      0.78      0.82         9
         432       1.00      1.00      1.00         4
         433       0.75      0.75      0.75         4
         434       0.71      1.00      0.83         5
         435       0.36      0.80      0.50         5
         436       0.40      0.50      0.44         4
         437       1.00      0.89      0.94         9
         438       0.42      1.00      0.59         5
         439       1.00      0.00      0.00         4
         440       0.89      0.89      0.89         9
         441       1.00      1.00      1.00         4
         442       0.43      0.67      0.52         9
         443       0.67      1.00      0.80         4
         444       0.00      0.00      0.00         4
         445       0.45      0.56      0.50         9
         446       0.57      1.00      0.73         4
         447       0.50      0.50      0.50         4
         448       1.00      0.75      0.86         4
         449       0.80      0.80      0.80         5
         450       0.40      0.50      0.44         4
         451       0.50      0.75      0.60         4
         452       0.00      0.00      0.00         4
         453       0.67      1.00      0.80         4
         454       0.67      0.50      0.57         4
         455       0.57      0.80      0.67         5
         456       0.88      0.78      0.82         9
         457       0.75      0.75      0.75         4
         458       0.29      0.40      0.33         5
         459       1.00      0.25      0.40         4
         460       0.50      0.50      0.50         4
         461       0.46      0.67      0.55         9
         462       1.00      0.75      0.86         4
         463       0.60      0.60      0.60         5
         464       1.00      0.75      0.86         4
         465       0.22      0.22      0.22         9
         466       0.45      0.56      0.50         9
         467       0.40      1.00      0.57         4
         468       0.80      1.00      0.89         4
         469       1.00      1.00      1.00         4
         470       0.56      0.56      0.56         9
         471       0.75      0.75      0.75         4
         472       1.00      0.00      0.00         4
         473       0.60      0.75      0.67         4
         474       0.00      0.00      0.00         4
         475       0.50      0.80      0.62         5
         476       0.00      0.00      0.00         4
         477       1.00      0.89      0.94         9
         478       0.38      0.33      0.35         9
         479       0.89      0.89      0.89         9
         480       0.70      0.78      0.74         9
         481       1.00      0.80      0.89         5
         482       0.83      0.56      0.67         9
         483       1.00      0.11      0.20         9
         484       1.00      0.00      0.00         4
         485       0.67      0.67      0.67         9
         486       1.00      1.00      1.00         4
         487       0.67      0.22      0.33         9
         488       1.00      1.00      1.00         5
         489       0.33      0.40      0.36         5
         490       0.83      1.00      0.91         5
         491       0.50      0.25      0.33         4
         492       0.00      0.00      0.00         9
         493       0.43      0.75      0.55         4
         494       0.88      0.78      0.82         9
         495       0.80      1.00      0.89         4
         496       1.00      0.00      0.00         4
         497       0.00      0.00      0.00         4
         498       0.67      0.80      0.73         5
         499       1.00      0.25      0.40         4
         500       0.60      0.75      0.67         4
         501       1.00      1.00      1.00         9
         502       1.00      0.00      0.00         4
         503       0.50      0.50      0.50         4
         504       0.50      0.75      0.60         4
         505       0.50      1.00      0.67         5
         506       1.00      0.20      0.33         5
         507       0.60      0.75      0.67         4
         508       0.75      0.75      0.75         4
         509       0.58      0.78      0.67         9
         510       0.83      1.00      0.91         5
         511       1.00      0.00      0.00         4
         512       1.00      0.00      0.00         4
         513       1.00      0.60      0.75         5
         514       1.00      0.00      0.00         4
         515       0.38      0.67      0.48         9
         516       0.57      0.89      0.70         9
         517       1.00      0.00      0.00         4
         518       0.67      0.50      0.57         4
         519       1.00      1.00      1.00         4
         520       0.90      1.00      0.95         9
         521       0.33      0.25      0.29         4
         522       0.80      0.89      0.84         9
         523       0.67      0.80      0.73         5
         524       0.58      0.78      0.67         9
         525       0.88      0.78      0.82         9
         526       0.00      0.00      0.00         4
         527       0.00      0.00      0.00         4
         528       0.12      0.25      0.17         4
         529       1.00      0.89      0.94         9
         530       0.33      0.11      0.17         9
         531       0.00      0.00      0.00         4
         532       0.60      0.75      0.67         4
         533       1.00      0.67      0.80         9
         534       1.00      1.00      1.00         4
         535       0.89      0.89      0.89         9
         536       1.00      0.00      0.00         4
         537       0.00      0.00      0.00         4
         538       0.86      0.67      0.75         9
         539       0.64      1.00      0.78         9
         540       0.33      0.80      0.47         5
         541       0.78      0.78      0.78         9
         542       0.64      1.00      0.78         9
         543       1.00      0.50      0.67         4
         544       0.75      0.33      0.46         9
         545       1.00      0.75      0.86         4
         546       0.67      0.50      0.57         4
         547       1.00      0.40      0.57         5
         548       0.80      1.00      0.89         4
         549       0.57      1.00      0.73         4
         550       0.67      1.00      0.80         4
         551       0.70      0.78      0.74         9
         552       1.00      1.00      1.00         4
         553       0.62      1.00      0.77         5
         554       0.75      0.75      0.75         4
         555       0.40      0.44      0.42         9
         556       0.56      0.56      0.56         9
         557       0.56      1.00      0.71         5
         558       1.00      1.00      1.00         4
         559       1.00      0.20      0.33         5
         560       0.50      0.25      0.33         4
         561       1.00      0.25      0.40         4
         562       0.73      0.89      0.80         9
         563       0.00      0.00      0.00         4
         564       0.75      0.75      0.75         4
         565       1.00      0.75      0.86         4
         566       0.40      0.80      0.53         5
         567       0.00      0.00      0.00         4
         568       0.25      0.25      0.25         4
         569       1.00      0.50      0.67         4
         570       1.00      1.00      1.00         9
         571       0.67      1.00      0.80         4
         572       0.44      0.44      0.44         9
         573       1.00      0.75      0.86         4
         574       1.00      0.50      0.67         4
         575       0.38      0.75      0.50         4
         576       0.44      1.00      0.62         4
         577       1.00      0.50      0.67         4
         578       0.67      1.00      0.80         4
         579       1.00      0.60      0.75         5
         580       0.50      1.00      0.67         4
         581       0.38      0.75      0.50         4
         582       1.00      0.75      0.86         4
         583       1.00      1.00      1.00         4
         584       0.60      0.75      0.67         4
         585       0.46      0.67      0.55         9
         586       0.60      0.75      0.67         4
         587       0.43      0.75      0.55         4
         588       1.00      0.50      0.67         4
         589       0.00      0.00      0.00         4
         590       0.83      1.00      0.91         5
         591       0.57      1.00      0.73         4
         592       0.00      0.00      0.00         4
         593       0.43      0.33      0.38         9
         594       0.17      0.20      0.18         5
         595       0.33      0.11      0.17         9
         596       0.25      0.25      0.25         4
         597       1.00      1.00      1.00         4
         598       0.67      0.50      0.57         4
         599       0.62      1.00      0.77         5
         600       0.00      0.00      0.00         9
         601       0.57      0.80      0.67         5
         602       1.00      0.00      0.00         4
         603       0.62      0.89      0.73         9
         604       1.00      0.80      0.89         5
         605       0.88      0.78      0.82         9
         606       0.43      0.33      0.38         9
         607       0.90      1.00      0.95         9
         608       0.60      0.75      0.67         4
         609       1.00      0.00      0.00         4
         610       1.00      0.78      0.88         9
         611       1.00      1.00      1.00         4
         612       1.00      1.00      1.00         4
         613       1.00      0.40      0.57         5
         614       0.75      0.75      0.75         4
         615       0.80      0.80      0.80         5
         616       0.75      0.75      0.75         4
         617       0.71      1.00      0.83         5
         618       1.00      0.40      0.57         5
         619       1.00      0.25      0.40         4
         620       0.00      0.00      0.00         4
         621       0.00      0.00      0.00         4
         622       0.25      0.20      0.22         5
         623       0.67      0.89      0.76         9
         624       0.60      0.75      0.67         4
         625       0.71      0.56      0.63         9
         626       1.00      0.78      0.88         9
         627       0.86      0.75      0.80         8
         628       0.89      0.89      0.89         9
         629       1.00      1.00      1.00         4
         630       0.47      0.78      0.58         9
         631       1.00      0.56      0.71         9
         632       0.17      0.11      0.13         9
         633       0.55      0.67      0.60         9
         634       1.00      0.50      0.67         4
         635       0.12      0.22      0.16         9
         636       0.50      0.22      0.31         9
         637       0.36      0.56      0.43         9
         638       1.00      0.25      0.40         4
         639       0.46      0.67      0.55         9
         640       0.00      0.00      0.00         4
         641       1.00      0.89      0.94         9
         642       0.00      0.00      0.00         4
         643       0.80      1.00      0.89         4
         644       0.00      0.00      0.00         9
         645       1.00      0.80      0.89         5
         646       0.60      0.33      0.43         9
         647       1.00      1.00      1.00         4
         648       0.00      0.00      0.00         4
         649       0.67      1.00      0.80         4
         650       0.75      0.75      0.75         4
         651       0.33      0.50      0.40         4
         652       1.00      0.25      0.40         4
         653       1.00      1.00      1.00         4
         654       0.80      1.00      0.89         4
         655       0.62      1.00      0.77         5
         656       0.62      0.89      0.73         9
         657       0.50      1.00      0.67         4
         658       0.00      0.00      0.00         4
         659       0.00      0.00      0.00         4
         660       1.00      0.89      0.94         9
         661       0.11      0.22      0.15         9
         662       0.90      1.00      0.95         9
         663       0.88      0.78      0.82         9
         664       0.38      0.56      0.45         9
         665       1.00      0.75      0.86         4
         666       0.33      0.40      0.36         5
         667       0.50      0.44      0.47         9
         668       1.00      0.50      0.67         4
         669       1.00      0.50      0.67         4
         670       0.67      0.50      0.57         4
         671       0.67      0.67      0.67         9
         672       0.80      1.00      0.89         4
         673       0.00      0.00      0.00         4
         674       0.27      0.75      0.40         4
         675       0.75      0.75      0.75         4
         676       0.88      0.78      0.82         9
         677       0.50      0.50      0.50         4
         678       1.00      0.89      0.94         9
         679       0.50      0.50      0.50         4
         680       0.67      0.80      0.73         5
         681       0.60      0.67      0.63         9
         682       0.90      1.00      0.95         9
         683       0.11      0.25      0.15         4
         684       0.50      0.33      0.40         9
         685       0.67      1.00      0.80         4
         686       1.00      1.00      1.00         5
         687       0.89      0.89      0.89         9
         688       0.82      1.00      0.90         9
         689       0.56      1.00      0.72         9
         690       1.00      1.00      1.00         4
         691       0.90      1.00      0.95         9
         692       0.50      0.75      0.60         4
         693       0.22      0.50      0.31         4
         694       0.00      0.00      0.00         4
         695       0.09      0.25      0.13         4
         696       1.00      0.50      0.67         4
         697       1.00      1.00      1.00         4
         698       0.90      1.00      0.95         9
         699       0.50      0.50      0.50         4
         700       0.11      0.50      0.17         4
         701       0.53      0.89      0.67         9
         702       0.83      1.00      0.91         5
         703       0.88      0.78      0.82         9
         704       1.00      1.00      1.00         5
         705       1.00      0.50      0.67         4
         706       1.00      0.78      0.88         9
         707       0.58      0.78      0.67         9
         708       0.00      0.00      0.00         4
         709       0.38      1.00      0.55         9
         710       0.62      1.00      0.77         5
         711       0.00      0.00      0.00         5
         712       0.09      0.25      0.13         4
         713       0.67      1.00      0.80         4
         714       1.00      0.50      0.67         4
         715       0.50      0.20      0.29         5
         716       0.75      0.75      0.75         4
         717       0.44      0.80      0.57         5
         718       1.00      0.56      0.71         9
         719       0.64      0.78      0.70         9
         720       1.00      0.89      0.94         9
         721       0.50      0.50      0.50         4
         722       0.57      0.89      0.70         9
         723       1.00      0.00      0.00         4
         724       1.00      0.75      0.86         4
         725       1.00      0.40      0.57         5
         726       0.50      0.50      0.50         4
         727       1.00      0.75      0.86         4
         728       0.71      1.00      0.83         5
         729       1.00      1.00      1.00         4
         730       0.33      0.50      0.40         4
         731       0.80      0.89      0.84         9
         732       0.75      0.60      0.67         5
         733       1.00      0.50      0.67         4
         734       0.08      0.11      0.09         9
         735       0.50      0.50      0.50         4
         736       0.67      1.00      0.80         4
         737       0.11      0.22      0.15         9
         738       0.50      0.20      0.29         5
         739       0.71      1.00      0.83         5
         740       0.80      1.00      0.89         4
         741       0.50      0.60      0.55         5
         742       0.25      0.50      0.33         4
         743       1.00      1.00      1.00         4
         744       1.00      0.20      0.33         5
         745       0.67      1.00      0.80         4
         746       1.00      0.40      0.57         5
         747       0.50      0.25      0.33         4
         748       1.00      1.00      1.00         5
         749       1.00      1.00      1.00         4
         750       0.80      1.00      0.89         4
         751       1.00      0.75      0.86         4
         752       0.50      0.25      0.33         4
         753       1.00      1.00      1.00         4
         754       0.00      0.00      0.00         4
         755       0.50      0.75      0.60         4
         756       1.00      1.00      1.00         4
         757       0.33      0.50      0.40         4
         758       0.67      0.80      0.73         5
         759       0.75      0.75      0.75         4
         760       0.33      0.25      0.29         4
         761       0.70      0.78      0.74         9
         762       0.75      0.60      0.67         5
         763       0.00      0.00      0.00         4
         764       0.57      1.00      0.73         4
         765       0.60      0.60      0.60         5
         766       0.50      0.25      0.33         4
         767       0.50      0.60      0.55         5
         768       0.43      0.75      0.55         4
         769       1.00      1.00      1.00         5
         770       0.60      1.00      0.75         9
         771       0.50      0.78      0.61         9
         772       0.80      0.80      0.80         5
         773       0.62      0.89      0.73         9
         774       0.75      0.75      0.75         4
         775       0.60      0.75      0.67         4
         776       0.70      0.78      0.74         9
         777       1.00      1.00      1.00         4
         778       0.82      1.00      0.90         9
         779       0.60      0.33      0.43         9
         780       0.71      0.56      0.63         9
         781       0.00      0.00      0.00         4
         782       0.82      1.00      0.90         9
         783       0.60      0.67      0.63         9
         784       1.00      1.00      1.00         4
         785       0.00      0.00      0.00         4
         786       1.00      1.00      1.00         5
         787       0.80      1.00      0.89         4
         788       0.00      0.00      0.00         4
         789       1.00      1.00      1.00         4
         790       0.80      0.80      0.80         5
         791       1.00      0.75      0.86         4
         792       0.80      1.00      0.89         4
         793       0.67      0.44      0.53         9
         794       0.80      1.00      0.89         4
         795       0.67      0.50      0.57         4
         796       0.75      0.75      0.75         4
         797       1.00      1.00      1.00         5
         798       0.83      0.56      0.67         9
         799       0.56      1.00      0.72         9
         800       0.58      0.78      0.67         9
         801       1.00      0.75      0.86         4
         802       0.25      0.33      0.29         9
         803       0.38      0.75      0.50         4
         804       0.60      0.75      0.67         4
         805       0.60      0.75      0.67         4
         806       1.00      1.00      1.00         4
         807       1.00      0.00      0.00         4
         808       0.60      0.60      0.60         5
         809       0.75      0.75      0.75         4
         810       0.67      0.67      0.67         9
         811       0.00      0.00      0.00         4
         812       0.50      0.50      0.50         4
         813       0.00      0.00      0.00         4
         814       1.00      0.00      0.00         4
         815       0.83      1.00      0.91         5
         816       0.60      0.75      0.67         4
         817       0.00      0.00      0.00         4
         818       1.00      0.89      0.94         9
         819       1.00      0.50      0.67         4
         820       0.08      0.25      0.12         4
         821       1.00      0.00      0.00         4
         822       0.60      0.75      0.67         4
         823       0.50      1.00      0.67         4
         824       0.50      0.25      0.33         4
         825       0.00      0.00      0.00         4
         826       0.67      0.50      0.57         4
         827       0.67      1.00      0.80         4
         828       0.50      0.44      0.47         9
         829       0.57      1.00      0.73         4
         830       1.00      1.00      1.00         4
         831       0.69      1.00      0.82         9
         832       0.11      0.25      0.15         4
         833       0.50      0.25      0.33         4
         834       1.00      1.00      1.00         4
         835       0.83      1.00      0.91         5
         836       1.00      1.00      1.00         5
         837       0.00      0.00      0.00         4
         838       1.00      1.00      1.00         5
         839       1.00      0.50      0.67         4
         840       0.33      0.50      0.40         4
         841       0.40      0.50      0.44         4
         842       1.00      0.00      0.00         4
         843       1.00      0.25      0.40         4
         844       0.50      0.80      0.62         5
         845       0.67      0.89      0.76         9
         846       0.57      0.89      0.70         9
         847       0.80      0.44      0.57         9
         848       0.50      0.40      0.44         5
         849       0.33      0.11      0.17         9
         850       0.86      0.67      0.75         9
         851       0.50      0.75      0.60         4
         852       1.00      1.00      1.00         4
         853       1.00      0.00      0.00         4
         854       1.00      0.00      0.00         5
         855       0.89      0.89      0.89         9
         856       0.67      1.00      0.80         4
         857       1.00      0.75      0.86         4
         858       0.78      0.78      0.78         9
         859       0.53      0.89      0.67         9
         860       1.00      1.00      1.00         5
         861       0.50      0.25      0.33         4
         862       0.50      0.11      0.18         9
         863       1.00      0.50      0.67         4
         864       0.57      1.00      0.73         4
         865       0.62      1.00      0.77         5
         866       0.00      0.00      0.00         4
         867       1.00      0.00      0.00         4
         868       0.00      0.00      0.00         4
         869       0.58      0.78      0.67         9
         870       0.50      0.50      0.50         4
         871       0.60      1.00      0.75         9
         872       0.71      1.00      0.83         5
         873       0.83      0.56      0.67         9
         874       0.20      0.50      0.29         4
         875       0.88      0.78      0.82         9
         876       0.67      0.50      0.57         4
         877       1.00      0.00      0.00         4
         878       1.00      0.80      0.89         5
         879       0.11      0.25      0.15         4
         880       0.50      0.60      0.55         5
         881       0.60      0.60      0.60         5
         882       0.05      0.11      0.07         9
         883       1.00      0.80      0.89         5
         884       0.71      1.00      0.83         5
         885       0.80      1.00      0.89         4
         886       1.00      0.50      0.67         4
         887       0.67      0.50      0.57         4
         888       0.60      0.75      0.67         4
         889       1.00      0.50      0.67         4
         890       1.00      0.75      0.86         4
         891       1.00      0.75      0.86         4
         892       1.00      0.25      0.40         4
         893       1.00      0.75      0.86         4

    accuracy                           0.61      4917
   macro avg       0.66      0.59      0.57      4917
weighted avg       0.66      0.61      0.59      4917

