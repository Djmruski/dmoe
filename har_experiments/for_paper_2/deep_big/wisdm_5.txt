CLASS_ORDER: [250, 815, 651, 279, 739, 288, 409, 764, 453, 592, 526, 748, 553, 754, 878, 558, 434, 9, 331, 522, 875, 245, 126, 316, 328, 479, 120, 658, 450, 666, 373, 182, 360, 803, 57, 813, 278, 290, 552, 282, 159, 785, 319, 720, 729, 664, 64, 39, 690, 281, 270, 486, 614, 355, 671, 428, 131, 860, 130, 576, 616, 304, 749, 62, 829, 312, 470, 496, 487, 367, 175, 786, 40, 723, 146, 185, 676, 24, 323, 285, 674, 385, 124, 726, 839, 564, 398, 135, 243, 276, 82, 329, 225, 454, 569, 232, 743, 240, 340, 605, 28, 524, 611, 498, 253, 286, 627, 160, 358, 482, 580, 5, 792, 845, 187, 587, 132, 163, 129, 603, 820, 2, 193, 808, 112, 333, 865, 326, 63, 884, 65, 507, 773, 661, 555, 698, 681, 807, 218, 481, 557, 511, 654, 228, 656, 503, 652, 531, 198, 771, 451, 877, 673, 361, 389, 400, 489, 271, 834, 847, 601, 4, 186, 494, 38, 525, 212, 543, 293, 782, 549, 419, 751, 275, 462, 554, 234, 606, 264, 227, 21, 490, 527, 386, 585, 411, 591, 168, 260, 724, 517, 883, 566, 480, 855, 483, 13, 374, 541, 809, 633, 796, 853, 842, 342, 194, 405, 657, 352, 180, 189, 155, 402, 816, 432, 458, 513, 104, 705, 794, 497, 80, 215, 795, 353, 707, 426, 475, 72, 613, 139, 74, 76, 20, 254, 56, 866, 231, 368, 315, 663, 469, 867, 274, 439, 306, 818, 722, 455, 437, 852, 145, 599, 125, 192, 646, 547, 687, 237, 719, 747, 79, 488, 118, 628, 504, 200, 701, 712, 258, 802, 516, 107, 655, 88, 208, 161, 233, 178, 429, 550, 121, 510, 805, 778, 408, 144, 449, 645, 619, 404, 390, 191, 467, 287, 371, 468, 575, 103, 378, 529, 704, 873, 188, 298, 874, 52, 33, 149, 582, 598, 495, 625, 501, 30, 745, 546, 36, 10, 94, 366, 887, 880, 68, 753, 246, 77, 321, 324, 833, 359, 672, 772, 418, 506, 578, 133, 430, 60, 297, 675, 343, 272, 387, 691, 446, 96, 466, 577, 50, 721, 763, 738, 623, 71, 216, 255, 711, 892, 715, 369, 637, 345, 733, 32, 417, 685, 357, 102, 66, 890, 750, 741, 43, 759, 643, 423, 162, 196, 679, 184, 886, 571, 7, 134, 856, 267, 110, 241, 214, 565, 586, 869, 251, 584, 83, 334, 472, 317, 841, 836, 817, 29, 478, 889, 857, 714, 849, 335, 596, 607, 484, 868, 86, 570, 433, 765, 604, 590, 823, 223, 471, 797, 559, 770, 354, 249, 303, 445, 509, 35, 95, 242, 58, 147, 493, 790, 22, 638, 14, 396, 780, 678, 425, 105, 84, 97, 181, 609, 269, 752, 485, 686, 872, 266, 55, 284, 535, 302, 239, 305, 465, 1, 257, 581, 518, 48, 226, 34, 73, 90, 11, 308, 822, 224, 123, 665, 202, 380, 199, 756, 327, 85, 338, 600, 783, 708, 881, 420, 399, 166, 91, 562, 295, 330, 629, 539, 363, 344, 706, 799, 263, 567, 863, 37, 636, 624, 365, 742, 744, 229, 593, 608, 709, 336, 725, 573, 372, 219, 696, 730, 370, 140, 294, 172, 800, 746, 662, 349, 171, 351, 832, 563, 594, 728, 341, 766, 337, 206, 167, 876, 375, 283, 521, 220, 89, 768, 499, 737, 459, 821, 388, 781, 119, 473, 538, 460, 767, 870, 310, 209, 436, 474, 183, 650, 415, 127, 668, 46, 75, 292, 667, 406, 893, 548, 579, 825, 203, 169, 639, 859, 774, 410, 851, 848, 515, 244, 383, 195, 100, 769, 414, 861, 828, 534, 364, 156, 205, 158, 136, 574, 610, 814, 622, 835, 528, 464, 699, 838, 422, 556, 170, 806, 362, 688, 421, 837, 561, 502, 831, 268, 572, 313, 826, 67, 141, 647, 618, 775, 779, 891, 514, 407, 27, 634, 716, 597, 117, 801, 236, 682, 536, 438, 457, 463, 221, 153, 392, 395, 379, 811, 81, 435, 717, 882, 23, 51, 677, 843, 659, 311, 356, 615, 261, 791, 862, 101, 632, 846, 92, 617, 309, 644, 589, 653, 776, 736, 116, 25, 115, 45, 213, 143, 138, 544, 588, 844, 3, 44, 440, 176, 551, 59, 54, 346, 322, 520, 26, 137, 301, 583, 320, 441, 692, 177, 165, 142, 858, 735, 47, 760, 702, 718, 734, 456, 492, 461, 108, 114, 394, 680, 265, 755, 670, 207, 476, 542, 740, 152, 560, 793, 710, 641, 697, 42, 500, 148, 789, 397, 19, 314, 174, 382, 109, 777, 31, 384, 602, 69, 864, 350, 93, 318, 757, 296, 235, 190, 262, 523, 41, 626, 642, 545, 204, 660, 17, 173, 277, 299, 16, 693, 695, 595, 871, 403, 798, 669, 732, 731, 447, 151, 98, 888, 18, 427, 630, 804, 758, 683, 533, 0, 452, 222, 713, 530, 70, 508, 442, 512, 307, 689, 444, 78, 197, 113, 810, 122, 99, 885, 621, 273, 631, 211, 111, 700, 879, 8, 12, 762, 332, 640, 850, 87, 157, 6, 259, 727, 413, 477, 519, 620, 537, 201, 377, 210, 784, 164, 830, 128, 381, 540, 703, 448, 252, 49, 424, 339, 106, 15, 61, 393, 401, 238, 248, 280, 391, 154, 827, 648, 347, 291, 787, 289, 230, 217, 532, 684, 150, 431, 505, 247, 568, 812, 325, 854, 179, 612, 443, 824, 761, 348, 416, 819, 649, 53, 300, 788, 376, 412, 694, 635, 491, 256, 840]
class_group: [(250, 815, 651, 279, 739, 288, 409, 764, 453, 592, 526, 748, 553, 754, 878, 558, 434, 9, 331, 522, 875, 245, 126, 316, 328, 479, 120, 658, 450, 666, 373, 182, 360, 803), (57, 813, 278, 290, 552, 282, 159, 785, 319, 720, 729, 664, 64, 39, 690, 281, 270, 486, 614, 355), (671, 428, 131, 860, 130, 576, 616, 304, 749, 62, 829, 312, 470, 496, 487, 367, 175, 786, 40, 723), (146, 185, 676, 24, 323, 285, 674, 385, 124, 726, 839, 564, 398, 135, 243, 276, 82, 329, 225, 454), (569, 232, 743, 240, 340, 605, 28, 524, 611, 498, 253, 286, 627, 160, 358, 482, 580, 5, 792, 845), (187, 587, 132, 163, 129, 603, 820, 2, 193, 808, 112, 333, 865, 326, 63, 884, 65, 507, 773, 661), (555, 698, 681, 807, 218, 481, 557, 511, 654, 228, 656, 503, 652, 531, 198, 771, 451, 877, 673, 361), (389, 400, 489, 271, 834, 847, 601, 4, 186, 494, 38, 525, 212, 543, 293, 782, 549, 419, 751, 275), (462, 554, 234, 606, 264, 227, 21, 490, 527, 386, 585, 411, 591, 168, 260, 724, 517, 883, 566, 480), (855, 483, 13, 374, 541, 809, 633, 796, 853, 842, 342, 194, 405, 657, 352, 180, 189, 155, 402, 816), (432, 458, 513, 104, 705, 794, 497, 80, 215, 795, 353, 707, 426, 475, 72, 613, 139, 74, 76, 20), (254, 56, 866, 231, 368, 315, 663, 469, 867, 274, 439, 306, 818, 722, 455, 437, 852, 145, 599, 125), (192, 646, 547, 687, 237, 719, 747, 79, 488, 118, 628, 504, 200, 701, 712, 258, 802, 516, 107, 655), (88, 208, 161, 233, 178, 429, 550, 121, 510, 805, 778, 408, 144, 449, 645, 619, 404, 390, 191, 467), (287, 371, 468, 575, 103, 378, 529, 704, 873, 188, 298, 874, 52, 33, 149, 582, 598, 495, 625, 501), (30, 745, 546, 36, 10, 94, 366, 887, 880, 68, 753, 246, 77, 321, 324, 833, 359, 672, 772, 418), (506, 578, 133, 430, 60, 297, 675, 343, 272, 387, 691, 446, 96, 466, 577, 50, 721, 763, 738, 623), (71, 216, 255, 711, 892, 715, 369, 637, 345, 733, 32, 417, 685, 357, 102, 66, 890, 750, 741, 43), (759, 643, 423, 162, 196, 679, 184, 886, 571, 7, 134, 856, 267, 110, 241, 214, 565, 586, 869, 251), (584, 83, 334, 472, 317, 841, 836, 817, 29, 478, 889, 857, 714, 849, 335, 596, 607, 484, 868, 86), (570, 433, 765, 604, 590, 823, 223, 471, 797, 559, 770, 354, 249, 303, 445, 509, 35, 95, 242, 58), (147, 493, 790, 22, 638, 14, 396, 780, 678, 425, 105, 84, 97, 181, 609, 269, 752, 485, 686, 872), (266, 55, 284, 535, 302, 239, 305, 465, 1, 257, 581, 518, 48, 226, 34, 73, 90, 11, 308, 822), (224, 123, 665, 202, 380, 199, 756, 327, 85, 338, 600, 783, 708, 881, 420, 399, 166, 91, 562, 295), (330, 629, 539, 363, 344, 706, 799, 263, 567, 863, 37, 636, 624, 365, 742, 744, 229, 593, 608, 709), (336, 725, 573, 372, 219, 696, 730, 370, 140, 294, 172, 800, 746, 662, 349, 171, 351, 832, 563, 594), (728, 341, 766, 337, 206, 167, 876, 375, 283, 521, 220, 89, 768, 499, 737, 459, 821, 388, 781, 119), (473, 538, 460, 767, 870, 310, 209, 436, 474, 183, 650, 415, 127, 668, 46, 75, 292, 667, 406, 893), (548, 579, 825, 203, 169, 639, 859, 774, 410, 851, 848, 515, 244, 383, 195, 100, 769, 414, 861, 828), (534, 364, 156, 205, 158, 136, 574, 610, 814, 622, 835, 528, 464, 699, 838, 422, 556, 170, 806, 362), (688, 421, 837, 561, 502, 831, 268, 572, 313, 826, 67, 141, 647, 618, 775, 779, 891, 514, 407, 27), (634, 716, 597, 117, 801, 236, 682, 536, 438, 457, 463, 221, 153, 392, 395, 379, 811, 81, 435, 717), (882, 23, 51, 677, 843, 659, 311, 356, 615, 261, 791, 862, 101, 632, 846, 92, 617, 309, 644, 589), (653, 776, 736, 116, 25, 115, 45, 213, 143, 138, 544, 588, 844, 3, 44, 440, 176, 551, 59, 54), (346, 322, 520, 26, 137, 301, 583, 320, 441, 692, 177, 165, 142, 858, 735, 47, 760, 702, 718, 734), (456, 492, 461, 108, 114, 394, 680, 265, 755, 670, 207, 476, 542, 740, 152, 560, 793, 710, 641, 697), (42, 500, 148, 789, 397, 19, 314, 174, 382, 109, 777, 31, 384, 602, 69, 864, 350, 93, 318, 757), (296, 235, 190, 262, 523, 41, 626, 642, 545, 204, 660, 17, 173, 277, 299, 16, 693, 695, 595, 871), (403, 798, 669, 732, 731, 447, 151, 98, 888, 18, 427, 630, 804, 758, 683, 533, 0, 452, 222, 713), (530, 70, 508, 442, 512, 307, 689, 444, 78, 197, 113, 810, 122, 99, 885, 621, 273, 631, 211, 111), (700, 879, 8, 12, 762, 332, 640, 850, 87, 157, 6, 259, 727, 413, 477, 519, 620, 537, 201, 377), (210, 784, 164, 830, 128, 381, 540, 703, 448, 252, 49, 424, 339, 106, 15, 61, 393, 401, 238, 248), (280, 391, 154, 827, 648, 347, 291, 787, 289, 230, 217, 532, 684, 150, 431, 505, 247, 568, 812, 325), (854, 179, 612, 443, 824, 761, 348, 416, 819, 649, 53, 300, 788, 376, 412, 694, 635, 491, 256, 840)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.6272	step1_train_accuracy: 62.3468
STEP-1	Epoch: 20/50	loss: 0.8666	step1_train_accuracy: 85.4641
STEP-1	Epoch: 30/50	loss: 0.5568	step1_train_accuracy: 90.8932
STEP-1	Epoch: 40/50	loss: 0.4037	step1_train_accuracy: 92.9947
STEP-1	Epoch: 50/50	loss: 0.3174	step1_train_accuracy: 92.8196
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2619	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.2486	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.2386	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.2315	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.2261	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.2218	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.2182	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.2152	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.2128	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.2106	gate_loss: 0.0000	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 94.4056	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.8554	step1_train_accuracy: 62.3955
STEP-1	Epoch: 20/50	loss: 0.8141	step1_train_accuracy: 88.0223
STEP-1	Epoch: 30/50	loss: 0.3927	step1_train_accuracy: 95.2646
STEP-1	Epoch: 40/50	loss: 0.2369	step1_train_accuracy: 97.7716
STEP-1	Epoch: 50/50	loss: 0.1626	step1_train_accuracy: 98.6072
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1650	gate_loss: 0.2879	step2_classification_accuracy: 94.4444	step_2_gate_accuracy: 90.5556
STEP-2	Epoch: 40/200	classification_loss: 0.1449	gate_loss: 0.1108	step2_classification_accuracy: 94.6296	step_2_gate_accuracy: 98.8889
STEP-2	Epoch: 60/200	classification_loss: 0.1248	gate_loss: 0.0531	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.1203	gate_loss: 0.0327	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.1165	gate_loss: 0.0237	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.1142	gate_loss: 0.0181	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.1117	gate_loss: 0.0145	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.1098	gate_loss: 0.0118	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.1084	gate_loss: 0.0104	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.1067	gate_loss: 0.0091	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 90.9091	gate_accuracy: 96.5035
	Task-1	val_accuracy: 95.5556	gate_accuracy: 97.7778
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 96.9957


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 1.9731	step1_train_accuracy: 59.7902
STEP-1	Epoch: 20/50	loss: 0.8733	step1_train_accuracy: 85.6643
STEP-1	Epoch: 30/50	loss: 0.4944	step1_train_accuracy: 90.9091
STEP-1	Epoch: 40/50	loss: 0.3300	step1_train_accuracy: 95.8042
STEP-1	Epoch: 50/50	loss: 0.2626	step1_train_accuracy: 94.7552
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2715	gate_loss: 0.6255	step2_classification_accuracy: 91.6216	step_2_gate_accuracy: 78.9189
STEP-2	Epoch: 40/200	classification_loss: 0.2288	gate_loss: 0.3125	step2_classification_accuracy: 91.8919	step_2_gate_accuracy: 93.9189
STEP-2	Epoch: 60/200	classification_loss: 0.1718	gate_loss: 0.1684	step2_classification_accuracy: 93.6486	step_2_gate_accuracy: 98.1081
STEP-2	Epoch: 80/200	classification_loss: 0.2016	gate_loss: 0.1208	step2_classification_accuracy: 93.1081	step_2_gate_accuracy: 97.9730
STEP-2	Epoch: 100/200	classification_loss: 0.1433	gate_loss: 0.0749	step2_classification_accuracy: 94.3243	step_2_gate_accuracy: 99.0541
STEP-2	Epoch: 120/200	classification_loss: 0.1336	gate_loss: 0.0568	step2_classification_accuracy: 94.4595	step_2_gate_accuracy: 99.5946
STEP-2	Epoch: 140/200	classification_loss: 0.1263	gate_loss: 0.0435	step2_classification_accuracy: 94.5946	step_2_gate_accuracy: 99.8649
STEP-2	Epoch: 160/200	classification_loss: 0.1632	gate_loss: 0.0458	step2_classification_accuracy: 94.0541	step_2_gate_accuracy: 99.3243
STEP-2	Epoch: 180/200	classification_loss: 0.1204	gate_loss: 0.0335	step2_classification_accuracy: 94.5946	step_2_gate_accuracy: 99.7297
STEP-2	Epoch: 200/200	classification_loss: 0.1186	gate_loss: 0.0282	step2_classification_accuracy: 94.5946	step_2_gate_accuracy: 99.8649
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 88.8112	gate_accuracy: 92.3077
	Task-1	val_accuracy: 92.2222	gate_accuracy: 93.3333
	Task-2	val_accuracy: 83.3333	gate_accuracy: 90.2778
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 92.1311


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 2.1295	step1_train_accuracy: 50.0000
STEP-1	Epoch: 20/50	loss: 1.1377	step1_train_accuracy: 79.4304
STEP-1	Epoch: 30/50	loss: 0.6379	step1_train_accuracy: 92.4051
STEP-1	Epoch: 40/50	loss: 0.3933	step1_train_accuracy: 96.8354
STEP-1	Epoch: 50/50	loss: 0.2639	step1_train_accuracy: 98.4177
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2198	gate_loss: 0.8568	step2_classification_accuracy: 93.1915	step_2_gate_accuracy: 71.7021
STEP-2	Epoch: 40/200	classification_loss: 0.1433	gate_loss: 0.3692	step2_classification_accuracy: 95.5319	step_2_gate_accuracy: 94.1489
STEP-2	Epoch: 60/200	classification_loss: 0.1311	gate_loss: 0.1756	step2_classification_accuracy: 95.4255	step_2_gate_accuracy: 97.5532
STEP-2	Epoch: 80/200	classification_loss: 0.1015	gate_loss: 0.0971	step2_classification_accuracy: 96.3830	step_2_gate_accuracy: 99.0425
STEP-2	Epoch: 100/200	classification_loss: 0.0926	gate_loss: 0.0612	step2_classification_accuracy: 96.3830	step_2_gate_accuracy: 99.5745
STEP-2	Epoch: 120/200	classification_loss: 0.0864	gate_loss: 0.0425	step2_classification_accuracy: 96.4894	step_2_gate_accuracy: 99.7872
STEP-2	Epoch: 140/200	classification_loss: 0.0815	gate_loss: 0.0333	step2_classification_accuracy: 96.5957	step_2_gate_accuracy: 99.8936
STEP-2	Epoch: 160/200	classification_loss: 0.0805	gate_loss: 0.0286	step2_classification_accuracy: 96.4894	step_2_gate_accuracy: 99.5745
STEP-2	Epoch: 180/200	classification_loss: 0.0763	gate_loss: 0.0220	step2_classification_accuracy: 96.5957	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0743	gate_loss: 0.0192	step2_classification_accuracy: 96.5957	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 83.9161	gate_accuracy: 89.5105
	Task-1	val_accuracy: 88.8889	gate_accuracy: 88.8889
	Task-2	val_accuracy: 81.9444	gate_accuracy: 90.2778
	Task-3	val_accuracy: 92.4051	gate_accuracy: 92.4051
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 90.1042


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.0581	step1_train_accuracy: 54.4199
STEP-1	Epoch: 20/50	loss: 0.9829	step1_train_accuracy: 77.3481
STEP-1	Epoch: 30/50	loss: 0.5773	step1_train_accuracy: 87.2928
STEP-1	Epoch: 40/50	loss: 0.4025	step1_train_accuracy: 89.7790
STEP-1	Epoch: 50/50	loss: 0.3429	step1_train_accuracy: 89.7790
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3548	gate_loss: 0.9367	step2_classification_accuracy: 89.7368	step_2_gate_accuracy: 70.4386
STEP-2	Epoch: 40/200	classification_loss: 0.2393	gate_loss: 0.3954	step2_classification_accuracy: 92.1053	step_2_gate_accuracy: 92.1930
STEP-2	Epoch: 60/200	classification_loss: 0.1928	gate_loss: 0.1920	step2_classification_accuracy: 93.3333	step_2_gate_accuracy: 96.3158
STEP-2	Epoch: 80/200	classification_loss: 0.1735	gate_loss: 0.1191	step2_classification_accuracy: 93.0702	step_2_gate_accuracy: 97.4561
STEP-2	Epoch: 100/200	classification_loss: 0.1694	gate_loss: 0.0858	step2_classification_accuracy: 93.1579	step_2_gate_accuracy: 98.0702
STEP-2	Epoch: 120/200	classification_loss: 0.1525	gate_loss: 0.0651	step2_classification_accuracy: 93.9474	step_2_gate_accuracy: 98.7719
STEP-2	Epoch: 140/200	classification_loss: 0.1496	gate_loss: 0.0547	step2_classification_accuracy: 93.9474	step_2_gate_accuracy: 98.6842
STEP-2	Epoch: 160/200	classification_loss: 0.1458	gate_loss: 0.0500	step2_classification_accuracy: 93.5088	step_2_gate_accuracy: 98.4211
STEP-2	Epoch: 180/200	classification_loss: 0.1382	gate_loss: 0.0444	step2_classification_accuracy: 94.2105	step_2_gate_accuracy: 98.4211
STEP-2	Epoch: 200/200	classification_loss: 0.1373	gate_loss: 0.0407	step2_classification_accuracy: 93.9474	step_2_gate_accuracy: 98.7719
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 78.3217	gate_accuracy: 83.2168
	Task-1	val_accuracy: 87.7778	gate_accuracy: 88.8889
	Task-2	val_accuracy: 87.5000	gate_accuracy: 93.0556
	Task-3	val_accuracy: 89.8734	gate_accuracy: 89.8734
	Task-4	val_accuracy: 85.7143	gate_accuracy: 92.3077
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 88.6316


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.0746	step1_train_accuracy: 58.4848
STEP-1	Epoch: 20/50	loss: 0.9732	step1_train_accuracy: 78.4848
STEP-1	Epoch: 30/50	loss: 0.5195	step1_train_accuracy: 88.1818
STEP-1	Epoch: 40/50	loss: 0.3397	step1_train_accuracy: 91.5152
STEP-1	Epoch: 50/50	loss: 0.2599	step1_train_accuracy: 93.6364
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3635	gate_loss: 1.1657	step2_classification_accuracy: 87.8358	step_2_gate_accuracy: 63.5075
STEP-2	Epoch: 40/200	classification_loss: 0.2765	gate_loss: 0.5089	step2_classification_accuracy: 89.6269	step_2_gate_accuracy: 87.3134
STEP-2	Epoch: 60/200	classification_loss: 0.2257	gate_loss: 0.2567	step2_classification_accuracy: 90.9701	step_2_gate_accuracy: 94.1791
STEP-2	Epoch: 80/200	classification_loss: 0.2191	gate_loss: 0.1677	step2_classification_accuracy: 91.1940	step_2_gate_accuracy: 95.6716
STEP-2	Epoch: 100/200	classification_loss: 0.2002	gate_loss: 0.1181	step2_classification_accuracy: 91.7910	step_2_gate_accuracy: 97.0896
STEP-2	Epoch: 120/200	classification_loss: 0.1825	gate_loss: 0.0920	step2_classification_accuracy: 92.1642	step_2_gate_accuracy: 97.5373
STEP-2	Epoch: 140/200	classification_loss: 0.1765	gate_loss: 0.0789	step2_classification_accuracy: 92.6866	step_2_gate_accuracy: 97.9104
STEP-2	Epoch: 160/200	classification_loss: 0.1895	gate_loss: 0.0737	step2_classification_accuracy: 92.0896	step_2_gate_accuracy: 97.4627
STEP-2	Epoch: 180/200	classification_loss: 0.1733	gate_loss: 0.0634	step2_classification_accuracy: 92.5373	step_2_gate_accuracy: 98.0597
STEP-2	Epoch: 200/200	classification_loss: 0.1666	gate_loss: 0.0570	step2_classification_accuracy: 92.6119	step_2_gate_accuracy: 97.9851
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.2238	gate_accuracy: 81.1189
	Task-1	val_accuracy: 84.4444	gate_accuracy: 86.6667
	Task-2	val_accuracy: 84.7222	gate_accuracy: 91.6667
	Task-3	val_accuracy: 89.8734	gate_accuracy: 88.6076
	Task-4	val_accuracy: 86.8132	gate_accuracy: 91.2088
	Task-5	val_accuracy: 76.8293	gate_accuracy: 82.9268
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 86.3555


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 3.0346	step1_train_accuracy: 36.4000
STEP-1	Epoch: 20/50	loss: 1.3565	step1_train_accuracy: 80.8000
STEP-1	Epoch: 30/50	loss: 0.7350	step1_train_accuracy: 88.0000
STEP-1	Epoch: 40/50	loss: 0.4996	step1_train_accuracy: 90.8000
STEP-1	Epoch: 50/50	loss: 0.3811	step1_train_accuracy: 93.2000
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4391	gate_loss: 1.2250	step2_classification_accuracy: 84.8052	step_2_gate_accuracy: 61.6883
STEP-2	Epoch: 40/200	classification_loss: 0.3284	gate_loss: 0.5271	step2_classification_accuracy: 87.6623	step_2_gate_accuracy: 85.2597
STEP-2	Epoch: 60/200	classification_loss: 0.2743	gate_loss: 0.2860	step2_classification_accuracy: 88.6364	step_2_gate_accuracy: 92.4675
STEP-2	Epoch: 80/200	classification_loss: 0.2560	gate_loss: 0.2022	step2_classification_accuracy: 90.2597	step_2_gate_accuracy: 94.1558
STEP-2	Epoch: 100/200	classification_loss: 0.2158	gate_loss: 0.1519	step2_classification_accuracy: 91.2338	step_2_gate_accuracy: 95.6494
STEP-2	Epoch: 120/200	classification_loss: 0.2213	gate_loss: 0.1395	step2_classification_accuracy: 91.1039	step_2_gate_accuracy: 94.8701
STEP-2	Epoch: 140/200	classification_loss: 0.1970	gate_loss: 0.1171	step2_classification_accuracy: 91.5584	step_2_gate_accuracy: 95.9740
STEP-2	Epoch: 160/200	classification_loss: 0.1887	gate_loss: 0.1033	step2_classification_accuracy: 91.7532	step_2_gate_accuracy: 96.6234
STEP-2	Epoch: 180/200	classification_loss: 0.1817	gate_loss: 0.0953	step2_classification_accuracy: 91.9481	step_2_gate_accuracy: 96.9481
STEP-2	Epoch: 200/200	classification_loss: 0.1739	gate_loss: 0.0869	step2_classification_accuracy: 92.2727	step_2_gate_accuracy: 97.4675
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.0280	gate_accuracy: 80.4196
	Task-1	val_accuracy: 84.4444	gate_accuracy: 86.6667
	Task-2	val_accuracy: 79.1667	gate_accuracy: 84.7222
	Task-3	val_accuracy: 92.4051	gate_accuracy: 89.8734
	Task-4	val_accuracy: 82.4176	gate_accuracy: 86.8132
	Task-5	val_accuracy: 75.6098	gate_accuracy: 81.7073
	Task-6	val_accuracy: 55.5556	gate_accuracy: 61.9048
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.2581


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 2.2416	step1_train_accuracy: 50.9677
STEP-1	Epoch: 20/50	loss: 0.9561	step1_train_accuracy: 85.8064
STEP-1	Epoch: 30/50	loss: 0.4176	step1_train_accuracy: 99.0323
STEP-1	Epoch: 40/50	loss: 0.2407	step1_train_accuracy: 99.3548
STEP-1	Epoch: 50/50	loss: 0.1585	step1_train_accuracy: 100.0000
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4281	gate_loss: 1.3600	step2_classification_accuracy: 85.8621	step_2_gate_accuracy: 54.7701
STEP-2	Epoch: 40/200	classification_loss: 0.3184	gate_loss: 0.5789	step2_classification_accuracy: 88.2759	step_2_gate_accuracy: 83.1609
STEP-2	Epoch: 60/200	classification_loss: 0.2557	gate_loss: 0.3023	step2_classification_accuracy: 89.9425	step_2_gate_accuracy: 91.8966
STEP-2	Epoch: 80/200	classification_loss: 0.2365	gate_loss: 0.2023	step2_classification_accuracy: 91.0920	step_2_gate_accuracy: 94.0805
STEP-2	Epoch: 100/200	classification_loss: 0.2099	gate_loss: 0.1513	step2_classification_accuracy: 91.4943	step_2_gate_accuracy: 95.8046
STEP-2	Epoch: 120/200	classification_loss: 0.1854	gate_loss: 0.1236	step2_classification_accuracy: 92.0690	step_2_gate_accuracy: 95.7471
STEP-2	Epoch: 140/200	classification_loss: 0.1933	gate_loss: 0.1114	step2_classification_accuracy: 91.6092	step_2_gate_accuracy: 96.4943
STEP-2	Epoch: 160/200	classification_loss: 0.1748	gate_loss: 0.0967	step2_classification_accuracy: 92.8736	step_2_gate_accuracy: 96.9540
STEP-2	Epoch: 180/200	classification_loss: 0.1714	gate_loss: 0.0885	step2_classification_accuracy: 92.9310	step_2_gate_accuracy: 96.9540
STEP-2	Epoch: 200/200	classification_loss: 0.1587	gate_loss: 0.0802	step2_classification_accuracy: 93.0460	step_2_gate_accuracy: 97.3563
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.2308	gate_accuracy: 74.1259
	Task-1	val_accuracy: 83.3333	gate_accuracy: 82.2222
	Task-2	val_accuracy: 76.3889	gate_accuracy: 81.9444
	Task-3	val_accuracy: 88.6076	gate_accuracy: 84.8101
	Task-4	val_accuracy: 83.5165	gate_accuracy: 86.8132
	Task-5	val_accuracy: 76.8293	gate_accuracy: 85.3659
	Task-6	val_accuracy: 60.3175	gate_accuracy: 66.6667
	Task-7	val_accuracy: 94.8052	gate_accuracy: 92.2078
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.4921


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.6178	step1_train_accuracy: 42.5000
STEP-1	Epoch: 20/50	loss: 1.1545	step1_train_accuracy: 84.2857
STEP-1	Epoch: 30/50	loss: 0.5494	step1_train_accuracy: 93.9286
STEP-1	Epoch: 40/50	loss: 0.3358	step1_train_accuracy: 95.0000
STEP-1	Epoch: 50/50	loss: 0.2403	step1_train_accuracy: 95.0000
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5071	gate_loss: 1.4625	step2_classification_accuracy: 82.8866	step_2_gate_accuracy: 53.4536
STEP-2	Epoch: 40/200	classification_loss: 0.3392	gate_loss: 0.6153	step2_classification_accuracy: 87.3711	step_2_gate_accuracy: 83.5052
STEP-2	Epoch: 60/200	classification_loss: 0.2839	gate_loss: 0.3277	step2_classification_accuracy: 88.8660	step_2_gate_accuracy: 91.3402
STEP-2	Epoch: 80/200	classification_loss: 0.2455	gate_loss: 0.2156	step2_classification_accuracy: 90.0515	step_2_gate_accuracy: 94.2268
STEP-2	Epoch: 100/200	classification_loss: 0.2286	gate_loss: 0.1688	step2_classification_accuracy: 90.7732	step_2_gate_accuracy: 95.1031
STEP-2	Epoch: 120/200	classification_loss: 0.2089	gate_loss: 0.1376	step2_classification_accuracy: 91.4433	step_2_gate_accuracy: 95.7216
STEP-2	Epoch: 140/200	classification_loss: 0.1954	gate_loss: 0.1177	step2_classification_accuracy: 91.8041	step_2_gate_accuracy: 96.2887
STEP-2	Epoch: 160/200	classification_loss: 0.1916	gate_loss: 0.1050	step2_classification_accuracy: 92.0103	step_2_gate_accuracy: 97.0619
STEP-2	Epoch: 180/200	classification_loss: 0.1930	gate_loss: 0.1003	step2_classification_accuracy: 91.5979	step_2_gate_accuracy: 96.7526
STEP-2	Epoch: 200/200	classification_loss: 0.1806	gate_loss: 0.0901	step2_classification_accuracy: 92.0619	step_2_gate_accuracy: 96.8557
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.5385	gate_accuracy: 72.7273
	Task-1	val_accuracy: 83.3333	gate_accuracy: 85.5556
	Task-2	val_accuracy: 79.1667	gate_accuracy: 83.3333
	Task-3	val_accuracy: 87.3418	gate_accuracy: 86.0759
	Task-4	val_accuracy: 83.5165	gate_accuracy: 84.6154
	Task-5	val_accuracy: 69.5122	gate_accuracy: 73.1707
	Task-6	val_accuracy: 65.0794	gate_accuracy: 65.0794
	Task-7	val_accuracy: 93.5065	gate_accuracy: 87.0130
	Task-8	val_accuracy: 72.8571	gate_accuracy: 77.1429
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.2699


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.1735	step1_train_accuracy: 44.5161
STEP-1	Epoch: 20/50	loss: 0.9102	step1_train_accuracy: 87.4194
STEP-1	Epoch: 30/50	loss: 0.5051	step1_train_accuracy: 94.1936
STEP-1	Epoch: 40/50	loss: 0.3250	step1_train_accuracy: 96.1290
STEP-1	Epoch: 50/50	loss: 0.2389	step1_train_accuracy: 97.4194
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5385	gate_loss: 1.5543	step2_classification_accuracy: 82.0093	step_2_gate_accuracy: 51.1682
STEP-2	Epoch: 40/200	classification_loss: 0.3741	gate_loss: 0.6614	step2_classification_accuracy: 86.5888	step_2_gate_accuracy: 83.0374
STEP-2	Epoch: 60/200	classification_loss: 0.3000	gate_loss: 0.3409	step2_classification_accuracy: 89.2991	step_2_gate_accuracy: 91.8692
STEP-2	Epoch: 80/200	classification_loss: 0.2514	gate_loss: 0.2271	step2_classification_accuracy: 90.7477	step_2_gate_accuracy: 94.0187
STEP-2	Epoch: 100/200	classification_loss: 0.2415	gate_loss: 0.1712	step2_classification_accuracy: 90.8411	step_2_gate_accuracy: 95.4673
STEP-2	Epoch: 120/200	classification_loss: 0.2112	gate_loss: 0.1380	step2_classification_accuracy: 91.5888	step_2_gate_accuracy: 95.9813
STEP-2	Epoch: 140/200	classification_loss: 0.2081	gate_loss: 0.1239	step2_classification_accuracy: 91.7290	step_2_gate_accuracy: 96.0280
STEP-2	Epoch: 160/200	classification_loss: 0.1973	gate_loss: 0.1090	step2_classification_accuracy: 92.0093	step_2_gate_accuracy: 96.4019
STEP-2	Epoch: 180/200	classification_loss: 0.1899	gate_loss: 0.1001	step2_classification_accuracy: 92.1028	step_2_gate_accuracy: 96.5888
STEP-2	Epoch: 200/200	classification_loss: 0.1775	gate_loss: 0.0924	step2_classification_accuracy: 92.5701	step_2_gate_accuracy: 96.9159
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.5385	gate_accuracy: 73.4266
	Task-1	val_accuracy: 81.1111	gate_accuracy: 84.4444
	Task-2	val_accuracy: 76.3889	gate_accuracy: 79.1667
	Task-3	val_accuracy: 88.6076	gate_accuracy: 84.8101
	Task-4	val_accuracy: 80.2198	gate_accuracy: 82.4176
	Task-5	val_accuracy: 63.4146	gate_accuracy: 71.9512
	Task-6	val_accuracy: 55.5556	gate_accuracy: 63.4921
	Task-7	val_accuracy: 88.3117	gate_accuracy: 85.7143
	Task-8	val_accuracy: 80.0000	gate_accuracy: 82.8571
	Task-9	val_accuracy: 83.1169	gate_accuracy: 76.6234
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.4360


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.1536	step1_train_accuracy: 52.6786
STEP-1	Epoch: 20/50	loss: 1.0899	step1_train_accuracy: 69.0476
STEP-1	Epoch: 30/50	loss: 0.6533	step1_train_accuracy: 83.9286
STEP-1	Epoch: 40/50	loss: 0.4474	step1_train_accuracy: 85.7143
STEP-1	Epoch: 50/50	loss: 0.3375	step1_train_accuracy: 89.2857
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5530	gate_loss: 1.6407	step2_classification_accuracy: 81.3248	step_2_gate_accuracy: 48.8889
STEP-2	Epoch: 40/200	classification_loss: 0.4017	gate_loss: 0.6919	step2_classification_accuracy: 85.3846	step_2_gate_accuracy: 81.7949
STEP-2	Epoch: 60/200	classification_loss: 0.2990	gate_loss: 0.3500	step2_classification_accuracy: 88.5470	step_2_gate_accuracy: 91.2393
STEP-2	Epoch: 80/200	classification_loss: 0.2500	gate_loss: 0.2238	step2_classification_accuracy: 90.4274	step_2_gate_accuracy: 94.1880
STEP-2	Epoch: 100/200	classification_loss: 0.2411	gate_loss: 0.1731	step2_classification_accuracy: 90.8547	step_2_gate_accuracy: 94.9573
STEP-2	Epoch: 120/200	classification_loss: 0.2188	gate_loss: 0.1399	step2_classification_accuracy: 91.0256	step_2_gate_accuracy: 96.4530
STEP-2	Epoch: 140/200	classification_loss: 0.2052	gate_loss: 0.1219	step2_classification_accuracy: 91.1111	step_2_gate_accuracy: 96.7521
STEP-2	Epoch: 160/200	classification_loss: 0.1941	gate_loss: 0.1075	step2_classification_accuracy: 92.0513	step_2_gate_accuracy: 96.8376
STEP-2	Epoch: 180/200	classification_loss: 0.1864	gate_loss: 0.0982	step2_classification_accuracy: 92.1795	step_2_gate_accuracy: 97.0085
STEP-2	Epoch: 200/200	classification_loss: 0.1799	gate_loss: 0.0903	step2_classification_accuracy: 92.2222	step_2_gate_accuracy: 97.2650
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.2378	gate_accuracy: 74.1259
	Task-1	val_accuracy: 81.1111	gate_accuracy: 84.4444
	Task-2	val_accuracy: 73.6111	gate_accuracy: 81.9444
	Task-3	val_accuracy: 86.0759	gate_accuracy: 86.0759
	Task-4	val_accuracy: 82.4176	gate_accuracy: 82.4176
	Task-5	val_accuracy: 68.2927	gate_accuracy: 74.3902
	Task-6	val_accuracy: 52.3810	gate_accuracy: 61.9048
	Task-7	val_accuracy: 90.9091	gate_accuracy: 87.0130
	Task-8	val_accuracy: 75.7143	gate_accuracy: 74.2857
	Task-9	val_accuracy: 70.1299	gate_accuracy: 68.8312
	Task-10	val_accuracy: 77.3810	gate_accuracy: 86.9048
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.5560


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.4031	step1_train_accuracy: 46.3816
STEP-1	Epoch: 20/50	loss: 1.0543	step1_train_accuracy: 82.2368
STEP-1	Epoch: 30/50	loss: 0.5105	step1_train_accuracy: 93.4211
STEP-1	Epoch: 40/50	loss: 0.3290	step1_train_accuracy: 97.0395
STEP-1	Epoch: 50/50	loss: 0.2327	step1_train_accuracy: 97.6974
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5396	gate_loss: 1.7069	step2_classification_accuracy: 81.4173	step_2_gate_accuracy: 46.0630
STEP-2	Epoch: 40/200	classification_loss: 0.4043	gate_loss: 0.7137	step2_classification_accuracy: 86.2992	step_2_gate_accuracy: 80.3150
STEP-2	Epoch: 60/200	classification_loss: 0.3139	gate_loss: 0.3632	step2_classification_accuracy: 88.7795	step_2_gate_accuracy: 90.3150
STEP-2	Epoch: 80/200	classification_loss: 0.2700	gate_loss: 0.2387	step2_classification_accuracy: 90.0787	step_2_gate_accuracy: 94.0945
STEP-2	Epoch: 100/200	classification_loss: 0.2473	gate_loss: 0.1810	step2_classification_accuracy: 90.5512	step_2_gate_accuracy: 95.1181
STEP-2	Epoch: 120/200	classification_loss: 0.2260	gate_loss: 0.1461	step2_classification_accuracy: 91.5354	step_2_gate_accuracy: 95.7874
STEP-2	Epoch: 140/200	classification_loss: 0.2097	gate_loss: 0.1227	step2_classification_accuracy: 91.7717	step_2_gate_accuracy: 96.5748
STEP-2	Epoch: 160/200	classification_loss: 0.2035	gate_loss: 0.1090	step2_classification_accuracy: 92.0472	step_2_gate_accuracy: 96.8110
STEP-2	Epoch: 180/200	classification_loss: 0.1899	gate_loss: 0.1016	step2_classification_accuracy: 91.9685	step_2_gate_accuracy: 96.5354
STEP-2	Epoch: 200/200	classification_loss: 0.1862	gate_loss: 0.0927	step2_classification_accuracy: 92.4409	step_2_gate_accuracy: 96.8898
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.7413	gate_accuracy: 68.5315
	Task-1	val_accuracy: 80.0000	gate_accuracy: 84.4444
	Task-2	val_accuracy: 75.0000	gate_accuracy: 81.9444
	Task-3	val_accuracy: 84.8101	gate_accuracy: 84.8101
	Task-4	val_accuracy: 78.0220	gate_accuracy: 74.7253
	Task-5	val_accuracy: 68.2927	gate_accuracy: 71.9512
	Task-6	val_accuracy: 49.2063	gate_accuracy: 57.1429
	Task-7	val_accuracy: 85.7143	gate_accuracy: 81.8182
	Task-8	val_accuracy: 74.2857	gate_accuracy: 72.8571
	Task-9	val_accuracy: 70.1299	gate_accuracy: 67.5325
	Task-10	val_accuracy: 77.3810	gate_accuracy: 85.7143
	Task-11	val_accuracy: 82.8947	gate_accuracy: 80.2632
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.8964


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.3509	step1_train_accuracy: 56.7213
STEP-1	Epoch: 20/50	loss: 0.8505	step1_train_accuracy: 90.4918
STEP-1	Epoch: 30/50	loss: 0.4406	step1_train_accuracy: 95.7377
STEP-1	Epoch: 40/50	loss: 0.2703	step1_train_accuracy: 97.7049
STEP-1	Epoch: 50/50	loss: 0.1969	step1_train_accuracy: 98.3607
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5917	gate_loss: 1.8186	step2_classification_accuracy: 80.7664	step_2_gate_accuracy: 43.7226
STEP-2	Epoch: 40/200	classification_loss: 0.4389	gate_loss: 0.7708	step2_classification_accuracy: 84.7810	step_2_gate_accuracy: 78.2847
STEP-2	Epoch: 60/200	classification_loss: 0.3356	gate_loss: 0.4016	step2_classification_accuracy: 87.9927	step_2_gate_accuracy: 89.1606
STEP-2	Epoch: 80/200	classification_loss: 0.2859	gate_loss: 0.2639	step2_classification_accuracy: 89.5255	step_2_gate_accuracy: 92.8102
STEP-2	Epoch: 100/200	classification_loss: 0.2510	gate_loss: 0.1977	step2_classification_accuracy: 90.2920	step_2_gate_accuracy: 93.9781
STEP-2	Epoch: 120/200	classification_loss: 0.2407	gate_loss: 0.1685	step2_classification_accuracy: 90.6934	step_2_gate_accuracy: 94.7445
STEP-2	Epoch: 140/200	classification_loss: 0.2324	gate_loss: 0.1509	step2_classification_accuracy: 90.6204	step_2_gate_accuracy: 94.7080
STEP-2	Epoch: 160/200	classification_loss: 0.2137	gate_loss: 0.1272	step2_classification_accuracy: 91.4599	step_2_gate_accuracy: 96.0584
STEP-2	Epoch: 180/200	classification_loss: 0.2033	gate_loss: 0.1212	step2_classification_accuracy: 91.6423	step_2_gate_accuracy: 95.9489
STEP-2	Epoch: 200/200	classification_loss: 0.1858	gate_loss: 0.1045	step2_classification_accuracy: 92.0073	step_2_gate_accuracy: 96.2774
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.7413	gate_accuracy: 67.8322
	Task-1	val_accuracy: 81.1111	gate_accuracy: 82.2222
	Task-2	val_accuracy: 68.0556	gate_accuracy: 76.3889
	Task-3	val_accuracy: 84.8101	gate_accuracy: 82.2785
	Task-4	val_accuracy: 78.0220	gate_accuracy: 82.4176
	Task-5	val_accuracy: 65.8537	gate_accuracy: 79.2683
	Task-6	val_accuracy: 50.7937	gate_accuracy: 53.9683
	Task-7	val_accuracy: 89.6104	gate_accuracy: 88.3117
	Task-8	val_accuracy: 75.7143	gate_accuracy: 78.5714
	Task-9	val_accuracy: 72.7273	gate_accuracy: 63.6364
	Task-10	val_accuracy: 77.3810	gate_accuracy: 85.7143
	Task-11	val_accuracy: 81.5789	gate_accuracy: 81.5789
	Task-12	val_accuracy: 84.2105	gate_accuracy: 77.6316
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.8519


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 2.4078	step1_train_accuracy: 44.8029
STEP-1	Epoch: 20/50	loss: 1.0002	step1_train_accuracy: 79.9283
STEP-1	Epoch: 30/50	loss: 0.5742	step1_train_accuracy: 88.1720
STEP-1	Epoch: 40/50	loss: 0.3879	step1_train_accuracy: 93.1900
STEP-1	Epoch: 50/50	loss: 0.2823	step1_train_accuracy: 97.1326
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5871	gate_loss: 1.8527	step2_classification_accuracy: 80.9184	step_2_gate_accuracy: 44.6259
STEP-2	Epoch: 40/200	classification_loss: 0.4388	gate_loss: 0.7608	step2_classification_accuracy: 85.4422	step_2_gate_accuracy: 78.7415
STEP-2	Epoch: 60/200	classification_loss: 0.3491	gate_loss: 0.4030	step2_classification_accuracy: 87.3810	step_2_gate_accuracy: 88.9796
STEP-2	Epoch: 80/200	classification_loss: 0.2922	gate_loss: 0.2701	step2_classification_accuracy: 89.0136	step_2_gate_accuracy: 92.5850
STEP-2	Epoch: 100/200	classification_loss: 0.2594	gate_loss: 0.2090	step2_classification_accuracy: 90.6122	step_2_gate_accuracy: 94.2517
STEP-2	Epoch: 120/200	classification_loss: 0.2550	gate_loss: 0.1821	step2_classification_accuracy: 90.6803	step_2_gate_accuracy: 94.1837
STEP-2	Epoch: 140/200	classification_loss: 0.2208	gate_loss: 0.1481	step2_classification_accuracy: 91.2245	step_2_gate_accuracy: 95.2041
STEP-2	Epoch: 160/200	classification_loss: 0.2138	gate_loss: 0.1307	step2_classification_accuracy: 91.4966	step_2_gate_accuracy: 95.7143
STEP-2	Epoch: 180/200	classification_loss: 0.2011	gate_loss: 0.1188	step2_classification_accuracy: 91.7687	step_2_gate_accuracy: 96.0544
STEP-2	Epoch: 200/200	classification_loss: 0.1975	gate_loss: 0.1105	step2_classification_accuracy: 91.8027	step_2_gate_accuracy: 96.5986
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.8392	gate_accuracy: 71.3287
	Task-1	val_accuracy: 76.6667	gate_accuracy: 83.3333
	Task-2	val_accuracy: 75.0000	gate_accuracy: 79.1667
	Task-3	val_accuracy: 82.2785	gate_accuracy: 83.5443
	Task-4	val_accuracy: 80.2198	gate_accuracy: 82.4176
	Task-5	val_accuracy: 67.0732	gate_accuracy: 67.0732
	Task-6	val_accuracy: 50.7937	gate_accuracy: 53.9683
	Task-7	val_accuracy: 87.0130	gate_accuracy: 81.8182
	Task-8	val_accuracy: 75.7143	gate_accuracy: 78.5714
	Task-9	val_accuracy: 75.3247	gate_accuracy: 76.6234
	Task-10	val_accuracy: 75.0000	gate_accuracy: 86.9048
	Task-11	val_accuracy: 78.9474	gate_accuracy: 71.0526
	Task-12	val_accuracy: 82.8947	gate_accuracy: 80.2632
	Task-13	val_accuracy: 62.8571	gate_accuracy: 60.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.7391


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.6251	step1_train_accuracy: 50.6993
STEP-1	Epoch: 20/50	loss: 1.0016	step1_train_accuracy: 87.4126
STEP-1	Epoch: 30/50	loss: 0.5787	step1_train_accuracy: 92.6573
STEP-1	Epoch: 40/50	loss: 0.4287	step1_train_accuracy: 95.8042
STEP-1	Epoch: 50/50	loss: 0.3079	step1_train_accuracy: 96.1538
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6288	gate_loss: 1.9976	step2_classification_accuracy: 79.9363	step_2_gate_accuracy: 40.7643
STEP-2	Epoch: 40/200	classification_loss: 0.4658	gate_loss: 0.8445	step2_classification_accuracy: 85.1592	step_2_gate_accuracy: 77.8025
STEP-2	Epoch: 60/200	classification_loss: 0.3609	gate_loss: 0.4293	step2_classification_accuracy: 88.1529	step_2_gate_accuracy: 89.3312
STEP-2	Epoch: 80/200	classification_loss: 0.3032	gate_loss: 0.2863	step2_classification_accuracy: 89.6497	step_2_gate_accuracy: 92.2930
STEP-2	Epoch: 100/200	classification_loss: 0.2767	gate_loss: 0.2246	step2_classification_accuracy: 90.3185	step_2_gate_accuracy: 93.6624
STEP-2	Epoch: 120/200	classification_loss: 0.2387	gate_loss: 0.1777	step2_classification_accuracy: 91.3376	step_2_gate_accuracy: 94.9682
STEP-2	Epoch: 140/200	classification_loss: 0.2228	gate_loss: 0.1501	step2_classification_accuracy: 91.6561	step_2_gate_accuracy: 95.6369
STEP-2	Epoch: 160/200	classification_loss: 0.2084	gate_loss: 0.1362	step2_classification_accuracy: 92.2293	step_2_gate_accuracy: 95.9236
STEP-2	Epoch: 180/200	classification_loss: 0.1973	gate_loss: 0.1224	step2_classification_accuracy: 92.7389	step_2_gate_accuracy: 96.2420
STEP-2	Epoch: 200/200	classification_loss: 0.1866	gate_loss: 0.1104	step2_classification_accuracy: 92.7707	step_2_gate_accuracy: 96.6879
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.7413	gate_accuracy: 65.7343
	Task-1	val_accuracy: 78.8889	gate_accuracy: 82.2222
	Task-2	val_accuracy: 66.6667	gate_accuracy: 73.6111
	Task-3	val_accuracy: 79.7468	gate_accuracy: 81.0127
	Task-4	val_accuracy: 75.8242	gate_accuracy: 74.7253
	Task-5	val_accuracy: 65.8537	gate_accuracy: 70.7317
	Task-6	val_accuracy: 50.7937	gate_accuracy: 65.0794
	Task-7	val_accuracy: 87.0130	gate_accuracy: 84.4156
	Task-8	val_accuracy: 77.1429	gate_accuracy: 77.1429
	Task-9	val_accuracy: 72.7273	gate_accuracy: 74.0260
	Task-10	val_accuracy: 73.8095	gate_accuracy: 77.3810
	Task-11	val_accuracy: 75.0000	gate_accuracy: 68.4211
	Task-12	val_accuracy: 81.5789	gate_accuracy: 77.6316
	Task-13	val_accuracy: 57.1429	gate_accuracy: 51.4286
	Task-14	val_accuracy: 84.5070	gate_accuracy: 77.4648
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.3006


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.3288	step1_train_accuracy: 47.3154
STEP-1	Epoch: 20/50	loss: 1.0467	step1_train_accuracy: 87.9195
STEP-1	Epoch: 30/50	loss: 0.4982	step1_train_accuracy: 93.2886
STEP-1	Epoch: 40/50	loss: 0.3123	step1_train_accuracy: 96.3087
STEP-1	Epoch: 50/50	loss: 0.2210	step1_train_accuracy: 97.3154
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6515	gate_loss: 2.0817	step2_classification_accuracy: 79.2216	step_2_gate_accuracy: 37.9341
STEP-2	Epoch: 40/200	classification_loss: 0.5032	gate_loss: 0.9014	step2_classification_accuracy: 83.5629	step_2_gate_accuracy: 76.0778
STEP-2	Epoch: 60/200	classification_loss: 0.3962	gate_loss: 0.4639	step2_classification_accuracy: 86.8862	step_2_gate_accuracy: 87.5150
STEP-2	Epoch: 80/200	classification_loss: 0.3275	gate_loss: 0.3078	step2_classification_accuracy: 88.6826	step_2_gate_accuracy: 91.1677
STEP-2	Epoch: 100/200	classification_loss: 0.2876	gate_loss: 0.2290	step2_classification_accuracy: 89.8802	step_2_gate_accuracy: 93.5629
STEP-2	Epoch: 120/200	classification_loss: 0.2477	gate_loss: 0.1847	step2_classification_accuracy: 91.0180	step_2_gate_accuracy: 94.7605
STEP-2	Epoch: 140/200	classification_loss: 0.2328	gate_loss: 0.1599	step2_classification_accuracy: 91.2575	step_2_gate_accuracy: 95.5389
STEP-2	Epoch: 160/200	classification_loss: 0.2121	gate_loss: 0.1379	step2_classification_accuracy: 91.9162	step_2_gate_accuracy: 96.1377
STEP-2	Epoch: 180/200	classification_loss: 0.2028	gate_loss: 0.1248	step2_classification_accuracy: 92.0060	step_2_gate_accuracy: 95.9581
STEP-2	Epoch: 200/200	classification_loss: 0.2001	gate_loss: 0.1170	step2_classification_accuracy: 92.3054	step_2_gate_accuracy: 96.3174
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.6434	gate_accuracy: 68.5315
	Task-1	val_accuracy: 77.7778	gate_accuracy: 84.4444
	Task-2	val_accuracy: 69.4444	gate_accuracy: 75.0000
	Task-3	val_accuracy: 82.2785	gate_accuracy: 81.0127
	Task-4	val_accuracy: 76.9231	gate_accuracy: 78.0220
	Task-5	val_accuracy: 58.5366	gate_accuracy: 63.4146
	Task-6	val_accuracy: 46.0317	gate_accuracy: 52.3810
	Task-7	val_accuracy: 81.8182	gate_accuracy: 76.6234
	Task-8	val_accuracy: 71.4286	gate_accuracy: 71.4286
	Task-9	val_accuracy: 67.5325	gate_accuracy: 68.8312
	Task-10	val_accuracy: 78.5714	gate_accuracy: 85.7143
	Task-11	val_accuracy: 77.6316	gate_accuracy: 75.0000
	Task-12	val_accuracy: 86.8421	gate_accuracy: 85.5263
	Task-13	val_accuracy: 78.5714	gate_accuracy: 68.5714
	Task-14	val_accuracy: 76.0563	gate_accuracy: 74.6479
	Task-15	val_accuracy: 71.6216	gate_accuracy: 79.7297
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.4402


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.5051	step1_train_accuracy: 47.9290
STEP-1	Epoch: 20/50	loss: 0.9501	step1_train_accuracy: 84.9112
STEP-1	Epoch: 30/50	loss: 0.4683	step1_train_accuracy: 94.9704
STEP-1	Epoch: 40/50	loss: 0.3215	step1_train_accuracy: 96.1538
STEP-1	Epoch: 50/50	loss: 0.2424	step1_train_accuracy: 96.4497
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6521	gate_loss: 2.0661	step2_classification_accuracy: 78.3898	step_2_gate_accuracy: 37.3164
STEP-2	Epoch: 40/200	classification_loss: 0.4895	gate_loss: 0.8585	step2_classification_accuracy: 83.6441	step_2_gate_accuracy: 76.9492
STEP-2	Epoch: 60/200	classification_loss: 0.3835	gate_loss: 0.4530	step2_classification_accuracy: 87.5424	step_2_gate_accuracy: 87.7966
STEP-2	Epoch: 80/200	classification_loss: 0.3237	gate_loss: 0.3043	step2_classification_accuracy: 88.8701	step_2_gate_accuracy: 91.5819
STEP-2	Epoch: 100/200	classification_loss: 0.2732	gate_loss: 0.2310	step2_classification_accuracy: 90.2260	step_2_gate_accuracy: 93.0508
STEP-2	Epoch: 120/200	classification_loss: 0.2567	gate_loss: 0.1979	step2_classification_accuracy: 90.7627	step_2_gate_accuracy: 93.6158
STEP-2	Epoch: 140/200	classification_loss: 0.2291	gate_loss: 0.1638	step2_classification_accuracy: 91.6384	step_2_gate_accuracy: 94.8870
STEP-2	Epoch: 160/200	classification_loss: 0.2226	gate_loss: 0.1518	step2_classification_accuracy: 91.7797	step_2_gate_accuracy: 95.5367
STEP-2	Epoch: 180/200	classification_loss: 0.1979	gate_loss: 0.1275	step2_classification_accuracy: 92.6836	step_2_gate_accuracy: 96.0734
STEP-2	Epoch: 200/200	classification_loss: 0.1927	gate_loss: 0.1200	step2_classification_accuracy: 92.4011	step_2_gate_accuracy: 96.0452
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.4406	gate_accuracy: 65.0350
	Task-1	val_accuracy: 77.7778	gate_accuracy: 82.2222
	Task-2	val_accuracy: 70.8333	gate_accuracy: 79.1667
	Task-3	val_accuracy: 75.9494	gate_accuracy: 77.2152
	Task-4	val_accuracy: 75.8242	gate_accuracy: 73.6264
	Task-5	val_accuracy: 63.4146	gate_accuracy: 68.2927
	Task-6	val_accuracy: 46.0317	gate_accuracy: 58.7302
	Task-7	val_accuracy: 83.1169	gate_accuracy: 75.3247
	Task-8	val_accuracy: 71.4286	gate_accuracy: 75.7143
	Task-9	val_accuracy: 68.8312	gate_accuracy: 67.5325
	Task-10	val_accuracy: 78.5714	gate_accuracy: 78.5714
	Task-11	val_accuracy: 75.0000	gate_accuracy: 73.6842
	Task-12	val_accuracy: 82.8947	gate_accuracy: 75.0000
	Task-13	val_accuracy: 70.0000	gate_accuracy: 70.0000
	Task-14	val_accuracy: 76.0563	gate_accuracy: 71.8310
	Task-15	val_accuracy: 71.6216	gate_accuracy: 68.9189
	Task-16	val_accuracy: 69.0476	gate_accuracy: 65.4762
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.0087


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 2.5299	step1_train_accuracy: 50.0000
STEP-1	Epoch: 20/50	loss: 1.0641	step1_train_accuracy: 81.2883
STEP-1	Epoch: 30/50	loss: 0.5875	step1_train_accuracy: 90.4908
STEP-1	Epoch: 40/50	loss: 0.3721	step1_train_accuracy: 95.3988
STEP-1	Epoch: 50/50	loss: 0.2570	step1_train_accuracy: 97.2393
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6661	gate_loss: 2.0756	step2_classification_accuracy: 78.7433	step_2_gate_accuracy: 38.5561
STEP-2	Epoch: 40/200	classification_loss: 0.5011	gate_loss: 0.8376	step2_classification_accuracy: 83.8770	step_2_gate_accuracy: 78.1283
STEP-2	Epoch: 60/200	classification_loss: 0.3833	gate_loss: 0.4383	step2_classification_accuracy: 87.4332	step_2_gate_accuracy: 87.8877
STEP-2	Epoch: 80/200	classification_loss: 0.3197	gate_loss: 0.2936	step2_classification_accuracy: 88.9305	step_2_gate_accuracy: 92.1390
STEP-2	Epoch: 100/200	classification_loss: 0.2796	gate_loss: 0.2264	step2_classification_accuracy: 90.0267	step_2_gate_accuracy: 93.7166
STEP-2	Epoch: 120/200	classification_loss: 0.2614	gate_loss: 0.1941	step2_classification_accuracy: 90.6150	step_2_gate_accuracy: 94.2513
STEP-2	Epoch: 140/200	classification_loss: 0.2360	gate_loss: 0.1596	step2_classification_accuracy: 91.2299	step_2_gate_accuracy: 94.6791
STEP-2	Epoch: 160/200	classification_loss: 0.2196	gate_loss: 0.1421	step2_classification_accuracy: 91.4706	step_2_gate_accuracy: 95.5615
STEP-2	Epoch: 180/200	classification_loss: 0.2061	gate_loss: 0.1267	step2_classification_accuracy: 92.1123	step_2_gate_accuracy: 96.2032
STEP-2	Epoch: 200/200	classification_loss: 0.2024	gate_loss: 0.1211	step2_classification_accuracy: 92.2460	step_2_gate_accuracy: 95.9358
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.7483	gate_accuracy: 66.4336
	Task-1	val_accuracy: 75.5556	gate_accuracy: 78.8889
	Task-2	val_accuracy: 70.8333	gate_accuracy: 72.2222
	Task-3	val_accuracy: 79.7468	gate_accuracy: 75.9494
	Task-4	val_accuracy: 76.9231	gate_accuracy: 80.2198
	Task-5	val_accuracy: 60.9756	gate_accuracy: 69.5122
	Task-6	val_accuracy: 34.9206	gate_accuracy: 46.0317
	Task-7	val_accuracy: 77.9221	gate_accuracy: 71.4286
	Task-8	val_accuracy: 75.7143	gate_accuracy: 74.2857
	Task-9	val_accuracy: 70.1299	gate_accuracy: 63.6364
	Task-10	val_accuracy: 72.6190	gate_accuracy: 75.0000
	Task-11	val_accuracy: 73.6842	gate_accuracy: 65.7895
	Task-12	val_accuracy: 81.5789	gate_accuracy: 78.9474
	Task-13	val_accuracy: 64.2857	gate_accuracy: 57.1429
	Task-14	val_accuracy: 81.6901	gate_accuracy: 77.4648
	Task-15	val_accuracy: 75.6757	gate_accuracy: 82.4324
	Task-16	val_accuracy: 67.8571	gate_accuracy: 65.4762
	Task-17	val_accuracy: 90.2439	gate_accuracy: 91.4634
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.0055


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 2.6696	step1_train_accuracy: 51.6923
STEP-1	Epoch: 20/50	loss: 1.1471	step1_train_accuracy: 77.2308
STEP-1	Epoch: 30/50	loss: 0.7145	step1_train_accuracy: 91.0769
STEP-1	Epoch: 40/50	loss: 0.4949	step1_train_accuracy: 94.1538
STEP-1	Epoch: 50/50	loss: 0.3594	step1_train_accuracy: 96.0000
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6924	gate_loss: 2.1414	step2_classification_accuracy: 77.3604	step_2_gate_accuracy: 34.3655
STEP-2	Epoch: 40/200	classification_loss: 0.5279	gate_loss: 0.8904	step2_classification_accuracy: 82.5888	step_2_gate_accuracy: 76.3959
STEP-2	Epoch: 60/200	classification_loss: 0.4002	gate_loss: 0.4726	step2_classification_accuracy: 86.4213	step_2_gate_accuracy: 87.4619
STEP-2	Epoch: 80/200	classification_loss: 0.3294	gate_loss: 0.3177	step2_classification_accuracy: 88.2487	step_2_gate_accuracy: 91.0914
STEP-2	Epoch: 100/200	classification_loss: 0.2884	gate_loss: 0.2436	step2_classification_accuracy: 89.7208	step_2_gate_accuracy: 92.8426
STEP-2	Epoch: 120/200	classification_loss: 0.2563	gate_loss: 0.1986	step2_classification_accuracy: 90.9137	step_2_gate_accuracy: 94.0101
STEP-2	Epoch: 140/200	classification_loss: 0.2440	gate_loss: 0.1731	step2_classification_accuracy: 91.2437	step_2_gate_accuracy: 94.6701
STEP-2	Epoch: 160/200	classification_loss: 0.2272	gate_loss: 0.1543	step2_classification_accuracy: 91.6751	step_2_gate_accuracy: 95.1777
STEP-2	Epoch: 180/200	classification_loss: 0.2169	gate_loss: 0.1407	step2_classification_accuracy: 91.9289	step_2_gate_accuracy: 95.4822
STEP-2	Epoch: 200/200	classification_loss: 0.2079	gate_loss: 0.1325	step2_classification_accuracy: 92.1574	step_2_gate_accuracy: 95.8629
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.9441	gate_accuracy: 70.6294
	Task-1	val_accuracy: 80.0000	gate_accuracy: 83.3333
	Task-2	val_accuracy: 62.5000	gate_accuracy: 70.8333
	Task-3	val_accuracy: 82.2785	gate_accuracy: 79.7468
	Task-4	val_accuracy: 74.7253	gate_accuracy: 79.1209
	Task-5	val_accuracy: 58.5366	gate_accuracy: 63.4146
	Task-6	val_accuracy: 44.4444	gate_accuracy: 57.1429
	Task-7	val_accuracy: 85.7143	gate_accuracy: 77.9221
	Task-8	val_accuracy: 74.2857	gate_accuracy: 74.2857
	Task-9	val_accuracy: 63.6364	gate_accuracy: 62.3377
	Task-10	val_accuracy: 76.1905	gate_accuracy: 83.3333
	Task-11	val_accuracy: 73.6842	gate_accuracy: 65.7895
	Task-12	val_accuracy: 78.9474	gate_accuracy: 73.6842
	Task-13	val_accuracy: 67.1429	gate_accuracy: 65.7143
	Task-14	val_accuracy: 73.2394	gate_accuracy: 67.6056
	Task-15	val_accuracy: 75.6757	gate_accuracy: 79.7297
	Task-16	val_accuracy: 69.0476	gate_accuracy: 65.4762
	Task-17	val_accuracy: 86.5854	gate_accuracy: 81.7073
	Task-18	val_accuracy: 67.9012	gate_accuracy: 60.4938
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.9844


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.3660	step1_train_accuracy: 46.6472
STEP-1	Epoch: 20/50	loss: 0.9906	step1_train_accuracy: 78.4257
STEP-1	Epoch: 30/50	loss: 0.5574	step1_train_accuracy: 91.2536
STEP-1	Epoch: 40/50	loss: 0.3698	step1_train_accuracy: 93.8775
STEP-1	Epoch: 50/50	loss: 0.2976	step1_train_accuracy: 94.7522
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6920	gate_loss: 2.1286	step2_classification_accuracy: 78.2609	step_2_gate_accuracy: 38.4058
STEP-2	Epoch: 40/200	classification_loss: 0.5200	gate_loss: 0.8667	step2_classification_accuracy: 83.2609	step_2_gate_accuracy: 76.1836
STEP-2	Epoch: 60/200	classification_loss: 0.3925	gate_loss: 0.4672	step2_classification_accuracy: 86.9807	step_2_gate_accuracy: 86.5942
STEP-2	Epoch: 80/200	classification_loss: 0.3344	gate_loss: 0.3248	step2_classification_accuracy: 88.5990	step_2_gate_accuracy: 90.7729
STEP-2	Epoch: 100/200	classification_loss: 0.2919	gate_loss: 0.2445	step2_classification_accuracy: 90.0242	step_2_gate_accuracy: 92.9710
STEP-2	Epoch: 120/200	classification_loss: 0.2620	gate_loss: 0.2025	step2_classification_accuracy: 90.9179	step_2_gate_accuracy: 94.1787
STEP-2	Epoch: 140/200	classification_loss: 0.2348	gate_loss: 0.1701	step2_classification_accuracy: 91.8357	step_2_gate_accuracy: 95.1449
STEP-2	Epoch: 160/200	classification_loss: 0.2181	gate_loss: 0.1509	step2_classification_accuracy: 92.2705	step_2_gate_accuracy: 95.4589
STEP-2	Epoch: 180/200	classification_loss: 0.2134	gate_loss: 0.1377	step2_classification_accuracy: 92.2705	step_2_gate_accuracy: 95.7488
STEP-2	Epoch: 200/200	classification_loss: 0.2055	gate_loss: 0.1281	step2_classification_accuracy: 92.4638	step_2_gate_accuracy: 95.9662
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.0490	gate_accuracy: 67.8322
	Task-1	val_accuracy: 75.5556	gate_accuracy: 82.2222
	Task-2	val_accuracy: 65.2778	gate_accuracy: 76.3889
	Task-3	val_accuracy: 73.4177	gate_accuracy: 79.7468
	Task-4	val_accuracy: 72.5275	gate_accuracy: 70.3297
	Task-5	val_accuracy: 51.2195	gate_accuracy: 59.7561
	Task-6	val_accuracy: 47.6190	gate_accuracy: 55.5556
	Task-7	val_accuracy: 83.1169	gate_accuracy: 77.9221
	Task-8	val_accuracy: 70.0000	gate_accuracy: 75.7143
	Task-9	val_accuracy: 61.0390	gate_accuracy: 64.9351
	Task-10	val_accuracy: 76.1905	gate_accuracy: 84.5238
	Task-11	val_accuracy: 65.7895	gate_accuracy: 52.6316
	Task-12	val_accuracy: 75.0000	gate_accuracy: 71.0526
	Task-13	val_accuracy: 62.8571	gate_accuracy: 60.0000
	Task-14	val_accuracy: 74.6479	gate_accuracy: 74.6479
	Task-15	val_accuracy: 68.9189	gate_accuracy: 71.6216
	Task-16	val_accuracy: 72.6190	gate_accuracy: 67.8571
	Task-17	val_accuracy: 90.2439	gate_accuracy: 89.0244
	Task-18	val_accuracy: 64.1975	gate_accuracy: 54.3210
	Task-19	val_accuracy: 70.9302	gate_accuracy: 65.1163
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.2088


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.1138	step1_train_accuracy: 55.5882
STEP-1	Epoch: 20/50	loss: 0.8347	step1_train_accuracy: 88.8235
STEP-1	Epoch: 30/50	loss: 0.4344	step1_train_accuracy: 93.2353
STEP-1	Epoch: 40/50	loss: 0.2699	step1_train_accuracy: 96.1765
STEP-1	Epoch: 50/50	loss: 0.1798	step1_train_accuracy: 99.4118
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7536	gate_loss: 2.1996	step2_classification_accuracy: 76.8203	step_2_gate_accuracy: 37.3041
STEP-2	Epoch: 40/200	classification_loss: 0.5712	gate_loss: 0.8900	step2_classification_accuracy: 82.0968	step_2_gate_accuracy: 74.9078
STEP-2	Epoch: 60/200	classification_loss: 0.4310	gate_loss: 0.4846	step2_classification_accuracy: 85.8756	step_2_gate_accuracy: 85.7143
STEP-2	Epoch: 80/200	classification_loss: 0.3553	gate_loss: 0.3249	step2_classification_accuracy: 88.4101	step_2_gate_accuracy: 90.4378
STEP-2	Epoch: 100/200	classification_loss: 0.3157	gate_loss: 0.2506	step2_classification_accuracy: 89.4009	step_2_gate_accuracy: 93.2028
STEP-2	Epoch: 120/200	classification_loss: 0.2807	gate_loss: 0.2062	step2_classification_accuracy: 90.1613	step_2_gate_accuracy: 94.1014
STEP-2	Epoch: 140/200	classification_loss: 0.2553	gate_loss: 0.1738	step2_classification_accuracy: 91.1521	step_2_gate_accuracy: 94.9309
STEP-2	Epoch: 160/200	classification_loss: 0.2437	gate_loss: 0.1589	step2_classification_accuracy: 91.3825	step_2_gate_accuracy: 95.5760
STEP-2	Epoch: 180/200	classification_loss: 0.2264	gate_loss: 0.1439	step2_classification_accuracy: 91.7972	step_2_gate_accuracy: 95.5760
STEP-2	Epoch: 200/200	classification_loss: 0.2143	gate_loss: 0.1294	step2_classification_accuracy: 91.8203	step_2_gate_accuracy: 95.9908
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.6434	gate_accuracy: 67.8322
	Task-1	val_accuracy: 73.3333	gate_accuracy: 78.8889
	Task-2	val_accuracy: 61.1111	gate_accuracy: 73.6111
	Task-3	val_accuracy: 77.2152	gate_accuracy: 82.2785
	Task-4	val_accuracy: 68.1319	gate_accuracy: 70.3297
	Task-5	val_accuracy: 59.7561	gate_accuracy: 59.7561
	Task-6	val_accuracy: 44.4444	gate_accuracy: 47.6190
	Task-7	val_accuracy: 79.2208	gate_accuracy: 79.2208
	Task-8	val_accuracy: 64.2857	gate_accuracy: 70.0000
	Task-9	val_accuracy: 67.5325	gate_accuracy: 67.5325
	Task-10	val_accuracy: 75.0000	gate_accuracy: 83.3333
	Task-11	val_accuracy: 68.4211	gate_accuracy: 67.1053
	Task-12	val_accuracy: 73.6842	gate_accuracy: 69.7368
	Task-13	val_accuracy: 60.0000	gate_accuracy: 52.8571
	Task-14	val_accuracy: 66.1972	gate_accuracy: 63.3803
	Task-15	val_accuracy: 70.2703	gate_accuracy: 79.7297
	Task-16	val_accuracy: 72.6190	gate_accuracy: 67.8571
	Task-17	val_accuracy: 86.5854	gate_accuracy: 87.8049
	Task-18	val_accuracy: 66.6667	gate_accuracy: 60.4938
	Task-19	val_accuracy: 66.2791	gate_accuracy: 65.1163
	Task-20	val_accuracy: 76.4706	gate_accuracy: 74.1176
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.2277


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.1975	step1_train_accuracy: 63.4561
STEP-1	Epoch: 20/50	loss: 0.8793	step1_train_accuracy: 84.7026
STEP-1	Epoch: 30/50	loss: 0.4615	step1_train_accuracy: 96.8839
STEP-1	Epoch: 40/50	loss: 0.2838	step1_train_accuracy: 98.0170
STEP-1	Epoch: 50/50	loss: 0.1936	step1_train_accuracy: 98.3003
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7274	gate_loss: 2.2611	step2_classification_accuracy: 77.5330	step_2_gate_accuracy: 33.6344
STEP-2	Epoch: 40/200	classification_loss: 0.5522	gate_loss: 0.9214	step2_classification_accuracy: 82.9736	step_2_gate_accuracy: 74.9780
STEP-2	Epoch: 60/200	classification_loss: 0.4071	gate_loss: 0.4854	step2_classification_accuracy: 87.2026	step_2_gate_accuracy: 86.8282
STEP-2	Epoch: 80/200	classification_loss: 0.3333	gate_loss: 0.3250	step2_classification_accuracy: 89.2952	step_2_gate_accuracy: 91.1454
STEP-2	Epoch: 100/200	classification_loss: 0.2867	gate_loss: 0.2464	step2_classification_accuracy: 90.4626	step_2_gate_accuracy: 93.1938
STEP-2	Epoch: 120/200	classification_loss: 0.2624	gate_loss: 0.2066	step2_classification_accuracy: 91.3216	step_2_gate_accuracy: 93.8987
STEP-2	Epoch: 140/200	classification_loss: 0.2331	gate_loss: 0.1723	step2_classification_accuracy: 92.0264	step_2_gate_accuracy: 94.9559
STEP-2	Epoch: 160/200	classification_loss: 0.2187	gate_loss: 0.1514	step2_classification_accuracy: 92.3789	step_2_gate_accuracy: 95.5947
STEP-2	Epoch: 180/200	classification_loss: 0.2063	gate_loss: 0.1376	step2_classification_accuracy: 92.7974	step_2_gate_accuracy: 96.1674
STEP-2	Epoch: 200/200	classification_loss: 0.1978	gate_loss: 0.1289	step2_classification_accuracy: 92.9736	step_2_gate_accuracy: 95.9912
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.4476	gate_accuracy: 62.9371
	Task-1	val_accuracy: 74.4444	gate_accuracy: 81.1111
	Task-2	val_accuracy: 69.4444	gate_accuracy: 75.0000
	Task-3	val_accuracy: 73.4177	gate_accuracy: 74.6835
	Task-4	val_accuracy: 73.6264	gate_accuracy: 69.2308
	Task-5	val_accuracy: 54.8780	gate_accuracy: 59.7561
	Task-6	val_accuracy: 46.0317	gate_accuracy: 44.4444
	Task-7	val_accuracy: 80.5195	gate_accuracy: 75.3247
	Task-8	val_accuracy: 71.4286	gate_accuracy: 74.2857
	Task-9	val_accuracy: 61.0390	gate_accuracy: 63.6364
	Task-10	val_accuracy: 75.0000	gate_accuracy: 84.5238
	Task-11	val_accuracy: 73.6842	gate_accuracy: 67.1053
	Task-12	val_accuracy: 77.6316	gate_accuracy: 77.6316
	Task-13	val_accuracy: 60.0000	gate_accuracy: 52.8571
	Task-14	val_accuracy: 71.8310	gate_accuracy: 67.6056
	Task-15	val_accuracy: 66.2162	gate_accuracy: 70.2703
	Task-16	val_accuracy: 67.8571	gate_accuracy: 58.3333
	Task-17	val_accuracy: 86.5854	gate_accuracy: 86.5854
	Task-18	val_accuracy: 61.7284	gate_accuracy: 61.7284
	Task-19	val_accuracy: 67.4419	gate_accuracy: 61.6279
	Task-20	val_accuracy: 77.6471	gate_accuracy: 75.2941
	Task-21	val_accuracy: 81.8182	gate_accuracy: 79.5455
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.4059


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.9228	step1_train_accuracy: 51.6129
STEP-1	Epoch: 20/50	loss: 0.9297	step1_train_accuracy: 84.1936
STEP-1	Epoch: 30/50	loss: 0.3819	step1_train_accuracy: 98.3871
STEP-1	Epoch: 40/50	loss: 0.2296	step1_train_accuracy: 98.3871
STEP-1	Epoch: 50/50	loss: 0.1593	step1_train_accuracy: 98.3871
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7817	gate_loss: 2.2939	step2_classification_accuracy: 75.2532	step_2_gate_accuracy: 33.3333
STEP-2	Epoch: 40/200	classification_loss: 0.6105	gate_loss: 0.9618	step2_classification_accuracy: 80.6118	step_2_gate_accuracy: 73.2911
STEP-2	Epoch: 60/200	classification_loss: 0.4537	gate_loss: 0.5291	step2_classification_accuracy: 85.4219	step_2_gate_accuracy: 84.8523
STEP-2	Epoch: 80/200	classification_loss: 0.3648	gate_loss: 0.3544	step2_classification_accuracy: 88.0802	step_2_gate_accuracy: 89.5570
STEP-2	Epoch: 100/200	classification_loss: 0.3156	gate_loss: 0.2683	step2_classification_accuracy: 89.5359	step_2_gate_accuracy: 92.0042
STEP-2	Epoch: 120/200	classification_loss: 0.2823	gate_loss: 0.2198	step2_classification_accuracy: 90.6540	step_2_gate_accuracy: 93.5865
STEP-2	Epoch: 140/200	classification_loss: 0.2628	gate_loss: 0.1906	step2_classification_accuracy: 91.0127	step_2_gate_accuracy: 94.5148
STEP-2	Epoch: 160/200	classification_loss: 0.2367	gate_loss: 0.1626	step2_classification_accuracy: 91.6456	step_2_gate_accuracy: 95.0422
STEP-2	Epoch: 180/200	classification_loss: 0.2177	gate_loss: 0.1447	step2_classification_accuracy: 92.2363	step_2_gate_accuracy: 95.5696
STEP-2	Epoch: 200/200	classification_loss: 0.2141	gate_loss: 0.1321	step2_classification_accuracy: 92.1730	step_2_gate_accuracy: 96.1181
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.1469	gate_accuracy: 63.6364
	Task-1	val_accuracy: 76.6667	gate_accuracy: 85.5556
	Task-2	val_accuracy: 58.3333	gate_accuracy: 69.4444
	Task-3	val_accuracy: 78.4810	gate_accuracy: 78.4810
	Task-4	val_accuracy: 74.7253	gate_accuracy: 75.8242
	Task-5	val_accuracy: 62.1951	gate_accuracy: 59.7561
	Task-6	val_accuracy: 39.6825	gate_accuracy: 38.0952
	Task-7	val_accuracy: 81.8182	gate_accuracy: 76.6234
	Task-8	val_accuracy: 64.2857	gate_accuracy: 61.4286
	Task-9	val_accuracy: 61.0390	gate_accuracy: 57.1429
	Task-10	val_accuracy: 70.2381	gate_accuracy: 73.8095
	Task-11	val_accuracy: 72.3684	gate_accuracy: 71.0526
	Task-12	val_accuracy: 72.3684	gate_accuracy: 64.4737
	Task-13	val_accuracy: 54.2857	gate_accuracy: 51.4286
	Task-14	val_accuracy: 76.0563	gate_accuracy: 73.2394
	Task-15	val_accuracy: 72.9730	gate_accuracy: 79.7297
	Task-16	val_accuracy: 66.6667	gate_accuracy: 63.0952
	Task-17	val_accuracy: 85.3659	gate_accuracy: 85.3659
	Task-18	val_accuracy: 71.6049	gate_accuracy: 65.4321
	Task-19	val_accuracy: 62.7907	gate_accuracy: 61.6279
	Task-20	val_accuracy: 82.3529	gate_accuracy: 80.0000
	Task-21	val_accuracy: 73.8636	gate_accuracy: 72.7273
	Task-22	val_accuracy: 71.4286	gate_accuracy: 68.8312
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.9031


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 2.3154	step1_train_accuracy: 56.8452
STEP-1	Epoch: 20/50	loss: 0.9408	step1_train_accuracy: 82.7381
STEP-1	Epoch: 30/50	loss: 0.5128	step1_train_accuracy: 91.3690
STEP-1	Epoch: 40/50	loss: 0.3329	step1_train_accuracy: 98.2143
STEP-1	Epoch: 50/50	loss: 0.2335	step1_train_accuracy: 98.5119
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7408	gate_loss: 2.4045	step2_classification_accuracy: 76.3563	step_2_gate_accuracy: 31.6397
STEP-2	Epoch: 40/200	classification_loss: 0.5545	gate_loss: 0.9772	step2_classification_accuracy: 82.6721	step_2_gate_accuracy: 74.0081
STEP-2	Epoch: 60/200	classification_loss: 0.4138	gate_loss: 0.5031	step2_classification_accuracy: 86.8826	step_2_gate_accuracy: 86.4170
STEP-2	Epoch: 80/200	classification_loss: 0.3383	gate_loss: 0.3383	step2_classification_accuracy: 88.7449	step_2_gate_accuracy: 90.7490
STEP-2	Epoch: 100/200	classification_loss: 0.2923	gate_loss: 0.2617	step2_classification_accuracy: 89.7773	step_2_gate_accuracy: 92.5911
STEP-2	Epoch: 120/200	classification_loss: 0.2655	gate_loss: 0.2112	step2_classification_accuracy: 90.8704	step_2_gate_accuracy: 93.9676
STEP-2	Epoch: 140/200	classification_loss: 0.2455	gate_loss: 0.1818	step2_classification_accuracy: 91.6802	step_2_gate_accuracy: 94.7773
STEP-2	Epoch: 160/200	classification_loss: 0.2282	gate_loss: 0.1579	step2_classification_accuracy: 92.2874	step_2_gate_accuracy: 95.1822
STEP-2	Epoch: 180/200	classification_loss: 0.2124	gate_loss: 0.1440	step2_classification_accuracy: 92.3077	step_2_gate_accuracy: 95.5466
STEP-2	Epoch: 200/200	classification_loss: 0.2064	gate_loss: 0.1345	step2_classification_accuracy: 92.5506	step_2_gate_accuracy: 95.7895
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.0490	gate_accuracy: 66.4336
	Task-1	val_accuracy: 77.7778	gate_accuracy: 80.0000
	Task-2	val_accuracy: 59.7222	gate_accuracy: 68.0556
	Task-3	val_accuracy: 67.0886	gate_accuracy: 65.8228
	Task-4	val_accuracy: 73.6264	gate_accuracy: 75.8242
	Task-5	val_accuracy: 62.1951	gate_accuracy: 60.9756
	Task-6	val_accuracy: 33.3333	gate_accuracy: 41.2698
	Task-7	val_accuracy: 77.9221	gate_accuracy: 74.0260
	Task-8	val_accuracy: 68.5714	gate_accuracy: 72.8571
	Task-9	val_accuracy: 57.1429	gate_accuracy: 59.7403
	Task-10	val_accuracy: 71.4286	gate_accuracy: 76.1905
	Task-11	val_accuracy: 67.1053	gate_accuracy: 65.7895
	Task-12	val_accuracy: 77.6316	gate_accuracy: 72.3684
	Task-13	val_accuracy: 58.5714	gate_accuracy: 55.7143
	Task-14	val_accuracy: 60.5634	gate_accuracy: 60.5634
	Task-15	val_accuracy: 71.6216	gate_accuracy: 77.0270
	Task-16	val_accuracy: 71.4286	gate_accuracy: 59.5238
	Task-17	val_accuracy: 85.3659	gate_accuracy: 86.5854
	Task-18	val_accuracy: 64.1975	gate_accuracy: 62.9630
	Task-19	val_accuracy: 69.7674	gate_accuracy: 67.4419
	Task-20	val_accuracy: 75.2941	gate_accuracy: 70.5882
	Task-21	val_accuracy: 72.7273	gate_accuracy: 73.8636
	Task-22	val_accuracy: 70.1299	gate_accuracy: 67.5325
	Task-23	val_accuracy: 73.8095	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.1957


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.0165	step1_train_accuracy: 51.7426
STEP-1	Epoch: 20/50	loss: 0.8648	step1_train_accuracy: 76.6756
STEP-1	Epoch: 30/50	loss: 0.5132	step1_train_accuracy: 89.8123
STEP-1	Epoch: 40/50	loss: 0.3749	step1_train_accuracy: 90.0804
STEP-1	Epoch: 50/50	loss: 0.2981	step1_train_accuracy: 91.1528
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8167	gate_loss: 2.3094	step2_classification_accuracy: 75.8755	step_2_gate_accuracy: 32.9572
STEP-2	Epoch: 40/200	classification_loss: 0.6263	gate_loss: 0.9437	step2_classification_accuracy: 80.9922	step_2_gate_accuracy: 74.1829
STEP-2	Epoch: 60/200	classification_loss: 0.4780	gate_loss: 0.5280	step2_classification_accuracy: 85.5058	step_2_gate_accuracy: 84.9805
STEP-2	Epoch: 80/200	classification_loss: 0.4024	gate_loss: 0.3647	step2_classification_accuracy: 87.4708	step_2_gate_accuracy: 89.6109
STEP-2	Epoch: 100/200	classification_loss: 0.3415	gate_loss: 0.2839	step2_classification_accuracy: 89.0272	step_2_gate_accuracy: 91.5564
STEP-2	Epoch: 120/200	classification_loss: 0.2995	gate_loss: 0.2261	step2_classification_accuracy: 90.4475	step_2_gate_accuracy: 93.5409
STEP-2	Epoch: 140/200	classification_loss: 0.2714	gate_loss: 0.1937	step2_classification_accuracy: 90.9339	step_2_gate_accuracy: 94.1440
STEP-2	Epoch: 160/200	classification_loss: 0.2583	gate_loss: 0.1750	step2_classification_accuracy: 91.5564	step_2_gate_accuracy: 94.4553
STEP-2	Epoch: 180/200	classification_loss: 0.2361	gate_loss: 0.1527	step2_classification_accuracy: 92.0039	step_2_gate_accuracy: 95.1362
STEP-2	Epoch: 200/200	classification_loss: 0.2316	gate_loss: 0.1464	step2_classification_accuracy: 92.0428	step_2_gate_accuracy: 95.3113
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.8462	gate_accuracy: 59.4406
	Task-1	val_accuracy: 75.5556	gate_accuracy: 74.4444
	Task-2	val_accuracy: 59.7222	gate_accuracy: 72.2222
	Task-3	val_accuracy: 69.6203	gate_accuracy: 72.1519
	Task-4	val_accuracy: 69.2308	gate_accuracy: 68.1319
	Task-5	val_accuracy: 52.4390	gate_accuracy: 59.7561
	Task-6	val_accuracy: 44.4444	gate_accuracy: 50.7937
	Task-7	val_accuracy: 84.4156	gate_accuracy: 72.7273
	Task-8	val_accuracy: 62.8571	gate_accuracy: 62.8571
	Task-9	val_accuracy: 63.6364	gate_accuracy: 64.9351
	Task-10	val_accuracy: 72.6190	gate_accuracy: 72.6190
	Task-11	val_accuracy: 76.3158	gate_accuracy: 75.0000
	Task-12	val_accuracy: 76.3158	gate_accuracy: 71.0526
	Task-13	val_accuracy: 54.2857	gate_accuracy: 50.0000
	Task-14	val_accuracy: 64.7887	gate_accuracy: 63.3803
	Task-15	val_accuracy: 64.8649	gate_accuracy: 75.6757
	Task-16	val_accuracy: 63.0952	gate_accuracy: 60.7143
	Task-17	val_accuracy: 82.9268	gate_accuracy: 78.0488
	Task-18	val_accuracy: 64.1975	gate_accuracy: 58.0247
	Task-19	val_accuracy: 62.7907	gate_accuracy: 55.8140
	Task-20	val_accuracy: 76.4706	gate_accuracy: 75.2941
	Task-21	val_accuracy: 77.2727	gate_accuracy: 76.1364
	Task-22	val_accuracy: 75.3247	gate_accuracy: 67.5325
	Task-23	val_accuracy: 66.6667	gate_accuracy: 61.9048
	Task-24	val_accuracy: 59.1398	gate_accuracy: 58.0645
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.2287


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 2.8567	step1_train_accuracy: 46.3259
STEP-1	Epoch: 20/50	loss: 1.0695	step1_train_accuracy: 78.5942
STEP-1	Epoch: 30/50	loss: 0.6275	step1_train_accuracy: 88.4984
STEP-1	Epoch: 40/50	loss: 0.4455	step1_train_accuracy: 92.3323
STEP-1	Epoch: 50/50	loss: 0.3399	step1_train_accuracy: 92.9712
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8266	gate_loss: 2.3012	step2_classification_accuracy: 74.8876	step_2_gate_accuracy: 33.1648
STEP-2	Epoch: 40/200	classification_loss: 0.6430	gate_loss: 0.9456	step2_classification_accuracy: 80.3184	step_2_gate_accuracy: 73.4270
STEP-2	Epoch: 60/200	classification_loss: 0.4914	gate_loss: 0.5291	step2_classification_accuracy: 84.6442	step_2_gate_accuracy: 85.3184
STEP-2	Epoch: 80/200	classification_loss: 0.4159	gate_loss: 0.3725	step2_classification_accuracy: 86.7603	step_2_gate_accuracy: 88.8390
STEP-2	Epoch: 100/200	classification_loss: 0.3392	gate_loss: 0.2846	step2_classification_accuracy: 88.7828	step_2_gate_accuracy: 91.0300
STEP-2	Epoch: 120/200	classification_loss: 0.3080	gate_loss: 0.2349	step2_classification_accuracy: 89.8315	step_2_gate_accuracy: 93.1461
STEP-2	Epoch: 140/200	classification_loss: 0.2822	gate_loss: 0.2054	step2_classification_accuracy: 90.2060	step_2_gate_accuracy: 93.8015
STEP-2	Epoch: 160/200	classification_loss: 0.2611	gate_loss: 0.1798	step2_classification_accuracy: 90.7491	step_2_gate_accuracy: 94.4007
STEP-2	Epoch: 180/200	classification_loss: 0.2553	gate_loss: 0.1708	step2_classification_accuracy: 90.7865	step_2_gate_accuracy: 94.6816
STEP-2	Epoch: 200/200	classification_loss: 0.2288	gate_loss: 0.1509	step2_classification_accuracy: 91.5356	step_2_gate_accuracy: 95.2996
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.3497	gate_accuracy: 62.9371
	Task-1	val_accuracy: 70.0000	gate_accuracy: 77.7778
	Task-2	val_accuracy: 63.8889	gate_accuracy: 65.2778
	Task-3	val_accuracy: 75.9494	gate_accuracy: 74.6835
	Task-4	val_accuracy: 70.3297	gate_accuracy: 69.2308
	Task-5	val_accuracy: 52.4390	gate_accuracy: 57.3171
	Task-6	val_accuracy: 44.4444	gate_accuracy: 49.2063
	Task-7	val_accuracy: 83.1169	gate_accuracy: 80.5195
	Task-8	val_accuracy: 68.5714	gate_accuracy: 74.2857
	Task-9	val_accuracy: 55.8442	gate_accuracy: 54.5455
	Task-10	val_accuracy: 70.2381	gate_accuracy: 71.4286
	Task-11	val_accuracy: 71.0526	gate_accuracy: 67.1053
	Task-12	val_accuracy: 73.6842	gate_accuracy: 69.7368
	Task-13	val_accuracy: 67.1429	gate_accuracy: 61.4286
	Task-14	val_accuracy: 63.3803	gate_accuracy: 64.7887
	Task-15	val_accuracy: 67.5676	gate_accuracy: 70.2703
	Task-16	val_accuracy: 69.0476	gate_accuracy: 64.2857
	Task-17	val_accuracy: 85.3659	gate_accuracy: 84.1463
	Task-18	val_accuracy: 74.0741	gate_accuracy: 61.7284
	Task-19	val_accuracy: 72.0930	gate_accuracy: 67.4419
	Task-20	val_accuracy: 74.1176	gate_accuracy: 75.2941
	Task-21	val_accuracy: 84.0909	gate_accuracy: 77.2727
	Task-22	val_accuracy: 74.0260	gate_accuracy: 66.2338
	Task-23	val_accuracy: 71.4286	gate_accuracy: 67.8571
	Task-24	val_accuracy: 56.9892	gate_accuracy: 49.4624
	Task-25	val_accuracy: 80.7692	gate_accuracy: 71.7949
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.5574


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 1.8821	step1_train_accuracy: 69.3548
STEP-1	Epoch: 20/50	loss: 0.6091	step1_train_accuracy: 88.9401
STEP-1	Epoch: 30/50	loss: 0.3145	step1_train_accuracy: 94.0092
STEP-1	Epoch: 40/50	loss: 0.2150	step1_train_accuracy: 95.6221
STEP-1	Epoch: 50/50	loss: 0.1572	step1_train_accuracy: 96.5438
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8340	gate_loss: 2.3719	step2_classification_accuracy: 74.3321	step_2_gate_accuracy: 31.8231
STEP-2	Epoch: 40/200	classification_loss: 0.6297	gate_loss: 0.9694	step2_classification_accuracy: 80.3610	step_2_gate_accuracy: 73.0144
STEP-2	Epoch: 60/200	classification_loss: 0.4769	gate_loss: 0.5423	step2_classification_accuracy: 84.3682	step_2_gate_accuracy: 84.2058
STEP-2	Epoch: 80/200	classification_loss: 0.3980	gate_loss: 0.3746	step2_classification_accuracy: 87.0578	step_2_gate_accuracy: 89.2058
STEP-2	Epoch: 100/200	classification_loss: 0.3343	gate_loss: 0.2829	step2_classification_accuracy: 88.8087	step_2_gate_accuracy: 91.7148
STEP-2	Epoch: 120/200	classification_loss: 0.2964	gate_loss: 0.2303	step2_classification_accuracy: 89.7653	step_2_gate_accuracy: 93.0144
STEP-2	Epoch: 140/200	classification_loss: 0.2661	gate_loss: 0.1986	step2_classification_accuracy: 90.5415	step_2_gate_accuracy: 94.0975
STEP-2	Epoch: 160/200	classification_loss: 0.2463	gate_loss: 0.1727	step2_classification_accuracy: 91.1552	step_2_gate_accuracy: 94.5307
STEP-2	Epoch: 180/200	classification_loss: 0.2269	gate_loss: 0.1536	step2_classification_accuracy: 91.8592	step_2_gate_accuracy: 95.3610
STEP-2	Epoch: 200/200	classification_loss: 0.2177	gate_loss: 0.1412	step2_classification_accuracy: 91.6787	step_2_gate_accuracy: 95.7220
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 54.5455	gate_accuracy: 65.7343
	Task-1	val_accuracy: 70.0000	gate_accuracy: 75.5556
	Task-2	val_accuracy: 55.5556	gate_accuracy: 68.0556
	Task-3	val_accuracy: 77.2152	gate_accuracy: 75.9494
	Task-4	val_accuracy: 67.0330	gate_accuracy: 68.1319
	Task-5	val_accuracy: 60.9756	gate_accuracy: 57.3171
	Task-6	val_accuracy: 36.5079	gate_accuracy: 46.0317
	Task-7	val_accuracy: 79.2208	gate_accuracy: 76.6234
	Task-8	val_accuracy: 65.7143	gate_accuracy: 61.4286
	Task-9	val_accuracy: 59.7403	gate_accuracy: 59.7403
	Task-10	val_accuracy: 73.8095	gate_accuracy: 80.9524
	Task-11	val_accuracy: 61.8421	gate_accuracy: 53.9474
	Task-12	val_accuracy: 73.6842	gate_accuracy: 69.7368
	Task-13	val_accuracy: 58.5714	gate_accuracy: 51.4286
	Task-14	val_accuracy: 67.6056	gate_accuracy: 67.6056
	Task-15	val_accuracy: 70.2703	gate_accuracy: 78.3784
	Task-16	val_accuracy: 58.3333	gate_accuracy: 60.7143
	Task-17	val_accuracy: 85.3659	gate_accuracy: 86.5854
	Task-18	val_accuracy: 65.4321	gate_accuracy: 54.3210
	Task-19	val_accuracy: 65.1163	gate_accuracy: 60.4651
	Task-20	val_accuracy: 69.4118	gate_accuracy: 70.5882
	Task-21	val_accuracy: 77.2727	gate_accuracy: 73.8636
	Task-22	val_accuracy: 75.3247	gate_accuracy: 68.8312
	Task-23	val_accuracy: 60.7143	gate_accuracy: 58.3333
	Task-24	val_accuracy: 58.0645	gate_accuracy: 52.6882
	Task-25	val_accuracy: 76.9231	gate_accuracy: 74.3590
	Task-26	val_accuracy: 72.4771	gate_accuracy: 62.3853
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.0571


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.7504	step1_train_accuracy: 54.4025
STEP-1	Epoch: 20/50	loss: 0.9621	step1_train_accuracy: 84.9057
STEP-1	Epoch: 30/50	loss: 0.4375	step1_train_accuracy: 94.0252
STEP-1	Epoch: 40/50	loss: 0.2738	step1_train_accuracy: 96.5409
STEP-1	Epoch: 50/50	loss: 0.1942	step1_train_accuracy: 97.7987
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8897	gate_loss: 2.4928	step2_classification_accuracy: 72.6829	step_2_gate_accuracy: 28.6237
STEP-2	Epoch: 40/200	classification_loss: 0.6930	gate_loss: 1.0706	step2_classification_accuracy: 78.9199	step_2_gate_accuracy: 69.8955
STEP-2	Epoch: 60/200	classification_loss: 0.5216	gate_loss: 0.5889	step2_classification_accuracy: 83.6237	step_2_gate_accuracy: 83.1010
STEP-2	Epoch: 80/200	classification_loss: 0.4296	gate_loss: 0.4055	step2_classification_accuracy: 86.1672	step_2_gate_accuracy: 87.9965
STEP-2	Epoch: 100/200	classification_loss: 0.3759	gate_loss: 0.3158	step2_classification_accuracy: 87.5784	step_2_gate_accuracy: 90.7143
STEP-2	Epoch: 120/200	classification_loss: 0.3277	gate_loss: 0.2539	step2_classification_accuracy: 88.9373	step_2_gate_accuracy: 92.2997
STEP-2	Epoch: 140/200	classification_loss: 0.3043	gate_loss: 0.2246	step2_classification_accuracy: 89.7735	step_2_gate_accuracy: 93.2404
STEP-2	Epoch: 160/200	classification_loss: 0.2757	gate_loss: 0.1930	step2_classification_accuracy: 90.2439	step_2_gate_accuracy: 94.2857
STEP-2	Epoch: 180/200	classification_loss: 0.2653	gate_loss: 0.1802	step2_classification_accuracy: 90.5923	step_2_gate_accuracy: 94.4251
STEP-2	Epoch: 200/200	classification_loss: 0.2433	gate_loss: 0.1566	step2_classification_accuracy: 91.4634	step_2_gate_accuracy: 95.1219
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.8462	gate_accuracy: 60.8392
	Task-1	val_accuracy: 68.8889	gate_accuracy: 75.5556
	Task-2	val_accuracy: 63.8889	gate_accuracy: 76.3889
	Task-3	val_accuracy: 73.4177	gate_accuracy: 75.9494
	Task-4	val_accuracy: 67.0330	gate_accuracy: 65.9341
	Task-5	val_accuracy: 41.4634	gate_accuracy: 47.5610
	Task-6	val_accuracy: 50.7937	gate_accuracy: 65.0794
	Task-7	val_accuracy: 84.4156	gate_accuracy: 79.2208
	Task-8	val_accuracy: 65.7143	gate_accuracy: 64.2857
	Task-9	val_accuracy: 62.3377	gate_accuracy: 58.4416
	Task-10	val_accuracy: 75.0000	gate_accuracy: 78.5714
	Task-11	val_accuracy: 65.7895	gate_accuracy: 61.8421
	Task-12	val_accuracy: 77.6316	gate_accuracy: 76.3158
	Task-13	val_accuracy: 62.8571	gate_accuracy: 65.7143
	Task-14	val_accuracy: 67.6056	gate_accuracy: 67.6056
	Task-15	val_accuracy: 68.9189	gate_accuracy: 77.0270
	Task-16	val_accuracy: 61.9048	gate_accuracy: 60.7143
	Task-17	val_accuracy: 75.6098	gate_accuracy: 79.2683
	Task-18	val_accuracy: 59.2593	gate_accuracy: 61.7284
	Task-19	val_accuracy: 67.4419	gate_accuracy: 65.1163
	Task-20	val_accuracy: 63.5294	gate_accuracy: 67.0588
	Task-21	val_accuracy: 78.4091	gate_accuracy: 75.0000
	Task-22	val_accuracy: 70.1299	gate_accuracy: 63.6364
	Task-23	val_accuracy: 67.8571	gate_accuracy: 60.7143
	Task-24	val_accuracy: 61.2903	gate_accuracy: 56.9892
	Task-25	val_accuracy: 75.6410	gate_accuracy: 76.9231
	Task-26	val_accuracy: 69.7248	gate_accuracy: 59.6330
	Task-27	val_accuracy: 50.6329	gate_accuracy: 45.5696
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.4369


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 2.9049	step1_train_accuracy: 39.8625
STEP-1	Epoch: 20/50	loss: 1.0962	step1_train_accuracy: 81.7869
STEP-1	Epoch: 30/50	loss: 0.5780	step1_train_accuracy: 96.9072
STEP-1	Epoch: 40/50	loss: 0.3672	step1_train_accuracy: 97.2509
STEP-1	Epoch: 50/50	loss: 0.2704	step1_train_accuracy: 97.2509
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8650	gate_loss: 2.4636	step2_classification_accuracy: 73.3165	step_2_gate_accuracy: 30.1347
STEP-2	Epoch: 40/200	classification_loss: 0.6568	gate_loss: 1.0397	step2_classification_accuracy: 80.2020	step_2_gate_accuracy: 70.6734
STEP-2	Epoch: 60/200	classification_loss: 0.5074	gate_loss: 0.5865	step2_classification_accuracy: 84.4276	step_2_gate_accuracy: 82.8620
STEP-2	Epoch: 80/200	classification_loss: 0.4218	gate_loss: 0.4092	step2_classification_accuracy: 86.7845	step_2_gate_accuracy: 88.1818
STEP-2	Epoch: 100/200	classification_loss: 0.3564	gate_loss: 0.3097	step2_classification_accuracy: 88.6027	step_2_gate_accuracy: 90.9764
STEP-2	Epoch: 120/200	classification_loss: 0.3118	gate_loss: 0.2542	step2_classification_accuracy: 89.5118	step_2_gate_accuracy: 92.2727
STEP-2	Epoch: 140/200	classification_loss: 0.2933	gate_loss: 0.2233	step2_classification_accuracy: 90.1684	step_2_gate_accuracy: 93.1481
STEP-2	Epoch: 160/200	classification_loss: 0.2570	gate_loss: 0.1881	step2_classification_accuracy: 91.0774	step_2_gate_accuracy: 94.3098
STEP-2	Epoch: 180/200	classification_loss: 0.2460	gate_loss: 0.1726	step2_classification_accuracy: 91.1616	step_2_gate_accuracy: 94.6970
STEP-2	Epoch: 200/200	classification_loss: 0.2311	gate_loss: 0.1548	step2_classification_accuracy: 91.6162	step_2_gate_accuracy: 95.0842
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 48.9510	gate_accuracy: 58.7413
	Task-1	val_accuracy: 63.3333	gate_accuracy: 72.2222
	Task-2	val_accuracy: 61.1111	gate_accuracy: 69.4444
	Task-3	val_accuracy: 72.1519	gate_accuracy: 72.1519
	Task-4	val_accuracy: 68.1319	gate_accuracy: 65.9341
	Task-5	val_accuracy: 53.6585	gate_accuracy: 62.1951
	Task-6	val_accuracy: 34.9206	gate_accuracy: 42.8571
	Task-7	val_accuracy: 77.9221	gate_accuracy: 74.0260
	Task-8	val_accuracy: 71.4286	gate_accuracy: 80.0000
	Task-9	val_accuracy: 57.1429	gate_accuracy: 58.4416
	Task-10	val_accuracy: 71.4286	gate_accuracy: 76.1905
	Task-11	val_accuracy: 75.0000	gate_accuracy: 72.3684
	Task-12	val_accuracy: 71.0526	gate_accuracy: 64.4737
	Task-13	val_accuracy: 62.8571	gate_accuracy: 61.4286
	Task-14	val_accuracy: 64.7887	gate_accuracy: 57.7465
	Task-15	val_accuracy: 74.3243	gate_accuracy: 85.1351
	Task-16	val_accuracy: 66.6667	gate_accuracy: 59.5238
	Task-17	val_accuracy: 81.7073	gate_accuracy: 76.8293
	Task-18	val_accuracy: 62.9630	gate_accuracy: 59.2593
	Task-19	val_accuracy: 62.7907	gate_accuracy: 61.6279
	Task-20	val_accuracy: 62.3529	gate_accuracy: 63.5294
	Task-21	val_accuracy: 77.2727	gate_accuracy: 76.1364
	Task-22	val_accuracy: 77.9221	gate_accuracy: 75.3247
	Task-23	val_accuracy: 66.6667	gate_accuracy: 58.3333
	Task-24	val_accuracy: 50.5376	gate_accuracy: 43.0108
	Task-25	val_accuracy: 74.3590	gate_accuracy: 71.7949
	Task-26	val_accuracy: 77.9817	gate_accuracy: 70.6422
	Task-27	val_accuracy: 58.2278	gate_accuracy: 53.1646
	Task-28	val_accuracy: 68.4932	gate_accuracy: 65.7534
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.6642


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 3.5268	step1_train_accuracy: 50.7042
STEP-1	Epoch: 20/50	loss: 1.0697	step1_train_accuracy: 81.3380
STEP-1	Epoch: 30/50	loss: 0.5406	step1_train_accuracy: 93.6620
STEP-1	Epoch: 40/50	loss: 0.3379	step1_train_accuracy: 96.8310
STEP-1	Epoch: 50/50	loss: 0.2490	step1_train_accuracy: 98.5916
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8882	gate_loss: 2.4648	step2_classification_accuracy: 73.2085	step_2_gate_accuracy: 29.9349
STEP-2	Epoch: 40/200	classification_loss: 0.6851	gate_loss: 1.0271	step2_classification_accuracy: 78.8599	step_2_gate_accuracy: 71.6287
STEP-2	Epoch: 60/200	classification_loss: 0.5133	gate_loss: 0.5783	step2_classification_accuracy: 83.9088	step_2_gate_accuracy: 83.1759
STEP-2	Epoch: 80/200	classification_loss: 0.4188	gate_loss: 0.4038	step2_classification_accuracy: 86.9055	step_2_gate_accuracy: 88.1433
STEP-2	Epoch: 100/200	classification_loss: 0.3680	gate_loss: 0.3187	step2_classification_accuracy: 88.2410	step_2_gate_accuracy: 90.7818
STEP-2	Epoch: 120/200	classification_loss: 0.3240	gate_loss: 0.2600	step2_classification_accuracy: 89.4300	step_2_gate_accuracy: 92.4430
STEP-2	Epoch: 140/200	classification_loss: 0.2990	gate_loss: 0.2214	step2_classification_accuracy: 90.4235	step_2_gate_accuracy: 93.4202
STEP-2	Epoch: 160/200	classification_loss: 0.2780	gate_loss: 0.2000	step2_classification_accuracy: 90.4886	step_2_gate_accuracy: 94.1531
STEP-2	Epoch: 180/200	classification_loss: 0.2553	gate_loss: 0.1788	step2_classification_accuracy: 91.0261	step_2_gate_accuracy: 94.5766
STEP-2	Epoch: 200/200	classification_loss: 0.2514	gate_loss: 0.1700	step2_classification_accuracy: 91.4821	step_2_gate_accuracy: 94.7557
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 40.5594	gate_accuracy: 51.7483
	Task-1	val_accuracy: 66.6667	gate_accuracy: 80.0000
	Task-2	val_accuracy: 59.7222	gate_accuracy: 68.0556
	Task-3	val_accuracy: 77.2152	gate_accuracy: 81.0127
	Task-4	val_accuracy: 65.9341	gate_accuracy: 69.2308
	Task-5	val_accuracy: 50.0000	gate_accuracy: 57.3171
	Task-6	val_accuracy: 38.0952	gate_accuracy: 36.5079
	Task-7	val_accuracy: 76.6234	gate_accuracy: 68.8312
	Task-8	val_accuracy: 65.7143	gate_accuracy: 68.5714
	Task-9	val_accuracy: 53.2468	gate_accuracy: 49.3506
	Task-10	val_accuracy: 70.2381	gate_accuracy: 75.0000
	Task-11	val_accuracy: 69.7368	gate_accuracy: 67.1053
	Task-12	val_accuracy: 77.6316	gate_accuracy: 77.6316
	Task-13	val_accuracy: 51.4286	gate_accuracy: 50.0000
	Task-14	val_accuracy: 64.7887	gate_accuracy: 67.6056
	Task-15	val_accuracy: 72.9730	gate_accuracy: 72.9730
	Task-16	val_accuracy: 66.6667	gate_accuracy: 65.4762
	Task-17	val_accuracy: 85.3659	gate_accuracy: 80.4878
	Task-18	val_accuracy: 50.6173	gate_accuracy: 48.1481
	Task-19	val_accuracy: 61.6279	gate_accuracy: 56.9767
	Task-20	val_accuracy: 70.5882	gate_accuracy: 74.1176
	Task-21	val_accuracy: 79.5455	gate_accuracy: 78.4091
	Task-22	val_accuracy: 76.6234	gate_accuracy: 70.1299
	Task-23	val_accuracy: 66.6667	gate_accuracy: 59.5238
	Task-24	val_accuracy: 58.0645	gate_accuracy: 60.2151
	Task-25	val_accuracy: 82.0513	gate_accuracy: 82.0513
	Task-26	val_accuracy: 69.7248	gate_accuracy: 66.9725
	Task-27	val_accuracy: 59.4937	gate_accuracy: 50.6329
	Task-28	val_accuracy: 67.1233	gate_accuracy: 65.7534
	Task-29	val_accuracy: 61.9718	gate_accuracy: 57.7465
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.2333


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 2.4262	step1_train_accuracy: 47.7901
STEP-1	Epoch: 20/50	loss: 0.9083	step1_train_accuracy: 86.4641
STEP-1	Epoch: 30/50	loss: 0.5099	step1_train_accuracy: 92.5414
STEP-1	Epoch: 40/50	loss: 0.3286	step1_train_accuracy: 94.1989
STEP-1	Epoch: 50/50	loss: 0.2393	step1_train_accuracy: 95.5801
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9299	gate_loss: 2.5271	step2_classification_accuracy: 71.9085	step_2_gate_accuracy: 28.5647
STEP-2	Epoch: 40/200	classification_loss: 0.7340	gate_loss: 1.0828	step2_classification_accuracy: 77.3817	step_2_gate_accuracy: 69.3849
STEP-2	Epoch: 60/200	classification_loss: 0.5694	gate_loss: 0.6260	step2_classification_accuracy: 82.3975	step_2_gate_accuracy: 81.6246
STEP-2	Epoch: 80/200	classification_loss: 0.4670	gate_loss: 0.4397	step2_classification_accuracy: 85.2681	step_2_gate_accuracy: 87.0978
STEP-2	Epoch: 100/200	classification_loss: 0.4056	gate_loss: 0.3460	step2_classification_accuracy: 87.3659	step_2_gate_accuracy: 89.4637
STEP-2	Epoch: 120/200	classification_loss: 0.3629	gate_loss: 0.2883	step2_classification_accuracy: 88.1388	step_2_gate_accuracy: 91.2461
STEP-2	Epoch: 140/200	classification_loss: 0.3240	gate_loss: 0.2421	step2_classification_accuracy: 89.2744	step_2_gate_accuracy: 92.4132
STEP-2	Epoch: 160/200	classification_loss: 0.3041	gate_loss: 0.2178	step2_classification_accuracy: 89.8580	step_2_gate_accuracy: 93.3123
STEP-2	Epoch: 180/200	classification_loss: 0.2831	gate_loss: 0.1935	step2_classification_accuracy: 90.3628	step_2_gate_accuracy: 94.1483
STEP-2	Epoch: 200/200	classification_loss: 0.2678	gate_loss: 0.1795	step2_classification_accuracy: 90.6151	step_2_gate_accuracy: 94.3218
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.3497	gate_accuracy: 58.7413
	Task-1	val_accuracy: 66.6667	gate_accuracy: 76.6667
	Task-2	val_accuracy: 65.2778	gate_accuracy: 68.0556
	Task-3	val_accuracy: 73.4177	gate_accuracy: 75.9494
	Task-4	val_accuracy: 62.6374	gate_accuracy: 64.8352
	Task-5	val_accuracy: 51.2195	gate_accuracy: 51.2195
	Task-6	val_accuracy: 46.0317	gate_accuracy: 55.5556
	Task-7	val_accuracy: 79.2208	gate_accuracy: 77.9221
	Task-8	val_accuracy: 64.2857	gate_accuracy: 65.7143
	Task-9	val_accuracy: 58.4416	gate_accuracy: 58.4416
	Task-10	val_accuracy: 67.8571	gate_accuracy: 66.6667
	Task-11	val_accuracy: 59.2105	gate_accuracy: 51.3158
	Task-12	val_accuracy: 68.4211	gate_accuracy: 68.4211
	Task-13	val_accuracy: 57.1429	gate_accuracy: 51.4286
	Task-14	val_accuracy: 69.0141	gate_accuracy: 67.6056
	Task-15	val_accuracy: 71.6216	gate_accuracy: 77.0270
	Task-16	val_accuracy: 65.4762	gate_accuracy: 58.3333
	Task-17	val_accuracy: 79.2683	gate_accuracy: 82.9268
	Task-18	val_accuracy: 59.2593	gate_accuracy: 55.5556
	Task-19	val_accuracy: 67.4419	gate_accuracy: 58.1395
	Task-20	val_accuracy: 65.8824	gate_accuracy: 69.4118
	Task-21	val_accuracy: 77.2727	gate_accuracy: 77.2727
	Task-22	val_accuracy: 67.5325	gate_accuracy: 61.0390
	Task-23	val_accuracy: 69.0476	gate_accuracy: 65.4762
	Task-24	val_accuracy: 55.9140	gate_accuracy: 50.5376
	Task-25	val_accuracy: 70.5128	gate_accuracy: 64.1026
	Task-26	val_accuracy: 71.5596	gate_accuracy: 66.0550
	Task-27	val_accuracy: 56.9620	gate_accuracy: 49.3671
	Task-28	val_accuracy: 68.4932	gate_accuracy: 67.1233
	Task-29	val_accuracy: 61.9718	gate_accuracy: 54.9296
	Task-30	val_accuracy: 73.3333	gate_accuracy: 65.5556
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.9139


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 2.7713	step1_train_accuracy: 41.9672
STEP-1	Epoch: 20/50	loss: 0.9879	step1_train_accuracy: 87.2131
STEP-1	Epoch: 30/50	loss: 0.4866	step1_train_accuracy: 95.7377
STEP-1	Epoch: 40/50	loss: 0.2934	step1_train_accuracy: 98.3607
STEP-1	Epoch: 50/50	loss: 0.2006	step1_train_accuracy: 98.0328
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9145	gate_loss: 2.5201	step2_classification_accuracy: 72.2936	step_2_gate_accuracy: 28.7615
STEP-2	Epoch: 40/200	classification_loss: 0.7223	gate_loss: 1.0672	step2_classification_accuracy: 78.2416	step_2_gate_accuracy: 70.5046
STEP-2	Epoch: 60/200	classification_loss: 0.5625	gate_loss: 0.6216	step2_classification_accuracy: 82.8746	step_2_gate_accuracy: 82.4771
STEP-2	Epoch: 80/200	classification_loss: 0.4490	gate_loss: 0.4322	step2_classification_accuracy: 85.9174	step_2_gate_accuracy: 87.0031
STEP-2	Epoch: 100/200	classification_loss: 0.3909	gate_loss: 0.3314	step2_classification_accuracy: 87.6300	step_2_gate_accuracy: 90.1988
STEP-2	Epoch: 120/200	classification_loss: 0.3515	gate_loss: 0.2835	step2_classification_accuracy: 88.3639	step_2_gate_accuracy: 91.3609
STEP-2	Epoch: 140/200	classification_loss: 0.3224	gate_loss: 0.2460	step2_classification_accuracy: 89.5872	step_2_gate_accuracy: 92.7064
STEP-2	Epoch: 160/200	classification_loss: 0.2913	gate_loss: 0.2099	step2_classification_accuracy: 90.3976	step_2_gate_accuracy: 93.9602
STEP-2	Epoch: 180/200	classification_loss: 0.2674	gate_loss: 0.1873	step2_classification_accuracy: 90.8716	step_2_gate_accuracy: 94.4495
STEP-2	Epoch: 200/200	classification_loss: 0.2538	gate_loss: 0.1716	step2_classification_accuracy: 91.4832	step_2_gate_accuracy: 94.7401
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 47.5524	gate_accuracy: 65.0350
	Task-1	val_accuracy: 67.7778	gate_accuracy: 76.6667
	Task-2	val_accuracy: 55.5556	gate_accuracy: 66.6667
	Task-3	val_accuracy: 69.6203	gate_accuracy: 75.9494
	Task-4	val_accuracy: 69.2308	gate_accuracy: 70.3297
	Task-5	val_accuracy: 50.0000	gate_accuracy: 57.3171
	Task-6	val_accuracy: 44.4444	gate_accuracy: 49.2063
	Task-7	val_accuracy: 75.3247	gate_accuracy: 76.6234
	Task-8	val_accuracy: 57.1429	gate_accuracy: 62.8571
	Task-9	val_accuracy: 63.6364	gate_accuracy: 62.3377
	Task-10	val_accuracy: 69.0476	gate_accuracy: 76.1905
	Task-11	val_accuracy: 65.7895	gate_accuracy: 65.7895
	Task-12	val_accuracy: 71.0526	gate_accuracy: 64.4737
	Task-13	val_accuracy: 62.8571	gate_accuracy: 60.0000
	Task-14	val_accuracy: 70.4225	gate_accuracy: 67.6056
	Task-15	val_accuracy: 72.9730	gate_accuracy: 78.3784
	Task-16	val_accuracy: 59.5238	gate_accuracy: 57.1429
	Task-17	val_accuracy: 78.0488	gate_accuracy: 80.4878
	Task-18	val_accuracy: 56.7901	gate_accuracy: 54.3210
	Task-19	val_accuracy: 61.6279	gate_accuracy: 62.7907
	Task-20	val_accuracy: 69.4118	gate_accuracy: 68.2353
	Task-21	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-22	val_accuracy: 71.4286	gate_accuracy: 62.3377
	Task-23	val_accuracy: 61.9048	gate_accuracy: 52.3810
	Task-24	val_accuracy: 63.4409	gate_accuracy: 59.1398
	Task-25	val_accuracy: 73.0769	gate_accuracy: 70.5128
	Task-26	val_accuracy: 70.6422	gate_accuracy: 64.2202
	Task-27	val_accuracy: 62.0253	gate_accuracy: 55.6962
	Task-28	val_accuracy: 63.0137	gate_accuracy: 60.2740
	Task-29	val_accuracy: 60.5634	gate_accuracy: 56.3380
	Task-30	val_accuracy: 71.1111	gate_accuracy: 65.5556
	Task-31	val_accuracy: 76.3158	gate_accuracy: 68.4211
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.4124


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 2.7671	step1_train_accuracy: 48.8235
STEP-1	Epoch: 20/50	loss: 1.1312	step1_train_accuracy: 74.1176
STEP-1	Epoch: 30/50	loss: 0.6042	step1_train_accuracy: 91.1765
STEP-1	Epoch: 40/50	loss: 0.3948	step1_train_accuracy: 93.5294
STEP-1	Epoch: 50/50	loss: 0.2891	step1_train_accuracy: 95.2941
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9401	gate_loss: 2.6022	step2_classification_accuracy: 71.0979	step_2_gate_accuracy: 26.2760
STEP-2	Epoch: 40/200	classification_loss: 0.7207	gate_loss: 1.0828	step2_classification_accuracy: 78.1602	step_2_gate_accuracy: 68.9763
STEP-2	Epoch: 60/200	classification_loss: 0.5592	gate_loss: 0.6243	step2_classification_accuracy: 82.6558	step_2_gate_accuracy: 81.8991
STEP-2	Epoch: 80/200	classification_loss: 0.4574	gate_loss: 0.4436	step2_classification_accuracy: 85.6973	step_2_gate_accuracy: 86.8991
STEP-2	Epoch: 100/200	classification_loss: 0.3817	gate_loss: 0.3347	step2_classification_accuracy: 87.9228	step_2_gate_accuracy: 90.4748
STEP-2	Epoch: 120/200	classification_loss: 0.3432	gate_loss: 0.2804	step2_classification_accuracy: 88.4273	step_2_gate_accuracy: 91.8249
STEP-2	Epoch: 140/200	classification_loss: 0.3034	gate_loss: 0.2375	step2_classification_accuracy: 89.7478	step_2_gate_accuracy: 92.9080
STEP-2	Epoch: 160/200	classification_loss: 0.2801	gate_loss: 0.2064	step2_classification_accuracy: 90.5786	step_2_gate_accuracy: 93.8576
STEP-2	Epoch: 180/200	classification_loss: 0.2701	gate_loss: 0.1927	step2_classification_accuracy: 90.9644	step_2_gate_accuracy: 93.9763
STEP-2	Epoch: 200/200	classification_loss: 0.2499	gate_loss: 0.1717	step2_classification_accuracy: 91.3205	step_2_gate_accuracy: 94.4362
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 41.9580	gate_accuracy: 55.2448
	Task-1	val_accuracy: 63.3333	gate_accuracy: 70.0000
	Task-2	val_accuracy: 68.0556	gate_accuracy: 73.6111
	Task-3	val_accuracy: 70.8861	gate_accuracy: 75.9494
	Task-4	val_accuracy: 70.3297	gate_accuracy: 72.5275
	Task-5	val_accuracy: 51.2195	gate_accuracy: 52.4390
	Task-6	val_accuracy: 39.6825	gate_accuracy: 46.0317
	Task-7	val_accuracy: 72.7273	gate_accuracy: 68.8312
	Task-8	val_accuracy: 64.2857	gate_accuracy: 68.5714
	Task-9	val_accuracy: 50.6494	gate_accuracy: 50.6494
	Task-10	val_accuracy: 73.8095	gate_accuracy: 77.3810
	Task-11	val_accuracy: 59.2105	gate_accuracy: 53.9474
	Task-12	val_accuracy: 56.5789	gate_accuracy: 55.2632
	Task-13	val_accuracy: 51.4286	gate_accuracy: 51.4286
	Task-14	val_accuracy: 56.3380	gate_accuracy: 53.5211
	Task-15	val_accuracy: 68.9189	gate_accuracy: 75.6757
	Task-16	val_accuracy: 67.8571	gate_accuracy: 65.4762
	Task-17	val_accuracy: 76.8293	gate_accuracy: 75.6098
	Task-18	val_accuracy: 60.4938	gate_accuracy: 51.8519
	Task-19	val_accuracy: 66.2791	gate_accuracy: 63.9535
	Task-20	val_accuracy: 75.2941	gate_accuracy: 68.2353
	Task-21	val_accuracy: 72.7273	gate_accuracy: 72.7273
	Task-22	val_accuracy: 70.1299	gate_accuracy: 58.4416
	Task-23	val_accuracy: 75.0000	gate_accuracy: 66.6667
	Task-24	val_accuracy: 60.2151	gate_accuracy: 49.4624
	Task-25	val_accuracy: 73.0769	gate_accuracy: 66.6667
	Task-26	val_accuracy: 56.8807	gate_accuracy: 47.7064
	Task-27	val_accuracy: 63.2911	gate_accuracy: 60.7595
	Task-28	val_accuracy: 61.6438	gate_accuracy: 61.6438
	Task-29	val_accuracy: 67.6056	gate_accuracy: 66.1972
	Task-30	val_accuracy: 68.8889	gate_accuracy: 61.1111
	Task-31	val_accuracy: 68.4211	gate_accuracy: 67.1053
	Task-32	val_accuracy: 61.1765	gate_accuracy: 57.6471
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.3343


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.3540	step1_train_accuracy: 56.4384
STEP-1	Epoch: 20/50	loss: 0.7729	step1_train_accuracy: 85.2055
STEP-1	Epoch: 30/50	loss: 0.3962	step1_train_accuracy: 95.0685
STEP-1	Epoch: 40/50	loss: 0.2502	step1_train_accuracy: 98.6301
STEP-1	Epoch: 50/50	loss: 0.1935	step1_train_accuracy: 98.6301
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9876	gate_loss: 2.5488	step2_classification_accuracy: 69.9280	step_2_gate_accuracy: 28.3573
STEP-2	Epoch: 40/200	classification_loss: 0.7611	gate_loss: 1.0946	step2_classification_accuracy: 77.0173	step_2_gate_accuracy: 69.0346
STEP-2	Epoch: 60/200	classification_loss: 0.5891	gate_loss: 0.6419	step2_classification_accuracy: 82.2622	step_2_gate_accuracy: 80.2161
STEP-2	Epoch: 80/200	classification_loss: 0.4892	gate_loss: 0.4577	step2_classification_accuracy: 84.8415	step_2_gate_accuracy: 85.3026
STEP-2	Epoch: 100/200	classification_loss: 0.4233	gate_loss: 0.3583	step2_classification_accuracy: 87.0461	step_2_gate_accuracy: 89.1643
STEP-2	Epoch: 120/200	classification_loss: 0.3697	gate_loss: 0.2945	step2_classification_accuracy: 88.2853	step_2_gate_accuracy: 90.8934
STEP-2	Epoch: 140/200	classification_loss: 0.3390	gate_loss: 0.2578	step2_classification_accuracy: 89.1787	step_2_gate_accuracy: 92.2478
STEP-2	Epoch: 160/200	classification_loss: 0.3048	gate_loss: 0.2191	step2_classification_accuracy: 90.0865	step_2_gate_accuracy: 93.1988
STEP-2	Epoch: 180/200	classification_loss: 0.2854	gate_loss: 0.2016	step2_classification_accuracy: 90.6628	step_2_gate_accuracy: 93.8473
STEP-2	Epoch: 200/200	classification_loss: 0.2729	gate_loss: 0.1885	step2_classification_accuracy: 90.7637	step_2_gate_accuracy: 94.1210
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 46.8531	gate_accuracy: 58.7413
	Task-1	val_accuracy: 65.5556	gate_accuracy: 72.2222
	Task-2	val_accuracy: 44.4444	gate_accuracy: 56.9444
	Task-3	val_accuracy: 64.5570	gate_accuracy: 67.0886
	Task-4	val_accuracy: 63.7363	gate_accuracy: 60.4396
	Task-5	val_accuracy: 53.6585	gate_accuracy: 60.9756
	Task-6	val_accuracy: 41.2698	gate_accuracy: 52.3810
	Task-7	val_accuracy: 72.7273	gate_accuracy: 67.5325
	Task-8	val_accuracy: 62.8571	gate_accuracy: 64.2857
	Task-9	val_accuracy: 58.4416	gate_accuracy: 54.5455
	Task-10	val_accuracy: 63.0952	gate_accuracy: 71.4286
	Task-11	val_accuracy: 63.1579	gate_accuracy: 52.6316
	Task-12	val_accuracy: 65.7895	gate_accuracy: 63.1579
	Task-13	val_accuracy: 48.5714	gate_accuracy: 44.2857
	Task-14	val_accuracy: 63.3803	gate_accuracy: 59.1549
	Task-15	val_accuracy: 67.5676	gate_accuracy: 77.0270
	Task-16	val_accuracy: 55.9524	gate_accuracy: 50.0000
	Task-17	val_accuracy: 84.1463	gate_accuracy: 84.1463
	Task-18	val_accuracy: 61.7284	gate_accuracy: 62.9630
	Task-19	val_accuracy: 62.7907	gate_accuracy: 56.9767
	Task-20	val_accuracy: 62.3529	gate_accuracy: 65.8824
	Task-21	val_accuracy: 73.8636	gate_accuracy: 70.4545
	Task-22	val_accuracy: 74.0260	gate_accuracy: 70.1299
	Task-23	val_accuracy: 70.2381	gate_accuracy: 60.7143
	Task-24	val_accuracy: 49.4624	gate_accuracy: 48.3871
	Task-25	val_accuracy: 80.7692	gate_accuracy: 76.9231
	Task-26	val_accuracy: 74.3119	gate_accuracy: 71.5596
	Task-27	val_accuracy: 56.9620	gate_accuracy: 53.1646
	Task-28	val_accuracy: 63.0137	gate_accuracy: 60.2740
	Task-29	val_accuracy: 74.6479	gate_accuracy: 71.8310
	Task-30	val_accuracy: 71.1111	gate_accuracy: 64.4444
	Task-31	val_accuracy: 71.0526	gate_accuracy: 65.7895
	Task-32	val_accuracy: 62.3529	gate_accuracy: 63.5294
	Task-33	val_accuracy: 74.7253	gate_accuracy: 65.9341
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.1991


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 2.3994	step1_train_accuracy: 58.3770
STEP-1	Epoch: 20/50	loss: 0.7961	step1_train_accuracy: 85.3403
STEP-1	Epoch: 30/50	loss: 0.3457	step1_train_accuracy: 95.5497
STEP-1	Epoch: 40/50	loss: 0.2844	step1_train_accuracy: 98.4293
STEP-1	Epoch: 50/50	loss: 0.1490	step1_train_accuracy: 98.6911
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0428	gate_loss: 2.5820	step2_classification_accuracy: 69.0896	step_2_gate_accuracy: 26.8347
STEP-2	Epoch: 40/200	classification_loss: 0.8154	gate_loss: 1.1595	step2_classification_accuracy: 75.7983	step_2_gate_accuracy: 66.4566
STEP-2	Epoch: 60/200	classification_loss: 0.6307	gate_loss: 0.6965	step2_classification_accuracy: 80.5042	step_2_gate_accuracy: 78.8936
STEP-2	Epoch: 80/200	classification_loss: 0.5281	gate_loss: 0.4997	step2_classification_accuracy: 83.9916	step_2_gate_accuracy: 84.4678
STEP-2	Epoch: 100/200	classification_loss: 0.4408	gate_loss: 0.3887	step2_classification_accuracy: 86.6387	step_2_gate_accuracy: 88.2073
STEP-2	Epoch: 120/200	classification_loss: 0.4056	gate_loss: 0.3266	step2_classification_accuracy: 87.3810	step_2_gate_accuracy: 89.7479
STEP-2	Epoch: 140/200	classification_loss: 0.3552	gate_loss: 0.2742	step2_classification_accuracy: 88.7535	step_2_gate_accuracy: 91.6667
STEP-2	Epoch: 160/200	classification_loss: 0.3330	gate_loss: 0.2442	step2_classification_accuracy: 89.5518	step_2_gate_accuracy: 92.1989
STEP-2	Epoch: 180/200	classification_loss: 0.3124	gate_loss: 0.2207	step2_classification_accuracy: 89.7059	step_2_gate_accuracy: 92.9552
STEP-2	Epoch: 200/200	classification_loss: 0.2875	gate_loss: 0.2049	step2_classification_accuracy: 90.5462	step_2_gate_accuracy: 93.4314
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 46.8531	gate_accuracy: 61.5385
	Task-1	val_accuracy: 68.8889	gate_accuracy: 76.6667
	Task-2	val_accuracy: 50.0000	gate_accuracy: 61.1111
	Task-3	val_accuracy: 62.0253	gate_accuracy: 64.5570
	Task-4	val_accuracy: 67.0330	gate_accuracy: 64.8352
	Task-5	val_accuracy: 54.8780	gate_accuracy: 56.0976
	Task-6	val_accuracy: 38.0952	gate_accuracy: 41.2698
	Task-7	val_accuracy: 70.1299	gate_accuracy: 71.4286
	Task-8	val_accuracy: 58.5714	gate_accuracy: 62.8571
	Task-9	val_accuracy: 54.5455	gate_accuracy: 51.9481
	Task-10	val_accuracy: 63.0952	gate_accuracy: 66.6667
	Task-11	val_accuracy: 65.7895	gate_accuracy: 64.4737
	Task-12	val_accuracy: 72.3684	gate_accuracy: 73.6842
	Task-13	val_accuracy: 60.0000	gate_accuracy: 55.7143
	Task-14	val_accuracy: 59.1549	gate_accuracy: 57.7465
	Task-15	val_accuracy: 71.6216	gate_accuracy: 75.6757
	Task-16	val_accuracy: 57.1429	gate_accuracy: 50.0000
	Task-17	val_accuracy: 71.9512	gate_accuracy: 67.0732
	Task-18	val_accuracy: 54.3210	gate_accuracy: 54.3210
	Task-19	val_accuracy: 54.6512	gate_accuracy: 54.6512
	Task-20	val_accuracy: 70.5882	gate_accuracy: 69.4118
	Task-21	val_accuracy: 73.8636	gate_accuracy: 72.7273
	Task-22	val_accuracy: 68.8312	gate_accuracy: 64.9351
	Task-23	val_accuracy: 64.2857	gate_accuracy: 53.5714
	Task-24	val_accuracy: 61.2903	gate_accuracy: 52.6882
	Task-25	val_accuracy: 65.3846	gate_accuracy: 60.2564
	Task-26	val_accuracy: 72.4771	gate_accuracy: 67.8899
	Task-27	val_accuracy: 63.2911	gate_accuracy: 56.9620
	Task-28	val_accuracy: 65.7534	gate_accuracy: 64.3836
	Task-29	val_accuracy: 52.1127	gate_accuracy: 49.2958
	Task-30	val_accuracy: 70.0000	gate_accuracy: 68.8889
	Task-31	val_accuracy: 63.1579	gate_accuracy: 52.6316
	Task-32	val_accuracy: 58.8235	gate_accuracy: 52.9412
	Task-33	val_accuracy: 62.6374	gate_accuracy: 57.1429
	Task-34	val_accuracy: 63.5417	gate_accuracy: 55.2083
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.1092


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.5636	step1_train_accuracy: 49.8516
STEP-1	Epoch: 20/50	loss: 0.8700	step1_train_accuracy: 79.2285
STEP-1	Epoch: 30/50	loss: 0.5126	step1_train_accuracy: 83.9763
STEP-1	Epoch: 40/50	loss: 0.4047	step1_train_accuracy: 88.7240
STEP-1	Epoch: 50/50	loss: 0.3521	step1_train_accuracy: 87.5371
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0365	gate_loss: 2.6172	step2_classification_accuracy: 69.3460	step_2_gate_accuracy: 26.1717
STEP-2	Epoch: 40/200	classification_loss: 0.8289	gate_loss: 1.1328	step2_classification_accuracy: 75.2044	step_2_gate_accuracy: 67.4387
STEP-2	Epoch: 60/200	classification_loss: 0.6479	gate_loss: 0.6871	step2_classification_accuracy: 80.5586	step_2_gate_accuracy: 79.2098
STEP-2	Epoch: 80/200	classification_loss: 0.5389	gate_loss: 0.5071	step2_classification_accuracy: 83.9237	step_2_gate_accuracy: 84.4687
STEP-2	Epoch: 100/200	classification_loss: 0.4711	gate_loss: 0.4023	step2_classification_accuracy: 85.6540	step_2_gate_accuracy: 87.3706
STEP-2	Epoch: 120/200	classification_loss: 0.4176	gate_loss: 0.3372	step2_classification_accuracy: 87.0436	step_2_gate_accuracy: 89.3597
STEP-2	Epoch: 140/200	classification_loss: 0.3694	gate_loss: 0.2863	step2_classification_accuracy: 88.4196	step_2_gate_accuracy: 90.8311
STEP-2	Epoch: 160/200	classification_loss: 0.3477	gate_loss: 0.2576	step2_classification_accuracy: 89.0599	step_2_gate_accuracy: 91.7166
STEP-2	Epoch: 180/200	classification_loss: 0.3218	gate_loss: 0.2330	step2_classification_accuracy: 89.5640	step_2_gate_accuracy: 92.3161
STEP-2	Epoch: 200/200	classification_loss: 0.3055	gate_loss: 0.2130	step2_classification_accuracy: 89.8774	step_2_gate_accuracy: 92.8610
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 46.1538	gate_accuracy: 65.7343
	Task-1	val_accuracy: 63.3333	gate_accuracy: 74.4444
	Task-2	val_accuracy: 54.1667	gate_accuracy: 68.0556
	Task-3	val_accuracy: 68.3544	gate_accuracy: 70.8861
	Task-4	val_accuracy: 65.9341	gate_accuracy: 62.6374
	Task-5	val_accuracy: 48.7805	gate_accuracy: 56.0976
	Task-6	val_accuracy: 31.7460	gate_accuracy: 42.8571
	Task-7	val_accuracy: 75.3247	gate_accuracy: 75.3247
	Task-8	val_accuracy: 68.5714	gate_accuracy: 74.2857
	Task-9	val_accuracy: 54.5455	gate_accuracy: 57.1429
	Task-10	val_accuracy: 71.4286	gate_accuracy: 65.4762
	Task-11	val_accuracy: 61.8421	gate_accuracy: 59.2105
	Task-12	val_accuracy: 60.5263	gate_accuracy: 55.2632
	Task-13	val_accuracy: 58.5714	gate_accuracy: 54.2857
	Task-14	val_accuracy: 61.9718	gate_accuracy: 66.1972
	Task-15	val_accuracy: 60.8108	gate_accuracy: 67.5676
	Task-16	val_accuracy: 61.9048	gate_accuracy: 61.9048
	Task-17	val_accuracy: 79.2683	gate_accuracy: 71.9512
	Task-18	val_accuracy: 65.4321	gate_accuracy: 59.2593
	Task-19	val_accuracy: 60.4651	gate_accuracy: 59.3023
	Task-20	val_accuracy: 65.8824	gate_accuracy: 56.4706
	Task-21	val_accuracy: 78.4091	gate_accuracy: 70.4545
	Task-22	val_accuracy: 62.3377	gate_accuracy: 61.0390
	Task-23	val_accuracy: 64.2857	gate_accuracy: 60.7143
	Task-24	val_accuracy: 55.9140	gate_accuracy: 54.8387
	Task-25	val_accuracy: 71.7949	gate_accuracy: 66.6667
	Task-26	val_accuracy: 70.6422	gate_accuracy: 59.6330
	Task-27	val_accuracy: 54.4304	gate_accuracy: 50.6329
	Task-28	val_accuracy: 56.1644	gate_accuracy: 50.6849
	Task-29	val_accuracy: 54.9296	gate_accuracy: 49.2958
	Task-30	val_accuracy: 67.7778	gate_accuracy: 66.6667
	Task-31	val_accuracy: 73.6842	gate_accuracy: 67.1053
	Task-32	val_accuracy: 64.7059	gate_accuracy: 52.9412
	Task-33	val_accuracy: 64.8352	gate_accuracy: 60.4396
	Task-34	val_accuracy: 68.7500	gate_accuracy: 59.3750
	Task-35	val_accuracy: 61.9048	gate_accuracy: 58.3333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.6672


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 2.6608	step1_train_accuracy: 32.0261
STEP-1	Epoch: 20/50	loss: 1.0342	step1_train_accuracy: 81.6993
STEP-1	Epoch: 30/50	loss: 0.5222	step1_train_accuracy: 93.4641
STEP-1	Epoch: 40/50	loss: 0.3169	step1_train_accuracy: 95.7516
STEP-1	Epoch: 50/50	loss: 0.2158	step1_train_accuracy: 98.3660
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0450	gate_loss: 2.6422	step2_classification_accuracy: 69.0318	step_2_gate_accuracy: 26.1671
STEP-2	Epoch: 40/200	classification_loss: 0.8202	gate_loss: 1.1374	step2_classification_accuracy: 75.7162	step_2_gate_accuracy: 67.2414
STEP-2	Epoch: 60/200	classification_loss: 0.6458	gate_loss: 0.6953	step2_classification_accuracy: 80.6499	step_2_gate_accuracy: 78.8594
STEP-2	Epoch: 80/200	classification_loss: 0.5302	gate_loss: 0.5076	step2_classification_accuracy: 83.8329	step_2_gate_accuracy: 84.0981
STEP-2	Epoch: 100/200	classification_loss: 0.4673	gate_loss: 0.4058	step2_classification_accuracy: 85.5040	step_2_gate_accuracy: 87.3873
STEP-2	Epoch: 120/200	classification_loss: 0.4233	gate_loss: 0.3458	step2_classification_accuracy: 86.7639	step_2_gate_accuracy: 89.0451
STEP-2	Epoch: 140/200	classification_loss: 0.3849	gate_loss: 0.2991	step2_classification_accuracy: 88.1034	step_2_gate_accuracy: 90.5172
STEP-2	Epoch: 160/200	classification_loss: 0.3419	gate_loss: 0.2572	step2_classification_accuracy: 89.0584	step_2_gate_accuracy: 91.9231
STEP-2	Epoch: 180/200	classification_loss: 0.3260	gate_loss: 0.2367	step2_classification_accuracy: 89.3103	step_2_gate_accuracy: 92.6525
STEP-2	Epoch: 200/200	classification_loss: 0.3088	gate_loss: 0.2170	step2_classification_accuracy: 89.6817	step_2_gate_accuracy: 93.3422
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.4476	gate_accuracy: 65.7343
	Task-1	val_accuracy: 71.1111	gate_accuracy: 77.7778
	Task-2	val_accuracy: 55.5556	gate_accuracy: 68.0556
	Task-3	val_accuracy: 73.4177	gate_accuracy: 75.9494
	Task-4	val_accuracy: 67.0330	gate_accuracy: 68.1319
	Task-5	val_accuracy: 48.7805	gate_accuracy: 51.2195
	Task-6	val_accuracy: 41.2698	gate_accuracy: 50.7937
	Task-7	val_accuracy: 71.4286	gate_accuracy: 66.2338
	Task-8	val_accuracy: 52.8571	gate_accuracy: 52.8571
	Task-9	val_accuracy: 59.7403	gate_accuracy: 63.6364
	Task-10	val_accuracy: 60.7143	gate_accuracy: 60.7143
	Task-11	val_accuracy: 64.4737	gate_accuracy: 55.2632
	Task-12	val_accuracy: 64.4737	gate_accuracy: 60.5263
	Task-13	val_accuracy: 55.7143	gate_accuracy: 51.4286
	Task-14	val_accuracy: 60.5634	gate_accuracy: 56.3380
	Task-15	val_accuracy: 77.0270	gate_accuracy: 83.7838
	Task-16	val_accuracy: 51.1905	gate_accuracy: 50.0000
	Task-17	val_accuracy: 73.1707	gate_accuracy: 68.2927
	Task-18	val_accuracy: 62.9630	gate_accuracy: 59.2593
	Task-19	val_accuracy: 65.1163	gate_accuracy: 61.6279
	Task-20	val_accuracy: 60.0000	gate_accuracy: 62.3529
	Task-21	val_accuracy: 84.0909	gate_accuracy: 77.2727
	Task-22	val_accuracy: 77.9221	gate_accuracy: 67.5325
	Task-23	val_accuracy: 71.4286	gate_accuracy: 69.0476
	Task-24	val_accuracy: 60.2151	gate_accuracy: 61.2903
	Task-25	val_accuracy: 70.5128	gate_accuracy: 71.7949
	Task-26	val_accuracy: 78.8991	gate_accuracy: 70.6422
	Task-27	val_accuracy: 58.2278	gate_accuracy: 51.8987
	Task-28	val_accuracy: 71.2329	gate_accuracy: 69.8630
	Task-29	val_accuracy: 53.5211	gate_accuracy: 49.2958
	Task-30	val_accuracy: 70.0000	gate_accuracy: 66.6667
	Task-31	val_accuracy: 69.7368	gate_accuracy: 64.4737
	Task-32	val_accuracy: 65.8824	gate_accuracy: 58.8235
	Task-33	val_accuracy: 65.9341	gate_accuracy: 53.8462
	Task-34	val_accuracy: 53.1250	gate_accuracy: 46.8750
	Task-35	val_accuracy: 63.0952	gate_accuracy: 57.1429
	Task-36	val_accuracy: 67.1053	gate_accuracy: 59.2105
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.5531


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.2081	step1_train_accuracy: 62.9121
STEP-1	Epoch: 20/50	loss: 0.8394	step1_train_accuracy: 85.7143
STEP-1	Epoch: 30/50	loss: 0.4300	step1_train_accuracy: 96.4286
STEP-1	Epoch: 40/50	loss: 0.2488	step1_train_accuracy: 98.0769
STEP-1	Epoch: 50/50	loss: 0.1751	step1_train_accuracy: 98.6264
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0335	gate_loss: 2.6312	step2_classification_accuracy: 69.0956	step_2_gate_accuracy: 28.8630
STEP-2	Epoch: 40/200	classification_loss: 0.8009	gate_loss: 1.1132	step2_classification_accuracy: 76.7571	step_2_gate_accuracy: 68.6563
STEP-2	Epoch: 60/200	classification_loss: 0.6439	gate_loss: 0.6837	step2_classification_accuracy: 81.2016	step_2_gate_accuracy: 79.3928
STEP-2	Epoch: 80/200	classification_loss: 0.5330	gate_loss: 0.4988	step2_classification_accuracy: 84.0568	step_2_gate_accuracy: 84.9871
STEP-2	Epoch: 100/200	classification_loss: 0.4521	gate_loss: 0.4014	step2_classification_accuracy: 85.8915	step_2_gate_accuracy: 87.6227
STEP-2	Epoch: 120/200	classification_loss: 0.4029	gate_loss: 0.3315	step2_classification_accuracy: 87.6227	step_2_gate_accuracy: 89.9742
STEP-2	Epoch: 140/200	classification_loss: 0.3728	gate_loss: 0.2919	step2_classification_accuracy: 88.3204	step_2_gate_accuracy: 91.2403
STEP-2	Epoch: 160/200	classification_loss: 0.3405	gate_loss: 0.2535	step2_classification_accuracy: 89.2636	step_2_gate_accuracy: 92.0930
STEP-2	Epoch: 180/200	classification_loss: 0.3147	gate_loss: 0.2314	step2_classification_accuracy: 89.8708	step_2_gate_accuracy: 92.9716
STEP-2	Epoch: 200/200	classification_loss: 0.2976	gate_loss: 0.2101	step2_classification_accuracy: 90.5168	step_2_gate_accuracy: 93.7339
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.0490	gate_accuracy: 61.5385
	Task-1	val_accuracy: 68.8889	gate_accuracy: 77.7778
	Task-2	val_accuracy: 54.1667	gate_accuracy: 62.5000
	Task-3	val_accuracy: 69.6203	gate_accuracy: 78.4810
	Task-4	val_accuracy: 59.3407	gate_accuracy: 54.9451
	Task-5	val_accuracy: 45.1220	gate_accuracy: 52.4390
	Task-6	val_accuracy: 36.5079	gate_accuracy: 42.8571
	Task-7	val_accuracy: 76.6234	gate_accuracy: 72.7273
	Task-8	val_accuracy: 58.5714	gate_accuracy: 64.2857
	Task-9	val_accuracy: 64.9351	gate_accuracy: 66.2338
	Task-10	val_accuracy: 66.6667	gate_accuracy: 63.0952
	Task-11	val_accuracy: 67.1053	gate_accuracy: 65.7895
	Task-12	val_accuracy: 65.7895	gate_accuracy: 65.7895
	Task-13	val_accuracy: 60.0000	gate_accuracy: 55.7143
	Task-14	val_accuracy: 69.0141	gate_accuracy: 66.1972
	Task-15	val_accuracy: 63.5135	gate_accuracy: 72.9730
	Task-16	val_accuracy: 65.4762	gate_accuracy: 65.4762
	Task-17	val_accuracy: 79.2683	gate_accuracy: 76.8293
	Task-18	val_accuracy: 60.4938	gate_accuracy: 60.4938
	Task-19	val_accuracy: 58.1395	gate_accuracy: 56.9767
	Task-20	val_accuracy: 62.3529	gate_accuracy: 62.3529
	Task-21	val_accuracy: 80.6818	gate_accuracy: 73.8636
	Task-22	val_accuracy: 75.3247	gate_accuracy: 70.1299
	Task-23	val_accuracy: 69.0476	gate_accuracy: 65.4762
	Task-24	val_accuracy: 54.8387	gate_accuracy: 47.3118
	Task-25	val_accuracy: 73.0769	gate_accuracy: 69.2308
	Task-26	val_accuracy: 75.2294	gate_accuracy: 69.7248
	Task-27	val_accuracy: 53.1646	gate_accuracy: 44.3038
	Task-28	val_accuracy: 71.2329	gate_accuracy: 65.7534
	Task-29	val_accuracy: 50.7042	gate_accuracy: 42.2535
	Task-30	val_accuracy: 70.0000	gate_accuracy: 61.1111
	Task-31	val_accuracy: 68.4211	gate_accuracy: 60.5263
	Task-32	val_accuracy: 62.3529	gate_accuracy: 57.6471
	Task-33	val_accuracy: 59.3407	gate_accuracy: 52.7473
	Task-34	val_accuracy: 65.6250	gate_accuracy: 63.5417
	Task-35	val_accuracy: 59.5238	gate_accuracy: 60.7143
	Task-36	val_accuracy: 73.6842	gate_accuracy: 67.1053
	Task-37	val_accuracy: 64.8352	gate_accuracy: 62.6374
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.7140


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 2.9882	step1_train_accuracy: 40.7534
STEP-1	Epoch: 20/50	loss: 1.1412	step1_train_accuracy: 75.3425
STEP-1	Epoch: 30/50	loss: 0.6786	step1_train_accuracy: 87.6712
STEP-1	Epoch: 40/50	loss: 0.4759	step1_train_accuracy: 91.4384
STEP-1	Epoch: 50/50	loss: 0.3781	step1_train_accuracy: 92.4658
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0519	gate_loss: 2.6625	step2_classification_accuracy: 68.7280	step_2_gate_accuracy: 26.3602
STEP-2	Epoch: 40/200	classification_loss: 0.8345	gate_loss: 1.1516	step2_classification_accuracy: 75.4912	step_2_gate_accuracy: 66.6625
STEP-2	Epoch: 60/200	classification_loss: 0.6640	gate_loss: 0.7018	step2_classification_accuracy: 80.2645	step_2_gate_accuracy: 78.7280
STEP-2	Epoch: 80/200	classification_loss: 0.5525	gate_loss: 0.5194	step2_classification_accuracy: 83.4887	step_2_gate_accuracy: 83.9673
STEP-2	Epoch: 100/200	classification_loss: 0.4783	gate_loss: 0.4171	step2_classification_accuracy: 85.4534	step_2_gate_accuracy: 86.8892
STEP-2	Epoch: 120/200	classification_loss: 0.4200	gate_loss: 0.3447	step2_classification_accuracy: 86.9270	step_2_gate_accuracy: 89.3829
STEP-2	Epoch: 140/200	classification_loss: 0.3789	gate_loss: 0.2962	step2_classification_accuracy: 88.1486	step_2_gate_accuracy: 90.8690
STEP-2	Epoch: 160/200	classification_loss: 0.3480	gate_loss: 0.2592	step2_classification_accuracy: 89.2065	step_2_gate_accuracy: 91.8892
STEP-2	Epoch: 180/200	classification_loss: 0.3322	gate_loss: 0.2388	step2_classification_accuracy: 89.3451	step_2_gate_accuracy: 92.3929
STEP-2	Epoch: 200/200	classification_loss: 0.3188	gate_loss: 0.2232	step2_classification_accuracy: 89.9244	step_2_gate_accuracy: 93.0605
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 48.2517	gate_accuracy: 62.9371
	Task-1	val_accuracy: 66.6667	gate_accuracy: 73.3333
	Task-2	val_accuracy: 43.0556	gate_accuracy: 54.1667
	Task-3	val_accuracy: 72.1519	gate_accuracy: 77.2152
	Task-4	val_accuracy: 69.2308	gate_accuracy: 64.8352
	Task-5	val_accuracy: 51.2195	gate_accuracy: 45.1220
	Task-6	val_accuracy: 42.8571	gate_accuracy: 44.4444
	Task-7	val_accuracy: 66.2338	gate_accuracy: 66.2338
	Task-8	val_accuracy: 55.7143	gate_accuracy: 60.0000
	Task-9	val_accuracy: 53.2468	gate_accuracy: 50.6494
	Task-10	val_accuracy: 66.6667	gate_accuracy: 77.3810
	Task-11	val_accuracy: 64.4737	gate_accuracy: 64.4737
	Task-12	val_accuracy: 61.8421	gate_accuracy: 57.8947
	Task-13	val_accuracy: 57.1429	gate_accuracy: 55.7143
	Task-14	val_accuracy: 66.1972	gate_accuracy: 61.9718
	Task-15	val_accuracy: 67.5676	gate_accuracy: 74.3243
	Task-16	val_accuracy: 53.5714	gate_accuracy: 52.3810
	Task-17	val_accuracy: 73.1707	gate_accuracy: 70.7317
	Task-18	val_accuracy: 50.6173	gate_accuracy: 43.2099
	Task-19	val_accuracy: 46.5116	gate_accuracy: 48.8372
	Task-20	val_accuracy: 68.2353	gate_accuracy: 65.8824
	Task-21	val_accuracy: 72.7273	gate_accuracy: 68.1818
	Task-22	val_accuracy: 59.7403	gate_accuracy: 55.8442
	Task-23	val_accuracy: 67.8571	gate_accuracy: 61.9048
	Task-24	val_accuracy: 58.0645	gate_accuracy: 50.5376
	Task-25	val_accuracy: 71.7949	gate_accuracy: 69.2308
	Task-26	val_accuracy: 66.0550	gate_accuracy: 56.8807
	Task-27	val_accuracy: 56.9620	gate_accuracy: 51.8987
	Task-28	val_accuracy: 65.7534	gate_accuracy: 60.2740
	Task-29	val_accuracy: 45.0704	gate_accuracy: 45.0704
	Task-30	val_accuracy: 66.6667	gate_accuracy: 57.7778
	Task-31	val_accuracy: 68.4211	gate_accuracy: 63.1579
	Task-32	val_accuracy: 57.6471	gate_accuracy: 55.2941
	Task-33	val_accuracy: 70.3297	gate_accuracy: 63.7363
	Task-34	val_accuracy: 68.7500	gate_accuracy: 61.4583
	Task-35	val_accuracy: 58.3333	gate_accuracy: 54.7619
	Task-36	val_accuracy: 60.5263	gate_accuracy: 50.0000
	Task-37	val_accuracy: 63.7363	gate_accuracy: 60.4396
	Task-38	val_accuracy: 79.4521	gate_accuracy: 78.0822
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.0558


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 3.1966	step1_train_accuracy: 50.6711
STEP-1	Epoch: 20/50	loss: 1.0485	step1_train_accuracy: 84.2282
STEP-1	Epoch: 30/50	loss: 0.4963	step1_train_accuracy: 91.6107
STEP-1	Epoch: 40/50	loss: 0.3312	step1_train_accuracy: 93.6242
STEP-1	Epoch: 50/50	loss: 0.2476	step1_train_accuracy: 93.9597
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1080	gate_loss: 2.6979	step2_classification_accuracy: 66.8550	step_2_gate_accuracy: 26.0934
STEP-2	Epoch: 40/200	classification_loss: 0.8805	gate_loss: 1.1954	step2_classification_accuracy: 73.8821	step_2_gate_accuracy: 64.9754
STEP-2	Epoch: 60/200	classification_loss: 0.6903	gate_loss: 0.7323	step2_classification_accuracy: 79.3980	step_2_gate_accuracy: 77.8256
STEP-2	Epoch: 80/200	classification_loss: 0.5754	gate_loss: 0.5353	step2_classification_accuracy: 82.7887	step_2_gate_accuracy: 83.5381
STEP-2	Epoch: 100/200	classification_loss: 0.4905	gate_loss: 0.4246	step2_classification_accuracy: 85.0737	step_2_gate_accuracy: 86.5848
STEP-2	Epoch: 120/200	classification_loss: 0.4509	gate_loss: 0.3704	step2_classification_accuracy: 85.7985	step_2_gate_accuracy: 88.4521
STEP-2	Epoch: 140/200	classification_loss: 0.4061	gate_loss: 0.3127	step2_classification_accuracy: 87.2113	step_2_gate_accuracy: 90.3071
STEP-2	Epoch: 160/200	classification_loss: 0.3734	gate_loss: 0.2789	step2_classification_accuracy: 88.1572	step_2_gate_accuracy: 91.2531
STEP-2	Epoch: 180/200	classification_loss: 0.3553	gate_loss: 0.2542	step2_classification_accuracy: 88.4767	step_2_gate_accuracy: 92.0147
STEP-2	Epoch: 200/200	classification_loss: 0.3391	gate_loss: 0.2406	step2_classification_accuracy: 89.2015	step_2_gate_accuracy: 92.5676
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 43.3566	gate_accuracy: 62.2378
	Task-1	val_accuracy: 66.6667	gate_accuracy: 78.8889
	Task-2	val_accuracy: 61.1111	gate_accuracy: 72.2222
	Task-3	val_accuracy: 64.5570	gate_accuracy: 73.4177
	Task-4	val_accuracy: 56.0440	gate_accuracy: 56.0440
	Task-5	val_accuracy: 36.5854	gate_accuracy: 40.2439
	Task-6	val_accuracy: 39.6825	gate_accuracy: 41.2698
	Task-7	val_accuracy: 68.8312	gate_accuracy: 67.5325
	Task-8	val_accuracy: 55.7143	gate_accuracy: 58.5714
	Task-9	val_accuracy: 50.6494	gate_accuracy: 51.9481
	Task-10	val_accuracy: 71.4286	gate_accuracy: 75.0000
	Task-11	val_accuracy: 69.7368	gate_accuracy: 65.7895
	Task-12	val_accuracy: 64.4737	gate_accuracy: 63.1579
	Task-13	val_accuracy: 54.2857	gate_accuracy: 48.5714
	Task-14	val_accuracy: 71.8310	gate_accuracy: 64.7887
	Task-15	val_accuracy: 71.6216	gate_accuracy: 81.0811
	Task-16	val_accuracy: 63.0952	gate_accuracy: 63.0952
	Task-17	val_accuracy: 79.2683	gate_accuracy: 76.8293
	Task-18	val_accuracy: 59.2593	gate_accuracy: 54.3210
	Task-19	val_accuracy: 56.9767	gate_accuracy: 56.9767
	Task-20	val_accuracy: 70.5882	gate_accuracy: 65.8824
	Task-21	val_accuracy: 73.8636	gate_accuracy: 73.8636
	Task-22	val_accuracy: 67.5325	gate_accuracy: 64.9351
	Task-23	val_accuracy: 67.8571	gate_accuracy: 60.7143
	Task-24	val_accuracy: 50.5376	gate_accuracy: 45.1613
	Task-25	val_accuracy: 75.6410	gate_accuracy: 70.5128
	Task-26	val_accuracy: 67.8899	gate_accuracy: 60.5505
	Task-27	val_accuracy: 54.4304	gate_accuracy: 51.8987
	Task-28	val_accuracy: 67.1233	gate_accuracy: 64.3836
	Task-29	val_accuracy: 43.6620	gate_accuracy: 38.0282
	Task-30	val_accuracy: 70.0000	gate_accuracy: 63.3333
	Task-31	val_accuracy: 57.8947	gate_accuracy: 56.5789
	Task-32	val_accuracy: 64.7059	gate_accuracy: 60.0000
	Task-33	val_accuracy: 59.3407	gate_accuracy: 54.9451
	Task-34	val_accuracy: 65.6250	gate_accuracy: 63.5417
	Task-35	val_accuracy: 60.7143	gate_accuracy: 53.5714
	Task-36	val_accuracy: 77.6316	gate_accuracy: 73.6842
	Task-37	val_accuracy: 59.3407	gate_accuracy: 56.0440
	Task-38	val_accuracy: 68.4932	gate_accuracy: 63.0137
	Task-39	val_accuracy: 68.0000	gate_accuracy: 68.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.5990


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 3.4232	step1_train_accuracy: 39.5683
STEP-1	Epoch: 20/50	loss: 1.2791	step1_train_accuracy: 74.4604
STEP-1	Epoch: 30/50	loss: 0.5433	step1_train_accuracy: 98.2014
STEP-1	Epoch: 40/50	loss: 0.3117	step1_train_accuracy: 98.2014
STEP-1	Epoch: 50/50	loss: 0.2030	step1_train_accuracy: 99.2806
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1120	gate_loss: 2.6737	step2_classification_accuracy: 67.0144	step_2_gate_accuracy: 26.8345
STEP-2	Epoch: 40/200	classification_loss: 0.8881	gate_loss: 1.1655	step2_classification_accuracy: 73.7770	step_2_gate_accuracy: 66.0312
STEP-2	Epoch: 60/200	classification_loss: 0.7112	gate_loss: 0.7393	step2_classification_accuracy: 79.0048	step_2_gate_accuracy: 77.3861
STEP-2	Epoch: 80/200	classification_loss: 0.5941	gate_loss: 0.5539	step2_classification_accuracy: 81.5707	step_2_gate_accuracy: 82.6019
STEP-2	Epoch: 100/200	classification_loss: 0.5206	gate_loss: 0.4444	step2_classification_accuracy: 84.3885	step_2_gate_accuracy: 85.9712
STEP-2	Epoch: 120/200	classification_loss: 0.4666	gate_loss: 0.3804	step2_classification_accuracy: 85.5276	step_2_gate_accuracy: 87.9257
STEP-2	Epoch: 140/200	classification_loss: 0.4379	gate_loss: 0.3434	step2_classification_accuracy: 86.3909	step_2_gate_accuracy: 88.9089
STEP-2	Epoch: 160/200	classification_loss: 0.3971	gate_loss: 0.3015	step2_classification_accuracy: 87.4460	step_2_gate_accuracy: 90.7794
STEP-2	Epoch: 180/200	classification_loss: 0.3674	gate_loss: 0.2688	step2_classification_accuracy: 88.1175	step_2_gate_accuracy: 91.4628
STEP-2	Epoch: 200/200	classification_loss: 0.3510	gate_loss: 0.2523	step2_classification_accuracy: 88.7770	step_2_gate_accuracy: 91.5468
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 51.7483	gate_accuracy: 62.9371
	Task-1	val_accuracy: 68.8889	gate_accuracy: 76.6667
	Task-2	val_accuracy: 52.7778	gate_accuracy: 62.5000
	Task-3	val_accuracy: 67.0886	gate_accuracy: 70.8861
	Task-4	val_accuracy: 68.1319	gate_accuracy: 68.1319
	Task-5	val_accuracy: 46.3415	gate_accuracy: 52.4390
	Task-6	val_accuracy: 38.0952	gate_accuracy: 44.4444
	Task-7	val_accuracy: 70.1299	gate_accuracy: 67.5325
	Task-8	val_accuracy: 54.2857	gate_accuracy: 55.7143
	Task-9	val_accuracy: 51.9481	gate_accuracy: 51.9481
	Task-10	val_accuracy: 73.8095	gate_accuracy: 78.5714
	Task-11	val_accuracy: 68.4211	gate_accuracy: 59.2105
	Task-12	val_accuracy: 51.3158	gate_accuracy: 53.9474
	Task-13	val_accuracy: 57.1429	gate_accuracy: 55.7143
	Task-14	val_accuracy: 64.7887	gate_accuracy: 60.5634
	Task-15	val_accuracy: 68.9189	gate_accuracy: 75.6757
	Task-16	val_accuracy: 58.3333	gate_accuracy: 58.3333
	Task-17	val_accuracy: 80.4878	gate_accuracy: 74.3902
	Task-18	val_accuracy: 51.8519	gate_accuracy: 44.4444
	Task-19	val_accuracy: 52.3256	gate_accuracy: 44.1860
	Task-20	val_accuracy: 62.3529	gate_accuracy: 61.1765
	Task-21	val_accuracy: 75.0000	gate_accuracy: 71.5909
	Task-22	val_accuracy: 67.5325	gate_accuracy: 63.6364
	Task-23	val_accuracy: 69.0476	gate_accuracy: 63.0952
	Task-24	val_accuracy: 54.8387	gate_accuracy: 46.2366
	Task-25	val_accuracy: 74.3590	gate_accuracy: 70.5128
	Task-26	val_accuracy: 65.1376	gate_accuracy: 54.1284
	Task-27	val_accuracy: 62.0253	gate_accuracy: 55.6962
	Task-28	val_accuracy: 63.0137	gate_accuracy: 58.9041
	Task-29	val_accuracy: 57.7465	gate_accuracy: 53.5211
	Task-30	val_accuracy: 63.3333	gate_accuracy: 58.8889
	Task-31	val_accuracy: 59.2105	gate_accuracy: 53.9474
	Task-32	val_accuracy: 60.0000	gate_accuracy: 62.3529
	Task-33	val_accuracy: 61.5385	gate_accuracy: 56.0440
	Task-34	val_accuracy: 75.0000	gate_accuracy: 69.7917
	Task-35	val_accuracy: 55.9524	gate_accuracy: 47.6190
	Task-36	val_accuracy: 60.5263	gate_accuracy: 51.3158
	Task-37	val_accuracy: 61.5385	gate_accuracy: 52.7473
	Task-38	val_accuracy: 64.3836	gate_accuracy: 60.2740
	Task-39	val_accuracy: 61.3333	gate_accuracy: 60.0000
	Task-40	val_accuracy: 70.0000	gate_accuracy: 64.2857
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.9941


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 2.2809	step1_train_accuracy: 57.1031
STEP-1	Epoch: 20/50	loss: 0.7078	step1_train_accuracy: 88.8579
STEP-1	Epoch: 30/50	loss: 0.3853	step1_train_accuracy: 98.3287
STEP-1	Epoch: 40/50	loss: 0.2515	step1_train_accuracy: 98.3287
STEP-1	Epoch: 50/50	loss: 0.1796	step1_train_accuracy: 98.3287
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1134	gate_loss: 2.7279	step2_classification_accuracy: 67.0258	step_2_gate_accuracy: 25.4801
STEP-2	Epoch: 40/200	classification_loss: 0.9173	gate_loss: 1.2124	step2_classification_accuracy: 73.2904	step_2_gate_accuracy: 65.3630
STEP-2	Epoch: 60/200	classification_loss: 0.7267	gate_loss: 0.7618	step2_classification_accuracy: 79.1218	step_2_gate_accuracy: 76.6979
STEP-2	Epoch: 80/200	classification_loss: 0.6162	gate_loss: 0.5797	step2_classification_accuracy: 81.9906	step_2_gate_accuracy: 82.1429
STEP-2	Epoch: 100/200	classification_loss: 0.5442	gate_loss: 0.4687	step2_classification_accuracy: 83.6651	step_2_gate_accuracy: 84.9883
STEP-2	Epoch: 120/200	classification_loss: 0.4721	gate_loss: 0.3883	step2_classification_accuracy: 85.5504	step_2_gate_accuracy: 87.6112
STEP-2	Epoch: 140/200	classification_loss: 0.4335	gate_loss: 0.3387	step2_classification_accuracy: 86.3700	step_2_gate_accuracy: 89.5082
STEP-2	Epoch: 160/200	classification_loss: 0.4020	gate_loss: 0.3065	step2_classification_accuracy: 87.2600	step_2_gate_accuracy: 89.9883
STEP-2	Epoch: 180/200	classification_loss: 0.3621	gate_loss: 0.2702	step2_classification_accuracy: 88.4543	step_2_gate_accuracy: 91.4286
STEP-2	Epoch: 200/200	classification_loss: 0.3475	gate_loss: 0.2496	step2_classification_accuracy: 88.8525	step_2_gate_accuracy: 91.7682
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 45.4545	gate_accuracy: 56.6434
	Task-1	val_accuracy: 66.6667	gate_accuracy: 77.7778
	Task-2	val_accuracy: 54.1667	gate_accuracy: 62.5000
	Task-3	val_accuracy: 70.8861	gate_accuracy: 75.9494
	Task-4	val_accuracy: 62.6374	gate_accuracy: 63.7363
	Task-5	val_accuracy: 43.9024	gate_accuracy: 47.5610
	Task-6	val_accuracy: 36.5079	gate_accuracy: 36.5079
	Task-7	val_accuracy: 68.8312	gate_accuracy: 70.1299
	Task-8	val_accuracy: 55.7143	gate_accuracy: 61.4286
	Task-9	val_accuracy: 49.3506	gate_accuracy: 51.9481
	Task-10	val_accuracy: 69.0476	gate_accuracy: 71.4286
	Task-11	val_accuracy: 63.1579	gate_accuracy: 52.6316
	Task-12	val_accuracy: 64.4737	gate_accuracy: 63.1579
	Task-13	val_accuracy: 48.5714	gate_accuracy: 47.1429
	Task-14	val_accuracy: 57.7465	gate_accuracy: 53.5211
	Task-15	val_accuracy: 68.9189	gate_accuracy: 68.9189
	Task-16	val_accuracy: 54.7619	gate_accuracy: 47.6190
	Task-17	val_accuracy: 80.4878	gate_accuracy: 73.1707
	Task-18	val_accuracy: 56.7901	gate_accuracy: 44.4444
	Task-19	val_accuracy: 62.7907	gate_accuracy: 63.9535
	Task-20	val_accuracy: 75.2941	gate_accuracy: 70.5882
	Task-21	val_accuracy: 78.4091	gate_accuracy: 73.8636
	Task-22	val_accuracy: 76.6234	gate_accuracy: 74.0260
	Task-23	val_accuracy: 69.0476	gate_accuracy: 64.2857
	Task-24	val_accuracy: 51.6129	gate_accuracy: 49.4624
	Task-25	val_accuracy: 61.5385	gate_accuracy: 69.2308
	Task-26	val_accuracy: 66.9725	gate_accuracy: 55.9633
	Task-27	val_accuracy: 58.2278	gate_accuracy: 55.6962
	Task-28	val_accuracy: 61.6438	gate_accuracy: 61.6438
	Task-29	val_accuracy: 63.3803	gate_accuracy: 53.5211
	Task-30	val_accuracy: 68.8889	gate_accuracy: 57.7778
	Task-31	val_accuracy: 73.6842	gate_accuracy: 60.5263
	Task-32	val_accuracy: 60.0000	gate_accuracy: 50.5882
	Task-33	val_accuracy: 54.9451	gate_accuracy: 49.4505
	Task-34	val_accuracy: 68.7500	gate_accuracy: 63.5417
	Task-35	val_accuracy: 53.5714	gate_accuracy: 51.1905
	Task-36	val_accuracy: 72.3684	gate_accuracy: 63.1579
	Task-37	val_accuracy: 70.3297	gate_accuracy: 63.7363
	Task-38	val_accuracy: 64.3836	gate_accuracy: 57.5342
	Task-39	val_accuracy: 62.6667	gate_accuracy: 56.0000
	Task-40	val_accuracy: 57.1429	gate_accuracy: 55.7143
	Task-41	val_accuracy: 58.8889	gate_accuracy: 52.2222
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.6187


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 2.3805	step1_train_accuracy: 66.9399
STEP-1	Epoch: 20/50	loss: 0.7553	step1_train_accuracy: 85.7924
STEP-1	Epoch: 30/50	loss: 0.4177	step1_train_accuracy: 95.3552
STEP-1	Epoch: 40/50	loss: 0.2839	step1_train_accuracy: 96.4481
STEP-1	Epoch: 50/50	loss: 0.1896	step1_train_accuracy: 98.0874
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1697	gate_loss: 2.6256	step2_classification_accuracy: 65.5835	step_2_gate_accuracy: 28.4554
STEP-2	Epoch: 40/200	classification_loss: 0.9502	gate_loss: 1.2167	step2_classification_accuracy: 72.9519	step_2_gate_accuracy: 64.7941
STEP-2	Epoch: 60/200	classification_loss: 0.7656	gate_loss: 0.7913	step2_classification_accuracy: 77.8947	step_2_gate_accuracy: 75.6407
STEP-2	Epoch: 80/200	classification_loss: 0.6513	gate_loss: 0.6069	step2_classification_accuracy: 80.8238	step_2_gate_accuracy: 81.2700
STEP-2	Epoch: 100/200	classification_loss: 0.5541	gate_loss: 0.4866	step2_classification_accuracy: 83.3524	step_2_gate_accuracy: 84.4737
STEP-2	Epoch: 120/200	classification_loss: 0.5152	gate_loss: 0.4247	step2_classification_accuracy: 84.4165	step_2_gate_accuracy: 86.3387
STEP-2	Epoch: 140/200	classification_loss: 0.4655	gate_loss: 0.3652	step2_classification_accuracy: 85.7895	step_2_gate_accuracy: 88.1121
STEP-2	Epoch: 160/200	classification_loss: 0.4224	gate_loss: 0.3176	step2_classification_accuracy: 86.6590	step_2_gate_accuracy: 90.0801
STEP-2	Epoch: 180/200	classification_loss: 0.3856	gate_loss: 0.2853	step2_classification_accuracy: 87.9405	step_2_gate_accuracy: 90.8810
STEP-2	Epoch: 200/200	classification_loss: 0.3650	gate_loss: 0.2636	step2_classification_accuracy: 88.3867	step_2_gate_accuracy: 91.7048
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 44.0559	gate_accuracy: 55.2448
	Task-1	val_accuracy: 71.1111	gate_accuracy: 76.6667
	Task-2	val_accuracy: 50.0000	gate_accuracy: 68.0556
	Task-3	val_accuracy: 69.6203	gate_accuracy: 73.4177
	Task-4	val_accuracy: 58.2418	gate_accuracy: 57.1429
	Task-5	val_accuracy: 47.5610	gate_accuracy: 50.0000
	Task-6	val_accuracy: 39.6825	gate_accuracy: 41.2698
	Task-7	val_accuracy: 75.3247	gate_accuracy: 74.0260
	Task-8	val_accuracy: 62.8571	gate_accuracy: 64.2857
	Task-9	val_accuracy: 51.9481	gate_accuracy: 46.7532
	Task-10	val_accuracy: 66.6667	gate_accuracy: 72.6190
	Task-11	val_accuracy: 63.1579	gate_accuracy: 57.8947
	Task-12	val_accuracy: 67.1053	gate_accuracy: 64.4737
	Task-13	val_accuracy: 60.0000	gate_accuracy: 51.4286
	Task-14	val_accuracy: 59.1549	gate_accuracy: 50.7042
	Task-15	val_accuracy: 64.8649	gate_accuracy: 71.6216
	Task-16	val_accuracy: 58.3333	gate_accuracy: 53.5714
	Task-17	val_accuracy: 78.0488	gate_accuracy: 73.1707
	Task-18	val_accuracy: 54.3210	gate_accuracy: 53.0864
	Task-19	val_accuracy: 56.9767	gate_accuracy: 59.3023
	Task-20	val_accuracy: 58.8235	gate_accuracy: 58.8235
	Task-21	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-22	val_accuracy: 68.8312	gate_accuracy: 63.6364
	Task-23	val_accuracy: 65.4762	gate_accuracy: 63.0952
	Task-24	val_accuracy: 52.6882	gate_accuracy: 52.6882
	Task-25	val_accuracy: 69.2308	gate_accuracy: 67.9487
	Task-26	val_accuracy: 55.9633	gate_accuracy: 45.8716
	Task-27	val_accuracy: 59.4937	gate_accuracy: 50.6329
	Task-28	val_accuracy: 68.4932	gate_accuracy: 65.7534
	Task-29	val_accuracy: 57.7465	gate_accuracy: 59.1549
	Task-30	val_accuracy: 68.8889	gate_accuracy: 66.6667
	Task-31	val_accuracy: 65.7895	gate_accuracy: 55.2632
	Task-32	val_accuracy: 64.7059	gate_accuracy: 58.8235
	Task-33	val_accuracy: 71.4286	gate_accuracy: 64.8352
	Task-34	val_accuracy: 56.2500	gate_accuracy: 47.9167
	Task-35	val_accuracy: 54.7619	gate_accuracy: 50.0000
	Task-36	val_accuracy: 75.0000	gate_accuracy: 67.1053
	Task-37	val_accuracy: 67.0330	gate_accuracy: 65.9341
	Task-38	val_accuracy: 76.7123	gate_accuracy: 71.2329
	Task-39	val_accuracy: 66.6667	gate_accuracy: 65.3333
	Task-40	val_accuracy: 64.2857	gate_accuracy: 60.0000
	Task-41	val_accuracy: 66.6667	gate_accuracy: 56.6667
	Task-42	val_accuracy: 61.5385	gate_accuracy: 47.2527
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.1464


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 3.0016	step1_train_accuracy: 49.8371
STEP-1	Epoch: 20/50	loss: 1.1368	step1_train_accuracy: 79.1531
STEP-1	Epoch: 30/50	loss: 0.5787	step1_train_accuracy: 86.6450
STEP-1	Epoch: 40/50	loss: 0.3958	step1_train_accuracy: 92.8339
STEP-1	Epoch: 50/50	loss: 0.2830	step1_train_accuracy: 93.1596
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10, 874: 10, 875: 10, 876: 10, 877: 10, 878: 10, 879: 10, 880: 10, 881: 10, 882: 10, 883: 10, 884: 10, 885: 10, 886: 10, 887: 10, 888: 10, 889: 10, 890: 10, 891: 10, 892: 10, 893: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1846	gate_loss: 2.7298	step2_classification_accuracy: 65.6488	step_2_gate_accuracy: 26.2975
STEP-2	Epoch: 40/200	classification_loss: 0.9477	gate_loss: 1.2307	step2_classification_accuracy: 72.6174	step_2_gate_accuracy: 64.6644
STEP-2	Epoch: 60/200	classification_loss: 0.7616	gate_loss: 0.7962	step2_classification_accuracy: 77.9754	step_2_gate_accuracy: 75.7271
STEP-2	Epoch: 80/200	classification_loss: 0.6343	gate_loss: 0.6002	step2_classification_accuracy: 81.2640	step_2_gate_accuracy: 81.5213
STEP-2	Epoch: 100/200	classification_loss: 0.5591	gate_loss: 0.4887	step2_classification_accuracy: 83.0984	step_2_gate_accuracy: 84.6980
STEP-2	Epoch: 120/200	classification_loss: 0.4923	gate_loss: 0.4104	step2_classification_accuracy: 85.1454	step_2_gate_accuracy: 87.1029
STEP-2	Epoch: 140/200	classification_loss: 0.4516	gate_loss: 0.3578	step2_classification_accuracy: 86.3311	step_2_gate_accuracy: 88.7584
STEP-2	Epoch: 160/200	classification_loss: 0.4118	gate_loss: 0.3180	step2_classification_accuracy: 87.2483	step_2_gate_accuracy: 89.9776
STEP-2	Epoch: 180/200	classification_loss: 0.3835	gate_loss: 0.2849	step2_classification_accuracy: 88.1208	step_2_gate_accuracy: 91.1409
STEP-2	Epoch: 200/200	classification_loss: 0.5392	gate_loss: 0.4525	step2_classification_accuracy: 83.4004	step_2_gate_accuracy: 84.4295
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 44.0559	gate_accuracy: 58.7413
	Task-1	val_accuracy: 64.4444	gate_accuracy: 73.3333
	Task-2	val_accuracy: 45.8333	gate_accuracy: 59.7222
	Task-3	val_accuracy: 59.4937	gate_accuracy: 68.3544
	Task-4	val_accuracy: 63.7363	gate_accuracy: 61.5385
	Task-5	val_accuracy: 45.1220	gate_accuracy: 53.6585
	Task-6	val_accuracy: 33.3333	gate_accuracy: 31.7460
	Task-7	val_accuracy: 64.9351	gate_accuracy: 63.6364
	Task-8	val_accuracy: 44.2857	gate_accuracy: 45.7143
	Task-9	val_accuracy: 54.5455	gate_accuracy: 54.5455
	Task-10	val_accuracy: 64.2857	gate_accuracy: 67.8571
	Task-11	val_accuracy: 67.1053	gate_accuracy: 63.1579
	Task-12	val_accuracy: 63.1579	gate_accuracy: 60.5263
	Task-13	val_accuracy: 47.1429	gate_accuracy: 45.7143
	Task-14	val_accuracy: 59.1549	gate_accuracy: 59.1549
	Task-15	val_accuracy: 72.9730	gate_accuracy: 82.4324
	Task-16	val_accuracy: 65.4762	gate_accuracy: 64.2857
	Task-17	val_accuracy: 74.3902	gate_accuracy: 64.6341
	Task-18	val_accuracy: 58.0247	gate_accuracy: 50.6173
	Task-19	val_accuracy: 52.3256	gate_accuracy: 51.1628
	Task-20	val_accuracy: 58.8235	gate_accuracy: 57.6471
	Task-21	val_accuracy: 76.1364	gate_accuracy: 70.4545
	Task-22	val_accuracy: 63.6364	gate_accuracy: 54.5455
	Task-23	val_accuracy: 67.8571	gate_accuracy: 64.2857
	Task-24	val_accuracy: 51.6129	gate_accuracy: 46.2366
	Task-25	val_accuracy: 61.5385	gate_accuracy: 61.5385
	Task-26	val_accuracy: 74.3119	gate_accuracy: 67.8899
	Task-27	val_accuracy: 56.9620	gate_accuracy: 50.6329
	Task-28	val_accuracy: 61.6438	gate_accuracy: 53.4247
	Task-29	val_accuracy: 49.2958	gate_accuracy: 52.1127
	Task-30	val_accuracy: 64.4444	gate_accuracy: 56.6667
	Task-31	val_accuracy: 56.5789	gate_accuracy: 51.3158
	Task-32	val_accuracy: 64.7059	gate_accuracy: 62.3529
	Task-33	val_accuracy: 58.2418	gate_accuracy: 48.3516
	Task-34	val_accuracy: 56.2500	gate_accuracy: 46.8750
	Task-35	val_accuracy: 55.9524	gate_accuracy: 50.0000
	Task-36	val_accuracy: 69.7368	gate_accuracy: 65.7895
	Task-37	val_accuracy: 67.0330	gate_accuracy: 57.1429
	Task-38	val_accuracy: 68.4932	gate_accuracy: 54.7945
	Task-39	val_accuracy: 73.3333	gate_accuracy: 66.6667
	Task-40	val_accuracy: 68.5714	gate_accuracy: 61.4286
	Task-41	val_accuracy: 65.5556	gate_accuracy: 61.1111
	Task-42	val_accuracy: 64.8352	gate_accuracy: 51.6484
	Task-43	val_accuracy: 59.7403	gate_accuracy: 49.3506
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 57.9890


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 06:37:31	Wall time: 05:49:53
CPU time: 23851.866958619	Wall time: 20993.925819396973
FAA: 73.49411626701828
FF: 36.38568790453516

TRAINER.METRIC.ACCURACY
0: [94.4055944055944]
1: [90.9090909090909, 95.55555555555556]
2: [88.81118881118881, 92.22222222222223, 83.33333333333334]
3: [83.91608391608392, 88.88888888888889, 81.94444444444444, 92.40506329113924]
4: [78.32167832167832, 87.77777777777777, 87.5, 89.87341772151899, 85.71428571428571]
5: [76.22377622377621, 84.44444444444444, 84.72222222222221, 89.87341772151899, 86.81318681318682, 76.82926829268293]
6: [72.02797202797203, 84.44444444444444, 79.16666666666666, 92.40506329113924, 82.41758241758241, 75.60975609756098, 55.55555555555556]
7: [69.23076923076923, 83.33333333333334, 76.38888888888889, 88.60759493670885, 83.51648351648352, 76.82926829268293, 60.317460317460316, 94.8051948051948]
8: [61.53846153846154, 83.33333333333334, 79.16666666666666, 87.34177215189874, 83.51648351648352, 69.51219512195121, 65.07936507936508, 93.5064935064935, 72.85714285714285]
9: [61.53846153846154, 81.11111111111111, 76.38888888888889, 88.60759493670885, 80.21978021978022, 63.41463414634146, 55.55555555555556, 88.31168831168831, 80.0, 83.11688311688312]
10: [62.23776223776224, 81.11111111111111, 73.61111111111111, 86.07594936708861, 82.41758241758241, 68.29268292682927, 52.38095238095239, 90.9090909090909, 75.71428571428571, 70.12987012987013, 77.38095238095238]
11: [58.74125874125874, 80.0, 75.0, 84.81012658227847, 78.02197802197803, 68.29268292682927, 49.2063492063492, 85.71428571428571, 74.28571428571429, 70.12987012987013, 77.38095238095238, 82.89473684210526]
12: [58.74125874125874, 81.11111111111111, 68.05555555555556, 84.81012658227847, 78.02197802197803, 65.85365853658537, 50.79365079365079, 89.6103896103896, 75.71428571428571, 72.72727272727273, 77.38095238095238, 81.57894736842105, 84.21052631578947]
13: [60.83916083916085, 76.66666666666667, 75.0, 82.27848101265823, 80.21978021978022, 67.07317073170732, 50.79365079365079, 87.01298701298701, 75.71428571428571, 75.32467532467533, 75.0, 78.94736842105263, 82.89473684210526, 62.857142857142854]
14: [58.74125874125874, 78.88888888888889, 66.66666666666666, 79.74683544303798, 75.82417582417582, 65.85365853658537, 50.79365079365079, 87.01298701298701, 77.14285714285715, 72.72727272727273, 73.80952380952381, 75.0, 81.57894736842105, 57.14285714285714, 84.50704225352112]
15: [56.64335664335665, 77.77777777777779, 69.44444444444444, 82.27848101265823, 76.92307692307693, 58.536585365853654, 46.03174603174603, 81.81818181818183, 71.42857142857143, 67.53246753246754, 78.57142857142857, 77.63157894736842, 86.8421052631579, 78.57142857142857, 76.05633802816901, 71.62162162162163]
16: [59.44055944055944, 77.77777777777779, 70.83333333333334, 75.9493670886076, 75.82417582417582, 63.41463414634146, 46.03174603174603, 83.11688311688312, 71.42857142857143, 68.83116883116884, 78.57142857142857, 75.0, 82.89473684210526, 70.0, 76.05633802816901, 71.62162162162163, 69.04761904761905]
17: [51.74825174825175, 75.55555555555556, 70.83333333333334, 79.74683544303798, 76.92307692307693, 60.97560975609756, 34.92063492063492, 77.92207792207793, 75.71428571428571, 70.12987012987013, 72.61904761904762, 73.68421052631578, 81.57894736842105, 64.28571428571429, 81.69014084507043, 75.67567567567568, 67.85714285714286, 90.2439024390244]
18: [55.94405594405595, 80.0, 62.5, 82.27848101265823, 74.72527472527473, 58.536585365853654, 44.44444444444444, 85.71428571428571, 74.28571428571429, 63.63636363636363, 76.19047619047619, 73.68421052631578, 78.94736842105263, 67.14285714285714, 73.23943661971832, 75.67567567567568, 69.04761904761905, 86.58536585365853, 67.90123456790124]
19: [51.048951048951054, 75.55555555555556, 65.27777777777779, 73.41772151898735, 72.52747252747253, 51.21951219512195, 47.61904761904761, 83.11688311688312, 70.0, 61.038961038961034, 76.19047619047619, 65.78947368421053, 75.0, 62.857142857142854, 74.64788732394366, 68.91891891891892, 72.61904761904762, 90.2439024390244, 64.19753086419753, 70.93023255813954]
20: [56.64335664335665, 73.33333333333333, 61.111111111111114, 77.21518987341773, 68.13186813186813, 59.756097560975604, 44.44444444444444, 79.22077922077922, 64.28571428571429, 67.53246753246754, 75.0, 68.42105263157895, 73.68421052631578, 60.0, 66.19718309859155, 70.27027027027027, 72.61904761904762, 86.58536585365853, 66.66666666666666, 66.27906976744185, 76.47058823529412]
21: [52.44755244755245, 74.44444444444444, 69.44444444444444, 73.41772151898735, 73.62637362637363, 54.87804878048781, 46.03174603174603, 80.51948051948052, 71.42857142857143, 61.038961038961034, 75.0, 73.68421052631578, 77.63157894736842, 60.0, 71.83098591549296, 66.21621621621621, 67.85714285714286, 86.58536585365853, 61.72839506172839, 67.44186046511628, 77.64705882352942, 81.81818181818183]
22: [53.14685314685315, 76.66666666666667, 58.333333333333336, 78.48101265822784, 74.72527472527473, 62.19512195121951, 39.682539682539684, 81.81818181818183, 64.28571428571429, 61.038961038961034, 70.23809523809523, 72.36842105263158, 72.36842105263158, 54.285714285714285, 76.05633802816901, 72.97297297297297, 66.66666666666666, 85.36585365853658, 71.60493827160494, 62.7906976744186, 82.35294117647058, 73.86363636363636, 71.42857142857143]
23: [51.048951048951054, 77.77777777777779, 59.72222222222222, 67.08860759493672, 73.62637362637363, 62.19512195121951, 33.33333333333333, 77.92207792207793, 68.57142857142857, 57.14285714285714, 71.42857142857143, 67.10526315789474, 77.63157894736842, 58.57142857142858, 60.56338028169014, 71.62162162162163, 71.42857142857143, 85.36585365853658, 64.19753086419753, 69.76744186046511, 75.29411764705883, 72.72727272727273, 70.12987012987013, 73.80952380952381]
24: [53.84615384615385, 75.55555555555556, 59.72222222222222, 69.62025316455697, 69.23076923076923, 52.4390243902439, 44.44444444444444, 84.4155844155844, 62.857142857142854, 63.63636363636363, 72.61904761904762, 76.31578947368422, 76.31578947368422, 54.285714285714285, 64.7887323943662, 64.86486486486487, 63.095238095238095, 82.92682926829268, 64.19753086419753, 62.7906976744186, 76.47058823529412, 77.27272727272727, 75.32467532467533, 66.66666666666666, 59.13978494623656]
25: [50.349650349650354, 70.0, 63.888888888888886, 75.9493670886076, 70.32967032967034, 52.4390243902439, 44.44444444444444, 83.11688311688312, 68.57142857142857, 55.84415584415584, 70.23809523809523, 71.05263157894737, 73.68421052631578, 67.14285714285714, 63.38028169014085, 67.56756756756756, 69.04761904761905, 85.36585365853658, 74.07407407407408, 72.09302325581395, 74.11764705882354, 84.0909090909091, 74.02597402597402, 71.42857142857143, 56.98924731182796, 80.76923076923077]
26: [54.54545454545454, 70.0, 55.55555555555556, 77.21518987341773, 67.03296703296702, 60.97560975609756, 36.507936507936506, 79.22077922077922, 65.71428571428571, 59.74025974025974, 73.80952380952381, 61.8421052631579, 73.68421052631578, 58.57142857142858, 67.6056338028169, 70.27027027027027, 58.333333333333336, 85.36585365853658, 65.4320987654321, 65.11627906976744, 69.41176470588235, 77.27272727272727, 75.32467532467533, 60.71428571428571, 58.06451612903226, 76.92307692307693, 72.47706422018348]
27: [53.84615384615385, 68.88888888888889, 63.888888888888886, 73.41772151898735, 67.03296703296702, 41.46341463414634, 50.79365079365079, 84.4155844155844, 65.71428571428571, 62.33766233766234, 75.0, 65.78947368421053, 77.63157894736842, 62.857142857142854, 67.6056338028169, 68.91891891891892, 61.904761904761905, 75.60975609756098, 59.25925925925925, 67.44186046511628, 63.52941176470588, 78.4090909090909, 70.12987012987013, 67.85714285714286, 61.29032258064516, 75.64102564102564, 69.72477064220183, 50.63291139240506]
28: [48.95104895104895, 63.33333333333333, 61.111111111111114, 72.15189873417721, 68.13186813186813, 53.65853658536586, 34.92063492063492, 77.92207792207793, 71.42857142857143, 57.14285714285714, 71.42857142857143, 75.0, 71.05263157894737, 62.857142857142854, 64.7887323943662, 74.32432432432432, 66.66666666666666, 81.70731707317073, 62.96296296296296, 62.7906976744186, 62.35294117647059, 77.27272727272727, 77.92207792207793, 66.66666666666666, 50.53763440860215, 74.35897435897436, 77.98165137614679, 58.22784810126582, 68.4931506849315]
29: [40.55944055944056, 66.66666666666666, 59.72222222222222, 77.21518987341773, 65.93406593406593, 50.0, 38.095238095238095, 76.62337662337663, 65.71428571428571, 53.246753246753244, 70.23809523809523, 69.73684210526315, 77.63157894736842, 51.42857142857142, 64.7887323943662, 72.97297297297297, 66.66666666666666, 85.36585365853658, 50.617283950617285, 61.627906976744185, 70.58823529411765, 79.54545454545455, 76.62337662337663, 66.66666666666666, 58.06451612903226, 82.05128205128204, 69.72477064220183, 59.49367088607595, 67.12328767123287, 61.97183098591549]
30: [50.349650349650354, 66.66666666666666, 65.27777777777779, 73.41772151898735, 62.637362637362635, 51.21951219512195, 46.03174603174603, 79.22077922077922, 64.28571428571429, 58.44155844155844, 67.85714285714286, 59.210526315789465, 68.42105263157895, 57.14285714285714, 69.01408450704226, 71.62162162162163, 65.47619047619048, 79.26829268292683, 59.25925925925925, 67.44186046511628, 65.88235294117646, 77.27272727272727, 67.53246753246754, 69.04761904761905, 55.91397849462365, 70.51282051282051, 71.55963302752293, 56.9620253164557, 68.4931506849315, 61.97183098591549, 73.33333333333333]
31: [47.55244755244755, 67.77777777777779, 55.55555555555556, 69.62025316455697, 69.23076923076923, 50.0, 44.44444444444444, 75.32467532467533, 57.14285714285714, 63.63636363636363, 69.04761904761905, 65.78947368421053, 71.05263157894737, 62.857142857142854, 70.4225352112676, 72.97297297297297, 59.523809523809526, 78.04878048780488, 56.79012345679012, 61.627906976744185, 69.41176470588235, 75.0, 71.42857142857143, 61.904761904761905, 63.44086021505376, 73.07692307692307, 70.64220183486239, 62.0253164556962, 63.013698630136986, 60.56338028169014, 71.11111111111111, 76.31578947368422]
32: [41.95804195804196, 63.33333333333333, 68.05555555555556, 70.88607594936708, 70.32967032967034, 51.21951219512195, 39.682539682539684, 72.72727272727273, 64.28571428571429, 50.649350649350644, 73.80952380952381, 59.210526315789465, 56.57894736842105, 51.42857142857142, 56.33802816901409, 68.91891891891892, 67.85714285714286, 76.82926829268293, 60.49382716049383, 66.27906976744185, 75.29411764705883, 72.72727272727273, 70.12987012987013, 75.0, 60.215053763440864, 73.07692307692307, 56.88073394495413, 63.29113924050633, 61.64383561643836, 67.6056338028169, 68.88888888888889, 68.42105263157895, 61.1764705882353]
33: [46.85314685314685, 65.55555555555556, 44.44444444444444, 64.55696202531645, 63.73626373626373, 53.65853658536586, 41.269841269841265, 72.72727272727273, 62.857142857142854, 58.44155844155844, 63.095238095238095, 63.1578947368421, 65.78947368421053, 48.57142857142857, 63.38028169014085, 67.56756756756756, 55.952380952380956, 84.14634146341463, 61.72839506172839, 62.7906976744186, 62.35294117647059, 73.86363636363636, 74.02597402597402, 70.23809523809523, 49.46236559139785, 80.76923076923077, 74.31192660550458, 56.9620253164557, 63.013698630136986, 74.64788732394366, 71.11111111111111, 71.05263157894737, 62.35294117647059, 74.72527472527473]
34: [46.85314685314685, 68.88888888888889, 50.0, 62.0253164556962, 67.03296703296702, 54.87804878048781, 38.095238095238095, 70.12987012987013, 58.57142857142858, 54.54545454545454, 63.095238095238095, 65.78947368421053, 72.36842105263158, 60.0, 59.154929577464785, 71.62162162162163, 57.14285714285714, 71.95121951219512, 54.32098765432099, 54.65116279069767, 70.58823529411765, 73.86363636363636, 68.83116883116884, 64.28571428571429, 61.29032258064516, 65.38461538461539, 72.47706422018348, 63.29113924050633, 65.75342465753424, 52.112676056338024, 70.0, 63.1578947368421, 58.82352941176471, 62.637362637362635, 63.541666666666664]
35: [46.15384615384615, 63.33333333333333, 54.166666666666664, 68.35443037974683, 65.93406593406593, 48.78048780487805, 31.746031746031743, 75.32467532467533, 68.57142857142857, 54.54545454545454, 71.42857142857143, 61.8421052631579, 60.526315789473685, 58.57142857142858, 61.97183098591549, 60.810810810810814, 61.904761904761905, 79.26829268292683, 65.4320987654321, 60.46511627906976, 65.88235294117646, 78.4090909090909, 62.33766233766234, 64.28571428571429, 55.91397849462365, 71.7948717948718, 70.64220183486239, 54.43037974683544, 56.16438356164384, 54.929577464788736, 67.77777777777779, 73.68421052631578, 64.70588235294117, 64.83516483516483, 68.75, 61.904761904761905]
36: [52.44755244755245, 71.11111111111111, 55.55555555555556, 73.41772151898735, 67.03296703296702, 48.78048780487805, 41.269841269841265, 71.42857142857143, 52.85714285714286, 59.74025974025974, 60.71428571428571, 64.47368421052632, 64.47368421052632, 55.714285714285715, 60.56338028169014, 77.02702702702703, 51.19047619047619, 73.17073170731707, 62.96296296296296, 65.11627906976744, 60.0, 84.0909090909091, 77.92207792207793, 71.42857142857143, 60.215053763440864, 70.51282051282051, 78.89908256880734, 58.22784810126582, 71.23287671232876, 53.52112676056338, 70.0, 69.73684210526315, 65.88235294117646, 65.93406593406593, 53.125, 63.095238095238095, 67.10526315789474]
37: [51.048951048951054, 68.88888888888889, 54.166666666666664, 69.62025316455697, 59.34065934065934, 45.1219512195122, 36.507936507936506, 76.62337662337663, 58.57142857142858, 64.93506493506493, 66.66666666666666, 67.10526315789474, 65.78947368421053, 60.0, 69.01408450704226, 63.51351351351351, 65.47619047619048, 79.26829268292683, 60.49382716049383, 58.139534883720934, 62.35294117647059, 80.68181818181817, 75.32467532467533, 69.04761904761905, 54.83870967741935, 73.07692307692307, 75.22935779816514, 53.16455696202531, 71.23287671232876, 50.70422535211267, 70.0, 68.42105263157895, 62.35294117647059, 59.34065934065934, 65.625, 59.523809523809526, 73.68421052631578, 64.83516483516483]
38: [48.25174825174825, 66.66666666666666, 43.05555555555556, 72.15189873417721, 69.23076923076923, 51.21951219512195, 42.857142857142854, 66.23376623376623, 55.714285714285715, 53.246753246753244, 66.66666666666666, 64.47368421052632, 61.8421052631579, 57.14285714285714, 66.19718309859155, 67.56756756756756, 53.57142857142857, 73.17073170731707, 50.617283950617285, 46.51162790697674, 68.23529411764706, 72.72727272727273, 59.74025974025974, 67.85714285714286, 58.06451612903226, 71.7948717948718, 66.05504587155964, 56.9620253164557, 65.75342465753424, 45.07042253521127, 66.66666666666666, 68.42105263157895, 57.647058823529406, 70.32967032967034, 68.75, 58.333333333333336, 60.526315789473685, 63.73626373626373, 79.45205479452055]
39: [43.35664335664335, 66.66666666666666, 61.111111111111114, 64.55696202531645, 56.043956043956044, 36.58536585365854, 39.682539682539684, 68.83116883116884, 55.714285714285715, 50.649350649350644, 71.42857142857143, 69.73684210526315, 64.47368421052632, 54.285714285714285, 71.83098591549296, 71.62162162162163, 63.095238095238095, 79.26829268292683, 59.25925925925925, 56.97674418604651, 70.58823529411765, 73.86363636363636, 67.53246753246754, 67.85714285714286, 50.53763440860215, 75.64102564102564, 67.88990825688074, 54.43037974683544, 67.12328767123287, 43.66197183098591, 70.0, 57.89473684210527, 64.70588235294117, 59.34065934065934, 65.625, 60.71428571428571, 77.63157894736842, 59.34065934065934, 68.4931506849315, 68.0]
40: [51.74825174825175, 68.88888888888889, 52.77777777777778, 67.08860759493672, 68.13186813186813, 46.34146341463415, 38.095238095238095, 70.12987012987013, 54.285714285714285, 51.94805194805194, 73.80952380952381, 68.42105263157895, 51.31578947368421, 57.14285714285714, 64.7887323943662, 68.91891891891892, 58.333333333333336, 80.48780487804879, 51.85185185185185, 52.32558139534884, 62.35294117647059, 75.0, 67.53246753246754, 69.04761904761905, 54.83870967741935, 74.35897435897436, 65.13761467889908, 62.0253164556962, 63.013698630136986, 57.74647887323944, 63.33333333333333, 59.210526315789465, 60.0, 61.53846153846154, 75.0, 55.952380952380956, 60.526315789473685, 61.53846153846154, 64.38356164383562, 61.33333333333333, 70.0]
41: [45.45454545454545, 66.66666666666666, 54.166666666666664, 70.88607594936708, 62.637362637362635, 43.90243902439025, 36.507936507936506, 68.83116883116884, 55.714285714285715, 49.35064935064935, 69.04761904761905, 63.1578947368421, 64.47368421052632, 48.57142857142857, 57.74647887323944, 68.91891891891892, 54.761904761904766, 80.48780487804879, 56.79012345679012, 62.7906976744186, 75.29411764705883, 78.4090909090909, 76.62337662337663, 69.04761904761905, 51.61290322580645, 61.53846153846154, 66.97247706422019, 58.22784810126582, 61.64383561643836, 63.38028169014085, 68.88888888888889, 73.68421052631578, 60.0, 54.94505494505495, 68.75, 53.57142857142857, 72.36842105263158, 70.32967032967034, 64.38356164383562, 62.66666666666667, 57.14285714285714, 58.88888888888889]
42: [44.05594405594406, 71.11111111111111, 50.0, 69.62025316455697, 58.24175824175825, 47.5609756097561, 39.682539682539684, 75.32467532467533, 62.857142857142854, 51.94805194805194, 66.66666666666666, 63.1578947368421, 67.10526315789474, 60.0, 59.154929577464785, 64.86486486486487, 58.333333333333336, 78.04878048780488, 54.32098765432099, 56.97674418604651, 58.82352941176471, 75.0, 68.83116883116884, 65.47619047619048, 52.68817204301075, 69.23076923076923, 55.96330275229357, 59.49367088607595, 68.4931506849315, 57.74647887323944, 68.88888888888889, 65.78947368421053, 64.70588235294117, 71.42857142857143, 56.25, 54.761904761904766, 75.0, 67.03296703296702, 76.71232876712328, 66.66666666666666, 64.28571428571429, 66.66666666666666, 61.53846153846154]
43: [44.05594405594406, 64.44444444444444, 45.83333333333333, 59.49367088607595, 63.73626373626373, 45.1219512195122, 33.33333333333333, 64.93506493506493, 44.285714285714285, 54.54545454545454, 64.28571428571429, 67.10526315789474, 63.1578947368421, 47.14285714285714, 59.154929577464785, 72.97297297297297, 65.47619047619048, 74.39024390243902, 58.0246913580247, 52.32558139534884, 58.82352941176471, 76.13636363636364, 63.63636363636363, 67.85714285714286, 51.61290322580645, 61.53846153846154, 74.31192660550458, 56.9620253164557, 61.64383561643836, 49.29577464788733, 64.44444444444444, 56.57894736842105, 64.70588235294117, 58.24175824175825, 56.25, 55.952380952380956, 69.73684210526315, 67.03296703296702, 68.4931506849315, 73.33333333333333, 68.57142857142857, 65.55555555555556, 64.83516483516483, 59.74025974025974]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 43.7500
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 62.1849
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 48.4848
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 64.4860
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 55.0000
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 53.6364
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 46.5909
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 60.9524
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 50.5155
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 51.8868
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 67.5439
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 68.5714
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 64.4231
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 54.6392
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 58.5859
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 70.8738
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 66.0714
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 66.0714
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 59.0909
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 56.5217
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 62.6087
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 72.8814
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 63.2075
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 62.8319
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 54.8387
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 65.4206
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 69.5035
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 50.9259
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 57.4257
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 48.9796
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 58.6777
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 65.7143
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 57.0175
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 47.9339
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 55.5556
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 48.6726
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 73.3333
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 73.5537
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 70.2970
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 63.7255
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 55.6701
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 73.9496
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 65.5738
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 53.3333

====================

f1_score(micro): 59.95525727069351
f1_score(macro): 56.67386997915551
Classification report:
              precision    recall  f1-score   support

           0       1.00      0.00      0.00         4
           1       1.00      0.25      0.40         4
           2       0.80      1.00      0.89         4
           3       1.00      0.75      0.86         4
           4       0.67      0.22      0.33         9
           5       0.75      0.67      0.71         9
           6       1.00      0.00      0.00         4
           7       1.00      0.80      0.89         5
           8       0.60      0.75      0.67         4
           9       1.00      0.56      0.71         9
          10       0.00      0.00      0.00         4
          11       1.00      0.00      0.00         4
          12       1.00      0.89      0.94         9
          13       1.00      0.00      0.00         4
          14       1.00      0.75      0.86         4
          15       1.00      0.00      0.00         4
          16       1.00      0.00      0.00         4
          17       1.00      0.75      0.86         4
          18       1.00      0.50      0.67         4
          19       0.33      0.22      0.27         9
          20       1.00      1.00      1.00         4
          21       1.00      0.78      0.88         9
          22       1.00      0.44      0.62         9
          23       0.83      0.56      0.67         9
          24       1.00      0.25      0.40         4
          25       0.17      0.25      0.20         4
          26       1.00      0.50      0.67         4
          27       0.00      0.00      0.00         9
          28       1.00      0.50      0.67         4
          29       0.75      0.75      0.75         4
          30       0.33      0.33      0.33         9
          31       0.62      0.56      0.59         9
          32       0.00      0.00      0.00         4
          33       1.00      0.25      0.40         4
          34       1.00      0.80      0.89         5
          35       0.80      0.44      0.57         9
          36       0.80      1.00      0.89         4
          37       0.82      1.00      0.90         9
          38       0.67      0.80      0.73         5
          39       1.00      0.50      0.67         4
          40       1.00      0.00      0.00         4
          41       1.00      1.00      1.00         9
          42       0.82      1.00      0.90         9
          43       1.00      0.22      0.36         9
          44       1.00      0.25      0.40         4
          45       0.80      0.80      0.80         5
          46       1.00      0.00      0.00         4
          47       0.75      0.75      0.75         4
          48       0.50      0.20      0.29         5
          49       1.00      0.50      0.67         4
          50       0.75      0.33      0.46         9
          51       0.88      0.78      0.82         9
          52       1.00      0.75      0.86         4
          53       1.00      0.75      0.86         4
          54       0.75      0.75      0.75         4
          55       1.00      0.25      0.40         4
          56       0.75      0.75      0.75         4
          57       1.00      0.00      0.00         4
          58       0.67      0.50      0.57         4
          59       0.00      0.00      0.00         4
          60       1.00      0.50      0.67         4
          61       0.67      0.40      0.50         5
          62       0.67      0.40      0.50         5
          63       0.25      0.25      0.25         4
          64       0.80      1.00      0.89         4
          65       0.00      0.00      0.00         4
          66       0.67      0.50      0.57         4
          67       0.67      0.40      0.50         5
          68       1.00      0.67      0.80         9
          69       0.67      0.40      0.50         5
          70       0.00      0.00      0.00         4
          71       1.00      0.89      0.94         9
          72       1.00      0.56      0.71         9
          73       0.60      0.75      0.67         4
          74       0.40      0.40      0.40         5
          75       0.00      0.00      0.00         4
          76       0.67      0.67      0.67         9
          77       0.90      1.00      0.95         9
          78       0.86      0.67      0.75         9
          79       1.00      0.50      0.67         4
          80       1.00      1.00      1.00         4
          81       1.00      0.50      0.67         4
          82       1.00      0.75      0.86         4
          83       0.60      0.75      0.67         4
          84       0.60      0.67      0.63         9
          85       1.00      0.25      0.40         4
          86       1.00      1.00      1.00         5
          87       1.00      0.50      0.67         4
          88       1.00      0.67      0.80         9
          89       0.75      0.75      0.75         4
          90       1.00      0.75      0.86         4
          91       1.00      0.25      0.40         4
          92       0.80      1.00      0.89         4
          93       1.00      0.25      0.40         4
          94       1.00      0.50      0.67         4
          95       1.00      0.00      0.00         9
          96       1.00      0.78      0.88         9
          97       0.78      0.78      0.78         9
          98       1.00      0.78      0.88         9
          99       0.00      0.00      0.00         9
         100       1.00      1.00      1.00         4
         101       0.75      0.75      0.75         4
         102       0.07      0.11      0.08         9
         103       1.00      0.80      0.89         5
         104       1.00      0.80      0.89         5
         105       1.00      0.75      0.86         4
         106       0.57      0.44      0.50         9
         107       0.11      0.50      0.18         4
         108       0.00      0.00      0.00         4
         109       1.00      0.75      0.86         4
         110       1.00      0.50      0.67         4
         111       1.00      1.00      1.00         5
         112       0.80      0.80      0.80         5
         113       0.50      0.80      0.62         5
         114       0.00      0.00      0.00         4
         115       0.58      0.78      0.67         9
         116       1.00      0.75      0.86         4
         117       0.00      0.00      0.00         4
         118       0.20      0.75      0.32         4
         119       0.62      0.56      0.59         9
         120       1.00      0.89      0.94         9
         121       0.80      1.00      0.89         4
         122       0.75      0.33      0.46         9
         123       0.18      0.50      0.27         4
         124       0.00      0.00      0.00         4
         125       0.80      1.00      0.89         4
         126       0.75      0.75      0.75         4
         127       0.80      1.00      0.89         4
         128       0.00      0.00      0.00         4
         129       0.73      0.89      0.80         9
         130       0.25      0.25      0.25         4
         131       1.00      0.00      0.00         4
         132       0.75      0.75      0.75         4
         133       0.06      0.11      0.08         9
         134       1.00      0.00      0.00         4
         135       1.00      0.50      0.67         4
         136       0.80      1.00      0.89         4
         137       0.00      0.00      0.00         4
         138       0.67      0.40      0.50         5
         139       0.67      1.00      0.80         4
         140       0.18      0.50      0.27         4
         141       0.00      0.00      0.00         4
         142       0.75      0.75      0.75         4
         143       1.00      0.50      0.67         4
         144       0.20      0.11      0.14         9
         145       0.60      0.75      0.67         4
         146       0.75      0.60      0.67         5
         147       0.50      0.50      0.50         4
         148       1.00      0.75      0.86         4
         149       0.50      0.75      0.60         4
         150       0.60      0.60      0.60         5
         151       0.75      0.75      0.75         4
         152       0.20      0.25      0.22         4
         153       0.00      0.00      0.00         4
         154       0.69      1.00      0.82         9
         155       0.40      0.80      0.53         5
         156       0.20      0.11      0.14         9
         157       0.33      0.50      0.40         4
         158       1.00      0.78      0.88         9
         159       1.00      1.00      1.00         5
         160       1.00      0.75      0.86         4
         161       1.00      0.50      0.67         4
         162       1.00      0.75      0.86         4
         163       0.78      0.78      0.78         9
         164       1.00      1.00      1.00         4
         165       1.00      0.75      0.86         4
         166       0.00      0.00      0.00         4
         167       1.00      0.80      0.89         5
         168       0.00      0.00      0.00         5
         169       0.67      0.50      0.57         4
         170       0.50      0.50      0.50         4
         171       0.00      0.00      0.00         4
         172       0.67      0.80      0.73         5
         173       0.67      0.50      0.57         4
         174       0.67      1.00      0.80         4
         175       1.00      0.75      0.86         4
         176       1.00      0.00      0.00         4
         177       0.00      0.00      0.00         9
         178       1.00      0.00      0.00         4
         179       0.60      0.75      0.67         4
         180       0.80      1.00      0.89         4
         181       1.00      0.78      0.88         9
         182       0.00      0.00      0.00         4
         183       0.67      0.50      0.57         4
         184       0.67      0.22      0.33         9
         185       0.00      0.00      0.00         4
         186       0.83      1.00      0.91         5
         187       0.83      1.00      0.91         5
         188       0.17      0.25      0.20         4
         189       0.67      0.50      0.57         4
         190       0.57      1.00      0.73         4
         191       0.50      0.50      0.50         4
         192       1.00      0.50      0.67         4
         193       0.60      0.75      0.67         4
         194       0.00      0.00      0.00         4
         195       0.25      0.50      0.33         4
         196       1.00      0.00      0.00         4
         197       0.38      0.75      0.50         4
         198       0.67      0.40      0.50         5
         199       0.00      0.00      0.00         4
         200       0.40      0.50      0.44         4
         201       0.75      0.75      0.75         4
         202       0.80      1.00      0.89         4
         203       0.67      0.80      0.73         5
         204       0.75      0.33      0.46         9
         205       0.60      0.60      0.60         5
         206       0.67      0.50      0.57         4
         207       1.00      0.00      0.00         9
         208       0.67      0.40      0.50         5
         209       0.67      0.50      0.57         4
         210       0.90      1.00      0.95         9
         211       1.00      0.20      0.33         5
         212       1.00      1.00      1.00         5
         213       0.73      0.89      0.80         9
         214       1.00      1.00      1.00         4
         215       0.00      0.00      0.00         4
         216       1.00      0.60      0.75         5
         217       1.00      0.80      0.89         5
         218       1.00      0.00      0.00         9
         219       0.80      1.00      0.89         4
         220       0.64      0.88      0.74         8
         221       1.00      1.00      1.00         4
         222       1.00      0.50      0.67         4
         223       1.00      1.00      1.00         5
         224       1.00      0.75      0.86         4
         225       0.50      0.11      0.18         9
         226       0.57      1.00      0.73         4
         227       0.40      0.50      0.44         4
         228       1.00      0.75      0.86         4
         229       1.00      0.80      0.89         5
         230       0.82      1.00      0.90         9
         231       1.00      0.78      0.88         9
         232       0.86      0.67      0.75         9
         233       0.83      1.00      0.91         5
         234       1.00      0.75      0.86         4
         235       0.67      1.00      0.80         4
         236       0.75      0.75      0.75         4
         237       0.20      0.25      0.22         4
         238       1.00      0.50      0.67         4
         239       0.67      0.80      0.73         5
         240       0.60      0.75      0.67         4
         241       0.60      0.75      0.67         4
         242       0.67      1.00      0.80         4
         243       0.44      0.89      0.59         9
         244       0.90      1.00      0.95         9
         245       1.00      0.50      0.67         4
         246       0.83      0.62      0.71         8
         247       0.80      1.00      0.89         4
         248       0.00      0.00      0.00         4
         249       0.33      0.44      0.38         9
         250       0.50      0.75      0.60         4
         251       0.60      0.67      0.63         9
         252       0.29      0.50      0.36         4
         253       1.00      0.50      0.67         4
         254       0.82      1.00      0.90         9
         255       0.20      0.40      0.27         5
         256       1.00      0.00      0.00         4
         257       0.58      0.78      0.67         9
         258       1.00      1.00      1.00         4
         259       0.80      1.00      0.89         4
         260       1.00      0.80      0.89         5
         261       1.00      0.50      0.67         4
         262       0.50      0.78      0.61         9
         263       0.75      0.75      0.75         4
         264       0.50      0.50      0.50         4
         265       0.60      0.75      0.67         4
         266       0.40      0.50      0.44         4
         267       0.75      0.60      0.67         5
         268       1.00      0.80      0.89         5
         269       0.00      0.00      0.00         4
         270       1.00      1.00      1.00         4
         271       1.00      0.75      0.86         4
         272       1.00      0.75      0.86         4
         273       0.11      0.11      0.11         9
         274       1.00      1.00      1.00         4
         275       1.00      0.00      0.00         4
         276       0.00      0.00      0.00         4
         277       1.00      0.75      0.86         4
         278       0.75      0.75      0.75         4
         279       0.38      0.75      0.50         4
         280       1.00      0.80      0.89         5
         281       1.00      0.75      0.86         4
         282       1.00      0.00      0.00         4
         283       0.00      0.00      0.00         4
         284       0.50      0.75      0.60         4
         285       0.00      0.00      0.00         4
         286       1.00      0.80      0.89         5
         287       1.00      1.00      1.00         5
         288       1.00      0.50      0.67         4
         289       0.80      0.57      0.67         7
         290       0.67      0.80      0.73         5
         291       0.54      0.78      0.64         9
         292       1.00      0.22      0.36         9
         293       0.50      0.50      0.50         4
         294       0.80      1.00      0.89         4
         295       0.40      0.80      0.53         5
         296       0.00      0.00      0.00         4
         297       0.25      0.25      0.25         4
         298       0.67      0.50      0.57         4
         299       0.25      0.50      0.33         4
         300       1.00      1.00      1.00         9
         301       0.11      0.11      0.11         9
         302       0.80      1.00      0.89         4
         303       0.50      0.33      0.40         9
         304       0.67      0.40      0.50         5
         305       0.50      1.00      0.67         4
         306       1.00      0.75      0.86         4
         307       0.67      1.00      0.80         4
         308       1.00      1.00      1.00         5
         309       0.40      0.50      0.44         4
         310       0.20      0.20      0.20         5
         311       0.75      0.75      0.75         4
         312       0.00      0.00      0.00         4
         313       0.80      1.00      0.89         4
         314       0.25      0.50      0.33         4
         315       0.83      1.00      0.91         5
         316       1.00      1.00      1.00         5
         317       0.50      0.75      0.60         4
         318       0.00      0.00      0.00         4
         319       0.75      1.00      0.86         9
         320       0.80      0.89      0.84         9
         321       0.71      0.56      0.63         9
         322       0.29      0.50      0.36         4
         323       1.00      0.00      0.00         4
         324       0.60      0.75      0.67         4
         325       0.71      1.00      0.83         5
         326       1.00      1.00      1.00         4
         327       0.50      0.50      0.50         4
         328       0.88      0.78      0.82         9
         329       0.80      1.00      0.89         4
         330       0.08      0.25      0.12         4
         331       0.80      1.00      0.89         4
         332       1.00      0.50      0.67         4
         333       0.40      0.50      0.44         4
         334       1.00      0.00      0.00         4
         335       0.33      1.00      0.50         4
         336       0.67      0.89      0.76         9
         337       1.00      0.25      0.40         4
         338       0.00      0.00      0.00         4
         339       1.00      0.75      0.86         4
         340       0.80      1.00      0.89         4
         341       1.00      0.60      0.75         5
         342       0.67      0.89      0.76         9
         343       0.36      0.44      0.40         9
         344       0.54      0.78      0.64         9
         345       0.50      0.50      0.50         4
         346       1.00      0.78      0.88         9
         347       0.50      0.75      0.60         4
         348       0.50      0.25      0.33         4
         349       0.57      1.00      0.73         4
         350       0.80      1.00      0.89         4
         351       0.43      0.60      0.50         5
         352       1.00      0.56      0.71         9
         353       1.00      0.75      0.86         4
         354       1.00      0.75      0.86         4
         355       0.67      0.80      0.73         5
         356       1.00      0.80      0.89         5
         357       0.80      1.00      0.89         4
         358       1.00      0.60      0.75         5
         359       0.80      1.00      0.89         4
         360       0.00      0.00      0.00         4
         361       0.47      0.78      0.58         9
         362       1.00      0.80      0.89         5
         363       1.00      0.75      0.86         4
         364       0.80      0.89      0.84         9
         365       0.50      0.60      0.55         5
         366       1.00      0.67      0.80         9
         367       0.00      0.00      0.00         4
         368       0.75      0.60      0.67         5
         369       0.67      0.50      0.57         4
         370       1.00      0.60      0.75         5
         371       1.00      0.75      0.86         4
         372       0.75      0.33      0.46         9
         373       0.78      0.78      0.78         9
         374       0.07      0.25      0.11         4
         375       0.56      1.00      0.71         5
         376       0.89      0.89      0.89         9
         377       1.00      0.00      0.00         4
         378       0.67      0.80      0.73         5
         379       0.60      0.75      0.67         4
         380       1.00      0.50      0.67         4
         381       0.86      0.67      0.75         9
         382       0.57      0.44      0.50         9
         383       0.80      0.80      0.80         5
         384       0.50      0.50      0.50         4
         385       0.00      0.00      0.00         4
         386       0.67      1.00      0.80         4
         387       0.00      0.00      0.00         4
         388       0.33      0.22      0.27         9
         389       0.16      0.75      0.26         4
         390       0.75      0.60      0.67         5
         391       0.70      0.78      0.74         9
         392       0.33      0.50      0.40         4
         393       0.83      1.00      0.91         5
         394       0.00      0.00      0.00         4
         395       0.90      1.00      0.95         9
         396       0.80      1.00      0.89         4
         397       1.00      0.60      0.75         5
         398       0.50      0.60      0.55         5
         399       0.62      0.89      0.73         9
         400       1.00      0.78      0.88         9
         401       0.60      0.75      0.67         4
         402       1.00      0.75      0.86         4
         403       0.50      0.50      0.50         4
         404       0.50      0.33      0.40         9
         405       1.00      0.00      0.00         4
         406       0.33      0.60      0.43         5
         407       0.50      0.40      0.44         5
         408       0.50      0.25      0.33         4
         409       0.60      0.60      0.60         5
         410       0.00      0.00      0.00         9
         411       0.33      0.75      0.46         4
         412       0.36      0.44      0.40         9
         413       0.57      1.00      0.73         4
         414       0.50      0.25      0.33         4
         415       0.50      1.00      0.67         4
         416       1.00      0.75      0.86         4
         417       0.67      0.80      0.73         5
         418       0.47      0.78      0.58         9
         419       0.50      0.75      0.60         4
         420       0.67      0.50      0.57         4
         421       0.78      0.78      0.78         9
         422       1.00      0.20      0.33         5
         423       0.00      0.00      0.00         4
         424       0.80      0.44      0.57         9
         425       0.50      0.80      0.62         5
         426       0.80      0.80      0.80         5
         427       0.80      1.00      0.89         4
         428       0.67      0.22      0.33         9
         429       0.00      0.00      0.00         4
         430       1.00      1.00      1.00         4
         431       0.71      1.00      0.83         5
         432       1.00      0.56      0.71         9
         433       0.89      0.89      0.89         9
         434       0.58      0.78      0.67         9
         435       0.83      1.00      0.91         5
         436       1.00      0.80      0.89         5
         437       1.00      1.00      1.00         4
         438       0.70      0.78      0.74         9
         439       0.00      0.00      0.00         4
         440       0.78      0.78      0.78         9
         441       0.75      0.75      0.75         4
         442       0.90      1.00      0.95         9
         443       1.00      0.25      0.40         4
         444       0.60      0.75      0.67         4
         445       0.60      0.75      0.67         4
         446       0.80      0.80      0.80         5
         447       1.00      1.00      1.00         4
         448       0.10      0.11      0.11         9
         449       0.33      0.25      0.29         4
         450       1.00      1.00      1.00         4
         451       0.35      0.67      0.46         9
         452       0.69      1.00      0.82         9
         453       1.00      1.00      1.00         4
         454       0.33      0.20      0.25         5
         455       1.00      1.00      1.00         5
         456       0.60      0.75      0.67         4
         457       0.25      0.11      0.15         9
         458       1.00      0.80      0.89         5
         459       1.00      1.00      1.00         9
         460       0.67      0.50      0.57         4
         461       0.62      1.00      0.77         5
         462       0.83      1.00      0.91         5
         463       0.20      0.25      0.22         4
         464       1.00      0.25      0.40         4
         465       0.40      0.50      0.44         4
         466       0.60      0.60      0.60         5
         467       1.00      0.25      0.40         4
         468       1.00      0.89      0.94         9
         469       0.80      1.00      0.89         4
         470       0.82      1.00      0.90         9
         471       0.00      0.00      0.00         4
         472       0.00      0.00      0.00         4
         473       0.75      0.75      0.75         4
         474       0.19      0.33      0.24         9
         475       0.80      0.89      0.84         9
         476       1.00      0.75      0.86         4
         477       0.60      0.75      0.67         4
         478       0.50      0.25      0.33         4
         479       0.80      0.80      0.80         5
         480       1.00      0.00      0.00         4
         481       0.67      0.50      0.57         4
         482       1.00      0.25      0.40         4
         483       0.80      0.89      0.84         9
         484       0.00      0.00      0.00         5
         485       0.67      1.00      0.80         4
         486       0.07      0.11      0.08         9
         487       1.00      0.75      0.86         4
         488       0.50      0.50      0.50         4
         489       0.62      1.00      0.76         8
         490       0.50      0.60      0.55         5
         491       0.60      1.00      0.75         9
         492       0.75      0.75      0.75         4
         493       0.62      1.00      0.77         5
         494       0.80      1.00      0.89         4
         495       0.75      1.00      0.86         9
         496       0.33      0.33      0.33         9
         497       0.00      0.00      0.00         4
         498       0.90      1.00      0.95         9
         499       1.00      0.00      0.00         9
         500       1.00      0.20      0.33         5
         501       0.00      0.00      0.00         4
         502       1.00      0.50      0.67         4
         503       0.50      0.67      0.57         9
         504       0.67      1.00      0.80         4
         505       0.69      1.00      0.82         9
         506       1.00      0.25      0.40         4
         507       0.60      0.60      0.60         5
         508       1.00      0.60      0.75         5
         509       0.50      0.25      0.33         4
         510       1.00      1.00      1.00         4
         511       0.83      1.00      0.91         5
         512       0.07      0.22      0.11         9
         513       0.08      0.22      0.11         9
         514       1.00      1.00      1.00         4
         515       0.75      0.75      0.75         4
         516       0.75      0.75      0.75         4
         517       0.50      0.50      0.50         4
         518       0.67      1.00      0.80         4
         519       0.75      0.75      0.75         4
         520       0.67      1.00      0.80         4
         521       0.50      0.50      0.50         4
         522       0.75      0.33      0.46         9
         523       0.53      0.89      0.67         9
         524       0.67      0.44      0.53         9
         525       0.50      0.75      0.60         4
         526       1.00      0.00      0.00         4
         527       0.71      1.00      0.83         5
         528       0.67      0.44      0.53         9
         529       0.00      0.00      0.00         4
         530       0.40      0.50      0.44         4
         531       1.00      1.00      1.00         4
         532       0.36      0.80      0.50         5
         533       0.89      0.89      0.89         9
         534       0.83      0.56      0.67         9
         535       0.67      0.89      0.76         9
         536       0.80      0.89      0.84         9
         537       0.50      0.44      0.47         9
         538       0.00      0.00      0.00         4
         539       0.31      0.56      0.40         9
         540       0.67      0.50      0.57         4
         541       0.40      0.50      0.44         4
         542       1.00      0.75      0.86         4
         543       0.89      0.89      0.89         9
         544       0.83      0.56      0.67         9
         545       0.90      1.00      0.95         9
         546       0.89      0.89      0.89         9
         547       0.33      0.50      0.40         4
         548       0.67      0.67      0.67         9
         549       0.00      0.00      0.00         4
         550       0.82      1.00      0.90         9
         551       0.53      1.00      0.69         9
         552       0.33      0.25      0.29         4
         553       0.33      0.80      0.47         5
         554       1.00      0.75      0.86         4
         555       0.73      0.89      0.80         9
         556       0.00      0.00      0.00         4
         557       0.00      0.00      0.00         4
         558       0.27      0.33      0.30         9
         559       0.00      0.00      0.00         4
         560       1.00      0.00      0.00         4
         561       0.40      0.50      0.44         4
         562       1.00      0.75      0.86         4
         563       1.00      0.75      0.86         4
         564       0.00      0.00      0.00         5
         565       0.33      0.60      0.43         5
         566       0.80      1.00      0.89         4
         567       0.00      0.00      0.00         4
         568       0.83      1.00      0.91         5
         569       0.78      0.78      0.78         9
         570       0.75      0.67      0.71         9
         571       1.00      0.00      0.00         4
         572       1.00      0.00      0.00         4
         573       0.53      0.89      0.67         9
         574       0.57      0.80      0.67         5
         575       0.60      0.67      0.63         9
         576       1.00      0.50      0.67         4
         577       0.40      0.40      0.40         5
         578       0.60      0.75      0.67         4
         579       0.45      0.56      0.50         9
         580       0.19      0.75      0.30         4
         581       1.00      0.75      0.86         4
         582       1.00      0.00      0.00         4
         583       0.83      1.00      0.91         5
         584       0.50      0.25      0.33         4
         585       0.50      0.40      0.44         5
         586       0.00      0.00      0.00         5
         587       0.50      0.50      0.50         4
         588       0.67      0.89      0.76         9
         589       0.67      0.80      0.73         5
         590       0.60      0.75      0.67         4
         591       1.00      0.50      0.67         4
         592       0.00      0.00      0.00         4
         593       0.60      0.75      0.67         4
         594       0.75      0.75      0.75         4
         595       1.00      0.25      0.40         4
         596       1.00      0.50      0.67         4
         597       0.67      0.40      0.50         5
         598       0.25      0.25      0.25         4
         599       0.50      0.50      0.50         4
         600       0.33      0.25      0.29         4
         601       0.00      0.00      0.00         9
         602       1.00      0.60      0.75         5
         603       1.00      0.75      0.86         4
         604       1.00      0.89      0.94         9
         605       0.57      1.00      0.73         4
         606       1.00      0.25      0.40         4
         607       0.40      0.80      0.53         5
         608       0.75      0.67      0.71         9
         609       0.50      0.75      0.60         4
         610       0.08      0.25      0.12         4
         611       0.75      0.75      0.75         4
         612       1.00      0.00      0.00         4
         613       0.00      0.00      0.00         4
         614       0.25      0.22      0.24         9
         615       0.88      0.78      0.82         9
         616       0.82      1.00      0.90         9
         617       0.00      0.00      0.00         4
         618       0.25      0.20      0.22         5
         619       1.00      1.00      1.00         4
         620       0.00      0.00      0.00         5
         621       0.43      0.43      0.43         7
         622       0.00      0.00      0.00         4
         623       0.57      1.00      0.73         4
         624       0.67      0.80      0.73         5
         625       1.00      0.89      0.94         9
         626       0.60      0.75      0.67         4
         627       0.43      0.75      0.55         4
         628       1.00      0.75      0.86         4
         629       0.33      0.25      0.29         4
         630       1.00      0.78      0.88         9
         631       0.33      0.22      0.27         9
         632       0.09      0.25      0.13         4
         633       0.75      1.00      0.86         9
         634       1.00      1.00      1.00         4
         635       0.75      0.75      0.75         4
         636       0.00      0.00      0.00         4
         637       0.80      0.80      0.80         5
         638       1.00      0.80      0.89         5
         639       0.80      1.00      0.89         4
         640       0.67      1.00      0.80         4
         641       0.75      0.33      0.46         9
         642       0.75      0.67      0.71         9
         643       0.00      0.00      0.00         4
         644       0.00      0.00      0.00         5
         645       0.29      1.00      0.44         4
         646       1.00      0.20      0.33         5
         647       0.62      0.89      0.73         9
         648       1.00      1.00      1.00         5
         649       0.33      0.25      0.29         4
         650       0.67      1.00      0.80         4
         651       1.00      0.78      0.88         9
         652       0.80      1.00      0.89         4
         653       1.00      0.75      0.86         4
         654       0.50      0.25      0.33         4
         655       0.80      1.00      0.89         4
         656       1.00      1.00      1.00         5
         657       0.50      0.50      0.50         4
         658       0.80      0.89      0.84         9
         659       0.12      0.11      0.12         9
         660       0.00      0.00      0.00         4
         661       0.17      0.25      0.20         4
         662       0.50      0.80      0.62         5
         663       0.00      0.00      0.00         4
         664       0.89      0.89      0.89         9
         665       0.67      0.40      0.50         5
         666       0.20      0.75      0.32         4
         667       0.33      0.50      0.40         4
         668       0.31      0.80      0.44         5
         669       0.90      1.00      0.95         9
         670       0.25      0.50      0.33         4
         671       0.00      0.00      0.00         4
         672       0.38      0.56      0.45         9
         673       0.40      0.44      0.42         9
         674       1.00      0.44      0.62         9
         675       0.80      1.00      0.89         4
         676       0.20      0.11      0.14         9
         677       1.00      0.00      0.00         4
         678       0.60      0.75      0.67         4
         679       1.00      0.00      0.00         4
         680       0.62      0.56      0.59         9
         681       0.00      0.00      0.00         4
         682       1.00      0.44      0.62         9
         683       0.50      0.25      0.33         4
         684       0.78      0.78      0.78         9
         685       0.67      0.89      0.76         9
         686       0.40      0.50      0.44         4
         687       0.83      1.00      0.91         5
         688       0.75      0.67      0.71         9
         689       0.57      0.44      0.50         9
         690       0.67      0.50      0.57         4
         691       1.00      0.00      0.00         4
         692       0.00      0.00      0.00         4
         693       1.00      0.50      0.67         4
         694       0.83      0.56      0.67         9
         695       1.00      0.78      0.88         9
         696       0.40      0.50      0.44         4
         697       1.00      1.00      1.00         9
         698       0.67      0.50      0.57         4
         699       1.00      0.50      0.67         4
         700       1.00      0.25      0.40         4
         701       0.40      0.50      0.44         4
         702       0.43      0.33      0.38         9
         703       1.00      0.20      0.33         5
         704       0.67      1.00      0.80         4
         705       0.25      1.00      0.40         4
         706       0.00      0.00      0.00         9
         707       0.00      0.00      0.00         4
         708       1.00      0.67      0.80         9
         709       0.86      0.67      0.75         9
         710       1.00      0.00      0.00         4
         711       1.00      0.67      0.80         9
         712       1.00      0.50      0.67         4
         713       0.62      0.89      0.73         9
         714       0.00      0.00      0.00         4
         715       0.73      0.89      0.80         9
         716       0.00      0.00      0.00         4
         717       1.00      0.50      0.67         4
         718       1.00      0.00      0.00         4
         719       0.50      0.33      0.40         9
         720       1.00      0.50      0.67         4
         721       0.43      0.75      0.55         4
         722       1.00      0.00      0.00         4
         723       0.70      0.78      0.74         9
         724       1.00      1.00      1.00         4
         725       0.67      0.50      0.57         4
         726       0.50      0.56      0.53         9
         727       0.50      0.20      0.29         5
         728       0.75      0.75      0.75         4
         729       1.00      0.00      0.00         4
         730       0.89      0.89      0.89         9
         731       0.00      0.00      0.00         9
         732       0.67      0.40      0.50         5
         733       1.00      1.00      1.00         5
         734       0.80      0.89      0.84         9
         735       0.75      0.60      0.67         5
         736       0.75      0.75      0.75         4
         737       1.00      0.89      0.94         9
         738       0.29      0.50      0.36         4
         739       1.00      1.00      1.00         4
         740       0.67      1.00      0.80         4
         741       0.50      0.33      0.40         9
         742       1.00      1.00      1.00         4
         743       0.00      0.00      0.00         4
         744       1.00      0.75      0.86         4
         745       0.80      1.00      0.89         4
         746       1.00      0.50      0.67         4
         747       0.57      0.80      0.67         5
         748       1.00      0.60      0.75         5
         749       0.43      0.60      0.50         5
         750       1.00      1.00      1.00         5
         751       0.75      1.00      0.86         9
         752       1.00      1.00      1.00         4
         753       0.06      0.25      0.10         4
         754       0.90      1.00      0.95         9
         755       0.67      0.50      0.57         4
         756       0.83      0.56      0.67         9
         757       0.00      0.00      0.00         4
         758       0.78      0.78      0.78         9
         759       0.78      0.78      0.78         9
         760       0.44      1.00      0.62         4
         761       0.60      0.67      0.63         9
         762       0.60      0.75      0.67         4
         763       1.00      0.75      0.86         4
         764       0.10      0.22      0.14         9
         765       0.80      1.00      0.89         4
         766       1.00      0.75      0.86         4
         767       0.50      0.75      0.60         4
         768       1.00      1.00      1.00         4
         769       0.33      0.25      0.29         4
         770       0.73      0.89      0.80         9
         771       0.42      1.00      0.59         5
         772       0.67      1.00      0.80         4
         773       0.56      1.00      0.72         9
         774       0.50      0.50      0.50         4
         775       1.00      1.00      1.00         4
         776       0.80      1.00      0.89         4
         777       0.50      0.75      0.60         4
         778       1.00      0.75      0.86         4
         779       0.60      0.60      0.60         5
         780       0.83      1.00      0.91         5
         781       1.00      0.89      0.94         9
         782       0.67      0.89      0.76         9
         783       0.71      1.00      0.83         5
         784       0.33      0.25      0.29         4
         785       0.80      1.00      0.89         4
         786       1.00      0.00      0.00         4
         787       1.00      0.00      0.00         4
         788       1.00      0.50      0.67         4
         789       0.50      0.50      0.50         4
         790       0.80      1.00      0.89         4
         791       0.40      0.50      0.44         4
         792       0.89      0.89      0.89         9
         793       0.75      0.43      0.55         7
         794       1.00      0.75      0.86         4
         795       0.75      0.75      0.75         4
         796       0.17      1.00      0.29         4
         797       0.33      0.20      0.25         5
         798       0.67      1.00      0.80         4
         799       0.00      0.00      0.00         4
         800       0.47      0.89      0.62         9
         801       0.57      0.80      0.67         5
         802       1.00      1.00      1.00         4
         803       0.89      0.89      0.89         9
         804       0.00      0.00      0.00         4
         805       0.00      0.00      0.00         4
         806       0.57      1.00      0.73         4
         807       1.00      0.25      0.40         4
         808       1.00      1.00      1.00         9
         809       0.78      0.78      0.78         9
         810       1.00      0.25      0.40         4
         811       1.00      1.00      1.00         4
         812       0.00      0.00      0.00         4
         813       1.00      0.00      0.00         4
         814       1.00      0.75      0.86         4
         815       1.00      0.75      0.86         4
         816       0.80      0.89      0.84         9
         817       0.00      0.00      0.00         4
         818       0.50      0.20      0.29         5
         819       1.00      0.50      0.67         4
         820       0.80      0.89      0.84         9
         821       1.00      1.00      1.00         4
         822       0.75      0.75      0.75         4
         823       0.75      0.75      0.75         4
         824       1.00      1.00      1.00         4
         825       1.00      0.00      0.00         4
         826       0.00      0.00      0.00         4
         827       1.00      0.00      0.00         4
         828       0.50      0.50      0.50         4
         829       0.50      0.25      0.33         4
         830       0.25      0.25      0.25         4
         831       1.00      0.44      0.62         9
         832       0.80      0.80      0.80         5
         833       0.60      0.75      0.67         4
         834       0.00      0.00      0.00         4
         835       0.50      1.00      0.67         9
         836       1.00      0.00      0.00         4
         837       0.80      1.00      0.89         4
         838       0.80      1.00      0.89         4
         839       0.67      0.89      0.76         9
         840       0.40      0.44      0.42         9
         841       0.75      0.75      0.75         4
         842       0.67      0.44      0.53         9
         843       0.44      1.00      0.62         4
         844       0.78      0.78      0.78         9
         845       0.75      1.00      0.86         6
         846       0.64      1.00      0.78         9
         847       0.83      1.00      0.91         5
         848       0.22      0.50      0.31         4
         849       0.00      0.00      0.00         4
         850       0.33      0.60      0.43         5
         851       1.00      0.75      0.86         4
         852       0.69      1.00      0.82         9
         853       1.00      1.00      1.00         4
         854       0.80      1.00      0.89         4
         855       0.71      0.56      0.63         9
         856       1.00      1.00      1.00         4
         857       0.40      0.50      0.44         4
         858       1.00      0.20      0.33         5
         859       1.00      1.00      1.00         4
         860       0.78      0.78      0.78         9
         861       0.64      0.78      0.70         9
         862       0.57      0.89      0.70         9
         863       1.00      1.00      1.00         4
         864       0.56      0.56      0.56         9
         865       0.00      0.00      0.00         4
         866       0.75      0.33      0.46         9
         867       0.50      0.50      0.50         4
         868       0.56      0.56      0.56         9
         869       1.00      0.00      0.00         4
         870       0.60      1.00      0.75         9
         871       0.25      0.50      0.33         4
         872       0.80      0.80      0.80         5
         873       1.00      1.00      1.00         4
         874       0.00      0.00      0.00         4
         875       0.00      0.00      0.00         4
         876       1.00      0.75      0.86         4
         877       0.89      0.89      0.89         9
         878       1.00      0.50      0.67         4
         879       0.60      0.75      0.67         4
         880       1.00      0.40      0.57         5
         881       0.33      0.11      0.17         9
         882       1.00      0.50      0.67         4
         883       0.00      0.00      0.00         4
         884       0.67      0.80      0.73         5
         885       0.71      1.00      0.83         5
         886       0.75      0.33      0.46         9
         887       0.33      0.25      0.29         4
         888       1.00      0.00      0.00         4
         889       1.00      0.50      0.67         4
         890       0.57      0.89      0.70         9
         891       0.75      0.60      0.67         5
         892       0.80      1.00      0.89         4
         893       0.62      1.00      0.77         5

    accuracy                           0.60      4917
   macro avg       0.66      0.58      0.57      4917
weighted avg       0.67      0.60      0.59      4917

