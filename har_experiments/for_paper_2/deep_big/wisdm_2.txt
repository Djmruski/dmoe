CLASS_ORDER: [373, 871, 521, 740, 287, 300, 536, 621, 15, 510, 611, 438, 264, 785, 217, 820, 634, 691, 638, 328, 693, 708, 869, 519, 735, 593, 62, 649, 586, 346, 89, 24, 580, 801, 620, 589, 551, 448, 607, 504, 449, 319, 572, 542, 872, 633, 748, 144, 191, 827, 250, 257, 74, 484, 799, 44, 291, 838, 193, 718, 696, 16, 223, 641, 434, 508, 119, 146, 209, 271, 360, 585, 766, 506, 255, 533, 141, 60, 117, 125, 124, 334, 717, 721, 100, 128, 584, 791, 288, 429, 149, 594, 662, 851, 769, 276, 169, 375, 218, 12, 564, 483, 728, 565, 836, 601, 497, 26, 68, 458, 732, 645, 432, 706, 235, 729, 865, 178, 802, 301, 499, 516, 393, 482, 265, 252, 403, 531, 35, 350, 267, 783, 843, 710, 236, 421, 417, 202, 107, 529, 8, 628, 616, 560, 552, 863, 579, 227, 698, 231, 724, 639, 471, 741, 170, 304, 230, 540, 312, 685, 332, 703, 151, 550, 55, 830, 160, 45, 70, 3, 20, 372, 92, 385, 427, 673, 757, 25, 454, 868, 358, 94, 481, 367, 595, 303, 249, 739, 112, 313, 494, 6, 366, 134, 537, 344, 63, 205, 509, 88, 534, 666, 343, 463, 849, 222, 539, 754, 790, 281, 127, 381, 201, 196, 450, 186, 48, 517, 177, 299, 275, 316, 370, 320, 624, 21, 665, 686, 622, 401, 49, 175, 669, 431, 58, 190, 547, 111, 204, 485, 695, 365, 520, 455, 289, 543, 331, 687, 237, 699, 544, 818, 386, 31, 891, 374, 79, 640, 258, 553, 602, 884, 500, 215, 747, 667, 7, 278, 102, 183, 692, 194, 682, 880, 548, 207, 228, 713, 168, 874, 433, 636, 844, 478, 378, 262, 726, 752, 410, 81, 581, 389, 719, 336, 285, 189, 405, 200, 465, 447, 269, 329, 804, 505, 154, 623, 76, 37, 889, 476, 773, 380, 234, 150, 306, 715, 83, 71, 486, 817, 803, 302, 672, 858, 779, 123, 451, 0, 677, 852, 310, 284, 496, 837, 538, 755, 805, 826, 604, 704, 888, 850, 382, 171, 414, 166, 221, 566, 842, 309, 30, 226, 198, 398, 711, 139, 541, 532, 82, 142, 182, 337, 603, 556, 172, 210, 219, 261, 587, 75, 576, 763, 138, 423, 835, 277, 188, 126, 32, 395, 163, 679, 80, 885, 105, 379, 810, 29, 786, 848, 283, 630, 294, 131, 661, 213, 57, 767, 441, 10, 104, 369, 292, 305, 615, 770, 442, 578, 430, 700, 266, 460, 50, 590, 861, 140, 839, 599, 524, 746, 153, 495, 846, 709, 825, 657, 376, 822, 561, 445, 644, 197, 474, 743, 480, 887, 796, 866, 396, 9, 753, 816, 84, 388, 461, 293, 91, 43, 730, 253, 133, 322, 428, 612, 66, 758, 46, 756, 777, 875, 259, 760, 280, 492, 655, 648, 582, 811, 394, 670, 631, 377, 42, 443, 148, 426, 570, 17, 629, 341, 73, 650, 167, 314, 654, 477, 242, 203, 353, 681, 646, 412, 408, 523, 557, 406, 558, 618, 368, 437, 65, 241, 290, 195, 663, 598, 511, 833, 51, 311, 632, 652, 828, 702, 38, 206, 295, 400, 774, 487, 567, 452, 419, 841, 176, 793, 425, 179, 371, 239, 653, 282, 893, 315, 77, 27, 864, 108, 444, 787, 248, 404, 792, 22, 143, 224, 180, 725, 106, 158, 347, 860, 467, 136, 815, 697, 52, 418, 518, 720, 211, 115, 469, 800, 173, 867, 321, 854, 472, 85, 470, 479, 14, 110, 244, 391, 525, 734, 270, 325, 647, 546, 610, 93, 502, 390, 613, 712, 39, 588, 78, 59, 765, 668, 751, 847, 789, 5, 272, 69, 626, 831, 857, 881, 47, 882, 362, 807, 97, 298, 296, 549, 664, 18, 130, 821, 13, 812, 152, 212, 330, 407, 493, 503, 363, 761, 707, 348, 813, 768, 466, 515, 642, 308, 317, 354, 147, 392, 64, 121, 876, 98, 187, 705, 883, 744, 157, 806, 165, 216, 824, 274, 457, 489, 823, 737, 462, 383, 535, 225, 364, 245, 129, 240, 878, 877, 41, 870, 214, 879, 422, 297, 559, 778, 413, 192, 135, 61, 137, 243, 263, 527, 886, 555, 436, 736, 545, 862, 859, 656, 738, 733, 676, 99, 164, 116, 498, 251, 11, 762, 352, 675, 355, 617, 435, 749, 120, 36, 625, 1, 109, 90, 349, 683, 609, 229, 659, 411, 577, 814, 528, 788, 34, 855, 415, 456, 345, 596, 782, 562, 96, 569, 273, 651, 40, 688, 174, 19, 464, 608, 440, 742, 507, 72, 409, 185, 808, 361, 181, 873, 359, 256, 771, 424, 583, 184, 781, 635, 750, 439, 614, 420, 246, 714, 627, 597, 338, 155, 571, 513, 387, 797, 118, 491, 351, 324, 514, 671, 220, 208, 161, 600, 809, 145, 784, 845, 488, 327, 453, 475, 162, 53, 384, 95, 764, 87, 591, 260, 122, 33, 684, 356, 342, 716, 114, 723, 113, 238, 592, 490, 103, 501, 4, 660, 333, 554, 568, 856, 678, 643, 853, 56, 268, 23, 526, 67, 680, 357, 759, 780, 892, 798, 731, 795, 606, 619, 745, 473, 323, 727, 722, 318, 690, 459, 132, 840, 2, 233, 397, 832, 890, 340, 530, 54, 335, 307, 468, 794, 247, 232, 512, 819, 605, 28, 446, 689, 574, 254, 775, 834, 674, 399, 701, 199, 326, 637, 522, 402, 776, 339, 86, 658, 573, 286, 829, 772, 159, 279, 156, 694, 416, 575, 101, 563]
class_group: [(373, 871, 521, 740, 287, 300, 536, 621, 15, 510, 611, 438, 264, 785, 217, 820, 634, 691, 638, 328, 693, 708, 869, 519, 735, 593, 62, 649, 586, 346, 89, 24, 580, 801), (620, 589, 551, 448, 607, 504, 449, 319, 572, 542, 872, 633, 748, 144, 191, 827, 250, 257, 74, 484), (799, 44, 291, 838, 193, 718, 696, 16, 223, 641, 434, 508, 119, 146, 209, 271, 360, 585, 766, 506), (255, 533, 141, 60, 117, 125, 124, 334, 717, 721, 100, 128, 584, 791, 288, 429, 149, 594, 662, 851), (769, 276, 169, 375, 218, 12, 564, 483, 728, 565, 836, 601, 497, 26, 68, 458, 732, 645, 432, 706), (235, 729, 865, 178, 802, 301, 499, 516, 393, 482, 265, 252, 403, 531, 35, 350, 267, 783, 843, 710), (236, 421, 417, 202, 107, 529, 8, 628, 616, 560, 552, 863, 579, 227, 698, 231, 724, 639, 471, 741), (170, 304, 230, 540, 312, 685, 332, 703, 151, 550, 55, 830, 160, 45, 70, 3, 20, 372, 92, 385), (427, 673, 757, 25, 454, 868, 358, 94, 481, 367, 595, 303, 249, 739, 112, 313, 494, 6, 366, 134), (537, 344, 63, 205, 509, 88, 534, 666, 343, 463, 849, 222, 539, 754, 790, 281, 127, 381, 201, 196), (450, 186, 48, 517, 177, 299, 275, 316, 370, 320, 624, 21, 665, 686, 622, 401, 49, 175, 669, 431), (58, 190, 547, 111, 204, 485, 695, 365, 520, 455, 289, 543, 331, 687, 237, 699, 544, 818, 386, 31), (891, 374, 79, 640, 258, 553, 602, 884, 500, 215, 747, 667, 7, 278, 102, 183, 692, 194, 682, 880), (548, 207, 228, 713, 168, 874, 433, 636, 844, 478, 378, 262, 726, 752, 410, 81, 581, 389, 719, 336), (285, 189, 405, 200, 465, 447, 269, 329, 804, 505, 154, 623, 76, 37, 889, 476, 773, 380, 234, 150), (306, 715, 83, 71, 486, 817, 803, 302, 672, 858, 779, 123, 451, 0, 677, 852, 310, 284, 496, 837), (538, 755, 805, 826, 604, 704, 888, 850, 382, 171, 414, 166, 221, 566, 842, 309, 30, 226, 198, 398), (711, 139, 541, 532, 82, 142, 182, 337, 603, 556, 172, 210, 219, 261, 587, 75, 576, 763, 138, 423), (835, 277, 188, 126, 32, 395, 163, 679, 80, 885, 105, 379, 810, 29, 786, 848, 283, 630, 294, 131), (661, 213, 57, 767, 441, 10, 104, 369, 292, 305, 615, 770, 442, 578, 430, 700, 266, 460, 50, 590), (861, 140, 839, 599, 524, 746, 153, 495, 846, 709, 825, 657, 376, 822, 561, 445, 644, 197, 474, 743), (480, 887, 796, 866, 396, 9, 753, 816, 84, 388, 461, 293, 91, 43, 730, 253, 133, 322, 428, 612), (66, 758, 46, 756, 777, 875, 259, 760, 280, 492, 655, 648, 582, 811, 394, 670, 631, 377, 42, 443), (148, 426, 570, 17, 629, 341, 73, 650, 167, 314, 654, 477, 242, 203, 353, 681, 646, 412, 408, 523), (557, 406, 558, 618, 368, 437, 65, 241, 290, 195, 663, 598, 511, 833, 51, 311, 632, 652, 828, 702), (38, 206, 295, 400, 774, 487, 567, 452, 419, 841, 176, 793, 425, 179, 371, 239, 653, 282, 893, 315), (77, 27, 864, 108, 444, 787, 248, 404, 792, 22, 143, 224, 180, 725, 106, 158, 347, 860, 467, 136), (815, 697, 52, 418, 518, 720, 211, 115, 469, 800, 173, 867, 321, 854, 472, 85, 470, 479, 14, 110), (244, 391, 525, 734, 270, 325, 647, 546, 610, 93, 502, 390, 613, 712, 39, 588, 78, 59, 765, 668), (751, 847, 789, 5, 272, 69, 626, 831, 857, 881, 47, 882, 362, 807, 97, 298, 296, 549, 664, 18), (130, 821, 13, 812, 152, 212, 330, 407, 493, 503, 363, 761, 707, 348, 813, 768, 466, 515, 642, 308), (317, 354, 147, 392, 64, 121, 876, 98, 187, 705, 883, 744, 157, 806, 165, 216, 824, 274, 457, 489), (823, 737, 462, 383, 535, 225, 364, 245, 129, 240, 878, 877, 41, 870, 214, 879, 422, 297, 559, 778), (413, 192, 135, 61, 137, 243, 263, 527, 886, 555, 436, 736, 545, 862, 859, 656, 738, 733, 676, 99), (164, 116, 498, 251, 11, 762, 352, 675, 355, 617, 435, 749, 120, 36, 625, 1, 109, 90, 349, 683), (609, 229, 659, 411, 577, 814, 528, 788, 34, 855, 415, 456, 345, 596, 782, 562, 96, 569, 273, 651), (40, 688, 174, 19, 464, 608, 440, 742, 507, 72, 409, 185, 808, 361, 181, 873, 359, 256, 771, 424), (583, 184, 781, 635, 750, 439, 614, 420, 246, 714, 627, 597, 338, 155, 571, 513, 387, 797, 118, 491), (351, 324, 514, 671, 220, 208, 161, 600, 809, 145, 784, 845, 488, 327, 453, 475, 162, 53, 384, 95), (764, 87, 591, 260, 122, 33, 684, 356, 342, 716, 114, 723, 113, 238, 592, 490, 103, 501, 4, 660), (333, 554, 568, 856, 678, 643, 853, 56, 268, 23, 526, 67, 680, 357, 759, 780, 892, 798, 731, 795), (606, 619, 745, 473, 323, 727, 722, 318, 690, 459, 132, 840, 2, 233, 397, 832, 890, 340, 530, 54), (335, 307, 468, 794, 247, 232, 512, 819, 605, 28, 446, 689, 574, 254, 775, 834, 674, 399, 701, 199), (326, 637, 522, 402, 776, 339, 86, 658, 573, 286, 829, 772, 159, 279, 156, 694, 416, 575, 101, 563)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.3580	step1_train_accuracy: 81.3019
STEP-1	Epoch: 20/50	loss: 0.6148	step1_train_accuracy: 89.1967
STEP-1	Epoch: 30/50	loss: 0.3466	step1_train_accuracy: 95.4294
STEP-1	Epoch: 40/50	loss: 0.2295	step1_train_accuracy: 97.5069
STEP-1	Epoch: 50/50	loss: 0.1648	step1_train_accuracy: 98.0609
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.0575	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.0516	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.0484	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0460	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0440	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0425	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0412	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0402	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0393	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0386	gate_loss: 0.0000	step2_classification_accuracy: 98.5294	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 98.3333	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.5617	step1_train_accuracy: 67.1271
STEP-1	Epoch: 20/50	loss: 0.6880	step1_train_accuracy: 89.5028
STEP-1	Epoch: 30/50	loss: 0.3865	step1_train_accuracy: 93.0939
STEP-1	Epoch: 40/50	loss: 0.2664	step1_train_accuracy: 96.6851
STEP-1	Epoch: 50/50	loss: 0.1994	step1_train_accuracy: 96.9613
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1559	gate_loss: 0.3017	step2_classification_accuracy: 95.1852	step_2_gate_accuracy: 91.2963
STEP-2	Epoch: 40/200	classification_loss: 0.1524	gate_loss: 0.1506	step2_classification_accuracy: 95.7407	step_2_gate_accuracy: 97.5926
STEP-2	Epoch: 60/200	classification_loss: 0.0675	gate_loss: 0.0621	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0642	gate_loss: 0.0363	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0611	gate_loss: 0.0257	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0594	gate_loss: 0.0194	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0582	gate_loss: 0.0160	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0574	gate_loss: 0.0137	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0570	gate_loss: 0.0118	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0567	gate_loss: 0.0102	step2_classification_accuracy: 96.8519	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 96.6667	gate_accuracy: 97.7778
	Task-1	val_accuracy: 75.5556	gate_accuracy: 84.4444
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 93.3333


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 1.8367	step1_train_accuracy: 59.6439
STEP-1	Epoch: 20/50	loss: 0.8450	step1_train_accuracy: 88.7240
STEP-1	Epoch: 30/50	loss: 0.4562	step1_train_accuracy: 92.8783
STEP-1	Epoch: 40/50	loss: 0.2984	step1_train_accuracy: 94.3620
STEP-1	Epoch: 50/50	loss: 0.2204	step1_train_accuracy: 96.1424
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2250	gate_loss: 0.5992	step2_classification_accuracy: 92.2973	step_2_gate_accuracy: 82.5676
STEP-2	Epoch: 40/200	classification_loss: 0.1534	gate_loss: 0.2735	step2_classification_accuracy: 93.6486	step_2_gate_accuracy: 94.3243
STEP-2	Epoch: 60/200	classification_loss: 0.1346	gate_loss: 0.1591	step2_classification_accuracy: 93.7838	step_2_gate_accuracy: 95.8108
STEP-2	Epoch: 80/200	classification_loss: 0.1251	gate_loss: 0.1162	step2_classification_accuracy: 93.7838	step_2_gate_accuracy: 96.0811
STEP-2	Epoch: 100/200	classification_loss: 0.1243	gate_loss: 0.0918	step2_classification_accuracy: 94.1892	step_2_gate_accuracy: 97.1622
STEP-2	Epoch: 120/200	classification_loss: 0.1196	gate_loss: 0.0784	step2_classification_accuracy: 94.1892	step_2_gate_accuracy: 96.8919
STEP-2	Epoch: 140/200	classification_loss: 0.1128	gate_loss: 0.0659	step2_classification_accuracy: 94.1892	step_2_gate_accuracy: 97.5676
STEP-2	Epoch: 160/200	classification_loss: 0.1196	gate_loss: 0.0645	step2_classification_accuracy: 94.0541	step_2_gate_accuracy: 96.8919
STEP-2	Epoch: 180/200	classification_loss: 0.1021	gate_loss: 0.0544	step2_classification_accuracy: 94.7297	step_2_gate_accuracy: 97.4324
STEP-2	Epoch: 200/200	classification_loss: 0.0981	gate_loss: 0.0506	step2_classification_accuracy: 94.7297	step_2_gate_accuracy: 97.5676
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 92.7778	gate_accuracy: 93.8889
	Task-1	val_accuracy: 76.6667	gate_accuracy: 87.7778
	Task-2	val_accuracy: 84.5238	gate_accuracy: 85.7143
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 90.3955


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 1.9664	step1_train_accuracy: 52.7331
STEP-1	Epoch: 20/50	loss: 0.9371	step1_train_accuracy: 86.8167
STEP-1	Epoch: 30/50	loss: 0.5419	step1_train_accuracy: 93.8907
STEP-1	Epoch: 40/50	loss: 0.3599	step1_train_accuracy: 96.4630
STEP-1	Epoch: 50/50	loss: 0.2583	step1_train_accuracy: 97.4277
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2548	gate_loss: 0.8644	step2_classification_accuracy: 90.7447	step_2_gate_accuracy: 72.5532
STEP-2	Epoch: 40/200	classification_loss: 0.1900	gate_loss: 0.3867	step2_classification_accuracy: 92.7660	step_2_gate_accuracy: 92.1277
STEP-2	Epoch: 60/200	classification_loss: 0.1433	gate_loss: 0.1903	step2_classification_accuracy: 93.9362	step_2_gate_accuracy: 96.8085
STEP-2	Epoch: 80/200	classification_loss: 0.1448	gate_loss: 0.1185	step2_classification_accuracy: 94.3617	step_2_gate_accuracy: 98.6170
STEP-2	Epoch: 100/200	classification_loss: 0.1252	gate_loss: 0.0869	step2_classification_accuracy: 94.3617	step_2_gate_accuracy: 98.1915
STEP-2	Epoch: 120/200	classification_loss: 0.1242	gate_loss: 0.0663	step2_classification_accuracy: 94.6809	step_2_gate_accuracy: 98.8298
STEP-2	Epoch: 140/200	classification_loss: 0.1176	gate_loss: 0.0556	step2_classification_accuracy: 94.4681	step_2_gate_accuracy: 98.8298
STEP-2	Epoch: 160/200	classification_loss: 0.1225	gate_loss: 0.0522	step2_classification_accuracy: 94.4681	step_2_gate_accuracy: 98.4043
STEP-2	Epoch: 180/200	classification_loss: 0.1186	gate_loss: 0.0425	step2_classification_accuracy: 94.5745	step_2_gate_accuracy: 99.1489
STEP-2	Epoch: 200/200	classification_loss: 0.1128	gate_loss: 0.0398	step2_classification_accuracy: 94.5745	step_2_gate_accuracy: 98.7234
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 87.2222	gate_accuracy: 92.2222
	Task-1	val_accuracy: 77.7778	gate_accuracy: 85.5556
	Task-2	val_accuracy: 82.1429	gate_accuracy: 88.0952
	Task-3	val_accuracy: 87.1795	gate_accuracy: 88.4615
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 89.3519


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.0474	step1_train_accuracy: 49.3548
STEP-1	Epoch: 20/50	loss: 0.9229	step1_train_accuracy: 91.9355
STEP-1	Epoch: 30/50	loss: 0.4490	step1_train_accuracy: 97.0968
STEP-1	Epoch: 40/50	loss: 0.2552	step1_train_accuracy: 98.7097
STEP-1	Epoch: 50/50	loss: 0.1711	step1_train_accuracy: 98.7097
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3210	gate_loss: 0.9234	step2_classification_accuracy: 89.2105	step_2_gate_accuracy: 76.0526
STEP-2	Epoch: 40/200	classification_loss: 0.2244	gate_loss: 0.3717	step2_classification_accuracy: 91.2281	step_2_gate_accuracy: 91.6667
STEP-2	Epoch: 60/200	classification_loss: 0.1617	gate_loss: 0.1998	step2_classification_accuracy: 93.9474	step_2_gate_accuracy: 95.9649
STEP-2	Epoch: 80/200	classification_loss: 0.1657	gate_loss: 0.1450	step2_classification_accuracy: 93.4211	step_2_gate_accuracy: 96.6667
STEP-2	Epoch: 100/200	classification_loss: 0.1462	gate_loss: 0.1114	step2_classification_accuracy: 94.2982	step_2_gate_accuracy: 97.5439
STEP-2	Epoch: 120/200	classification_loss: 0.1253	gate_loss: 0.0880	step2_classification_accuracy: 94.4737	step_2_gate_accuracy: 97.7193
STEP-2	Epoch: 140/200	classification_loss: 0.1268	gate_loss: 0.0758	step2_classification_accuracy: 94.4737	step_2_gate_accuracy: 97.8070
STEP-2	Epoch: 160/200	classification_loss: 0.1242	gate_loss: 0.0694	step2_classification_accuracy: 94.6491	step_2_gate_accuracy: 97.7193
STEP-2	Epoch: 180/200	classification_loss: 0.1114	gate_loss: 0.0571	step2_classification_accuracy: 95.2632	step_2_gate_accuracy: 98.4211
STEP-2	Epoch: 200/200	classification_loss: 0.1087	gate_loss: 0.0563	step2_classification_accuracy: 95.0000	step_2_gate_accuracy: 98.2456
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.4444	gate_accuracy: 88.3333
	Task-1	val_accuracy: 73.3333	gate_accuracy: 84.4444
	Task-2	val_accuracy: 79.7619	gate_accuracy: 82.1429
	Task-3	val_accuracy: 84.6154	gate_accuracy: 85.8974
	Task-4	val_accuracy: 80.7692	gate_accuracy: 80.7692
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 85.0980


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.4515	step1_train_accuracy: 54.1985
STEP-1	Epoch: 20/50	loss: 1.1580	step1_train_accuracy: 86.2595
STEP-1	Epoch: 30/50	loss: 0.5696	step1_train_accuracy: 93.8931
STEP-1	Epoch: 40/50	loss: 0.4181	step1_train_accuracy: 96.9466
STEP-1	Epoch: 50/50	loss: 0.2869	step1_train_accuracy: 96.1832
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3665	gate_loss: 1.0960	step2_classification_accuracy: 87.7612	step_2_gate_accuracy: 70.5970
STEP-2	Epoch: 40/200	classification_loss: 0.2772	gate_loss: 0.4533	step2_classification_accuracy: 90.1493	step_2_gate_accuracy: 89.0298
STEP-2	Epoch: 60/200	classification_loss: 0.2267	gate_loss: 0.2455	step2_classification_accuracy: 91.8657	step_2_gate_accuracy: 93.6567
STEP-2	Epoch: 80/200	classification_loss: 0.2370	gate_loss: 0.1915	step2_classification_accuracy: 91.6418	step_2_gate_accuracy: 94.0298
STEP-2	Epoch: 100/200	classification_loss: 0.1683	gate_loss: 0.1251	step2_classification_accuracy: 93.0597	step_2_gate_accuracy: 95.7463
STEP-2	Epoch: 120/200	classification_loss: 0.1584	gate_loss: 0.1060	step2_classification_accuracy: 93.6567	step_2_gate_accuracy: 96.6418
STEP-2	Epoch: 140/200	classification_loss: 0.1672	gate_loss: 0.0973	step2_classification_accuracy: 93.6567	step_2_gate_accuracy: 96.7164
STEP-2	Epoch: 160/200	classification_loss: 0.1394	gate_loss: 0.0782	step2_classification_accuracy: 94.4030	step_2_gate_accuracy: 97.4627
STEP-2	Epoch: 180/200	classification_loss: 0.1337	gate_loss: 0.0716	step2_classification_accuracy: 94.0299	step_2_gate_accuracy: 97.3881
STEP-2	Epoch: 200/200	classification_loss: 0.1335	gate_loss: 0.0709	step2_classification_accuracy: 94.3284	step_2_gate_accuracy: 98.1343
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 82.7778	gate_accuracy: 87.2222
	Task-1	val_accuracy: 74.4444	gate_accuracy: 86.6667
	Task-2	val_accuracy: 76.1905	gate_accuracy: 77.3810
	Task-3	val_accuracy: 80.7692	gate_accuracy: 84.6154
	Task-4	val_accuracy: 78.2051	gate_accuracy: 80.7692
	Task-5	val_accuracy: 66.6667	gate_accuracy: 75.7576
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 83.1597


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 1.8834	step1_train_accuracy: 67.8474
STEP-1	Epoch: 20/50	loss: 0.7395	step1_train_accuracy: 87.7384
STEP-1	Epoch: 30/50	loss: 0.3863	step1_train_accuracy: 92.0981
STEP-1	Epoch: 40/50	loss: 0.2528	step1_train_accuracy: 98.3651
STEP-1	Epoch: 50/50	loss: 0.1657	step1_train_accuracy: 98.3651
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3712	gate_loss: 1.1578	step2_classification_accuracy: 86.9481	step_2_gate_accuracy: 67.5974
STEP-2	Epoch: 40/200	classification_loss: 0.2278	gate_loss: 0.4566	step2_classification_accuracy: 91.8831	step_2_gate_accuracy: 90.3896
STEP-2	Epoch: 60/200	classification_loss: 0.1874	gate_loss: 0.2413	step2_classification_accuracy: 93.1818	step_2_gate_accuracy: 94.0909
STEP-2	Epoch: 80/200	classification_loss: 0.1592	gate_loss: 0.1582	step2_classification_accuracy: 94.0260	step_2_gate_accuracy: 95.8442
STEP-2	Epoch: 100/200	classification_loss: 0.1428	gate_loss: 0.1146	step2_classification_accuracy: 94.6753	step_2_gate_accuracy: 97.4026
STEP-2	Epoch: 120/200	classification_loss: 0.1425	gate_loss: 0.0924	step2_classification_accuracy: 94.4805	step_2_gate_accuracy: 97.9221
STEP-2	Epoch: 140/200	classification_loss: 0.1233	gate_loss: 0.0757	step2_classification_accuracy: 94.9351	step_2_gate_accuracy: 97.8571
STEP-2	Epoch: 160/200	classification_loss: 0.1190	gate_loss: 0.0665	step2_classification_accuracy: 95.0649	step_2_gate_accuracy: 97.9221
STEP-2	Epoch: 180/200	classification_loss: 0.1172	gate_loss: 0.0575	step2_classification_accuracy: 95.1299	step_2_gate_accuracy: 98.2467
STEP-2	Epoch: 200/200	classification_loss: 0.1072	gate_loss: 0.0502	step2_classification_accuracy: 95.5844	step_2_gate_accuracy: 98.7013
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.5556	gate_accuracy: 83.8889
	Task-1	val_accuracy: 68.8889	gate_accuracy: 77.7778
	Task-2	val_accuracy: 85.7143	gate_accuracy: 86.9048
	Task-3	val_accuracy: 79.4872	gate_accuracy: 83.3333
	Task-4	val_accuracy: 75.6410	gate_accuracy: 75.6410
	Task-5	val_accuracy: 75.7576	gate_accuracy: 81.8182
	Task-6	val_accuracy: 92.3913	gate_accuracy: 89.1304
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.9341


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 2.1695	step1_train_accuracy: 48.7097
STEP-1	Epoch: 20/50	loss: 0.9703	step1_train_accuracy: 82.5806
STEP-1	Epoch: 30/50	loss: 0.5655	step1_train_accuracy: 91.9355
STEP-1	Epoch: 40/50	loss: 0.3857	step1_train_accuracy: 94.5161
STEP-1	Epoch: 50/50	loss: 0.2883	step1_train_accuracy: 95.8064
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3737	gate_loss: 1.3296	step2_classification_accuracy: 88.2184	step_2_gate_accuracy: 60.9195
STEP-2	Epoch: 40/200	classification_loss: 0.2609	gate_loss: 0.5514	step2_classification_accuracy: 91.7816	step_2_gate_accuracy: 87.9310
STEP-2	Epoch: 60/200	classification_loss: 0.2019	gate_loss: 0.2799	step2_classification_accuracy: 92.9310	step_2_gate_accuracy: 93.6207
STEP-2	Epoch: 80/200	classification_loss: 0.1638	gate_loss: 0.1773	step2_classification_accuracy: 94.1954	step_2_gate_accuracy: 95.7471
STEP-2	Epoch: 100/200	classification_loss: 0.1500	gate_loss: 0.1332	step2_classification_accuracy: 94.3103	step_2_gate_accuracy: 96.7241
STEP-2	Epoch: 120/200	classification_loss: 0.1272	gate_loss: 0.0971	step2_classification_accuracy: 95.5747	step_2_gate_accuracy: 97.8161
STEP-2	Epoch: 140/200	classification_loss: 0.1207	gate_loss: 0.0779	step2_classification_accuracy: 95.3448	step_2_gate_accuracy: 98.2759
STEP-2	Epoch: 160/200	classification_loss: 0.1253	gate_loss: 0.0753	step2_classification_accuracy: 95.1724	step_2_gate_accuracy: 98.1034
STEP-2	Epoch: 180/200	classification_loss: 0.1045	gate_loss: 0.0581	step2_classification_accuracy: 95.5747	step_2_gate_accuracy: 98.6782
STEP-2	Epoch: 200/200	classification_loss: 0.1054	gate_loss: 0.0540	step2_classification_accuracy: 95.5747	step_2_gate_accuracy: 98.7931
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 81.1111	gate_accuracy: 85.5556
	Task-1	val_accuracy: 71.1111	gate_accuracy: 81.1111
	Task-2	val_accuracy: 76.1905	gate_accuracy: 78.5714
	Task-3	val_accuracy: 79.4872	gate_accuracy: 79.4872
	Task-4	val_accuracy: 79.4872	gate_accuracy: 75.6410
	Task-5	val_accuracy: 71.2121	gate_accuracy: 78.7879
	Task-6	val_accuracy: 92.3913	gate_accuracy: 90.2174
	Task-7	val_accuracy: 85.7143	gate_accuracy: 81.8182
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.1477


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.4781	step1_train_accuracy: 54.9206
STEP-1	Epoch: 20/50	loss: 1.0081	step1_train_accuracy: 77.7778
STEP-1	Epoch: 30/50	loss: 0.5201	step1_train_accuracy: 96.8254
STEP-1	Epoch: 40/50	loss: 0.3283	step1_train_accuracy: 97.4603
STEP-1	Epoch: 50/50	loss: 0.2297	step1_train_accuracy: 98.7302
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3574	gate_loss: 1.4226	step2_classification_accuracy: 89.0206	step_2_gate_accuracy: 57.7320
STEP-2	Epoch: 40/200	classification_loss: 0.2611	gate_loss: 0.5951	step2_classification_accuracy: 91.5979	step_2_gate_accuracy: 85.0000
STEP-2	Epoch: 60/200	classification_loss: 0.1937	gate_loss: 0.3027	step2_classification_accuracy: 93.0928	step_2_gate_accuracy: 93.1959
STEP-2	Epoch: 80/200	classification_loss: 0.1657	gate_loss: 0.1868	step2_classification_accuracy: 94.1237	step_2_gate_accuracy: 95.9278
STEP-2	Epoch: 100/200	classification_loss: 0.1426	gate_loss: 0.1348	step2_classification_accuracy: 94.9485	step_2_gate_accuracy: 96.9588
STEP-2	Epoch: 120/200	classification_loss: 0.1265	gate_loss: 0.1070	step2_classification_accuracy: 95.2062	step_2_gate_accuracy: 97.4742
STEP-2	Epoch: 140/200	classification_loss: 0.1204	gate_loss: 0.0901	step2_classification_accuracy: 95.6701	step_2_gate_accuracy: 97.9381
STEP-2	Epoch: 160/200	classification_loss: 0.1114	gate_loss: 0.0759	step2_classification_accuracy: 95.8247	step_2_gate_accuracy: 98.5567
STEP-2	Epoch: 180/200	classification_loss: 0.1115	gate_loss: 0.0695	step2_classification_accuracy: 95.6186	step_2_gate_accuracy: 97.8351
STEP-2	Epoch: 200/200	classification_loss: 0.1121	gate_loss: 0.0647	step2_classification_accuracy: 95.7216	step_2_gate_accuracy: 98.0412
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.1111	gate_accuracy: 82.7778
	Task-1	val_accuracy: 66.6667	gate_accuracy: 80.0000
	Task-2	val_accuracy: 79.7619	gate_accuracy: 86.9048
	Task-3	val_accuracy: 82.0513	gate_accuracy: 80.7692
	Task-4	val_accuracy: 74.3590	gate_accuracy: 76.9231
	Task-5	val_accuracy: 69.6970	gate_accuracy: 77.2727
	Task-6	val_accuracy: 94.5652	gate_accuracy: 92.3913
	Task-7	val_accuracy: 87.0130	gate_accuracy: 71.4286
	Task-8	val_accuracy: 88.6076	gate_accuracy: 84.8101
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.9175


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.0914	step1_train_accuracy: 47.6048
STEP-1	Epoch: 20/50	loss: 0.9272	step1_train_accuracy: 82.0359
STEP-1	Epoch: 30/50	loss: 0.5619	step1_train_accuracy: 89.5210
STEP-1	Epoch: 40/50	loss: 0.4132	step1_train_accuracy: 93.1138
STEP-1	Epoch: 50/50	loss: 0.3087	step1_train_accuracy: 94.9102
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4491	gate_loss: 1.4933	step2_classification_accuracy: 85.8411	step_2_gate_accuracy: 56.3551
STEP-2	Epoch: 40/200	classification_loss: 0.3117	gate_loss: 0.6113	step2_classification_accuracy: 89.8598	step_2_gate_accuracy: 84.8131
STEP-2	Epoch: 60/200	classification_loss: 0.2441	gate_loss: 0.3217	step2_classification_accuracy: 92.1963	step_2_gate_accuracy: 92.1963
STEP-2	Epoch: 80/200	classification_loss: 0.2144	gate_loss: 0.2120	step2_classification_accuracy: 92.3832	step_2_gate_accuracy: 95.0000
STEP-2	Epoch: 100/200	classification_loss: 0.1885	gate_loss: 0.1555	step2_classification_accuracy: 93.2710	step_2_gate_accuracy: 95.7944
STEP-2	Epoch: 120/200	classification_loss: 0.1734	gate_loss: 0.1239	step2_classification_accuracy: 94.0654	step_2_gate_accuracy: 96.9626
STEP-2	Epoch: 140/200	classification_loss: 0.1623	gate_loss: 0.1051	step2_classification_accuracy: 93.7850	step_2_gate_accuracy: 97.0561
STEP-2	Epoch: 160/200	classification_loss: 0.1512	gate_loss: 0.0941	step2_classification_accuracy: 94.1589	step_2_gate_accuracy: 97.2430
STEP-2	Epoch: 180/200	classification_loss: 0.1426	gate_loss: 0.0821	step2_classification_accuracy: 94.7196	step_2_gate_accuracy: 97.5701
STEP-2	Epoch: 200/200	classification_loss: 0.1402	gate_loss: 0.0767	step2_classification_accuracy: 94.8131	step_2_gate_accuracy: 97.6168
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.6667	gate_accuracy: 79.4444
	Task-1	val_accuracy: 66.6667	gate_accuracy: 74.4444
	Task-2	val_accuracy: 61.9048	gate_accuracy: 69.0476
	Task-3	val_accuracy: 80.7692	gate_accuracy: 82.0513
	Task-4	val_accuracy: 76.9231	gate_accuracy: 75.6410
	Task-5	val_accuracy: 68.1818	gate_accuracy: 78.7879
	Task-6	val_accuracy: 92.3913	gate_accuracy: 85.8696
	Task-7	val_accuracy: 79.2208	gate_accuracy: 72.7273
	Task-8	val_accuracy: 88.6076	gate_accuracy: 82.2785
	Task-9	val_accuracy: 89.1566	gate_accuracy: 90.3614
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.1621


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.3956	step1_train_accuracy: 45.8621
STEP-1	Epoch: 20/50	loss: 1.0524	step1_train_accuracy: 86.8966
STEP-1	Epoch: 30/50	loss: 0.5020	step1_train_accuracy: 96.8966
STEP-1	Epoch: 40/50	loss: 0.3042	step1_train_accuracy: 97.9310
STEP-1	Epoch: 50/50	loss: 0.2198	step1_train_accuracy: 97.9310
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4895	gate_loss: 1.6669	step2_classification_accuracy: 85.2137	step_2_gate_accuracy: 50.5556
STEP-2	Epoch: 40/200	classification_loss: 0.3612	gate_loss: 0.7335	step2_classification_accuracy: 88.1197	step_2_gate_accuracy: 80.7265
STEP-2	Epoch: 60/200	classification_loss: 0.2749	gate_loss: 0.3647	step2_classification_accuracy: 90.5983	step_2_gate_accuracy: 91.8803
STEP-2	Epoch: 80/200	classification_loss: 0.2159	gate_loss: 0.2352	step2_classification_accuracy: 92.3932	step_2_gate_accuracy: 94.4017
STEP-2	Epoch: 100/200	classification_loss: 0.1998	gate_loss: 0.1712	step2_classification_accuracy: 92.8205	step_2_gate_accuracy: 96.0256
STEP-2	Epoch: 120/200	classification_loss: 0.1741	gate_loss: 0.1267	step2_classification_accuracy: 93.5897	step_2_gate_accuracy: 97.2650
STEP-2	Epoch: 140/200	classification_loss: 0.1632	gate_loss: 0.1044	step2_classification_accuracy: 94.0598	step_2_gate_accuracy: 97.3932
STEP-2	Epoch: 160/200	classification_loss: 0.1532	gate_loss: 0.0868	step2_classification_accuracy: 93.9316	step_2_gate_accuracy: 97.9060
STEP-2	Epoch: 180/200	classification_loss: 0.1531	gate_loss: 0.0820	step2_classification_accuracy: 94.1453	step_2_gate_accuracy: 97.7350
STEP-2	Epoch: 200/200	classification_loss: 0.1358	gate_loss: 0.0686	step2_classification_accuracy: 94.4444	step_2_gate_accuracy: 98.3333
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.2222	gate_accuracy: 82.2222
	Task-1	val_accuracy: 73.3333	gate_accuracy: 81.1111
	Task-2	val_accuracy: 69.0476	gate_accuracy: 76.1905
	Task-3	val_accuracy: 73.0769	gate_accuracy: 80.7692
	Task-4	val_accuracy: 75.6410	gate_accuracy: 80.7692
	Task-5	val_accuracy: 68.1818	gate_accuracy: 78.7879
	Task-6	val_accuracy: 81.5217	gate_accuracy: 75.0000
	Task-7	val_accuracy: 85.7143	gate_accuracy: 87.0130
	Task-8	val_accuracy: 84.8101	gate_accuracy: 74.6835
	Task-9	val_accuracy: 86.7470	gate_accuracy: 89.1566
	Task-10	val_accuracy: 87.6712	gate_accuracy: 84.9315
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.0204


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.1515	step1_train_accuracy: 54.1311
STEP-1	Epoch: 20/50	loss: 0.9145	step1_train_accuracy: 81.7664
STEP-1	Epoch: 30/50	loss: 0.4985	step1_train_accuracy: 92.8775
STEP-1	Epoch: 40/50	loss: 0.3106	step1_train_accuracy: 96.2963
STEP-1	Epoch: 50/50	loss: 0.2143	step1_train_accuracy: 98.0057
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5105	gate_loss: 1.6581	step2_classification_accuracy: 84.8031	step_2_gate_accuracy: 53.2677
STEP-2	Epoch: 40/200	classification_loss: 0.3537	gate_loss: 0.6909	step2_classification_accuracy: 88.3465	step_2_gate_accuracy: 81.7717
STEP-2	Epoch: 60/200	classification_loss: 0.2771	gate_loss: 0.3520	step2_classification_accuracy: 91.0236	step_2_gate_accuracy: 91.1417
STEP-2	Epoch: 80/200	classification_loss: 0.2323	gate_loss: 0.2227	step2_classification_accuracy: 92.4016	step_2_gate_accuracy: 94.8031
STEP-2	Epoch: 100/200	classification_loss: 0.2117	gate_loss: 0.1693	step2_classification_accuracy: 93.1890	step_2_gate_accuracy: 95.7087
STEP-2	Epoch: 120/200	classification_loss: 0.1960	gate_loss: 0.1388	step2_classification_accuracy: 93.3465	step_2_gate_accuracy: 96.1811
STEP-2	Epoch: 140/200	classification_loss: 0.1834	gate_loss: 0.1143	step2_classification_accuracy: 93.7402	step_2_gate_accuracy: 96.9685
STEP-2	Epoch: 160/200	classification_loss: 0.1645	gate_loss: 0.0989	step2_classification_accuracy: 94.1339	step_2_gate_accuracy: 96.9685
STEP-2	Epoch: 180/200	classification_loss: 0.1538	gate_loss: 0.0891	step2_classification_accuracy: 94.8425	step_2_gate_accuracy: 97.4803
STEP-2	Epoch: 200/200	classification_loss: 0.1412	gate_loss: 0.0760	step2_classification_accuracy: 94.8031	step_2_gate_accuracy: 97.8740
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.0000	gate_accuracy: 81.1111
	Task-1	val_accuracy: 67.7778	gate_accuracy: 77.7778
	Task-2	val_accuracy: 59.5238	gate_accuracy: 71.4286
	Task-3	val_accuracy: 75.6410	gate_accuracy: 75.6410
	Task-4	val_accuracy: 79.4872	gate_accuracy: 80.7692
	Task-5	val_accuracy: 66.6667	gate_accuracy: 77.2727
	Task-6	val_accuracy: 88.0435	gate_accuracy: 86.9565
	Task-7	val_accuracy: 87.0130	gate_accuracy: 75.3247
	Task-8	val_accuracy: 87.3418	gate_accuracy: 83.5443
	Task-9	val_accuracy: 85.5422	gate_accuracy: 80.7229
	Task-10	val_accuracy: 95.8904	gate_accuracy: 90.4110
	Task-11	val_accuracy: 84.0909	gate_accuracy: 82.9545
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.4307


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.6323	step1_train_accuracy: 41.8530
STEP-1	Epoch: 20/50	loss: 1.2492	step1_train_accuracy: 69.0096
STEP-1	Epoch: 30/50	loss: 0.7960	step1_train_accuracy: 85.9425
STEP-1	Epoch: 40/50	loss: 0.5529	step1_train_accuracy: 89.1374
STEP-1	Epoch: 50/50	loss: 0.4026	step1_train_accuracy: 92.0128
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4891	gate_loss: 1.7120	step2_classification_accuracy: 85.1825	step_2_gate_accuracy: 49.7445
STEP-2	Epoch: 40/200	classification_loss: 0.3600	gate_loss: 0.7085	step2_classification_accuracy: 88.3942	step_2_gate_accuracy: 80.4380
STEP-2	Epoch: 60/200	classification_loss: 0.2737	gate_loss: 0.3546	step2_classification_accuracy: 91.3869	step_2_gate_accuracy: 91.4234
STEP-2	Epoch: 80/200	classification_loss: 0.2116	gate_loss: 0.2207	step2_classification_accuracy: 93.0657	step_2_gate_accuracy: 94.9270
STEP-2	Epoch: 100/200	classification_loss: 0.1896	gate_loss: 0.1619	step2_classification_accuracy: 93.6131	step_2_gate_accuracy: 95.6934
STEP-2	Epoch: 120/200	classification_loss: 0.1759	gate_loss: 0.1306	step2_classification_accuracy: 93.9416	step_2_gate_accuracy: 96.7153
STEP-2	Epoch: 140/200	classification_loss: 0.1682	gate_loss: 0.1117	step2_classification_accuracy: 94.1241	step_2_gate_accuracy: 97.0073
STEP-2	Epoch: 160/200	classification_loss: 0.1512	gate_loss: 0.0943	step2_classification_accuracy: 94.3431	step_2_gate_accuracy: 97.1898
STEP-2	Epoch: 180/200	classification_loss: 0.1393	gate_loss: 0.0801	step2_classification_accuracy: 94.4891	step_2_gate_accuracy: 97.8467
STEP-2	Epoch: 200/200	classification_loss: 0.1351	gate_loss: 0.0726	step2_classification_accuracy: 94.8175	step_2_gate_accuracy: 97.8832
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.5556	gate_accuracy: 83.3333
	Task-1	val_accuracy: 63.3333	gate_accuracy: 72.2222
	Task-2	val_accuracy: 69.0476	gate_accuracy: 77.3810
	Task-3	val_accuracy: 76.9231	gate_accuracy: 82.0513
	Task-4	val_accuracy: 71.7949	gate_accuracy: 67.9487
	Task-5	val_accuracy: 69.6970	gate_accuracy: 78.7879
	Task-6	val_accuracy: 88.0435	gate_accuracy: 85.8696
	Task-7	val_accuracy: 85.7143	gate_accuracy: 77.9221
	Task-8	val_accuracy: 83.5443	gate_accuracy: 78.4810
	Task-9	val_accuracy: 81.9277	gate_accuracy: 79.5181
	Task-10	val_accuracy: 91.7808	gate_accuracy: 87.6712
	Task-11	val_accuracy: 86.3636	gate_accuracy: 84.0909
	Task-12	val_accuracy: 70.5128	gate_accuracy: 76.9231
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.7557


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 2.4945	step1_train_accuracy: 61.0345
STEP-1	Epoch: 20/50	loss: 1.0188	step1_train_accuracy: 76.8966
STEP-1	Epoch: 30/50	loss: 0.5540	step1_train_accuracy: 92.0690
STEP-1	Epoch: 40/50	loss: 0.3824	step1_train_accuracy: 95.5172
STEP-1	Epoch: 50/50	loss: 0.2906	step1_train_accuracy: 95.1724
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5290	gate_loss: 1.8690	step2_classification_accuracy: 83.6054	step_2_gate_accuracy: 49.1156
STEP-2	Epoch: 40/200	classification_loss: 0.3822	gate_loss: 0.7331	step2_classification_accuracy: 87.8912	step_2_gate_accuracy: 81.8707
STEP-2	Epoch: 60/200	classification_loss: 0.2995	gate_loss: 0.3717	step2_classification_accuracy: 90.3061	step_2_gate_accuracy: 90.5102
STEP-2	Epoch: 80/200	classification_loss: 0.2469	gate_loss: 0.2368	step2_classification_accuracy: 91.3605	step_2_gate_accuracy: 94.1156
STEP-2	Epoch: 100/200	classification_loss: 0.2255	gate_loss: 0.1750	step2_classification_accuracy: 92.2109	step_2_gate_accuracy: 95.7823
STEP-2	Epoch: 120/200	classification_loss: 0.1994	gate_loss: 0.1379	step2_classification_accuracy: 92.9252	step_2_gate_accuracy: 96.4626
STEP-2	Epoch: 140/200	classification_loss: 0.1855	gate_loss: 0.1160	step2_classification_accuracy: 93.1973	step_2_gate_accuracy: 96.8708
STEP-2	Epoch: 160/200	classification_loss: 0.1942	gate_loss: 0.1075	step2_classification_accuracy: 93.0612	step_2_gate_accuracy: 96.9728
STEP-2	Epoch: 180/200	classification_loss: 0.1719	gate_loss: 0.0894	step2_classification_accuracy: 93.6054	step_2_gate_accuracy: 97.4490
STEP-2	Epoch: 200/200	classification_loss: 0.1579	gate_loss: 0.0788	step2_classification_accuracy: 94.1156	step_2_gate_accuracy: 97.6531
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.4444	gate_accuracy: 82.2222
	Task-1	val_accuracy: 63.3333	gate_accuracy: 76.6667
	Task-2	val_accuracy: 63.0952	gate_accuracy: 67.8571
	Task-3	val_accuracy: 75.6410	gate_accuracy: 79.4872
	Task-4	val_accuracy: 74.3590	gate_accuracy: 74.3590
	Task-5	val_accuracy: 72.7273	gate_accuracy: 80.3030
	Task-6	val_accuracy: 89.1304	gate_accuracy: 83.6957
	Task-7	val_accuracy: 87.0130	gate_accuracy: 87.0130
	Task-8	val_accuracy: 78.4810	gate_accuracy: 70.8861
	Task-9	val_accuracy: 81.9277	gate_accuracy: 78.3133
	Task-10	val_accuracy: 89.0411	gate_accuracy: 82.1918
	Task-11	val_accuracy: 80.6818	gate_accuracy: 80.6818
	Task-12	val_accuracy: 76.9231	gate_accuracy: 79.4872
	Task-13	val_accuracy: 86.3014	gate_accuracy: 83.5616
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.2453


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.5955	step1_train_accuracy: 51.6014
STEP-1	Epoch: 20/50	loss: 0.9783	step1_train_accuracy: 87.9004
STEP-1	Epoch: 30/50	loss: 0.5183	step1_train_accuracy: 93.9502
STEP-1	Epoch: 40/50	loss: 0.3084	step1_train_accuracy: 93.2384
STEP-1	Epoch: 50/50	loss: 0.2289	step1_train_accuracy: 94.3061
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5316	gate_loss: 1.8389	step2_classification_accuracy: 83.6624	step_2_gate_accuracy: 48.4395
STEP-2	Epoch: 40/200	classification_loss: 0.3978	gate_loss: 0.7455	step2_classification_accuracy: 87.5796	step_2_gate_accuracy: 81.3057
STEP-2	Epoch: 60/200	classification_loss: 0.3078	gate_loss: 0.3797	step2_classification_accuracy: 90.2229	step_2_gate_accuracy: 90.5096
STEP-2	Epoch: 80/200	classification_loss: 0.2483	gate_loss: 0.2435	step2_classification_accuracy: 92.6115	step_2_gate_accuracy: 93.9172
STEP-2	Epoch: 100/200	classification_loss: 0.2163	gate_loss: 0.1777	step2_classification_accuracy: 93.1529	step_2_gate_accuracy: 95.7006
STEP-2	Epoch: 120/200	classification_loss: 0.2102	gate_loss: 0.1464	step2_classification_accuracy: 93.0255	step_2_gate_accuracy: 96.2420
STEP-2	Epoch: 140/200	classification_loss: 0.1834	gate_loss: 0.1175	step2_classification_accuracy: 93.7898	step_2_gate_accuracy: 96.9745
STEP-2	Epoch: 160/200	classification_loss: 0.1736	gate_loss: 0.1039	step2_classification_accuracy: 93.6943	step_2_gate_accuracy: 97.1656
STEP-2	Epoch: 180/200	classification_loss: 0.1621	gate_loss: 0.0903	step2_classification_accuracy: 93.9809	step_2_gate_accuracy: 97.5159
STEP-2	Epoch: 200/200	classification_loss: 0.1498	gate_loss: 0.0791	step2_classification_accuracy: 94.6178	step_2_gate_accuracy: 98.0573
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.4444	gate_accuracy: 85.5556
	Task-1	val_accuracy: 63.3333	gate_accuracy: 68.8889
	Task-2	val_accuracy: 67.8571	gate_accuracy: 72.6190
	Task-3	val_accuracy: 80.7692	gate_accuracy: 79.4872
	Task-4	val_accuracy: 73.0769	gate_accuracy: 67.9487
	Task-5	val_accuracy: 71.2121	gate_accuracy: 77.2727
	Task-6	val_accuracy: 91.3043	gate_accuracy: 84.7826
	Task-7	val_accuracy: 83.1169	gate_accuracy: 79.2208
	Task-8	val_accuracy: 72.1519	gate_accuracy: 70.8861
	Task-9	val_accuracy: 86.7470	gate_accuracy: 84.3373
	Task-10	val_accuracy: 86.3014	gate_accuracy: 78.0822
	Task-11	val_accuracy: 81.8182	gate_accuracy: 84.0909
	Task-12	val_accuracy: 76.9231	gate_accuracy: 79.4872
	Task-13	val_accuracy: 79.4521	gate_accuracy: 78.0822
	Task-14	val_accuracy: 77.1429	gate_accuracy: 81.4286
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.7432


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.6735	step1_train_accuracy: 57.1429
STEP-1	Epoch: 20/50	loss: 0.8968	step1_train_accuracy: 85.7143
STEP-1	Epoch: 30/50	loss: 0.4201	step1_train_accuracy: 95.3488
STEP-1	Epoch: 40/50	loss: 0.3529	step1_train_accuracy: 98.3389
STEP-1	Epoch: 50/50	loss: 0.1831	step1_train_accuracy: 98.3389
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5418	gate_loss: 1.9391	step2_classification_accuracy: 82.9641	step_2_gate_accuracy: 46.0479
STEP-2	Epoch: 40/200	classification_loss: 0.3826	gate_loss: 0.7620	step2_classification_accuracy: 87.3952	step_2_gate_accuracy: 81.3473
STEP-2	Epoch: 60/200	classification_loss: 0.2768	gate_loss: 0.3686	step2_classification_accuracy: 90.5689	step_2_gate_accuracy: 91.5569
STEP-2	Epoch: 80/200	classification_loss: 0.2453	gate_loss: 0.2406	step2_classification_accuracy: 91.3772	step_2_gate_accuracy: 94.1317
STEP-2	Epoch: 100/200	classification_loss: 0.1988	gate_loss: 0.1686	step2_classification_accuracy: 92.8743	step_2_gate_accuracy: 95.8683
STEP-2	Epoch: 120/200	classification_loss: 0.1876	gate_loss: 0.1350	step2_classification_accuracy: 92.9940	step_2_gate_accuracy: 96.3773
STEP-2	Epoch: 140/200	classification_loss: 0.1770	gate_loss: 0.1128	step2_classification_accuracy: 93.0240	step_2_gate_accuracy: 97.4251
STEP-2	Epoch: 160/200	classification_loss: 0.1672	gate_loss: 0.0978	step2_classification_accuracy: 93.4731	step_2_gate_accuracy: 97.2755
STEP-2	Epoch: 180/200	classification_loss: 0.1573	gate_loss: 0.0878	step2_classification_accuracy: 93.7126	step_2_gate_accuracy: 97.6647
STEP-2	Epoch: 200/200	classification_loss: 0.1531	gate_loss: 0.0805	step2_classification_accuracy: 93.8323	step_2_gate_accuracy: 97.5449
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.7778	gate_accuracy: 78.8889
	Task-1	val_accuracy: 72.2222	gate_accuracy: 81.1111
	Task-2	val_accuracy: 65.4762	gate_accuracy: 76.1905
	Task-3	val_accuracy: 80.7692	gate_accuracy: 84.6154
	Task-4	val_accuracy: 70.5128	gate_accuracy: 75.6410
	Task-5	val_accuracy: 65.1515	gate_accuracy: 71.2121
	Task-6	val_accuracy: 76.0870	gate_accuracy: 71.7391
	Task-7	val_accuracy: 83.1169	gate_accuracy: 81.8182
	Task-8	val_accuracy: 69.6203	gate_accuracy: 69.6203
	Task-9	val_accuracy: 83.1325	gate_accuracy: 80.7229
	Task-10	val_accuracy: 84.9315	gate_accuracy: 82.1918
	Task-11	val_accuracy: 86.3636	gate_accuracy: 82.9545
	Task-12	val_accuracy: 79.4872	gate_accuracy: 83.3333
	Task-13	val_accuracy: 73.9726	gate_accuracy: 72.6027
	Task-14	val_accuracy: 77.1429	gate_accuracy: 77.1429
	Task-15	val_accuracy: 89.3333	gate_accuracy: 84.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.4457


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.3249	step1_train_accuracy: 53.7102
STEP-1	Epoch: 20/50	loss: 0.8898	step1_train_accuracy: 88.3392
STEP-1	Epoch: 30/50	loss: 0.4725	step1_train_accuracy: 95.4064
STEP-1	Epoch: 40/50	loss: 0.3050	step1_train_accuracy: 97.8799
STEP-1	Epoch: 50/50	loss: 0.2241	step1_train_accuracy: 98.9399
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6117	gate_loss: 1.9399	step2_classification_accuracy: 80.7627	step_2_gate_accuracy: 43.0226
STEP-2	Epoch: 40/200	classification_loss: 0.4442	gate_loss: 0.8184	step2_classification_accuracy: 85.5932	step_2_gate_accuracy: 78.0226
STEP-2	Epoch: 60/200	classification_loss: 0.3164	gate_loss: 0.4155	step2_classification_accuracy: 89.9153	step_2_gate_accuracy: 89.2655
STEP-2	Epoch: 80/200	classification_loss: 0.2648	gate_loss: 0.2678	step2_classification_accuracy: 91.5537	step_2_gate_accuracy: 93.3333
STEP-2	Epoch: 100/200	classification_loss: 0.2327	gate_loss: 0.1974	step2_classification_accuracy: 92.3729	step_2_gate_accuracy: 94.8870
STEP-2	Epoch: 120/200	classification_loss: 0.2221	gate_loss: 0.1613	step2_classification_accuracy: 92.5424	step_2_gate_accuracy: 95.5932
STEP-2	Epoch: 140/200	classification_loss: 0.2020	gate_loss: 0.1399	step2_classification_accuracy: 93.2203	step_2_gate_accuracy: 95.9322
STEP-2	Epoch: 160/200	classification_loss: 0.1876	gate_loss: 0.1192	step2_classification_accuracy: 93.2768	step_2_gate_accuracy: 96.6384
STEP-2	Epoch: 180/200	classification_loss: 0.1815	gate_loss: 0.1069	step2_classification_accuracy: 93.5876	step_2_gate_accuracy: 96.8644
STEP-2	Epoch: 200/200	classification_loss: 0.1670	gate_loss: 0.0955	step2_classification_accuracy: 93.8418	step_2_gate_accuracy: 97.3446
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.8889	gate_accuracy: 82.7778
	Task-1	val_accuracy: 65.5556	gate_accuracy: 68.8889
	Task-2	val_accuracy: 54.7619	gate_accuracy: 67.8571
	Task-3	val_accuracy: 74.3590	gate_accuracy: 75.6410
	Task-4	val_accuracy: 69.2308	gate_accuracy: 71.7949
	Task-5	val_accuracy: 60.6061	gate_accuracy: 68.1818
	Task-6	val_accuracy: 85.8696	gate_accuracy: 79.3478
	Task-7	val_accuracy: 76.6234	gate_accuracy: 74.0260
	Task-8	val_accuracy: 79.7468	gate_accuracy: 78.4810
	Task-9	val_accuracy: 80.7229	gate_accuracy: 81.9277
	Task-10	val_accuracy: 87.6712	gate_accuracy: 84.9315
	Task-11	val_accuracy: 85.2273	gate_accuracy: 81.8182
	Task-12	val_accuracy: 75.6410	gate_accuracy: 75.6410
	Task-13	val_accuracy: 80.8219	gate_accuracy: 82.1918
	Task-14	val_accuracy: 71.4286	gate_accuracy: 78.5714
	Task-15	val_accuracy: 85.3333	gate_accuracy: 81.3333
	Task-16	val_accuracy: 64.7887	gate_accuracy: 60.5634
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.6551


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 1.7936	step1_train_accuracy: 68.9119
STEP-1	Epoch: 20/50	loss: 0.6924	step1_train_accuracy: 85.7513
STEP-1	Epoch: 30/50	loss: 0.3949	step1_train_accuracy: 91.4508
STEP-1	Epoch: 40/50	loss: 0.2645	step1_train_accuracy: 95.0777
STEP-1	Epoch: 50/50	loss: 0.2000	step1_train_accuracy: 96.1140
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6311	gate_loss: 2.0350	step2_classification_accuracy: 80.6150	step_2_gate_accuracy: 42.5134
STEP-2	Epoch: 40/200	classification_loss: 0.4562	gate_loss: 0.8256	step2_classification_accuracy: 85.0535	step_2_gate_accuracy: 78.2620
STEP-2	Epoch: 60/200	classification_loss: 0.3568	gate_loss: 0.4310	step2_classification_accuracy: 88.1551	step_2_gate_accuracy: 88.3155
STEP-2	Epoch: 80/200	classification_loss: 0.2893	gate_loss: 0.2809	step2_classification_accuracy: 89.9733	step_2_gate_accuracy: 92.7273
STEP-2	Epoch: 100/200	classification_loss: 0.2557	gate_loss: 0.2071	step2_classification_accuracy: 91.5775	step_2_gate_accuracy: 94.1444
STEP-2	Epoch: 120/200	classification_loss: 0.2342	gate_loss: 0.1720	step2_classification_accuracy: 92.1390	step_2_gate_accuracy: 95.3209
STEP-2	Epoch: 140/200	classification_loss: 0.2224	gate_loss: 0.1468	step2_classification_accuracy: 92.3529	step_2_gate_accuracy: 95.5348
STEP-2	Epoch: 160/200	classification_loss: 0.1999	gate_loss: 0.1270	step2_classification_accuracy: 92.9412	step_2_gate_accuracy: 96.3369
STEP-2	Epoch: 180/200	classification_loss: 0.1862	gate_loss: 0.1101	step2_classification_accuracy: 93.2888	step_2_gate_accuracy: 96.9519
STEP-2	Epoch: 200/200	classification_loss: 0.1842	gate_loss: 0.1041	step2_classification_accuracy: 93.1551	step_2_gate_accuracy: 97.0588
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 68.8889	gate_accuracy: 75.5556
	Task-1	val_accuracy: 60.0000	gate_accuracy: 73.3333
	Task-2	val_accuracy: 59.5238	gate_accuracy: 69.0476
	Task-3	val_accuracy: 75.6410	gate_accuracy: 80.7692
	Task-4	val_accuracy: 74.3590	gate_accuracy: 73.0769
	Task-5	val_accuracy: 63.6364	gate_accuracy: 72.7273
	Task-6	val_accuracy: 84.7826	gate_accuracy: 76.0870
	Task-7	val_accuracy: 81.8182	gate_accuracy: 79.2208
	Task-8	val_accuracy: 74.6835	gate_accuracy: 73.4177
	Task-9	val_accuracy: 79.5181	gate_accuracy: 74.6988
	Task-10	val_accuracy: 90.4110	gate_accuracy: 89.0411
	Task-11	val_accuracy: 81.8182	gate_accuracy: 80.6818
	Task-12	val_accuracy: 73.0769	gate_accuracy: 78.2051
	Task-13	val_accuracy: 80.8219	gate_accuracy: 82.1918
	Task-14	val_accuracy: 68.5714	gate_accuracy: 72.8571
	Task-15	val_accuracy: 81.3333	gate_accuracy: 80.0000
	Task-16	val_accuracy: 64.7887	gate_accuracy: 56.3380
	Task-17	val_accuracy: 72.1649	gate_accuracy: 74.2268
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.6527


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 1.9818	step1_train_accuracy: 62.1387
STEP-1	Epoch: 20/50	loss: 0.7517	step1_train_accuracy: 94.2197
STEP-1	Epoch: 30/50	loss: 0.3432	step1_train_accuracy: 96.5318
STEP-1	Epoch: 40/50	loss: 0.2085	step1_train_accuracy: 97.6879
STEP-1	Epoch: 50/50	loss: 0.1452	step1_train_accuracy: 98.8439
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6308	gate_loss: 2.0714	step2_classification_accuracy: 80.6599	step_2_gate_accuracy: 42.1574
STEP-2	Epoch: 40/200	classification_loss: 0.4737	gate_loss: 0.8422	step2_classification_accuracy: 84.7970	step_2_gate_accuracy: 77.0558
STEP-2	Epoch: 60/200	classification_loss: 0.3589	gate_loss: 0.4378	step2_classification_accuracy: 88.5533	step_2_gate_accuracy: 88.6041
STEP-2	Epoch: 80/200	classification_loss: 0.2839	gate_loss: 0.2858	step2_classification_accuracy: 90.5584	step_2_gate_accuracy: 92.4873
STEP-2	Epoch: 100/200	classification_loss: 0.2535	gate_loss: 0.2152	step2_classification_accuracy: 91.3959	step_2_gate_accuracy: 94.2893
STEP-2	Epoch: 120/200	classification_loss: 0.2216	gate_loss: 0.1712	step2_classification_accuracy: 92.2081	step_2_gate_accuracy: 95.3299
STEP-2	Epoch: 140/200	classification_loss: 0.2043	gate_loss: 0.1438	step2_classification_accuracy: 92.7157	step_2_gate_accuracy: 95.9898
STEP-2	Epoch: 160/200	classification_loss: 0.1889	gate_loss: 0.1269	step2_classification_accuracy: 93.2234	step_2_gate_accuracy: 96.1929
STEP-2	Epoch: 180/200	classification_loss: 0.1903	gate_loss: 0.1189	step2_classification_accuracy: 92.9695	step_2_gate_accuracy: 96.4467
STEP-2	Epoch: 200/200	classification_loss: 0.1711	gate_loss: 0.1042	step2_classification_accuracy: 93.6802	step_2_gate_accuracy: 96.8020
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.8889	gate_accuracy: 80.0000
	Task-1	val_accuracy: 58.8889	gate_accuracy: 75.5556
	Task-2	val_accuracy: 54.7619	gate_accuracy: 57.1429
	Task-3	val_accuracy: 79.4872	gate_accuracy: 78.2051
	Task-4	val_accuracy: 67.9487	gate_accuracy: 71.7949
	Task-5	val_accuracy: 62.1212	gate_accuracy: 77.2727
	Task-6	val_accuracy: 80.4348	gate_accuracy: 77.1739
	Task-7	val_accuracy: 72.7273	gate_accuracy: 70.1299
	Task-8	val_accuracy: 75.9494	gate_accuracy: 68.3544
	Task-9	val_accuracy: 75.9036	gate_accuracy: 74.6988
	Task-10	val_accuracy: 87.6712	gate_accuracy: 83.5616
	Task-11	val_accuracy: 86.3636	gate_accuracy: 88.6364
	Task-12	val_accuracy: 74.3590	gate_accuracy: 80.7692
	Task-13	val_accuracy: 78.0822	gate_accuracy: 73.9726
	Task-14	val_accuracy: 71.4286	gate_accuracy: 70.0000
	Task-15	val_accuracy: 81.3333	gate_accuracy: 81.3333
	Task-16	val_accuracy: 66.1972	gate_accuracy: 57.7465
	Task-17	val_accuracy: 74.2268	gate_accuracy: 77.3196
	Task-18	val_accuracy: 75.8621	gate_accuracy: 74.7126
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.1081


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.1499	step1_train_accuracy: 54.6584
STEP-1	Epoch: 20/50	loss: 1.0536	step1_train_accuracy: 78.2609
STEP-1	Epoch: 30/50	loss: 0.6163	step1_train_accuracy: 90.0621
STEP-1	Epoch: 40/50	loss: 0.4292	step1_train_accuracy: 92.2360
STEP-1	Epoch: 50/50	loss: 0.3127	step1_train_accuracy: 91.9255
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7087	gate_loss: 2.0422	step2_classification_accuracy: 78.8647	step_2_gate_accuracy: 43.0435
STEP-2	Epoch: 40/200	classification_loss: 0.5264	gate_loss: 0.8307	step2_classification_accuracy: 83.5507	step_2_gate_accuracy: 77.4879
STEP-2	Epoch: 60/200	classification_loss: 0.4096	gate_loss: 0.4574	step2_classification_accuracy: 86.9324	step_2_gate_accuracy: 87.1739
STEP-2	Epoch: 80/200	classification_loss: 0.3457	gate_loss: 0.3151	step2_classification_accuracy: 88.2850	step_2_gate_accuracy: 91.2077
STEP-2	Epoch: 100/200	classification_loss: 0.2822	gate_loss: 0.2259	step2_classification_accuracy: 90.6039	step_2_gate_accuracy: 93.7681
STEP-2	Epoch: 120/200	classification_loss: 0.2625	gate_loss: 0.1867	step2_classification_accuracy: 90.9179	step_2_gate_accuracy: 94.6377
STEP-2	Epoch: 140/200	classification_loss: 0.2426	gate_loss: 0.1620	step2_classification_accuracy: 91.4010	step_2_gate_accuracy: 94.9517
STEP-2	Epoch: 160/200	classification_loss: 0.2224	gate_loss: 0.1387	step2_classification_accuracy: 92.2947	step_2_gate_accuracy: 95.8696
STEP-2	Epoch: 180/200	classification_loss: 0.2107	gate_loss: 0.1259	step2_classification_accuracy: 92.5121	step_2_gate_accuracy: 95.9179
STEP-2	Epoch: 200/200	classification_loss: 0.2030	gate_loss: 0.1148	step2_classification_accuracy: 92.7536	step_2_gate_accuracy: 96.4493
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.6667	gate_accuracy: 75.0000
	Task-1	val_accuracy: 64.4444	gate_accuracy: 74.4444
	Task-2	val_accuracy: 54.7619	gate_accuracy: 61.9048
	Task-3	val_accuracy: 73.0769	gate_accuracy: 78.2051
	Task-4	val_accuracy: 71.7949	gate_accuracy: 71.7949
	Task-5	val_accuracy: 63.6364	gate_accuracy: 71.2121
	Task-6	val_accuracy: 83.6957	gate_accuracy: 73.9130
	Task-7	val_accuracy: 77.9221	gate_accuracy: 77.9221
	Task-8	val_accuracy: 67.0886	gate_accuracy: 62.0253
	Task-9	val_accuracy: 79.5181	gate_accuracy: 73.4940
	Task-10	val_accuracy: 87.6712	gate_accuracy: 82.1918
	Task-11	val_accuracy: 84.0909	gate_accuracy: 80.6818
	Task-12	val_accuracy: 73.0769	gate_accuracy: 78.2051
	Task-13	val_accuracy: 69.8630	gate_accuracy: 71.2329
	Task-14	val_accuracy: 68.5714	gate_accuracy: 71.4286
	Task-15	val_accuracy: 90.6667	gate_accuracy: 86.6667
	Task-16	val_accuracy: 54.9296	gate_accuracy: 54.9296
	Task-17	val_accuracy: 71.1340	gate_accuracy: 73.1959
	Task-18	val_accuracy: 77.0115	gate_accuracy: 75.8621
	Task-19	val_accuracy: 85.0000	gate_accuracy: 82.5000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.9847


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.1505	step1_train_accuracy: 63.2153
STEP-1	Epoch: 20/50	loss: 0.9718	step1_train_accuracy: 77.1117
STEP-1	Epoch: 30/50	loss: 0.5874	step1_train_accuracy: 85.2861
STEP-1	Epoch: 40/50	loss: 0.4269	step1_train_accuracy: 92.6431
STEP-1	Epoch: 50/50	loss: 0.3307	step1_train_accuracy: 93.4605
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7134	gate_loss: 2.0953	step2_classification_accuracy: 77.0737	step_2_gate_accuracy: 41.2442
STEP-2	Epoch: 40/200	classification_loss: 0.5420	gate_loss: 0.8756	step2_classification_accuracy: 83.2719	step_2_gate_accuracy: 75.8986
STEP-2	Epoch: 60/200	classification_loss: 0.4078	gate_loss: 0.4797	step2_classification_accuracy: 87.2350	step_2_gate_accuracy: 86.2673
STEP-2	Epoch: 80/200	classification_loss: 0.3339	gate_loss: 0.3245	step2_classification_accuracy: 89.3318	step_2_gate_accuracy: 91.0369
STEP-2	Epoch: 100/200	classification_loss: 0.2906	gate_loss: 0.2428	step2_classification_accuracy: 90.7143	step_2_gate_accuracy: 93.4332
STEP-2	Epoch: 120/200	classification_loss: 0.2667	gate_loss: 0.1985	step2_classification_accuracy: 91.0138	step_2_gate_accuracy: 94.6544
STEP-2	Epoch: 140/200	classification_loss: 0.2404	gate_loss: 0.1690	step2_classification_accuracy: 91.5899	step_2_gate_accuracy: 94.9309
STEP-2	Epoch: 160/200	classification_loss: 0.2264	gate_loss: 0.1481	step2_classification_accuracy: 92.0507	step_2_gate_accuracy: 95.2535
STEP-2	Epoch: 180/200	classification_loss: 0.2157	gate_loss: 0.1332	step2_classification_accuracy: 92.2581	step_2_gate_accuracy: 96.1751
STEP-2	Epoch: 200/200	classification_loss: 0.2036	gate_loss: 0.1195	step2_classification_accuracy: 92.9263	step_2_gate_accuracy: 96.5438
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 78.3333	gate_accuracy: 83.8889
	Task-1	val_accuracy: 54.4444	gate_accuracy: 66.6667
	Task-2	val_accuracy: 58.3333	gate_accuracy: 65.4762
	Task-3	val_accuracy: 71.7949	gate_accuracy: 71.7949
	Task-4	val_accuracy: 71.7949	gate_accuracy: 70.5128
	Task-5	val_accuracy: 57.5758	gate_accuracy: 63.6364
	Task-6	val_accuracy: 84.7826	gate_accuracy: 78.2609
	Task-7	val_accuracy: 80.5195	gate_accuracy: 77.9221
	Task-8	val_accuracy: 73.4177	gate_accuracy: 63.2911
	Task-9	val_accuracy: 85.5422	gate_accuracy: 89.1566
	Task-10	val_accuracy: 89.0411	gate_accuracy: 84.9315
	Task-11	val_accuracy: 84.0909	gate_accuracy: 81.8182
	Task-12	val_accuracy: 73.0769	gate_accuracy: 78.2051
	Task-13	val_accuracy: 73.9726	gate_accuracy: 73.9726
	Task-14	val_accuracy: 65.7143	gate_accuracy: 65.7143
	Task-15	val_accuracy: 84.0000	gate_accuracy: 73.3333
	Task-16	val_accuracy: 60.5634	gate_accuracy: 50.7042
	Task-17	val_accuracy: 68.0412	gate_accuracy: 65.9794
	Task-18	val_accuracy: 68.9655	gate_accuracy: 68.9655
	Task-19	val_accuracy: 66.2500	gate_accuracy: 63.7500
	Task-20	val_accuracy: 59.7826	gate_accuracy: 48.9130
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.5243


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 1.8507	step1_train_accuracy: 62.4658
STEP-1	Epoch: 20/50	loss: 0.8050	step1_train_accuracy: 79.4521
STEP-1	Epoch: 30/50	loss: 0.4911	step1_train_accuracy: 93.4247
STEP-1	Epoch: 40/50	loss: 0.3371	step1_train_accuracy: 96.4384
STEP-1	Epoch: 50/50	loss: 0.2189	step1_train_accuracy: 97.8082
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7591	gate_loss: 2.1475	step2_classification_accuracy: 76.2996	step_2_gate_accuracy: 38.7445
STEP-2	Epoch: 40/200	classification_loss: 0.5581	gate_loss: 0.8932	step2_classification_accuracy: 83.0617	step_2_gate_accuracy: 75.3524
STEP-2	Epoch: 60/200	classification_loss: 0.4384	gate_loss: 0.4887	step2_classification_accuracy: 86.4317	step_2_gate_accuracy: 86.3656
STEP-2	Epoch: 80/200	classification_loss: 0.3571	gate_loss: 0.3308	step2_classification_accuracy: 88.7665	step_2_gate_accuracy: 90.4405
STEP-2	Epoch: 100/200	classification_loss: 0.3078	gate_loss: 0.2499	step2_classification_accuracy: 90.1762	step_2_gate_accuracy: 92.7533
STEP-2	Epoch: 120/200	classification_loss: 0.2841	gate_loss: 0.2082	step2_classification_accuracy: 90.5066	step_2_gate_accuracy: 93.8767
STEP-2	Epoch: 140/200	classification_loss: 0.2557	gate_loss: 0.1746	step2_classification_accuracy: 91.4097	step_2_gate_accuracy: 95.1322
STEP-2	Epoch: 160/200	classification_loss: 0.2420	gate_loss: 0.1552	step2_classification_accuracy: 91.6079	step_2_gate_accuracy: 95.3965
STEP-2	Epoch: 180/200	classification_loss: 0.2247	gate_loss: 0.1373	step2_classification_accuracy: 92.1366	step_2_gate_accuracy: 95.7929
STEP-2	Epoch: 200/200	classification_loss: 0.2123	gate_loss: 0.1234	step2_classification_accuracy: 92.5110	step_2_gate_accuracy: 96.2555
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.0000	gate_accuracy: 78.3333
	Task-1	val_accuracy: 61.1111	gate_accuracy: 75.5556
	Task-2	val_accuracy: 61.9048	gate_accuracy: 66.6667
	Task-3	val_accuracy: 70.5128	gate_accuracy: 76.9231
	Task-4	val_accuracy: 71.7949	gate_accuracy: 67.9487
	Task-5	val_accuracy: 50.0000	gate_accuracy: 78.7879
	Task-6	val_accuracy: 82.6087	gate_accuracy: 77.1739
	Task-7	val_accuracy: 75.3247	gate_accuracy: 70.1299
	Task-8	val_accuracy: 60.7595	gate_accuracy: 60.7595
	Task-9	val_accuracy: 78.3133	gate_accuracy: 74.6988
	Task-10	val_accuracy: 83.5616	gate_accuracy: 83.5616
	Task-11	val_accuracy: 75.0000	gate_accuracy: 77.2727
	Task-12	val_accuracy: 73.0769	gate_accuracy: 75.6410
	Task-13	val_accuracy: 75.3425	gate_accuracy: 78.0822
	Task-14	val_accuracy: 70.0000	gate_accuracy: 71.4286
	Task-15	val_accuracy: 85.3333	gate_accuracy: 85.3333
	Task-16	val_accuracy: 53.5211	gate_accuracy: 46.4789
	Task-17	val_accuracy: 73.1959	gate_accuracy: 73.1959
	Task-18	val_accuracy: 74.7126	gate_accuracy: 72.4138
	Task-19	val_accuracy: 71.2500	gate_accuracy: 70.0000
	Task-20	val_accuracy: 63.0435	gate_accuracy: 58.6957
	Task-21	val_accuracy: 80.2198	gate_accuracy: 76.9231
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.8480


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.2216	step1_train_accuracy: 49.5468
STEP-1	Epoch: 20/50	loss: 0.7908	step1_train_accuracy: 84.2900
STEP-1	Epoch: 30/50	loss: 0.4990	step1_train_accuracy: 86.7069
STEP-1	Epoch: 40/50	loss: 0.3744	step1_train_accuracy: 90.3323
STEP-1	Epoch: 50/50	loss: 0.2912	step1_train_accuracy: 91.8429
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7706	gate_loss: 2.2086	step2_classification_accuracy: 76.7089	step_2_gate_accuracy: 37.0042
STEP-2	Epoch: 40/200	classification_loss: 0.5809	gate_loss: 0.9225	step2_classification_accuracy: 81.7932	step_2_gate_accuracy: 74.3882
STEP-2	Epoch: 60/200	classification_loss: 0.4456	gate_loss: 0.5095	step2_classification_accuracy: 86.9409	step_2_gate_accuracy: 85.8017
STEP-2	Epoch: 80/200	classification_loss: 0.3830	gate_loss: 0.3540	step2_classification_accuracy: 88.1224	step_2_gate_accuracy: 89.9789
STEP-2	Epoch: 100/200	classification_loss: 0.3240	gate_loss: 0.2652	step2_classification_accuracy: 89.5570	step_2_gate_accuracy: 92.5316
STEP-2	Epoch: 120/200	classification_loss: 0.2850	gate_loss: 0.2143	step2_classification_accuracy: 90.6540	step_2_gate_accuracy: 94.2827
STEP-2	Epoch: 140/200	classification_loss: 0.2659	gate_loss: 0.1820	step2_classification_accuracy: 91.1392	step_2_gate_accuracy: 94.8734
STEP-2	Epoch: 160/200	classification_loss: 0.2427	gate_loss: 0.1550	step2_classification_accuracy: 91.9620	step_2_gate_accuracy: 95.5274
STEP-2	Epoch: 180/200	classification_loss: 0.2345	gate_loss: 0.1446	step2_classification_accuracy: 92.1519	step_2_gate_accuracy: 95.7384
STEP-2	Epoch: 200/200	classification_loss: 0.2200	gate_loss: 0.1313	step2_classification_accuracy: 92.3840	step_2_gate_accuracy: 96.3924
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.5556	gate_accuracy: 73.3333
	Task-1	val_accuracy: 53.3333	gate_accuracy: 58.8889
	Task-2	val_accuracy: 57.1429	gate_accuracy: 71.4286
	Task-3	val_accuracy: 75.6410	gate_accuracy: 76.9231
	Task-4	val_accuracy: 67.9487	gate_accuracy: 70.5128
	Task-5	val_accuracy: 53.0303	gate_accuracy: 62.1212
	Task-6	val_accuracy: 82.6087	gate_accuracy: 72.8261
	Task-7	val_accuracy: 72.7273	gate_accuracy: 70.1299
	Task-8	val_accuracy: 81.0127	gate_accuracy: 78.4810
	Task-9	val_accuracy: 78.3133	gate_accuracy: 77.1084
	Task-10	val_accuracy: 90.4110	gate_accuracy: 86.3014
	Task-11	val_accuracy: 81.8182	gate_accuracy: 80.6818
	Task-12	val_accuracy: 67.9487	gate_accuracy: 71.7949
	Task-13	val_accuracy: 72.6027	gate_accuracy: 71.2329
	Task-14	val_accuracy: 74.2857	gate_accuracy: 75.7143
	Task-15	val_accuracy: 81.3333	gate_accuracy: 81.3333
	Task-16	val_accuracy: 54.9296	gate_accuracy: 49.2958
	Task-17	val_accuracy: 72.1649	gate_accuracy: 68.0412
	Task-18	val_accuracy: 72.4138	gate_accuracy: 71.2644
	Task-19	val_accuracy: 63.7500	gate_accuracy: 68.7500
	Task-20	val_accuracy: 66.3043	gate_accuracy: 61.9565
	Task-21	val_accuracy: 78.0220	gate_accuracy: 74.7253
	Task-22	val_accuracy: 50.6024	gate_accuracy: 54.2169
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.8397


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 2.3945	step1_train_accuracy: 50.0000
STEP-1	Epoch: 20/50	loss: 0.8863	step1_train_accuracy: 82.2785
STEP-1	Epoch: 30/50	loss: 0.4994	step1_train_accuracy: 90.5063
STEP-1	Epoch: 40/50	loss: 0.3391	step1_train_accuracy: 92.0886
STEP-1	Epoch: 50/50	loss: 0.2542	step1_train_accuracy: 94.3038
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7689	gate_loss: 2.2227	step2_classification_accuracy: 76.9838	step_2_gate_accuracy: 40.1215
STEP-2	Epoch: 40/200	classification_loss: 0.5908	gate_loss: 0.9314	step2_classification_accuracy: 81.7004	step_2_gate_accuracy: 74.3320
STEP-2	Epoch: 60/200	classification_loss: 0.4569	gate_loss: 0.5137	step2_classification_accuracy: 85.5870	step_2_gate_accuracy: 85.6883
STEP-2	Epoch: 80/200	classification_loss: 0.3787	gate_loss: 0.3572	step2_classification_accuracy: 87.6113	step_2_gate_accuracy: 90.0405
STEP-2	Epoch: 100/200	classification_loss: 0.3273	gate_loss: 0.2662	step2_classification_accuracy: 89.4939	step_2_gate_accuracy: 92.8138
STEP-2	Epoch: 120/200	classification_loss: 0.2944	gate_loss: 0.2206	step2_classification_accuracy: 89.9595	step_2_gate_accuracy: 93.4615
STEP-2	Epoch: 140/200	classification_loss: 0.2594	gate_loss: 0.1783	step2_classification_accuracy: 91.1134	step_2_gate_accuracy: 94.9798
STEP-2	Epoch: 160/200	classification_loss: 0.2577	gate_loss: 0.1681	step2_classification_accuracy: 91.2551	step_2_gate_accuracy: 94.9393
STEP-2	Epoch: 180/200	classification_loss: 0.2335	gate_loss: 0.1422	step2_classification_accuracy: 92.0445	step_2_gate_accuracy: 96.0121
STEP-2	Epoch: 200/200	classification_loss: 0.2209	gate_loss: 0.1285	step2_classification_accuracy: 92.1457	step_2_gate_accuracy: 96.0729
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.1111	gate_accuracy: 75.0000
	Task-1	val_accuracy: 55.5556	gate_accuracy: 65.5556
	Task-2	val_accuracy: 60.7143	gate_accuracy: 72.6190
	Task-3	val_accuracy: 58.9744	gate_accuracy: 56.4103
	Task-4	val_accuracy: 73.0769	gate_accuracy: 71.7949
	Task-5	val_accuracy: 57.5758	gate_accuracy: 66.6667
	Task-6	val_accuracy: 82.6087	gate_accuracy: 80.4348
	Task-7	val_accuracy: 79.2208	gate_accuracy: 80.5195
	Task-8	val_accuracy: 73.4177	gate_accuracy: 65.8228
	Task-9	val_accuracy: 81.9277	gate_accuracy: 75.9036
	Task-10	val_accuracy: 90.4110	gate_accuracy: 84.9315
	Task-11	val_accuracy: 77.2727	gate_accuracy: 78.4091
	Task-12	val_accuracy: 69.2308	gate_accuracy: 71.7949
	Task-13	val_accuracy: 73.9726	gate_accuracy: 75.3425
	Task-14	val_accuracy: 65.7143	gate_accuracy: 67.1429
	Task-15	val_accuracy: 82.6667	gate_accuracy: 78.6667
	Task-16	val_accuracy: 60.5634	gate_accuracy: 60.5634
	Task-17	val_accuracy: 67.0103	gate_accuracy: 62.8866
	Task-18	val_accuracy: 71.2644	gate_accuracy: 72.4138
	Task-19	val_accuracy: 70.0000	gate_accuracy: 68.7500
	Task-20	val_accuracy: 65.2174	gate_accuracy: 61.9565
	Task-21	val_accuracy: 79.1209	gate_accuracy: 76.9231
	Task-22	val_accuracy: 62.6506	gate_accuracy: 66.2651
	Task-23	val_accuracy: 67.0886	gate_accuracy: 68.3544
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.2329


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.6146	step1_train_accuracy: 45.7680
STEP-1	Epoch: 20/50	loss: 1.0346	step1_train_accuracy: 80.5643
STEP-1	Epoch: 30/50	loss: 0.5923	step1_train_accuracy: 88.4013
STEP-1	Epoch: 40/50	loss: 0.4133	step1_train_accuracy: 91.2226
STEP-1	Epoch: 50/50	loss: 0.3208	step1_train_accuracy: 93.1034
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8280	gate_loss: 2.3556	step2_classification_accuracy: 74.6887	step_2_gate_accuracy: 35.3502
STEP-2	Epoch: 40/200	classification_loss: 0.6169	gate_loss: 1.0079	step2_classification_accuracy: 80.2335	step_2_gate_accuracy: 71.6731
STEP-2	Epoch: 60/200	classification_loss: 0.4661	gate_loss: 0.5475	step2_classification_accuracy: 85.3113	step_2_gate_accuracy: 84.6498
STEP-2	Epoch: 80/200	classification_loss: 0.3901	gate_loss: 0.3710	step2_classification_accuracy: 87.6654	step_2_gate_accuracy: 89.8249
STEP-2	Epoch: 100/200	classification_loss: 0.3420	gate_loss: 0.2849	step2_classification_accuracy: 88.9689	step_2_gate_accuracy: 92.0817
STEP-2	Epoch: 120/200	classification_loss: 0.2963	gate_loss: 0.2251	step2_classification_accuracy: 90.3113	step_2_gate_accuracy: 93.5798
STEP-2	Epoch: 140/200	classification_loss: 0.2791	gate_loss: 0.1986	step2_classification_accuracy: 90.5447	step_2_gate_accuracy: 94.5136
STEP-2	Epoch: 160/200	classification_loss: 0.2518	gate_loss: 0.1692	step2_classification_accuracy: 91.3813	step_2_gate_accuracy: 94.8833
STEP-2	Epoch: 180/200	classification_loss: 0.2418	gate_loss: 0.1542	step2_classification_accuracy: 91.7121	step_2_gate_accuracy: 95.5058
STEP-2	Epoch: 200/200	classification_loss: 0.2360	gate_loss: 0.1436	step2_classification_accuracy: 91.7899	step_2_gate_accuracy: 95.5642
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.8889	gate_accuracy: 75.5556
	Task-1	val_accuracy: 52.2222	gate_accuracy: 63.3333
	Task-2	val_accuracy: 60.7143	gate_accuracy: 72.6190
	Task-3	val_accuracy: 71.7949	gate_accuracy: 76.9231
	Task-4	val_accuracy: 66.6667	gate_accuracy: 62.8205
	Task-5	val_accuracy: 60.6061	gate_accuracy: 68.1818
	Task-6	val_accuracy: 80.4348	gate_accuracy: 79.3478
	Task-7	val_accuracy: 81.8182	gate_accuracy: 85.7143
	Task-8	val_accuracy: 63.2911	gate_accuracy: 65.8228
	Task-9	val_accuracy: 75.9036	gate_accuracy: 67.4699
	Task-10	val_accuracy: 87.6712	gate_accuracy: 79.4521
	Task-11	val_accuracy: 81.8182	gate_accuracy: 79.5455
	Task-12	val_accuracy: 70.5128	gate_accuracy: 76.9231
	Task-13	val_accuracy: 69.8630	gate_accuracy: 68.4932
	Task-14	val_accuracy: 67.1429	gate_accuracy: 70.0000
	Task-15	val_accuracy: 80.0000	gate_accuracy: 78.6667
	Task-16	val_accuracy: 56.3380	gate_accuracy: 54.9296
	Task-17	val_accuracy: 70.1031	gate_accuracy: 63.9175
	Task-18	val_accuracy: 75.8621	gate_accuracy: 77.0115
	Task-19	val_accuracy: 70.0000	gate_accuracy: 73.7500
	Task-20	val_accuracy: 73.9130	gate_accuracy: 68.4783
	Task-21	val_accuracy: 81.3187	gate_accuracy: 76.9231
	Task-22	val_accuracy: 59.0361	gate_accuracy: 67.4699
	Task-23	val_accuracy: 73.4177	gate_accuracy: 73.4177
	Task-24	val_accuracy: 63.7500	gate_accuracy: 62.5000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.7985


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 2.4644	step1_train_accuracy: 60.6509
STEP-1	Epoch: 20/50	loss: 0.8194	step1_train_accuracy: 86.6864
STEP-1	Epoch: 30/50	loss: 0.4237	step1_train_accuracy: 92.6036
STEP-1	Epoch: 40/50	loss: 0.2888	step1_train_accuracy: 94.6746
STEP-1	Epoch: 50/50	loss: 0.2151	step1_train_accuracy: 97.6331
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8165	gate_loss: 2.3093	step2_classification_accuracy: 74.7753	step_2_gate_accuracy: 37.5843
STEP-2	Epoch: 40/200	classification_loss: 0.6280	gate_loss: 0.9838	step2_classification_accuracy: 80.5993	step_2_gate_accuracy: 72.3034
STEP-2	Epoch: 60/200	classification_loss: 0.4706	gate_loss: 0.5491	step2_classification_accuracy: 85.3184	step_2_gate_accuracy: 83.8015
STEP-2	Epoch: 80/200	classification_loss: 0.3878	gate_loss: 0.3767	step2_classification_accuracy: 87.8277	step_2_gate_accuracy: 88.6142
STEP-2	Epoch: 100/200	classification_loss: 0.3349	gate_loss: 0.2856	step2_classification_accuracy: 89.4195	step_2_gate_accuracy: 91.8914
STEP-2	Epoch: 120/200	classification_loss: 0.3089	gate_loss: 0.2398	step2_classification_accuracy: 90.3371	step_2_gate_accuracy: 93.1461
STEP-2	Epoch: 140/200	classification_loss: 0.2686	gate_loss: 0.1954	step2_classification_accuracy: 91.2734	step_2_gate_accuracy: 94.5318
STEP-2	Epoch: 160/200	classification_loss: 0.2567	gate_loss: 0.1749	step2_classification_accuracy: 91.6854	step_2_gate_accuracy: 94.7940
STEP-2	Epoch: 180/200	classification_loss: 0.2444	gate_loss: 0.1599	step2_classification_accuracy: 91.9288	step_2_gate_accuracy: 95.1311
STEP-2	Epoch: 200/200	classification_loss: 0.2329	gate_loss: 0.1467	step2_classification_accuracy: 92.0787	step_2_gate_accuracy: 95.4494
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.2222	gate_accuracy: 75.0000
	Task-1	val_accuracy: 56.6667	gate_accuracy: 63.3333
	Task-2	val_accuracy: 58.3333	gate_accuracy: 66.6667
	Task-3	val_accuracy: 62.8205	gate_accuracy: 70.5128
	Task-4	val_accuracy: 66.6667	gate_accuracy: 65.3846
	Task-5	val_accuracy: 59.0909	gate_accuracy: 72.7273
	Task-6	val_accuracy: 72.8261	gate_accuracy: 67.3913
	Task-7	val_accuracy: 75.3247	gate_accuracy: 72.7273
	Task-8	val_accuracy: 64.5570	gate_accuracy: 58.2278
	Task-9	val_accuracy: 72.2892	gate_accuracy: 63.8554
	Task-10	val_accuracy: 83.5616	gate_accuracy: 79.4521
	Task-11	val_accuracy: 73.8636	gate_accuracy: 73.8636
	Task-12	val_accuracy: 71.7949	gate_accuracy: 75.6410
	Task-13	val_accuracy: 72.6027	gate_accuracy: 72.6027
	Task-14	val_accuracy: 67.1429	gate_accuracy: 70.0000
	Task-15	val_accuracy: 78.6667	gate_accuracy: 74.6667
	Task-16	val_accuracy: 66.1972	gate_accuracy: 59.1549
	Task-17	val_accuracy: 68.0412	gate_accuracy: 60.8247
	Task-18	val_accuracy: 75.8621	gate_accuracy: 74.7126
	Task-19	val_accuracy: 62.5000	gate_accuracy: 55.0000
	Task-20	val_accuracy: 68.4783	gate_accuracy: 65.2174
	Task-21	val_accuracy: 84.6154	gate_accuracy: 80.2198
	Task-22	val_accuracy: 59.0361	gate_accuracy: 61.4458
	Task-23	val_accuracy: 64.5570	gate_accuracy: 65.8228
	Task-24	val_accuracy: 68.7500	gate_accuracy: 72.5000
	Task-25	val_accuracy: 74.1176	gate_accuracy: 72.9412
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.0358


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 3.0328	step1_train_accuracy: 34.9673
STEP-1	Epoch: 20/50	loss: 1.0921	step1_train_accuracy: 79.4118
STEP-1	Epoch: 30/50	loss: 0.5404	step1_train_accuracy: 91.1765
STEP-1	Epoch: 40/50	loss: 0.3660	step1_train_accuracy: 94.7712
STEP-1	Epoch: 50/50	loss: 0.2761	step1_train_accuracy: 97.7124
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8317	gate_loss: 2.2671	step2_classification_accuracy: 74.2599	step_2_gate_accuracy: 35.0541
STEP-2	Epoch: 40/200	classification_loss: 0.6384	gate_loss: 0.9747	step2_classification_accuracy: 80.9386	step_2_gate_accuracy: 71.6426
STEP-2	Epoch: 60/200	classification_loss: 0.4955	gate_loss: 0.5635	step2_classification_accuracy: 85.2347	step_2_gate_accuracy: 83.6643
STEP-2	Epoch: 80/200	classification_loss: 0.3983	gate_loss: 0.3870	step2_classification_accuracy: 87.7256	step_2_gate_accuracy: 88.8087
STEP-2	Epoch: 100/200	classification_loss: 0.3639	gate_loss: 0.3046	step2_classification_accuracy: 88.7365	step_2_gate_accuracy: 90.9928
STEP-2	Epoch: 120/200	classification_loss: 0.3169	gate_loss: 0.2449	step2_classification_accuracy: 90.0181	step_2_gate_accuracy: 92.7256
STEP-2	Epoch: 140/200	classification_loss: 0.2791	gate_loss: 0.2035	step2_classification_accuracy: 90.9928	step_2_gate_accuracy: 94.0794
STEP-2	Epoch: 160/200	classification_loss: 0.2779	gate_loss: 0.1908	step2_classification_accuracy: 91.1191	step_2_gate_accuracy: 93.9531
STEP-2	Epoch: 180/200	classification_loss: 0.2522	gate_loss: 0.1684	step2_classification_accuracy: 91.5162	step_2_gate_accuracy: 94.9097
STEP-2	Epoch: 200/200	classification_loss: 0.2412	gate_loss: 0.1538	step2_classification_accuracy: 92.0217	step_2_gate_accuracy: 95.1625
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.1111	gate_accuracy: 75.0000
	Task-1	val_accuracy: 54.4444	gate_accuracy: 63.3333
	Task-2	val_accuracy: 52.3810	gate_accuracy: 58.3333
	Task-3	val_accuracy: 70.5128	gate_accuracy: 75.6410
	Task-4	val_accuracy: 65.3846	gate_accuracy: 70.5128
	Task-5	val_accuracy: 62.1212	gate_accuracy: 72.7273
	Task-6	val_accuracy: 81.5217	gate_accuracy: 80.4348
	Task-7	val_accuracy: 72.7273	gate_accuracy: 68.8312
	Task-8	val_accuracy: 73.4177	gate_accuracy: 67.0886
	Task-9	val_accuracy: 75.9036	gate_accuracy: 77.1084
	Task-10	val_accuracy: 84.9315	gate_accuracy: 86.3014
	Task-11	val_accuracy: 78.4091	gate_accuracy: 75.0000
	Task-12	val_accuracy: 66.6667	gate_accuracy: 69.2308
	Task-13	val_accuracy: 71.2329	gate_accuracy: 72.6027
	Task-14	val_accuracy: 68.5714	gate_accuracy: 70.0000
	Task-15	val_accuracy: 77.3333	gate_accuracy: 72.0000
	Task-16	val_accuracy: 66.1972	gate_accuracy: 60.5634
	Task-17	val_accuracy: 76.2887	gate_accuracy: 70.1031
	Task-18	val_accuracy: 70.1149	gate_accuracy: 71.2644
	Task-19	val_accuracy: 61.2500	gate_accuracy: 63.7500
	Task-20	val_accuracy: 61.9565	gate_accuracy: 58.6957
	Task-21	val_accuracy: 81.3187	gate_accuracy: 78.0220
	Task-22	val_accuracy: 59.0361	gate_accuracy: 56.6265
	Task-23	val_accuracy: 63.2911	gate_accuracy: 64.5570
	Task-24	val_accuracy: 61.2500	gate_accuracy: 52.5000
	Task-25	val_accuracy: 78.8235	gate_accuracy: 76.4706
	Task-26	val_accuracy: 80.5195	gate_accuracy: 70.1299
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.7288


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 3.3579	step1_train_accuracy: 36.5854
STEP-1	Epoch: 20/50	loss: 1.3080	step1_train_accuracy: 77.2358
STEP-1	Epoch: 30/50	loss: 0.7419	step1_train_accuracy: 86.5854
STEP-1	Epoch: 40/50	loss: 0.5516	step1_train_accuracy: 88.2114
STEP-1	Epoch: 50/50	loss: 0.4383	step1_train_accuracy: 90.6504
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8398	gate_loss: 2.3367	step2_classification_accuracy: 74.1463	step_2_gate_accuracy: 35.5401
STEP-2	Epoch: 40/200	classification_loss: 0.6425	gate_loss: 0.9765	step2_classification_accuracy: 80.0000	step_2_gate_accuracy: 72.5087
STEP-2	Epoch: 60/200	classification_loss: 0.5091	gate_loss: 0.5600	step2_classification_accuracy: 84.0941	step_2_gate_accuracy: 83.2753
STEP-2	Epoch: 80/200	classification_loss: 0.4148	gate_loss: 0.3900	step2_classification_accuracy: 86.4634	step_2_gate_accuracy: 88.4669
STEP-2	Epoch: 100/200	classification_loss: 0.3741	gate_loss: 0.3088	step2_classification_accuracy: 88.1707	step_2_gate_accuracy: 90.8711
STEP-2	Epoch: 120/200	classification_loss: 0.3265	gate_loss: 0.2484	step2_classification_accuracy: 89.4599	step_2_gate_accuracy: 92.6307
STEP-2	Epoch: 140/200	classification_loss: 0.3029	gate_loss: 0.2158	step2_classification_accuracy: 89.9477	step_2_gate_accuracy: 93.6411
STEP-2	Epoch: 160/200	classification_loss: 0.2834	gate_loss: 0.1935	step2_classification_accuracy: 90.3659	step_2_gate_accuracy: 94.1463
STEP-2	Epoch: 180/200	classification_loss: 0.2662	gate_loss: 0.1749	step2_classification_accuracy: 90.9408	step_2_gate_accuracy: 94.5993
STEP-2	Epoch: 200/200	classification_loss: 0.2490	gate_loss: 0.1578	step2_classification_accuracy: 91.3937	step_2_gate_accuracy: 95.0523
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.7778	gate_accuracy: 72.2222
	Task-1	val_accuracy: 56.6667	gate_accuracy: 74.4444
	Task-2	val_accuracy: 51.1905	gate_accuracy: 64.2857
	Task-3	val_accuracy: 65.3846	gate_accuracy: 70.5128
	Task-4	val_accuracy: 71.7949	gate_accuracy: 73.0769
	Task-5	val_accuracy: 53.0303	gate_accuracy: 65.1515
	Task-6	val_accuracy: 75.0000	gate_accuracy: 71.7391
	Task-7	val_accuracy: 76.6234	gate_accuracy: 67.5325
	Task-8	val_accuracy: 72.1519	gate_accuracy: 64.5570
	Task-9	val_accuracy: 85.5422	gate_accuracy: 74.6988
	Task-10	val_accuracy: 84.9315	gate_accuracy: 82.1918
	Task-11	val_accuracy: 79.5455	gate_accuracy: 79.5455
	Task-12	val_accuracy: 67.9487	gate_accuracy: 75.6410
	Task-13	val_accuracy: 78.0822	gate_accuracy: 75.3425
	Task-14	val_accuracy: 68.5714	gate_accuracy: 70.0000
	Task-15	val_accuracy: 68.0000	gate_accuracy: 70.6667
	Task-16	val_accuracy: 63.3803	gate_accuracy: 57.7465
	Task-17	val_accuracy: 65.9794	gate_accuracy: 58.7629
	Task-18	val_accuracy: 68.9655	gate_accuracy: 70.1149
	Task-19	val_accuracy: 66.2500	gate_accuracy: 65.0000
	Task-20	val_accuracy: 61.9565	gate_accuracy: 58.6957
	Task-21	val_accuracy: 83.5165	gate_accuracy: 82.4176
	Task-22	val_accuracy: 59.0361	gate_accuracy: 61.4458
	Task-23	val_accuracy: 70.8861	gate_accuracy: 73.4177
	Task-24	val_accuracy: 63.7500	gate_accuracy: 60.0000
	Task-25	val_accuracy: 76.4706	gate_accuracy: 63.5294
	Task-26	val_accuracy: 81.8182	gate_accuracy: 75.3247
	Task-27	val_accuracy: 62.9032	gate_accuracy: 53.2258
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.2078


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 2.3460	step1_train_accuracy: 55.1532
STEP-1	Epoch: 20/50	loss: 0.8592	step1_train_accuracy: 88.5794
STEP-1	Epoch: 30/50	loss: 0.4356	step1_train_accuracy: 92.4791
STEP-1	Epoch: 40/50	loss: 0.2781	step1_train_accuracy: 95.8217
STEP-1	Epoch: 50/50	loss: 0.2062	step1_train_accuracy: 95.8217
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8795	gate_loss: 2.3887	step2_classification_accuracy: 73.5522	step_2_gate_accuracy: 34.2088
STEP-2	Epoch: 40/200	classification_loss: 0.6738	gate_loss: 0.9978	step2_classification_accuracy: 79.3939	step_2_gate_accuracy: 71.6498
STEP-2	Epoch: 60/200	classification_loss: 0.5324	gate_loss: 0.5733	step2_classification_accuracy: 83.4007	step_2_gate_accuracy: 83.1818
STEP-2	Epoch: 80/200	classification_loss: 0.4348	gate_loss: 0.3960	step2_classification_accuracy: 86.1279	step_2_gate_accuracy: 88.4007
STEP-2	Epoch: 100/200	classification_loss: 0.3848	gate_loss: 0.3080	step2_classification_accuracy: 88.0976	step_2_gate_accuracy: 90.9764
STEP-2	Epoch: 120/200	classification_loss: 0.3423	gate_loss: 0.2492	step2_classification_accuracy: 89.4444	step_2_gate_accuracy: 92.7104
STEP-2	Epoch: 140/200	classification_loss: 0.3087	gate_loss: 0.2122	step2_classification_accuracy: 90.6229	step_2_gate_accuracy: 93.7205
STEP-2	Epoch: 160/200	classification_loss: 0.2899	gate_loss: 0.1857	step2_classification_accuracy: 90.5387	step_2_gate_accuracy: 94.7138
STEP-2	Epoch: 180/200	classification_loss: 0.2588	gate_loss: 0.1592	step2_classification_accuracy: 91.6162	step_2_gate_accuracy: 95.3704
STEP-2	Epoch: 200/200	classification_loss: 0.2495	gate_loss: 0.1474	step2_classification_accuracy: 91.3636	step_2_gate_accuracy: 95.5724
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 67.2222	gate_accuracy: 80.5556
	Task-1	val_accuracy: 47.7778	gate_accuracy: 55.5556
	Task-2	val_accuracy: 47.6190	gate_accuracy: 60.7143
	Task-3	val_accuracy: 65.3846	gate_accuracy: 67.9487
	Task-4	val_accuracy: 64.1026	gate_accuracy: 60.2564
	Task-5	val_accuracy: 50.0000	gate_accuracy: 62.1212
	Task-6	val_accuracy: 84.7826	gate_accuracy: 82.6087
	Task-7	val_accuracy: 72.7273	gate_accuracy: 64.9351
	Task-8	val_accuracy: 63.2911	gate_accuracy: 60.7595
	Task-9	val_accuracy: 80.7229	gate_accuracy: 77.1084
	Task-10	val_accuracy: 84.9315	gate_accuracy: 80.8219
	Task-11	val_accuracy: 78.4091	gate_accuracy: 79.5455
	Task-12	val_accuracy: 80.7692	gate_accuracy: 85.8974
	Task-13	val_accuracy: 72.6027	gate_accuracy: 65.7534
	Task-14	val_accuracy: 68.5714	gate_accuracy: 74.2857
	Task-15	val_accuracy: 73.3333	gate_accuracy: 70.6667
	Task-16	val_accuracy: 57.7465	gate_accuracy: 53.5211
	Task-17	val_accuracy: 65.9794	gate_accuracy: 61.8557
	Task-18	val_accuracy: 72.4138	gate_accuracy: 67.8161
	Task-19	val_accuracy: 62.5000	gate_accuracy: 63.7500
	Task-20	val_accuracy: 60.8696	gate_accuracy: 55.4348
	Task-21	val_accuracy: 82.4176	gate_accuracy: 78.0220
	Task-22	val_accuracy: 55.4217	gate_accuracy: 59.0361
	Task-23	val_accuracy: 65.8228	gate_accuracy: 68.3544
	Task-24	val_accuracy: 61.2500	gate_accuracy: 58.7500
	Task-25	val_accuracy: 78.8235	gate_accuracy: 74.1176
	Task-26	val_accuracy: 81.8182	gate_accuracy: 76.6234
	Task-27	val_accuracy: 43.5484	gate_accuracy: 38.7097
	Task-28	val_accuracy: 67.7778	gate_accuracy: 65.5556
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.0476


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.5653	step1_train_accuracy: 40.1899
STEP-1	Epoch: 20/50	loss: 1.0810	step1_train_accuracy: 73.1013
STEP-1	Epoch: 30/50	loss: 0.6469	step1_train_accuracy: 91.4557
STEP-1	Epoch: 40/50	loss: 0.4074	step1_train_accuracy: 95.8861
STEP-1	Epoch: 50/50	loss: 0.2758	step1_train_accuracy: 98.7342
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8824	gate_loss: 2.3469	step2_classification_accuracy: 73.2248	step_2_gate_accuracy: 34.2182
STEP-2	Epoch: 40/200	classification_loss: 0.6796	gate_loss: 0.9837	step2_classification_accuracy: 79.8371	step_2_gate_accuracy: 72.6059
STEP-2	Epoch: 60/200	classification_loss: 0.5291	gate_loss: 0.5667	step2_classification_accuracy: 83.9902	step_2_gate_accuracy: 83.3713
STEP-2	Epoch: 80/200	classification_loss: 0.4353	gate_loss: 0.3932	step2_classification_accuracy: 87.1498	step_2_gate_accuracy: 88.5831
STEP-2	Epoch: 100/200	classification_loss: 0.3694	gate_loss: 0.2998	step2_classification_accuracy: 88.6482	step_2_gate_accuracy: 91.2215
STEP-2	Epoch: 120/200	classification_loss: 0.3447	gate_loss: 0.2543	step2_classification_accuracy: 89.2345	step_2_gate_accuracy: 92.1987
STEP-2	Epoch: 140/200	classification_loss: 0.3053	gate_loss: 0.2113	step2_classification_accuracy: 90.1792	step_2_gate_accuracy: 93.6645
STEP-2	Epoch: 160/200	classification_loss: 0.2844	gate_loss: 0.1881	step2_classification_accuracy: 90.7166	step_2_gate_accuracy: 94.1531
STEP-2	Epoch: 180/200	classification_loss: 0.2609	gate_loss: 0.1668	step2_classification_accuracy: 91.4495	step_2_gate_accuracy: 95.1466
STEP-2	Epoch: 200/200	classification_loss: 0.2556	gate_loss: 0.1567	step2_classification_accuracy: 91.1238	step_2_gate_accuracy: 94.9837
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.8889	gate_accuracy: 73.3333
	Task-1	val_accuracy: 51.1111	gate_accuracy: 66.6667
	Task-2	val_accuracy: 44.0476	gate_accuracy: 60.7143
	Task-3	val_accuracy: 67.9487	gate_accuracy: 67.9487
	Task-4	val_accuracy: 69.2308	gate_accuracy: 67.9487
	Task-5	val_accuracy: 53.0303	gate_accuracy: 63.6364
	Task-6	val_accuracy: 79.3478	gate_accuracy: 72.8261
	Task-7	val_accuracy: 81.8182	gate_accuracy: 79.2208
	Task-8	val_accuracy: 81.0127	gate_accuracy: 74.6835
	Task-9	val_accuracy: 77.1084	gate_accuracy: 74.6988
	Task-10	val_accuracy: 82.1918	gate_accuracy: 79.4521
	Task-11	val_accuracy: 79.5455	gate_accuracy: 79.5455
	Task-12	val_accuracy: 65.3846	gate_accuracy: 74.3590
	Task-13	val_accuracy: 76.7123	gate_accuracy: 71.2329
	Task-14	val_accuracy: 64.2857	gate_accuracy: 55.7143
	Task-15	val_accuracy: 77.3333	gate_accuracy: 74.6667
	Task-16	val_accuracy: 61.9718	gate_accuracy: 59.1549
	Task-17	val_accuracy: 64.9485	gate_accuracy: 58.7629
	Task-18	val_accuracy: 67.8161	gate_accuracy: 70.1149
	Task-19	val_accuracy: 62.5000	gate_accuracy: 60.0000
	Task-20	val_accuracy: 59.7826	gate_accuracy: 54.3478
	Task-21	val_accuracy: 74.7253	gate_accuracy: 68.1319
	Task-22	val_accuracy: 62.6506	gate_accuracy: 65.0602
	Task-23	val_accuracy: 60.7595	gate_accuracy: 63.2911
	Task-24	val_accuracy: 61.2500	gate_accuracy: 61.2500
	Task-25	val_accuracy: 80.0000	gate_accuracy: 71.7647
	Task-26	val_accuracy: 75.3247	gate_accuracy: 57.1429
	Task-27	val_accuracy: 46.7742	gate_accuracy: 46.7742
	Task-28	val_accuracy: 67.7778	gate_accuracy: 65.5556
	Task-29	val_accuracy: 77.2152	gate_accuracy: 68.3544
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.2626


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 2.8887	step1_train_accuracy: 44.3750
STEP-1	Epoch: 20/50	loss: 1.0549	step1_train_accuracy: 80.3125
STEP-1	Epoch: 30/50	loss: 0.4955	step1_train_accuracy: 92.1875
STEP-1	Epoch: 40/50	loss: 0.3139	step1_train_accuracy: 95.6250
STEP-1	Epoch: 50/50	loss: 0.2260	step1_train_accuracy: 96.8750
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9346	gate_loss: 2.4053	step2_classification_accuracy: 71.8612	step_2_gate_accuracy: 33.9748
STEP-2	Epoch: 40/200	classification_loss: 0.7235	gate_loss: 1.0317	step2_classification_accuracy: 78.6278	step_2_gate_accuracy: 70.1893
STEP-2	Epoch: 60/200	classification_loss: 0.5735	gate_loss: 0.6231	step2_classification_accuracy: 82.8233	step_2_gate_accuracy: 80.9779
STEP-2	Epoch: 80/200	classification_loss: 0.4745	gate_loss: 0.4356	step2_classification_accuracy: 85.8360	step_2_gate_accuracy: 86.9558
STEP-2	Epoch: 100/200	classification_loss: 0.3990	gate_loss: 0.3393	step2_classification_accuracy: 87.3186	step_2_gate_accuracy: 89.7319
STEP-2	Epoch: 120/200	classification_loss: 0.3655	gate_loss: 0.2856	step2_classification_accuracy: 88.4700	step_2_gate_accuracy: 91.5300
STEP-2	Epoch: 140/200	classification_loss: 0.3183	gate_loss: 0.2309	step2_classification_accuracy: 89.6372	step_2_gate_accuracy: 93.2492
STEP-2	Epoch: 160/200	classification_loss: 0.2956	gate_loss: 0.2089	step2_classification_accuracy: 90.3155	step_2_gate_accuracy: 93.4385
STEP-2	Epoch: 180/200	classification_loss: 0.2758	gate_loss: 0.1834	step2_classification_accuracy: 91.0252	step_2_gate_accuracy: 94.3533
STEP-2	Epoch: 200/200	classification_loss: 0.2577	gate_loss: 0.1652	step2_classification_accuracy: 91.1672	step_2_gate_accuracy: 94.8896
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.7778	gate_accuracy: 64.4444
	Task-1	val_accuracy: 46.6667	gate_accuracy: 61.1111
	Task-2	val_accuracy: 55.9524	gate_accuracy: 69.0476
	Task-3	val_accuracy: 69.2308	gate_accuracy: 78.2051
	Task-4	val_accuracy: 74.3590	gate_accuracy: 75.6410
	Task-5	val_accuracy: 68.1818	gate_accuracy: 78.7879
	Task-6	val_accuracy: 80.4348	gate_accuracy: 77.1739
	Task-7	val_accuracy: 79.2208	gate_accuracy: 80.5195
	Task-8	val_accuracy: 67.0886	gate_accuracy: 62.0253
	Task-9	val_accuracy: 75.9036	gate_accuracy: 71.0843
	Task-10	val_accuracy: 75.3425	gate_accuracy: 71.2329
	Task-11	val_accuracy: 77.2727	gate_accuracy: 78.4091
	Task-12	val_accuracy: 64.1026	gate_accuracy: 67.9487
	Task-13	val_accuracy: 64.3836	gate_accuracy: 64.3836
	Task-14	val_accuracy: 67.1429	gate_accuracy: 65.7143
	Task-15	val_accuracy: 69.3333	gate_accuracy: 64.0000
	Task-16	val_accuracy: 45.0704	gate_accuracy: 40.8451
	Task-17	val_accuracy: 70.1031	gate_accuracy: 59.7938
	Task-18	val_accuracy: 71.2644	gate_accuracy: 67.8161
	Task-19	val_accuracy: 72.5000	gate_accuracy: 71.2500
	Task-20	val_accuracy: 59.7826	gate_accuracy: 55.4348
	Task-21	val_accuracy: 73.6264	gate_accuracy: 69.2308
	Task-22	val_accuracy: 60.2410	gate_accuracy: 65.0602
	Task-23	val_accuracy: 63.2911	gate_accuracy: 65.8228
	Task-24	val_accuracy: 61.2500	gate_accuracy: 51.2500
	Task-25	val_accuracy: 74.1176	gate_accuracy: 70.5882
	Task-26	val_accuracy: 85.7143	gate_accuracy: 85.7143
	Task-27	val_accuracy: 54.8387	gate_accuracy: 51.6129
	Task-28	val_accuracy: 70.0000	gate_accuracy: 68.8889
	Task-29	val_accuracy: 72.1519	gate_accuracy: 67.0886
	Task-30	val_accuracy: 48.7500	gate_accuracy: 51.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.8079


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 2.5176	step1_train_accuracy: 52.3669
STEP-1	Epoch: 20/50	loss: 0.9488	step1_train_accuracy: 87.5740
STEP-1	Epoch: 30/50	loss: 0.5005	step1_train_accuracy: 95.5621
STEP-1	Epoch: 40/50	loss: 0.3208	step1_train_accuracy: 95.8580
STEP-1	Epoch: 50/50	loss: 0.2262	step1_train_accuracy: 96.4497
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9602	gate_loss: 2.4045	step2_classification_accuracy: 70.4893	step_2_gate_accuracy: 34.0979
STEP-2	Epoch: 40/200	classification_loss: 0.7493	gate_loss: 1.0464	step2_classification_accuracy: 77.4771	step_2_gate_accuracy: 69.7401
STEP-2	Epoch: 60/200	classification_loss: 0.5898	gate_loss: 0.6245	step2_classification_accuracy: 81.9572	step_2_gate_accuracy: 81.3609
STEP-2	Epoch: 80/200	classification_loss: 0.4875	gate_loss: 0.4430	step2_classification_accuracy: 84.8930	step_2_gate_accuracy: 86.8807
STEP-2	Epoch: 100/200	classification_loss: 0.4194	gate_loss: 0.3431	step2_classification_accuracy: 86.8349	step_2_gate_accuracy: 89.7095
STEP-2	Epoch: 120/200	classification_loss: 0.3609	gate_loss: 0.2744	step2_classification_accuracy: 88.6544	step_2_gate_accuracy: 91.5902
STEP-2	Epoch: 140/200	classification_loss: 0.3402	gate_loss: 0.2441	step2_classification_accuracy: 88.9450	step_2_gate_accuracy: 92.3853
STEP-2	Epoch: 160/200	classification_loss: 0.3080	gate_loss: 0.2110	step2_classification_accuracy: 89.8930	step_2_gate_accuracy: 93.7003
STEP-2	Epoch: 180/200	classification_loss: 0.2941	gate_loss: 0.1947	step2_classification_accuracy: 89.8930	step_2_gate_accuracy: 93.9602
STEP-2	Epoch: 200/200	classification_loss: 0.2780	gate_loss: 0.1749	step2_classification_accuracy: 90.5505	step_2_gate_accuracy: 94.6789
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.6667	gate_accuracy: 68.8889
	Task-1	val_accuracy: 50.0000	gate_accuracy: 60.0000
	Task-2	val_accuracy: 47.6190	gate_accuracy: 69.0476
	Task-3	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-4	val_accuracy: 67.9487	gate_accuracy: 71.7949
	Task-5	val_accuracy: 56.0606	gate_accuracy: 68.1818
	Task-6	val_accuracy: 70.6522	gate_accuracy: 66.3043
	Task-7	val_accuracy: 76.6234	gate_accuracy: 70.1299
	Task-8	val_accuracy: 60.7595	gate_accuracy: 60.7595
	Task-9	val_accuracy: 83.1325	gate_accuracy: 78.3133
	Task-10	val_accuracy: 73.9726	gate_accuracy: 68.4932
	Task-11	val_accuracy: 73.8636	gate_accuracy: 70.4545
	Task-12	val_accuracy: 61.5385	gate_accuracy: 56.4103
	Task-13	val_accuracy: 72.6027	gate_accuracy: 72.6027
	Task-14	val_accuracy: 70.0000	gate_accuracy: 70.0000
	Task-15	val_accuracy: 74.6667	gate_accuracy: 70.6667
	Task-16	val_accuracy: 61.9718	gate_accuracy: 53.5211
	Task-17	val_accuracy: 64.9485	gate_accuracy: 63.9175
	Task-18	val_accuracy: 70.1149	gate_accuracy: 66.6667
	Task-19	val_accuracy: 61.2500	gate_accuracy: 62.5000
	Task-20	val_accuracy: 55.4348	gate_accuracy: 50.0000
	Task-21	val_accuracy: 83.5165	gate_accuracy: 81.3187
	Task-22	val_accuracy: 57.8313	gate_accuracy: 57.8313
	Task-23	val_accuracy: 58.2278	gate_accuracy: 60.7595
	Task-24	val_accuracy: 57.5000	gate_accuracy: 57.5000
	Task-25	val_accuracy: 74.1176	gate_accuracy: 69.4118
	Task-26	val_accuracy: 81.8182	gate_accuracy: 72.7273
	Task-27	val_accuracy: 54.8387	gate_accuracy: 48.3871
	Task-28	val_accuracy: 63.3333	gate_accuracy: 65.5556
	Task-29	val_accuracy: 75.9494	gate_accuracy: 68.3544
	Task-30	val_accuracy: 56.2500	gate_accuracy: 52.5000
	Task-31	val_accuracy: 58.3333	gate_accuracy: 51.1905
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.9385


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 2.8940	step1_train_accuracy: 37.5000
STEP-1	Epoch: 20/50	loss: 1.0845	step1_train_accuracy: 79.5732
STEP-1	Epoch: 30/50	loss: 0.5425	step1_train_accuracy: 90.8537
STEP-1	Epoch: 40/50	loss: 0.3690	step1_train_accuracy: 96.3415
STEP-1	Epoch: 50/50	loss: 0.2830	step1_train_accuracy: 97.2561
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9975	gate_loss: 2.4259	step2_classification_accuracy: 70.5045	step_2_gate_accuracy: 33.7092
STEP-2	Epoch: 40/200	classification_loss: 0.7760	gate_loss: 1.0672	step2_classification_accuracy: 77.0475	step_2_gate_accuracy: 68.5905
STEP-2	Epoch: 60/200	classification_loss: 0.6187	gate_loss: 0.6496	step2_classification_accuracy: 81.5875	step_2_gate_accuracy: 80.4154
STEP-2	Epoch: 80/200	classification_loss: 0.5206	gate_loss: 0.4736	step2_classification_accuracy: 84.4510	step_2_gate_accuracy: 85.6677
STEP-2	Epoch: 100/200	classification_loss: 0.4480	gate_loss: 0.3696	step2_classification_accuracy: 86.7211	step_2_gate_accuracy: 88.6944
STEP-2	Epoch: 120/200	classification_loss: 0.3943	gate_loss: 0.3004	step2_classification_accuracy: 87.9674	step_2_gate_accuracy: 90.6083
STEP-2	Epoch: 140/200	classification_loss: 0.3548	gate_loss: 0.2535	step2_classification_accuracy: 88.9169	step_2_gate_accuracy: 92.1662
STEP-2	Epoch: 160/200	classification_loss: 0.3321	gate_loss: 0.2277	step2_classification_accuracy: 89.6439	step_2_gate_accuracy: 92.8190
STEP-2	Epoch: 180/200	classification_loss: 0.3172	gate_loss: 0.2080	step2_classification_accuracy: 89.7478	step_2_gate_accuracy: 93.3976
STEP-2	Epoch: 200/200	classification_loss: 0.2878	gate_loss: 0.1841	step2_classification_accuracy: 90.4748	step_2_gate_accuracy: 93.9763
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.6667	gate_accuracy: 74.4444
	Task-1	val_accuracy: 41.1111	gate_accuracy: 45.5556
	Task-2	val_accuracy: 44.0476	gate_accuracy: 47.6190
	Task-3	val_accuracy: 64.1026	gate_accuracy: 65.3846
	Task-4	val_accuracy: 67.9487	gate_accuracy: 73.0769
	Task-5	val_accuracy: 56.0606	gate_accuracy: 71.2121
	Task-6	val_accuracy: 73.9130	gate_accuracy: 65.2174
	Task-7	val_accuracy: 67.5325	gate_accuracy: 62.3377
	Task-8	val_accuracy: 68.3544	gate_accuracy: 62.0253
	Task-9	val_accuracy: 75.9036	gate_accuracy: 69.8795
	Task-10	val_accuracy: 80.8219	gate_accuracy: 80.8219
	Task-11	val_accuracy: 76.1364	gate_accuracy: 75.0000
	Task-12	val_accuracy: 67.9487	gate_accuracy: 75.6410
	Task-13	val_accuracy: 69.8630	gate_accuracy: 69.8630
	Task-14	val_accuracy: 65.7143	gate_accuracy: 61.4286
	Task-15	val_accuracy: 80.0000	gate_accuracy: 78.6667
	Task-16	val_accuracy: 56.3380	gate_accuracy: 47.8873
	Task-17	val_accuracy: 72.1649	gate_accuracy: 67.0103
	Task-18	val_accuracy: 72.4138	gate_accuracy: 74.7126
	Task-19	val_accuracy: 52.5000	gate_accuracy: 53.7500
	Task-20	val_accuracy: 61.9565	gate_accuracy: 59.7826
	Task-21	val_accuracy: 83.5165	gate_accuracy: 82.4176
	Task-22	val_accuracy: 61.4458	gate_accuracy: 59.0361
	Task-23	val_accuracy: 58.2278	gate_accuracy: 58.2278
	Task-24	val_accuracy: 61.2500	gate_accuracy: 57.5000
	Task-25	val_accuracy: 78.8235	gate_accuracy: 70.5882
	Task-26	val_accuracy: 76.6234	gate_accuracy: 67.5325
	Task-27	val_accuracy: 48.3871	gate_accuracy: 41.9355
	Task-28	val_accuracy: 64.4444	gate_accuracy: 63.3333
	Task-29	val_accuracy: 77.2152	gate_accuracy: 74.6835
	Task-30	val_accuracy: 50.0000	gate_accuracy: 52.5000
	Task-31	val_accuracy: 71.4286	gate_accuracy: 67.8571
	Task-32	val_accuracy: 60.9756	gate_accuracy: 54.8780
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.0742


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.4079	step1_train_accuracy: 52.7066
STEP-1	Epoch: 20/50	loss: 1.0058	step1_train_accuracy: 78.9174
STEP-1	Epoch: 30/50	loss: 0.4782	step1_train_accuracy: 90.3134
STEP-1	Epoch: 40/50	loss: 0.3336	step1_train_accuracy: 92.3077
STEP-1	Epoch: 50/50	loss: 0.2666	step1_train_accuracy: 93.4473
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0191	gate_loss: 2.3487	step2_classification_accuracy: 69.4524	step_2_gate_accuracy: 35.3314
STEP-2	Epoch: 40/200	classification_loss: 0.7876	gate_loss: 1.0375	step2_classification_accuracy: 76.6282	step_2_gate_accuracy: 70.3026
STEP-2	Epoch: 60/200	classification_loss: 0.6055	gate_loss: 0.6306	step2_classification_accuracy: 82.0029	step_2_gate_accuracy: 81.1527
STEP-2	Epoch: 80/200	classification_loss: 0.5188	gate_loss: 0.4670	step2_classification_accuracy: 83.8040	step_2_gate_accuracy: 86.0086
STEP-2	Epoch: 100/200	classification_loss: 0.4509	gate_loss: 0.3742	step2_classification_accuracy: 86.0231	step_2_gate_accuracy: 88.3429
STEP-2	Epoch: 120/200	classification_loss: 0.3946	gate_loss: 0.3065	step2_classification_accuracy: 87.4207	step_2_gate_accuracy: 90.5187
STEP-2	Epoch: 140/200	classification_loss: 0.3482	gate_loss: 0.2577	step2_classification_accuracy: 88.6744	step_2_gate_accuracy: 92.0317
STEP-2	Epoch: 160/200	classification_loss: 0.3231	gate_loss: 0.2278	step2_classification_accuracy: 89.5101	step_2_gate_accuracy: 92.9395
STEP-2	Epoch: 180/200	classification_loss: 0.3003	gate_loss: 0.2041	step2_classification_accuracy: 90.3314	step_2_gate_accuracy: 93.7608
STEP-2	Epoch: 200/200	classification_loss: 0.2855	gate_loss: 0.1904	step2_classification_accuracy: 90.4755	step_2_gate_accuracy: 93.9625
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.2222	gate_accuracy: 75.0000
	Task-1	val_accuracy: 54.4444	gate_accuracy: 64.4444
	Task-2	val_accuracy: 47.6190	gate_accuracy: 55.9524
	Task-3	val_accuracy: 69.2308	gate_accuracy: 73.0769
	Task-4	val_accuracy: 62.8205	gate_accuracy: 66.6667
	Task-5	val_accuracy: 54.5455	gate_accuracy: 60.6061
	Task-6	val_accuracy: 80.4348	gate_accuracy: 71.7391
	Task-7	val_accuracy: 75.3247	gate_accuracy: 72.7273
	Task-8	val_accuracy: 59.4937	gate_accuracy: 64.5570
	Task-9	val_accuracy: 69.8795	gate_accuracy: 66.2651
	Task-10	val_accuracy: 76.7123	gate_accuracy: 73.9726
	Task-11	val_accuracy: 77.2727	gate_accuracy: 73.8636
	Task-12	val_accuracy: 67.9487	gate_accuracy: 71.7949
	Task-13	val_accuracy: 68.4932	gate_accuracy: 67.1233
	Task-14	val_accuracy: 70.0000	gate_accuracy: 70.0000
	Task-15	val_accuracy: 76.0000	gate_accuracy: 64.0000
	Task-16	val_accuracy: 60.5634	gate_accuracy: 50.7042
	Task-17	val_accuracy: 69.0722	gate_accuracy: 61.8557
	Task-18	val_accuracy: 77.0115	gate_accuracy: 71.2644
	Task-19	val_accuracy: 63.7500	gate_accuracy: 71.2500
	Task-20	val_accuracy: 58.6957	gate_accuracy: 51.0870
	Task-21	val_accuracy: 78.0220	gate_accuracy: 65.9341
	Task-22	val_accuracy: 61.4458	gate_accuracy: 63.8554
	Task-23	val_accuracy: 63.2911	gate_accuracy: 60.7595
	Task-24	val_accuracy: 55.0000	gate_accuracy: 50.0000
	Task-25	val_accuracy: 80.0000	gate_accuracy: 77.6471
	Task-26	val_accuracy: 77.9221	gate_accuracy: 74.0260
	Task-27	val_accuracy: 48.3871	gate_accuracy: 40.3226
	Task-28	val_accuracy: 68.8889	gate_accuracy: 66.6667
	Task-29	val_accuracy: 75.9494	gate_accuracy: 73.4177
	Task-30	val_accuracy: 57.5000	gate_accuracy: 51.2500
	Task-31	val_accuracy: 65.4762	gate_accuracy: 59.5238
	Task-32	val_accuracy: 60.9756	gate_accuracy: 53.6585
	Task-33	val_accuracy: 54.5455	gate_accuracy: 53.4091
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.8544


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 3.3717	step1_train_accuracy: 43.2727
STEP-1	Epoch: 20/50	loss: 1.2817	step1_train_accuracy: 78.5455
STEP-1	Epoch: 30/50	loss: 0.6662	step1_train_accuracy: 91.6364
STEP-1	Epoch: 40/50	loss: 0.4431	step1_train_accuracy: 94.1818
STEP-1	Epoch: 50/50	loss: 0.3116	step1_train_accuracy: 96.7273
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0120	gate_loss: 2.5147	step2_classification_accuracy: 69.3277	step_2_gate_accuracy: 31.7927
STEP-2	Epoch: 40/200	classification_loss: 0.7823	gate_loss: 1.0921	step2_classification_accuracy: 76.5126	step_2_gate_accuracy: 67.8431
STEP-2	Epoch: 60/200	classification_loss: 0.6124	gate_loss: 0.6642	step2_classification_accuracy: 81.3165	step_2_gate_accuracy: 79.8739
STEP-2	Epoch: 80/200	classification_loss: 0.5031	gate_loss: 0.4728	step2_classification_accuracy: 84.2997	step_2_gate_accuracy: 85.7703
STEP-2	Epoch: 100/200	classification_loss: 0.4373	gate_loss: 0.3737	step2_classification_accuracy: 86.5126	step_2_gate_accuracy: 88.2633
STEP-2	Epoch: 120/200	classification_loss: 0.3850	gate_loss: 0.3044	step2_classification_accuracy: 88.0392	step_2_gate_accuracy: 90.5882
STEP-2	Epoch: 140/200	classification_loss: 0.3581	gate_loss: 0.2626	step2_classification_accuracy: 88.9076	step_2_gate_accuracy: 92.1709
STEP-2	Epoch: 160/200	classification_loss: 0.3252	gate_loss: 0.2294	step2_classification_accuracy: 89.5798	step_2_gate_accuracy: 93.0532
STEP-2	Epoch: 180/200	classification_loss: 0.2982	gate_loss: 0.2008	step2_classification_accuracy: 90.3782	step_2_gate_accuracy: 93.9076
STEP-2	Epoch: 200/200	classification_loss: 0.2839	gate_loss: 0.1849	step2_classification_accuracy: 90.5882	step_2_gate_accuracy: 94.3557
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.5556	gate_accuracy: 78.3333
	Task-1	val_accuracy: 47.7778	gate_accuracy: 63.3333
	Task-2	val_accuracy: 51.1905	gate_accuracy: 66.6667
	Task-3	val_accuracy: 66.6667	gate_accuracy: 74.3590
	Task-4	val_accuracy: 67.9487	gate_accuracy: 67.9487
	Task-5	val_accuracy: 50.0000	gate_accuracy: 59.0909
	Task-6	val_accuracy: 76.0870	gate_accuracy: 70.6522
	Task-7	val_accuracy: 71.4286	gate_accuracy: 68.8312
	Task-8	val_accuracy: 60.7595	gate_accuracy: 51.8987
	Task-9	val_accuracy: 80.7229	gate_accuracy: 80.7229
	Task-10	val_accuracy: 82.1918	gate_accuracy: 75.3425
	Task-11	val_accuracy: 67.0455	gate_accuracy: 68.1818
	Task-12	val_accuracy: 64.1026	gate_accuracy: 70.5128
	Task-13	val_accuracy: 73.9726	gate_accuracy: 67.1233
	Task-14	val_accuracy: 61.4286	gate_accuracy: 68.5714
	Task-15	val_accuracy: 76.0000	gate_accuracy: 70.6667
	Task-16	val_accuracy: 54.9296	gate_accuracy: 52.1127
	Task-17	val_accuracy: 64.9485	gate_accuracy: 60.8247
	Task-18	val_accuracy: 65.5172	gate_accuracy: 60.9195
	Task-19	val_accuracy: 56.2500	gate_accuracy: 61.2500
	Task-20	val_accuracy: 59.7826	gate_accuracy: 60.8696
	Task-21	val_accuracy: 82.4176	gate_accuracy: 80.2198
	Task-22	val_accuracy: 59.0361	gate_accuracy: 56.6265
	Task-23	val_accuracy: 45.5696	gate_accuracy: 40.5063
	Task-24	val_accuracy: 62.5000	gate_accuracy: 60.0000
	Task-25	val_accuracy: 78.8235	gate_accuracy: 76.4706
	Task-26	val_accuracy: 79.2208	gate_accuracy: 72.7273
	Task-27	val_accuracy: 45.1613	gate_accuracy: 40.3226
	Task-28	val_accuracy: 71.1111	gate_accuracy: 71.1111
	Task-29	val_accuracy: 73.4177	gate_accuracy: 67.0886
	Task-30	val_accuracy: 52.5000	gate_accuracy: 53.7500
	Task-31	val_accuracy: 63.0952	gate_accuracy: 58.3333
	Task-32	val_accuracy: 62.1951	gate_accuracy: 52.4390
	Task-33	val_accuracy: 62.5000	gate_accuracy: 64.7727
	Task-34	val_accuracy: 62.3188	gate_accuracy: 56.5217
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.0000


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.5650	step1_train_accuracy: 47.0405
STEP-1	Epoch: 20/50	loss: 0.7630	step1_train_accuracy: 89.4081
STEP-1	Epoch: 30/50	loss: 0.3688	step1_train_accuracy: 94.0810
STEP-1	Epoch: 40/50	loss: 0.2758	step1_train_accuracy: 97.1963
STEP-1	Epoch: 50/50	loss: 0.3014	step1_train_accuracy: 98.4424
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0209	gate_loss: 2.4857	step2_classification_accuracy: 69.1689	step_2_gate_accuracy: 33.2970
STEP-2	Epoch: 40/200	classification_loss: 0.8103	gate_loss: 1.0844	step2_classification_accuracy: 75.6948	step_2_gate_accuracy: 67.7520
STEP-2	Epoch: 60/200	classification_loss: 0.6539	gate_loss: 0.6804	step2_classification_accuracy: 80.3951	step_2_gate_accuracy: 79.2915
STEP-2	Epoch: 80/200	classification_loss: 0.5428	gate_loss: 0.5015	step2_classification_accuracy: 83.6512	step_2_gate_accuracy: 84.4278
STEP-2	Epoch: 100/200	classification_loss: 0.4772	gate_loss: 0.4089	step2_classification_accuracy: 85.2725	step_2_gate_accuracy: 87.0708
STEP-2	Epoch: 120/200	classification_loss: 0.4310	gate_loss: 0.3419	step2_classification_accuracy: 86.5668	step_2_gate_accuracy: 89.3597
STEP-2	Epoch: 140/200	classification_loss: 0.4002	gate_loss: 0.3052	step2_classification_accuracy: 87.3433	step_2_gate_accuracy: 90.4905
STEP-2	Epoch: 160/200	classification_loss: 0.3635	gate_loss: 0.2690	step2_classification_accuracy: 88.2970	step_2_gate_accuracy: 91.4986
STEP-2	Epoch: 180/200	classification_loss: 0.3438	gate_loss: 0.2460	step2_classification_accuracy: 88.6785	step_2_gate_accuracy: 91.9210
STEP-2	Epoch: 200/200	classification_loss: 0.3179	gate_loss: 0.2162	step2_classification_accuracy: 89.4414	step_2_gate_accuracy: 93.0926
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 56.6667	gate_accuracy: 65.5556
	Task-1	val_accuracy: 48.8889	gate_accuracy: 63.3333
	Task-2	val_accuracy: 53.5714	gate_accuracy: 69.0476
	Task-3	val_accuracy: 64.1026	gate_accuracy: 66.6667
	Task-4	val_accuracy: 65.3846	gate_accuracy: 70.5128
	Task-5	val_accuracy: 53.0303	gate_accuracy: 66.6667
	Task-6	val_accuracy: 79.3478	gate_accuracy: 79.3478
	Task-7	val_accuracy: 72.7273	gate_accuracy: 66.2338
	Task-8	val_accuracy: 72.1519	gate_accuracy: 64.5570
	Task-9	val_accuracy: 69.8795	gate_accuracy: 65.0602
	Task-10	val_accuracy: 75.3425	gate_accuracy: 69.8630
	Task-11	val_accuracy: 71.5909	gate_accuracy: 67.0455
	Task-12	val_accuracy: 69.2308	gate_accuracy: 67.9487
	Task-13	val_accuracy: 72.6027	gate_accuracy: 68.4932
	Task-14	val_accuracy: 65.7143	gate_accuracy: 68.5714
	Task-15	val_accuracy: 72.0000	gate_accuracy: 68.0000
	Task-16	val_accuracy: 50.7042	gate_accuracy: 47.8873
	Task-17	val_accuracy: 67.0103	gate_accuracy: 57.7320
	Task-18	val_accuracy: 68.9655	gate_accuracy: 60.9195
	Task-19	val_accuracy: 65.0000	gate_accuracy: 60.0000
	Task-20	val_accuracy: 56.5217	gate_accuracy: 53.2609
	Task-21	val_accuracy: 74.7253	gate_accuracy: 70.3297
	Task-22	val_accuracy: 61.4458	gate_accuracy: 66.2651
	Task-23	val_accuracy: 68.3544	gate_accuracy: 62.0253
	Task-24	val_accuracy: 58.7500	gate_accuracy: 52.5000
	Task-25	val_accuracy: 75.2941	gate_accuracy: 64.7059
	Task-26	val_accuracy: 74.0260	gate_accuracy: 67.5325
	Task-27	val_accuracy: 51.6129	gate_accuracy: 40.3226
	Task-28	val_accuracy: 52.2222	gate_accuracy: 44.4444
	Task-29	val_accuracy: 78.4810	gate_accuracy: 68.3544
	Task-30	val_accuracy: 55.0000	gate_accuracy: 56.2500
	Task-31	val_accuracy: 66.6667	gate_accuracy: 60.7143
	Task-32	val_accuracy: 62.1951	gate_accuracy: 53.6585
	Task-33	val_accuracy: 45.4545	gate_accuracy: 40.9091
	Task-34	val_accuracy: 68.1159	gate_accuracy: 63.7681
	Task-35	val_accuracy: 51.2500	gate_accuracy: 51.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.0667


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 3.1188	step1_train_accuracy: 47.7987
STEP-1	Epoch: 20/50	loss: 1.2426	step1_train_accuracy: 71.6981
STEP-1	Epoch: 30/50	loss: 0.6832	step1_train_accuracy: 88.6792
STEP-1	Epoch: 40/50	loss: 0.4670	step1_train_accuracy: 90.8805
STEP-1	Epoch: 50/50	loss: 0.3466	step1_train_accuracy: 92.4528
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1016	gate_loss: 2.5517	step2_classification_accuracy: 66.9629	step_2_gate_accuracy: 30.7825
STEP-2	Epoch: 40/200	classification_loss: 0.8705	gate_loss: 1.1446	step2_classification_accuracy: 74.1379	step_2_gate_accuracy: 66.9098
STEP-2	Epoch: 60/200	classification_loss: 0.6830	gate_loss: 0.7104	step2_classification_accuracy: 79.3103	step_2_gate_accuracy: 78.1963
STEP-2	Epoch: 80/200	classification_loss: 0.5677	gate_loss: 0.5201	step2_classification_accuracy: 82.4536	step_2_gate_accuracy: 83.5942
STEP-2	Epoch: 100/200	classification_loss: 0.4994	gate_loss: 0.4173	step2_classification_accuracy: 84.7215	step_2_gate_accuracy: 86.8833
STEP-2	Epoch: 120/200	classification_loss: 0.4398	gate_loss: 0.3454	step2_classification_accuracy: 86.1406	step_2_gate_accuracy: 89.3369
STEP-2	Epoch: 140/200	classification_loss: 0.3931	gate_loss: 0.2949	step2_classification_accuracy: 87.7586	step_2_gate_accuracy: 90.7825
STEP-2	Epoch: 160/200	classification_loss: 0.3740	gate_loss: 0.2650	step2_classification_accuracy: 88.0371	step_2_gate_accuracy: 91.6313
STEP-2	Epoch: 180/200	classification_loss: 0.3525	gate_loss: 0.2437	step2_classification_accuracy: 88.5942	step_2_gate_accuracy: 92.3342
STEP-2	Epoch: 200/200	classification_loss: 0.3282	gate_loss: 0.2179	step2_classification_accuracy: 89.0186	step_2_gate_accuracy: 93.2361
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.5556	gate_accuracy: 65.5556
	Task-1	val_accuracy: 44.4444	gate_accuracy: 64.4444
	Task-2	val_accuracy: 44.0476	gate_accuracy: 52.3810
	Task-3	val_accuracy: 67.9487	gate_accuracy: 70.5128
	Task-4	val_accuracy: 57.6923	gate_accuracy: 56.4103
	Task-5	val_accuracy: 48.4848	gate_accuracy: 63.6364
	Task-6	val_accuracy: 61.9565	gate_accuracy: 66.3043
	Task-7	val_accuracy: 77.9221	gate_accuracy: 74.0260
	Task-8	val_accuracy: 73.4177	gate_accuracy: 73.4177
	Task-9	val_accuracy: 77.1084	gate_accuracy: 74.6988
	Task-10	val_accuracy: 76.7123	gate_accuracy: 75.3425
	Task-11	val_accuracy: 71.5909	gate_accuracy: 71.5909
	Task-12	val_accuracy: 65.3846	gate_accuracy: 66.6667
	Task-13	val_accuracy: 75.3425	gate_accuracy: 68.4932
	Task-14	val_accuracy: 64.2857	gate_accuracy: 62.8571
	Task-15	val_accuracy: 74.6667	gate_accuracy: 68.0000
	Task-16	val_accuracy: 47.8873	gate_accuracy: 46.4789
	Task-17	val_accuracy: 70.1031	gate_accuracy: 62.8866
	Task-18	val_accuracy: 73.5632	gate_accuracy: 71.2644
	Task-19	val_accuracy: 56.2500	gate_accuracy: 53.7500
	Task-20	val_accuracy: 58.6957	gate_accuracy: 48.9130
	Task-21	val_accuracy: 75.8242	gate_accuracy: 70.3297
	Task-22	val_accuracy: 54.2169	gate_accuracy: 55.4217
	Task-23	val_accuracy: 59.4937	gate_accuracy: 54.4304
	Task-24	val_accuracy: 60.0000	gate_accuracy: 58.7500
	Task-25	val_accuracy: 71.7647	gate_accuracy: 67.0588
	Task-26	val_accuracy: 74.0260	gate_accuracy: 68.8312
	Task-27	val_accuracy: 50.0000	gate_accuracy: 38.7097
	Task-28	val_accuracy: 66.6667	gate_accuracy: 64.4444
	Task-29	val_accuracy: 77.2152	gate_accuracy: 67.0886
	Task-30	val_accuracy: 57.5000	gate_accuracy: 60.0000
	Task-31	val_accuracy: 58.3333	gate_accuracy: 50.0000
	Task-32	val_accuracy: 59.7561	gate_accuracy: 50.0000
	Task-33	val_accuracy: 52.2727	gate_accuracy: 52.2727
	Task-34	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-35	val_accuracy: 47.5000	gate_accuracy: 43.7500
	Task-36	val_accuracy: 36.7089	gate_accuracy: 36.7089
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.3836


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.6468	step1_train_accuracy: 50.5780
STEP-1	Epoch: 20/50	loss: 0.8594	step1_train_accuracy: 86.1272
STEP-1	Epoch: 30/50	loss: 0.3900	step1_train_accuracy: 97.3988
STEP-1	Epoch: 40/50	loss: 0.2506	step1_train_accuracy: 97.6879
STEP-1	Epoch: 50/50	loss: 0.1798	step1_train_accuracy: 97.6879
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0667	gate_loss: 2.5741	step2_classification_accuracy: 68.3075	step_2_gate_accuracy: 30.5685
STEP-2	Epoch: 40/200	classification_loss: 0.8439	gate_loss: 1.1356	step2_classification_accuracy: 75.5039	step_2_gate_accuracy: 67.8941
STEP-2	Epoch: 60/200	classification_loss: 0.6747	gate_loss: 0.7011	step2_classification_accuracy: 80.0904	step_2_gate_accuracy: 78.9535
STEP-2	Epoch: 80/200	classification_loss: 0.5690	gate_loss: 0.5210	step2_classification_accuracy: 82.7778	step_2_gate_accuracy: 84.3798
STEP-2	Epoch: 100/200	classification_loss: 0.4924	gate_loss: 0.4139	step2_classification_accuracy: 84.8966	step_2_gate_accuracy: 87.4677
STEP-2	Epoch: 120/200	classification_loss: 0.4547	gate_loss: 0.3584	step2_classification_accuracy: 86.1240	step_2_gate_accuracy: 88.9922
STEP-2	Epoch: 140/200	classification_loss: 0.4147	gate_loss: 0.3109	step2_classification_accuracy: 87.2222	step_2_gate_accuracy: 90.4780
STEP-2	Epoch: 160/200	classification_loss: 0.3813	gate_loss: 0.2740	step2_classification_accuracy: 87.9328	step_2_gate_accuracy: 91.6796
STEP-2	Epoch: 180/200	classification_loss: 0.3713	gate_loss: 0.2588	step2_classification_accuracy: 87.9587	step_2_gate_accuracy: 92.0155
STEP-2	Epoch: 200/200	classification_loss: 0.3450	gate_loss: 0.2333	step2_classification_accuracy: 89.0698	step_2_gate_accuracy: 92.6615
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.2222	gate_accuracy: 68.3333
	Task-1	val_accuracy: 43.3333	gate_accuracy: 61.1111
	Task-2	val_accuracy: 44.0476	gate_accuracy: 57.1429
	Task-3	val_accuracy: 67.9487	gate_accuracy: 70.5128
	Task-4	val_accuracy: 62.8205	gate_accuracy: 62.8205
	Task-5	val_accuracy: 50.0000	gate_accuracy: 60.6061
	Task-6	val_accuracy: 67.3913	gate_accuracy: 59.7826
	Task-7	val_accuracy: 72.7273	gate_accuracy: 75.3247
	Task-8	val_accuracy: 65.8228	gate_accuracy: 59.4937
	Task-9	val_accuracy: 75.9036	gate_accuracy: 68.6747
	Task-10	val_accuracy: 79.4521	gate_accuracy: 79.4521
	Task-11	val_accuracy: 65.9091	gate_accuracy: 67.0455
	Task-12	val_accuracy: 64.1026	gate_accuracy: 70.5128
	Task-13	val_accuracy: 68.4932	gate_accuracy: 68.4932
	Task-14	val_accuracy: 65.7143	gate_accuracy: 60.0000
	Task-15	val_accuracy: 72.0000	gate_accuracy: 66.6667
	Task-16	val_accuracy: 54.9296	gate_accuracy: 52.1127
	Task-17	val_accuracy: 63.9175	gate_accuracy: 59.7938
	Task-18	val_accuracy: 63.2184	gate_accuracy: 66.6667
	Task-19	val_accuracy: 57.5000	gate_accuracy: 61.2500
	Task-20	val_accuracy: 63.0435	gate_accuracy: 56.5217
	Task-21	val_accuracy: 76.9231	gate_accuracy: 72.5275
	Task-22	val_accuracy: 59.0361	gate_accuracy: 61.4458
	Task-23	val_accuracy: 67.0886	gate_accuracy: 67.0886
	Task-24	val_accuracy: 55.0000	gate_accuracy: 48.7500
	Task-25	val_accuracy: 70.5882	gate_accuracy: 69.4118
	Task-26	val_accuracy: 75.3247	gate_accuracy: 74.0260
	Task-27	val_accuracy: 32.2581	gate_accuracy: 22.5806
	Task-28	val_accuracy: 57.7778	gate_accuracy: 63.3333
	Task-29	val_accuracy: 78.4810	gate_accuracy: 68.3544
	Task-30	val_accuracy: 55.0000	gate_accuracy: 51.2500
	Task-31	val_accuracy: 51.1905	gate_accuracy: 45.2381
	Task-32	val_accuracy: 63.4146	gate_accuracy: 54.8780
	Task-33	val_accuracy: 56.8182	gate_accuracy: 51.1364
	Task-34	val_accuracy: 68.1159	gate_accuracy: 65.2174
	Task-35	val_accuracy: 50.0000	gate_accuracy: 51.2500
	Task-36	val_accuracy: 48.1013	gate_accuracy: 48.1013
	Task-37	val_accuracy: 63.9535	gate_accuracy: 54.6512
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.4534


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 2.4049	step1_train_accuracy: 52.7859
STEP-1	Epoch: 20/50	loss: 0.8032	step1_train_accuracy: 85.0440
STEP-1	Epoch: 30/50	loss: 0.5898	step1_train_accuracy: 93.5484
STEP-1	Epoch: 40/50	loss: 0.2976	step1_train_accuracy: 94.7214
STEP-1	Epoch: 50/50	loss: 0.2261	step1_train_accuracy: 95.3079
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0974	gate_loss: 2.6334	step2_classification_accuracy: 67.4937	step_2_gate_accuracy: 29.7355
STEP-2	Epoch: 40/200	classification_loss: 0.8670	gate_loss: 1.1609	step2_classification_accuracy: 74.2695	step_2_gate_accuracy: 66.8010
STEP-2	Epoch: 60/200	classification_loss: 0.6995	gate_loss: 0.7237	step2_classification_accuracy: 78.7531	step_2_gate_accuracy: 77.7834
STEP-2	Epoch: 80/200	classification_loss: 0.5704	gate_loss: 0.5215	step2_classification_accuracy: 82.4055	step_2_gate_accuracy: 83.6524
STEP-2	Epoch: 100/200	classification_loss: 0.5044	gate_loss: 0.4179	step2_classification_accuracy: 84.7985	step_2_gate_accuracy: 87.1411
STEP-2	Epoch: 120/200	classification_loss: 0.4458	gate_loss: 0.3459	step2_classification_accuracy: 86.1587	step_2_gate_accuracy: 89.1310
STEP-2	Epoch: 140/200	classification_loss: 0.4064	gate_loss: 0.2980	step2_classification_accuracy: 87.4937	step_2_gate_accuracy: 90.7809
STEP-2	Epoch: 160/200	classification_loss: 0.3877	gate_loss: 0.2766	step2_classification_accuracy: 87.8086	step_2_gate_accuracy: 91.1587
STEP-2	Epoch: 180/200	classification_loss: 0.3499	gate_loss: 0.2383	step2_classification_accuracy: 88.9798	step_2_gate_accuracy: 92.5693
STEP-2	Epoch: 200/200	classification_loss: 0.5266	gate_loss: 0.4383	step2_classification_accuracy: 83.5516	step_2_gate_accuracy: 85.1763
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.1111	gate_accuracy: 69.4444
	Task-1	val_accuracy: 41.1111	gate_accuracy: 61.1111
	Task-2	val_accuracy: 40.4762	gate_accuracy: 52.3810
	Task-3	val_accuracy: 64.1026	gate_accuracy: 71.7949
	Task-4	val_accuracy: 55.1282	gate_accuracy: 57.6923
	Task-5	val_accuracy: 45.4545	gate_accuracy: 59.0909
	Task-6	val_accuracy: 70.6522	gate_accuracy: 66.3043
	Task-7	val_accuracy: 66.2338	gate_accuracy: 57.1429
	Task-8	val_accuracy: 74.6835	gate_accuracy: 67.0886
	Task-9	val_accuracy: 73.4940	gate_accuracy: 67.4699
	Task-10	val_accuracy: 78.0822	gate_accuracy: 73.9726
	Task-11	val_accuracy: 60.2273	gate_accuracy: 56.8182
	Task-12	val_accuracy: 67.9487	gate_accuracy: 70.5128
	Task-13	val_accuracy: 72.6027	gate_accuracy: 68.4932
	Task-14	val_accuracy: 62.8571	gate_accuracy: 64.2857
	Task-15	val_accuracy: 81.3333	gate_accuracy: 78.6667
	Task-16	val_accuracy: 46.4789	gate_accuracy: 39.4366
	Task-17	val_accuracy: 57.7320	gate_accuracy: 49.4845
	Task-18	val_accuracy: 71.2644	gate_accuracy: 65.5172
	Task-19	val_accuracy: 65.0000	gate_accuracy: 67.5000
	Task-20	val_accuracy: 51.0870	gate_accuracy: 46.7391
	Task-21	val_accuracy: 76.9231	gate_accuracy: 65.9341
	Task-22	val_accuracy: 59.0361	gate_accuracy: 60.2410
	Task-23	val_accuracy: 68.3544	gate_accuracy: 68.3544
	Task-24	val_accuracy: 56.2500	gate_accuracy: 51.2500
	Task-25	val_accuracy: 75.2941	gate_accuracy: 64.7059
	Task-26	val_accuracy: 84.4156	gate_accuracy: 77.9221
	Task-27	val_accuracy: 45.1613	gate_accuracy: 40.3226
	Task-28	val_accuracy: 61.1111	gate_accuracy: 61.1111
	Task-29	val_accuracy: 78.4810	gate_accuracy: 72.1519
	Task-30	val_accuracy: 61.2500	gate_accuracy: 58.7500
	Task-31	val_accuracy: 55.9524	gate_accuracy: 50.0000
	Task-32	val_accuracy: 67.0732	gate_accuracy: 43.9024
	Task-33	val_accuracy: 54.5455	gate_accuracy: 51.1364
	Task-34	val_accuracy: 62.3188	gate_accuracy: 57.9710
	Task-35	val_accuracy: 57.5000	gate_accuracy: 55.0000
	Task-36	val_accuracy: 39.2405	gate_accuracy: 40.5063
	Task-37	val_accuracy: 67.4419	gate_accuracy: 61.6279
	Task-38	val_accuracy: 56.4706	gate_accuracy: 50.5882
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.3077


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 2.5710	step1_train_accuracy: 43.3735
STEP-1	Epoch: 20/50	loss: 1.0430	step1_train_accuracy: 79.8193
STEP-1	Epoch: 30/50	loss: 0.5883	step1_train_accuracy: 91.8675
STEP-1	Epoch: 40/50	loss: 0.4094	step1_train_accuracy: 93.6747
STEP-1	Epoch: 50/50	loss: 0.3080	step1_train_accuracy: 93.9759
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0877	gate_loss: 2.5352	step2_classification_accuracy: 67.7150	step_2_gate_accuracy: 31.9656
STEP-2	Epoch: 40/200	classification_loss: 0.8700	gate_loss: 1.1391	step2_classification_accuracy: 74.4717	step_2_gate_accuracy: 66.8428
STEP-2	Epoch: 60/200	classification_loss: 0.6973	gate_loss: 0.7265	step2_classification_accuracy: 79.4840	step_2_gate_accuracy: 78.2555
STEP-2	Epoch: 80/200	classification_loss: 0.5945	gate_loss: 0.5450	step2_classification_accuracy: 82.1376	step_2_gate_accuracy: 83.3661
STEP-2	Epoch: 100/200	classification_loss: 0.5144	gate_loss: 0.4325	step2_classification_accuracy: 84.3366	step_2_gate_accuracy: 86.6216
STEP-2	Epoch: 120/200	classification_loss: 0.4665	gate_loss: 0.3720	step2_classification_accuracy: 85.8845	step_2_gate_accuracy: 88.5012
STEP-2	Epoch: 140/200	classification_loss: 0.4279	gate_loss: 0.3228	step2_classification_accuracy: 86.8673	step_2_gate_accuracy: 89.9877
STEP-2	Epoch: 160/200	classification_loss: 0.4010	gate_loss: 0.2944	step2_classification_accuracy: 87.4816	step_2_gate_accuracy: 90.5283
STEP-2	Epoch: 180/200	classification_loss: 0.3720	gate_loss: 0.2656	step2_classification_accuracy: 88.2555	step_2_gate_accuracy: 91.5111
STEP-2	Epoch: 200/200	classification_loss: 0.3594	gate_loss: 0.2473	step2_classification_accuracy: 88.5381	step_2_gate_accuracy: 92.2973
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.3333	gate_accuracy: 68.8889
	Task-1	val_accuracy: 42.2222	gate_accuracy: 51.1111
	Task-2	val_accuracy: 51.1905	gate_accuracy: 65.4762
	Task-3	val_accuracy: 64.1026	gate_accuracy: 67.9487
	Task-4	val_accuracy: 64.1026	gate_accuracy: 66.6667
	Task-5	val_accuracy: 45.4545	gate_accuracy: 60.6061
	Task-6	val_accuracy: 65.2174	gate_accuracy: 65.2174
	Task-7	val_accuracy: 64.9351	gate_accuracy: 64.9351
	Task-8	val_accuracy: 62.0253	gate_accuracy: 56.9620
	Task-9	val_accuracy: 74.6988	gate_accuracy: 68.6747
	Task-10	val_accuracy: 71.2329	gate_accuracy: 63.0137
	Task-11	val_accuracy: 73.8636	gate_accuracy: 75.0000
	Task-12	val_accuracy: 67.9487	gate_accuracy: 73.0769
	Task-13	val_accuracy: 64.3836	gate_accuracy: 67.1233
	Task-14	val_accuracy: 61.4286	gate_accuracy: 62.8571
	Task-15	val_accuracy: 69.3333	gate_accuracy: 57.3333
	Task-16	val_accuracy: 52.1127	gate_accuracy: 50.7042
	Task-17	val_accuracy: 70.1031	gate_accuracy: 63.9175
	Task-18	val_accuracy: 72.4138	gate_accuracy: 70.1149
	Task-19	val_accuracy: 57.5000	gate_accuracy: 58.7500
	Task-20	val_accuracy: 72.8261	gate_accuracy: 68.4783
	Task-21	val_accuracy: 73.6264	gate_accuracy: 67.0330
	Task-22	val_accuracy: 60.2410	gate_accuracy: 56.6265
	Task-23	val_accuracy: 56.9620	gate_accuracy: 51.8987
	Task-24	val_accuracy: 63.7500	gate_accuracy: 57.5000
	Task-25	val_accuracy: 67.0588	gate_accuracy: 65.8824
	Task-26	val_accuracy: 76.6234	gate_accuracy: 75.3247
	Task-27	val_accuracy: 48.3871	gate_accuracy: 41.9355
	Task-28	val_accuracy: 58.8889	gate_accuracy: 54.4444
	Task-29	val_accuracy: 69.6203	gate_accuracy: 68.3544
	Task-30	val_accuracy: 53.7500	gate_accuracy: 51.2500
	Task-31	val_accuracy: 65.4762	gate_accuracy: 58.3333
	Task-32	val_accuracy: 64.6341	gate_accuracy: 57.3171
	Task-33	val_accuracy: 47.7273	gate_accuracy: 47.7273
	Task-34	val_accuracy: 57.9710	gate_accuracy: 55.0725
	Task-35	val_accuracy: 53.7500	gate_accuracy: 56.2500
	Task-36	val_accuracy: 44.3038	gate_accuracy: 43.0380
	Task-37	val_accuracy: 62.7907	gate_accuracy: 58.1395
	Task-38	val_accuracy: 50.5882	gate_accuracy: 44.7059
	Task-39	val_accuracy: 56.6265	gate_accuracy: 53.0120
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.6661


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 3.5717	step1_train_accuracy: 40.5512
STEP-1	Epoch: 20/50	loss: 1.4650	step1_train_accuracy: 79.5276
STEP-1	Epoch: 30/50	loss: 0.6742	step1_train_accuracy: 93.3071
STEP-1	Epoch: 40/50	loss: 0.3800	step1_train_accuracy: 97.6378
STEP-1	Epoch: 50/50	loss: 0.2607	step1_train_accuracy: 98.4252
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1111	gate_loss: 2.5164	step2_classification_accuracy: 67.3861	step_2_gate_accuracy: 32.3621
STEP-2	Epoch: 40/200	classification_loss: 0.8887	gate_loss: 1.1138	step2_classification_accuracy: 74.1127	step_2_gate_accuracy: 67.8417
STEP-2	Epoch: 60/200	classification_loss: 0.7218	gate_loss: 0.7243	step2_classification_accuracy: 78.6811	step_2_gate_accuracy: 78.4293
STEP-2	Epoch: 80/200	classification_loss: 0.6043	gate_loss: 0.5441	step2_classification_accuracy: 82.2062	step_2_gate_accuracy: 83.4532
STEP-2	Epoch: 100/200	classification_loss: 0.5354	gate_loss: 0.4417	step2_classification_accuracy: 83.8249	step_2_gate_accuracy: 86.1151
STEP-2	Epoch: 120/200	classification_loss: 0.4915	gate_loss: 0.3850	step2_classification_accuracy: 85.1679	step_2_gate_accuracy: 87.8657
STEP-2	Epoch: 140/200	classification_loss: 0.4421	gate_loss: 0.3309	step2_classification_accuracy: 86.3909	step_2_gate_accuracy: 89.6043
STEP-2	Epoch: 160/200	classification_loss: 0.4184	gate_loss: 0.2977	step2_classification_accuracy: 87.4700	step_2_gate_accuracy: 90.6235
STEP-2	Epoch: 180/200	classification_loss: 0.3985	gate_loss: 0.2797	step2_classification_accuracy: 87.5540	step_2_gate_accuracy: 90.7674
STEP-2	Epoch: 200/200	classification_loss: 0.3642	gate_loss: 0.2456	step2_classification_accuracy: 88.3333	step_2_gate_accuracy: 92.1703
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 55.0000	gate_accuracy: 65.0000
	Task-1	val_accuracy: 45.5556	gate_accuracy: 56.6667
	Task-2	val_accuracy: 45.2381	gate_accuracy: 52.3810
	Task-3	val_accuracy: 75.6410	gate_accuracy: 76.9231
	Task-4	val_accuracy: 58.9744	gate_accuracy: 60.2564
	Task-5	val_accuracy: 59.0909	gate_accuracy: 65.1515
	Task-6	val_accuracy: 69.5652	gate_accuracy: 67.3913
	Task-7	val_accuracy: 71.4286	gate_accuracy: 68.8312
	Task-8	val_accuracy: 64.5570	gate_accuracy: 62.0253
	Task-9	val_accuracy: 69.8795	gate_accuracy: 63.8554
	Task-10	val_accuracy: 75.3425	gate_accuracy: 71.2329
	Task-11	val_accuracy: 68.1818	gate_accuracy: 67.0455
	Task-12	val_accuracy: 60.2564	gate_accuracy: 62.8205
	Task-13	val_accuracy: 68.4932	gate_accuracy: 67.1233
	Task-14	val_accuracy: 65.7143	gate_accuracy: 62.8571
	Task-15	val_accuracy: 72.0000	gate_accuracy: 64.0000
	Task-16	val_accuracy: 56.3380	gate_accuracy: 49.2958
	Task-17	val_accuracy: 67.0103	gate_accuracy: 64.9485
	Task-18	val_accuracy: 57.4713	gate_accuracy: 56.3218
	Task-19	val_accuracy: 46.2500	gate_accuracy: 51.2500
	Task-20	val_accuracy: 56.5217	gate_accuracy: 55.4348
	Task-21	val_accuracy: 81.3187	gate_accuracy: 72.5275
	Task-22	val_accuracy: 63.8554	gate_accuracy: 62.6506
	Task-23	val_accuracy: 50.6329	gate_accuracy: 51.8987
	Task-24	val_accuracy: 48.7500	gate_accuracy: 43.7500
	Task-25	val_accuracy: 74.1176	gate_accuracy: 70.5882
	Task-26	val_accuracy: 71.4286	gate_accuracy: 64.9351
	Task-27	val_accuracy: 48.3871	gate_accuracy: 48.3871
	Task-28	val_accuracy: 70.0000	gate_accuracy: 66.6667
	Task-29	val_accuracy: 77.2152	gate_accuracy: 73.4177
	Task-30	val_accuracy: 53.7500	gate_accuracy: 52.5000
	Task-31	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-32	val_accuracy: 62.1951	gate_accuracy: 47.5610
	Task-33	val_accuracy: 52.2727	gate_accuracy: 47.7273
	Task-34	val_accuracy: 53.6232	gate_accuracy: 50.7246
	Task-35	val_accuracy: 46.2500	gate_accuracy: 51.2500
	Task-36	val_accuracy: 37.9747	gate_accuracy: 30.3797
	Task-37	val_accuracy: 61.6279	gate_accuracy: 56.9767
	Task-38	val_accuracy: 61.1765	gate_accuracy: 55.2941
	Task-39	val_accuracy: 53.0120	gate_accuracy: 45.7831
	Task-40	val_accuracy: 60.9375	gate_accuracy: 60.9375
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.5525


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 3.0044	step1_train_accuracy: 40.2685
STEP-1	Epoch: 20/50	loss: 1.1863	step1_train_accuracy: 70.1342
STEP-1	Epoch: 30/50	loss: 0.6692	step1_train_accuracy: 88.5906
STEP-1	Epoch: 40/50	loss: 0.4307	step1_train_accuracy: 92.6174
STEP-1	Epoch: 50/50	loss: 0.3075	step1_train_accuracy: 97.3154
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1490	gate_loss: 2.5918	step2_classification_accuracy: 65.5386	step_2_gate_accuracy: 30.4098
STEP-2	Epoch: 40/200	classification_loss: 0.9315	gate_loss: 1.1882	step2_classification_accuracy: 72.9977	step_2_gate_accuracy: 65.8782
STEP-2	Epoch: 60/200	classification_loss: 0.7411	gate_loss: 0.7658	step2_classification_accuracy: 78.5480	step_2_gate_accuracy: 76.4988
STEP-2	Epoch: 80/200	classification_loss: 0.6204	gate_loss: 0.5744	step2_classification_accuracy: 81.8267	step_2_gate_accuracy: 81.7682
STEP-2	Epoch: 100/200	classification_loss: 0.5325	gate_loss: 0.4561	step2_classification_accuracy: 83.6651	step_2_gate_accuracy: 85.4801
STEP-2	Epoch: 120/200	classification_loss: 0.4840	gate_loss: 0.3901	step2_classification_accuracy: 85.2342	step_2_gate_accuracy: 87.8220
STEP-2	Epoch: 140/200	classification_loss: 0.4302	gate_loss: 0.3298	step2_classification_accuracy: 86.7681	step_2_gate_accuracy: 89.7190
STEP-2	Epoch: 160/200	classification_loss: 0.4071	gate_loss: 0.3013	step2_classification_accuracy: 87.0960	step_2_gate_accuracy: 90.6206
STEP-2	Epoch: 180/200	classification_loss: 0.3774	gate_loss: 0.2718	step2_classification_accuracy: 88.1616	step_2_gate_accuracy: 91.5457
STEP-2	Epoch: 200/200	classification_loss: 0.3555	gate_loss: 0.2506	step2_classification_accuracy: 88.7588	step_2_gate_accuracy: 92.3536
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.5556	gate_accuracy: 61.1111
	Task-1	val_accuracy: 44.4444	gate_accuracy: 63.3333
	Task-2	val_accuracy: 35.7143	gate_accuracy: 54.7619
	Task-3	val_accuracy: 64.1026	gate_accuracy: 69.2308
	Task-4	val_accuracy: 61.5385	gate_accuracy: 64.1026
	Task-5	val_accuracy: 53.0303	gate_accuracy: 63.6364
	Task-6	val_accuracy: 80.4348	gate_accuracy: 79.3478
	Task-7	val_accuracy: 70.1299	gate_accuracy: 66.2338
	Task-8	val_accuracy: 62.0253	gate_accuracy: 64.5570
	Task-9	val_accuracy: 73.4940	gate_accuracy: 71.0843
	Task-10	val_accuracy: 63.0137	gate_accuracy: 65.7534
	Task-11	val_accuracy: 75.0000	gate_accuracy: 72.7273
	Task-12	val_accuracy: 64.1026	gate_accuracy: 69.2308
	Task-13	val_accuracy: 61.6438	gate_accuracy: 64.3836
	Task-14	val_accuracy: 61.4286	gate_accuracy: 62.8571
	Task-15	val_accuracy: 70.6667	gate_accuracy: 66.6667
	Task-16	val_accuracy: 63.3803	gate_accuracy: 59.1549
	Task-17	val_accuracy: 64.9485	gate_accuracy: 59.7938
	Task-18	val_accuracy: 75.8621	gate_accuracy: 68.9655
	Task-19	val_accuracy: 50.0000	gate_accuracy: 50.0000
	Task-20	val_accuracy: 56.5217	gate_accuracy: 56.5217
	Task-21	val_accuracy: 81.3187	gate_accuracy: 74.7253
	Task-22	val_accuracy: 59.0361	gate_accuracy: 51.8072
	Task-23	val_accuracy: 54.4304	gate_accuracy: 59.4937
	Task-24	val_accuracy: 60.0000	gate_accuracy: 52.5000
	Task-25	val_accuracy: 76.4706	gate_accuracy: 74.1176
	Task-26	val_accuracy: 79.2208	gate_accuracy: 74.0260
	Task-27	val_accuracy: 41.9355	gate_accuracy: 43.5484
	Task-28	val_accuracy: 54.4444	gate_accuracy: 52.2222
	Task-29	val_accuracy: 75.9494	gate_accuracy: 73.4177
	Task-30	val_accuracy: 45.0000	gate_accuracy: 46.2500
	Task-31	val_accuracy: 58.3333	gate_accuracy: 51.1905
	Task-32	val_accuracy: 58.5366	gate_accuracy: 54.8780
	Task-33	val_accuracy: 51.1364	gate_accuracy: 50.0000
	Task-34	val_accuracy: 57.9710	gate_accuracy: 59.4203
	Task-35	val_accuracy: 47.5000	gate_accuracy: 45.0000
	Task-36	val_accuracy: 37.9747	gate_accuracy: 34.1772
	Task-37	val_accuracy: 65.1163	gate_accuracy: 61.6279
	Task-38	val_accuracy: 50.5882	gate_accuracy: 47.0588
	Task-39	val_accuracy: 50.6024	gate_accuracy: 49.3976
	Task-40	val_accuracy: 53.1250	gate_accuracy: 53.1250
	Task-41	val_accuracy: 67.5676	gate_accuracy: 63.5135
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.2708


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 2.4218	step1_train_accuracy: 56.3636
STEP-1	Epoch: 20/50	loss: 0.8469	step1_train_accuracy: 89.0909
STEP-1	Epoch: 30/50	loss: 0.4436	step1_train_accuracy: 94.5455
STEP-1	Epoch: 40/50	loss: 0.2771	step1_train_accuracy: 95.7576
STEP-1	Epoch: 50/50	loss: 0.1987	step1_train_accuracy: 97.2727
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1718	gate_loss: 2.6248	step2_classification_accuracy: 65.4233	step_2_gate_accuracy: 30.4348
STEP-2	Epoch: 40/200	classification_loss: 0.9404	gate_loss: 1.1992	step2_classification_accuracy: 72.6773	step_2_gate_accuracy: 65.4577
STEP-2	Epoch: 60/200	classification_loss: 0.7501	gate_loss: 0.7791	step2_classification_accuracy: 78.1922	step_2_gate_accuracy: 76.5332
STEP-2	Epoch: 80/200	classification_loss: 0.6366	gate_loss: 0.5869	step2_classification_accuracy: 81.2815	step_2_gate_accuracy: 81.9336
STEP-2	Epoch: 100/200	classification_loss: 0.5519	gate_loss: 0.4723	step2_classification_accuracy: 83.2494	step_2_gate_accuracy: 84.9428
STEP-2	Epoch: 120/200	classification_loss: 0.5000	gate_loss: 0.4060	step2_classification_accuracy: 84.7826	step_2_gate_accuracy: 86.9794
STEP-2	Epoch: 140/200	classification_loss: 0.4644	gate_loss: 0.3646	step2_classification_accuracy: 85.6751	step_2_gate_accuracy: 88.4668
STEP-2	Epoch: 160/200	classification_loss: 0.4240	gate_loss: 0.3162	step2_classification_accuracy: 86.8192	step_2_gate_accuracy: 90.1945
STEP-2	Epoch: 180/200	classification_loss: 0.3831	gate_loss: 0.2782	step2_classification_accuracy: 87.7918	step_2_gate_accuracy: 91.5217
STEP-2	Epoch: 200/200	classification_loss: 0.4140	gate_loss: 0.3144	step2_classification_accuracy: 86.8307	step_2_gate_accuracy: 89.6453
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 52.7778	gate_accuracy: 61.1111
	Task-1	val_accuracy: 48.8889	gate_accuracy: 63.3333
	Task-2	val_accuracy: 46.4286	gate_accuracy: 59.5238
	Task-3	val_accuracy: 61.5385	gate_accuracy: 69.2308
	Task-4	val_accuracy: 60.2564	gate_accuracy: 61.5385
	Task-5	val_accuracy: 50.0000	gate_accuracy: 63.6364
	Task-6	val_accuracy: 71.7391	gate_accuracy: 70.6522
	Task-7	val_accuracy: 71.4286	gate_accuracy: 67.5325
	Task-8	val_accuracy: 58.2278	gate_accuracy: 60.7595
	Task-9	val_accuracy: 67.4699	gate_accuracy: 65.0602
	Task-10	val_accuracy: 69.8630	gate_accuracy: 64.3836
	Task-11	val_accuracy: 65.9091	gate_accuracy: 65.9091
	Task-12	val_accuracy: 67.9487	gate_accuracy: 73.0769
	Task-13	val_accuracy: 75.3425	gate_accuracy: 67.1233
	Task-14	val_accuracy: 64.2857	gate_accuracy: 67.1429
	Task-15	val_accuracy: 74.6667	gate_accuracy: 68.0000
	Task-16	val_accuracy: 54.9296	gate_accuracy: 57.7465
	Task-17	val_accuracy: 70.1031	gate_accuracy: 55.6701
	Task-18	val_accuracy: 73.5632	gate_accuracy: 70.1149
	Task-19	val_accuracy: 56.2500	gate_accuracy: 61.2500
	Task-20	val_accuracy: 65.2174	gate_accuracy: 60.8696
	Task-21	val_accuracy: 82.4176	gate_accuracy: 72.5275
	Task-22	val_accuracy: 56.6265	gate_accuracy: 53.0120
	Task-23	val_accuracy: 56.9620	gate_accuracy: 53.1646
	Task-24	val_accuracy: 56.2500	gate_accuracy: 48.7500
	Task-25	val_accuracy: 75.2941	gate_accuracy: 71.7647
	Task-26	val_accuracy: 79.2208	gate_accuracy: 66.2338
	Task-27	val_accuracy: 54.8387	gate_accuracy: 45.1613
	Task-28	val_accuracy: 62.2222	gate_accuracy: 57.7778
	Task-29	val_accuracy: 77.2152	gate_accuracy: 65.8228
	Task-30	val_accuracy: 51.2500	gate_accuracy: 50.0000
	Task-31	val_accuracy: 63.0952	gate_accuracy: 58.3333
	Task-32	val_accuracy: 53.6585	gate_accuracy: 48.7805
	Task-33	val_accuracy: 56.8182	gate_accuracy: 54.5455
	Task-34	val_accuracy: 63.7681	gate_accuracy: 65.2174
	Task-35	val_accuracy: 43.7500	gate_accuracy: 45.0000
	Task-36	val_accuracy: 41.7722	gate_accuracy: 39.2405
	Task-37	val_accuracy: 68.6047	gate_accuracy: 63.9535
	Task-38	val_accuracy: 57.6471	gate_accuracy: 44.7059
	Task-39	val_accuracy: 45.7831	gate_accuracy: 49.3976
	Task-40	val_accuracy: 39.0625	gate_accuracy: 39.0625
	Task-41	val_accuracy: 59.4595	gate_accuracy: 58.1081
	Task-42	val_accuracy: 65.0602	gate_accuracy: 66.2651
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.9606


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 2.5995	step1_train_accuracy: 49.0506
STEP-1	Epoch: 20/50	loss: 0.8237	step1_train_accuracy: 82.5949
STEP-1	Epoch: 30/50	loss: 0.4293	step1_train_accuracy: 96.8354
STEP-1	Epoch: 40/50	loss: 0.2683	step1_train_accuracy: 97.4684
STEP-1	Epoch: 50/50	loss: 0.1822	step1_train_accuracy: 98.4177
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10, 874: 10, 875: 10, 876: 10, 877: 10, 878: 10, 879: 10, 880: 10, 881: 10, 882: 10, 883: 10, 884: 10, 885: 10, 886: 10, 887: 10, 888: 10, 889: 10, 890: 10, 891: 10, 892: 10, 893: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1475	gate_loss: 2.6531	step2_classification_accuracy: 66.3758	step_2_gate_accuracy: 30.2013
STEP-2	Epoch: 40/200	classification_loss: 0.9281	gate_loss: 1.1824	step2_classification_accuracy: 73.3669	step_2_gate_accuracy: 66.1409
STEP-2	Epoch: 60/200	classification_loss: 0.7593	gate_loss: 0.7812	step2_classification_accuracy: 77.8188	step_2_gate_accuracy: 76.3423
STEP-2	Epoch: 80/200	classification_loss: 0.6426	gate_loss: 0.5962	step2_classification_accuracy: 80.8054	step_2_gate_accuracy: 81.3423
STEP-2	Epoch: 100/200	classification_loss: 0.5652	gate_loss: 0.4902	step2_classification_accuracy: 82.7852	step_2_gate_accuracy: 84.6644
STEP-2	Epoch: 120/200	classification_loss: 0.5033	gate_loss: 0.4101	step2_classification_accuracy: 84.9329	step_2_gate_accuracy: 87.1588
STEP-2	Epoch: 140/200	classification_loss: 0.4552	gate_loss: 0.3540	step2_classification_accuracy: 86.0962	step_2_gate_accuracy: 89.1387
STEP-2	Epoch: 160/200	classification_loss: 0.4181	gate_loss: 0.3153	step2_classification_accuracy: 87.0470	step_2_gate_accuracy: 90.2685
STEP-2	Epoch: 180/200	classification_loss: 0.3972	gate_loss: 0.2888	step2_classification_accuracy: 87.5280	step_2_gate_accuracy: 90.9284
STEP-2	Epoch: 200/200	classification_loss: 0.3698	gate_loss: 0.2645	step2_classification_accuracy: 88.2998	step_2_gate_accuracy: 91.6778
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 44.4444	gate_accuracy: 58.3333
	Task-1	val_accuracy: 46.6667	gate_accuracy: 55.5556
	Task-2	val_accuracy: 44.0476	gate_accuracy: 57.1429
	Task-3	val_accuracy: 67.9487	gate_accuracy: 71.7949
	Task-4	val_accuracy: 52.5641	gate_accuracy: 57.6923
	Task-5	val_accuracy: 53.0303	gate_accuracy: 68.1818
	Task-6	val_accuracy: 76.0870	gate_accuracy: 71.7391
	Task-7	val_accuracy: 70.1299	gate_accuracy: 70.1299
	Task-8	val_accuracy: 60.7595	gate_accuracy: 60.7595
	Task-9	val_accuracy: 63.8554	gate_accuracy: 61.4458
	Task-10	val_accuracy: 80.8219	gate_accuracy: 76.7123
	Task-11	val_accuracy: 65.9091	gate_accuracy: 69.3182
	Task-12	val_accuracy: 57.6923	gate_accuracy: 65.3846
	Task-13	val_accuracy: 72.6027	gate_accuracy: 61.6438
	Task-14	val_accuracy: 62.8571	gate_accuracy: 67.1429
	Task-15	val_accuracy: 74.6667	gate_accuracy: 66.6667
	Task-16	val_accuracy: 50.7042	gate_accuracy: 45.0704
	Task-17	val_accuracy: 62.8866	gate_accuracy: 61.8557
	Task-18	val_accuracy: 67.8161	gate_accuracy: 67.8161
	Task-19	val_accuracy: 60.0000	gate_accuracy: 58.7500
	Task-20	val_accuracy: 64.1304	gate_accuracy: 58.6957
	Task-21	val_accuracy: 74.7253	gate_accuracy: 68.1319
	Task-22	val_accuracy: 62.6506	gate_accuracy: 65.0602
	Task-23	val_accuracy: 56.9620	gate_accuracy: 59.4937
	Task-24	val_accuracy: 51.2500	gate_accuracy: 45.0000
	Task-25	val_accuracy: 69.4118	gate_accuracy: 63.5294
	Task-26	val_accuracy: 88.3117	gate_accuracy: 80.5195
	Task-27	val_accuracy: 43.5484	gate_accuracy: 37.0968
	Task-28	val_accuracy: 61.1111	gate_accuracy: 54.4444
	Task-29	val_accuracy: 77.2152	gate_accuracy: 72.1519
	Task-30	val_accuracy: 42.5000	gate_accuracy: 42.5000
	Task-31	val_accuracy: 63.0952	gate_accuracy: 61.9048
	Task-32	val_accuracy: 62.1951	gate_accuracy: 51.2195
	Task-33	val_accuracy: 57.9545	gate_accuracy: 57.9545
	Task-34	val_accuracy: 68.1159	gate_accuracy: 66.6667
	Task-35	val_accuracy: 46.2500	gate_accuracy: 48.7500
	Task-36	val_accuracy: 31.6456	gate_accuracy: 35.4430
	Task-37	val_accuracy: 66.2791	gate_accuracy: 60.4651
	Task-38	val_accuracy: 51.7647	gate_accuracy: 47.0588
	Task-39	val_accuracy: 50.6024	gate_accuracy: 45.7831
	Task-40	val_accuracy: 54.6875	gate_accuracy: 50.0000
	Task-41	val_accuracy: 56.7568	gate_accuracy: 50.0000
	Task-42	val_accuracy: 60.2410	gate_accuracy: 59.0361
	Task-43	val_accuracy: 63.2911	gate_accuracy: 58.2278
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.4550


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 06:32:31	Wall time: 05:45:05
CPU time: 23551.213700177	Wall time: 20705.268929719925
FAA: 71.19390963019187
FF: 31.96854183194644

TRAINER.METRIC.ACCURACY
0: [98.33333333333333]
1: [96.66666666666667, 75.55555555555556]
2: [92.77777777777779, 76.66666666666667, 84.52380952380952]
3: [87.22222222222223, 77.77777777777779, 82.14285714285714, 87.17948717948718]
4: [84.44444444444444, 73.33333333333333, 79.76190476190477, 84.61538461538461, 80.76923076923077]
5: [82.77777777777777, 74.44444444444444, 76.19047619047619, 80.76923076923077, 78.2051282051282, 66.66666666666666]
6: [80.55555555555556, 68.88888888888889, 85.71428571428571, 79.48717948717949, 75.64102564102564, 75.75757575757575, 92.3913043478261]
7: [81.11111111111111, 71.11111111111111, 76.19047619047619, 79.48717948717949, 79.48717948717949, 71.21212121212122, 92.3913043478261, 85.71428571428571]
8: [76.11111111111111, 66.66666666666666, 79.76190476190477, 82.05128205128204, 74.35897435897436, 69.6969696969697, 94.56521739130434, 87.01298701298701, 88.60759493670885]
9: [76.66666666666667, 66.66666666666666, 61.904761904761905, 80.76923076923077, 76.92307692307693, 68.18181818181817, 92.3913043478261, 79.22077922077922, 88.60759493670885, 89.1566265060241]
10: [77.22222222222223, 73.33333333333333, 69.04761904761905, 73.07692307692307, 75.64102564102564, 68.18181818181817, 81.52173913043478, 85.71428571428571, 84.81012658227847, 86.74698795180723, 87.67123287671232]
11: [75.0, 67.77777777777779, 59.523809523809526, 75.64102564102564, 79.48717948717949, 66.66666666666666, 88.04347826086956, 87.01298701298701, 87.34177215189874, 85.54216867469879, 95.8904109589041, 84.0909090909091]
12: [75.55555555555556, 63.33333333333333, 69.04761904761905, 76.92307692307693, 71.7948717948718, 69.6969696969697, 88.04347826086956, 85.71428571428571, 83.54430379746836, 81.92771084337349, 91.78082191780823, 86.36363636363636, 70.51282051282051]
13: [74.44444444444444, 63.33333333333333, 63.095238095238095, 75.64102564102564, 74.35897435897436, 72.72727272727273, 89.13043478260869, 87.01298701298701, 78.48101265822784, 81.92771084337349, 89.04109589041096, 80.68181818181817, 76.92307692307693, 86.3013698630137]
14: [74.44444444444444, 63.33333333333333, 67.85714285714286, 80.76923076923077, 73.07692307692307, 71.21212121212122, 91.30434782608695, 83.11688311688312, 72.15189873417721, 86.74698795180723, 86.3013698630137, 81.81818181818183, 76.92307692307693, 79.45205479452055, 77.14285714285715]
15: [72.77777777777777, 72.22222222222221, 65.47619047619048, 80.76923076923077, 70.51282051282051, 65.15151515151516, 76.08695652173914, 83.11688311688312, 69.62025316455697, 83.13253012048193, 84.93150684931507, 86.36363636363636, 79.48717948717949, 73.97260273972603, 77.14285714285715, 89.33333333333333]
16: [73.88888888888889, 65.55555555555556, 54.761904761904766, 74.35897435897436, 69.23076923076923, 60.60606060606061, 85.86956521739131, 76.62337662337663, 79.74683544303798, 80.72289156626506, 87.67123287671232, 85.22727272727273, 75.64102564102564, 80.82191780821918, 71.42857142857143, 85.33333333333334, 64.7887323943662]
17: [68.88888888888889, 60.0, 59.523809523809526, 75.64102564102564, 74.35897435897436, 63.63636363636363, 84.78260869565217, 81.81818181818183, 74.68354430379746, 79.51807228915662, 90.41095890410958, 81.81818181818183, 73.07692307692307, 80.82191780821918, 68.57142857142857, 81.33333333333333, 64.7887323943662, 72.16494845360825]
18: [73.88888888888889, 58.88888888888889, 54.761904761904766, 79.48717948717949, 67.94871794871796, 62.121212121212125, 80.43478260869566, 72.72727272727273, 75.9493670886076, 75.90361445783132, 87.67123287671232, 86.36363636363636, 74.35897435897436, 78.08219178082192, 71.42857142857143, 81.33333333333333, 66.19718309859155, 74.22680412371135, 75.86206896551724]
19: [71.66666666666667, 64.44444444444444, 54.761904761904766, 73.07692307692307, 71.7948717948718, 63.63636363636363, 83.69565217391305, 77.92207792207793, 67.08860759493672, 79.51807228915662, 87.67123287671232, 84.0909090909091, 73.07692307692307, 69.86301369863014, 68.57142857142857, 90.66666666666666, 54.929577464788736, 71.1340206185567, 77.01149425287356, 85.0]
20: [78.33333333333333, 54.44444444444444, 58.333333333333336, 71.7948717948718, 71.7948717948718, 57.57575757575758, 84.78260869565217, 80.51948051948052, 73.41772151898735, 85.54216867469879, 89.04109589041096, 84.0909090909091, 73.07692307692307, 73.97260273972603, 65.71428571428571, 84.0, 60.56338028169014, 68.04123711340206, 68.96551724137932, 66.25, 59.78260869565217]
21: [70.0, 61.111111111111114, 61.904761904761905, 70.51282051282051, 71.7948717948718, 50.0, 82.6086956521739, 75.32467532467533, 60.75949367088608, 78.3132530120482, 83.56164383561644, 75.0, 73.07692307692307, 75.34246575342466, 70.0, 85.33333333333334, 53.52112676056338, 73.19587628865979, 74.71264367816092, 71.25, 63.04347826086957, 80.21978021978022]
22: [70.55555555555556, 53.333333333333336, 57.14285714285714, 75.64102564102564, 67.94871794871796, 53.03030303030303, 82.6086956521739, 72.72727272727273, 81.0126582278481, 78.3132530120482, 90.41095890410958, 81.81818181818183, 67.94871794871796, 72.6027397260274, 74.28571428571429, 81.33333333333333, 54.929577464788736, 72.16494845360825, 72.41379310344827, 63.74999999999999, 66.30434782608695, 78.02197802197803, 50.602409638554214]
23: [66.11111111111111, 55.55555555555556, 60.71428571428571, 58.97435897435898, 73.07692307692307, 57.57575757575758, 82.6086956521739, 79.22077922077922, 73.41772151898735, 81.92771084337349, 90.41095890410958, 77.27272727272727, 69.23076923076923, 73.97260273972603, 65.71428571428571, 82.66666666666667, 60.56338028169014, 67.0103092783505, 71.26436781609196, 70.0, 65.21739130434783, 79.12087912087912, 62.65060240963856, 67.08860759493672]
24: [63.888888888888886, 52.22222222222223, 60.71428571428571, 71.7948717948718, 66.66666666666666, 60.60606060606061, 80.43478260869566, 81.81818181818183, 63.29113924050633, 75.90361445783132, 87.67123287671232, 81.81818181818183, 70.51282051282051, 69.86301369863014, 67.14285714285714, 80.0, 56.33802816901409, 70.10309278350515, 75.86206896551724, 70.0, 73.91304347826086, 81.31868131868131, 59.036144578313255, 73.41772151898735, 63.74999999999999]
25: [62.22222222222222, 56.666666666666664, 58.333333333333336, 62.82051282051282, 66.66666666666666, 59.09090909090909, 72.82608695652173, 75.32467532467533, 64.55696202531645, 72.28915662650603, 83.56164383561644, 73.86363636363636, 71.7948717948718, 72.6027397260274, 67.14285714285714, 78.66666666666666, 66.19718309859155, 68.04123711340206, 75.86206896551724, 62.5, 68.47826086956522, 84.61538461538461, 59.036144578313255, 64.55696202531645, 68.75, 74.11764705882354]
26: [66.11111111111111, 54.44444444444444, 52.38095238095239, 70.51282051282051, 65.38461538461539, 62.121212121212125, 81.52173913043478, 72.72727272727273, 73.41772151898735, 75.90361445783132, 84.93150684931507, 78.4090909090909, 66.66666666666666, 71.23287671232876, 68.57142857142857, 77.33333333333333, 66.19718309859155, 76.28865979381443, 70.11494252873564, 61.25000000000001, 61.95652173913043, 81.31868131868131, 59.036144578313255, 63.29113924050633, 61.25000000000001, 78.82352941176471, 80.51948051948052]
27: [62.77777777777778, 56.666666666666664, 51.19047619047619, 65.38461538461539, 71.7948717948718, 53.03030303030303, 75.0, 76.62337662337663, 72.15189873417721, 85.54216867469879, 84.93150684931507, 79.54545454545455, 67.94871794871796, 78.08219178082192, 68.57142857142857, 68.0, 63.38028169014085, 65.97938144329896, 68.96551724137932, 66.25, 61.95652173913043, 83.51648351648352, 59.036144578313255, 70.88607594936708, 63.74999999999999, 76.47058823529412, 81.81818181818183, 62.903225806451616]
28: [67.22222222222223, 47.77777777777778, 47.61904761904761, 65.38461538461539, 64.1025641025641, 50.0, 84.78260869565217, 72.72727272727273, 63.29113924050633, 80.72289156626506, 84.93150684931507, 78.4090909090909, 80.76923076923077, 72.6027397260274, 68.57142857142857, 73.33333333333333, 57.74647887323944, 65.97938144329896, 72.41379310344827, 62.5, 60.86956521739131, 82.41758241758241, 55.42168674698795, 65.82278481012658, 61.25000000000001, 78.82352941176471, 81.81818181818183, 43.54838709677419, 67.77777777777779]
29: [63.888888888888886, 51.11111111111111, 44.047619047619044, 67.94871794871796, 69.23076923076923, 53.03030303030303, 79.34782608695652, 81.81818181818183, 81.0126582278481, 77.10843373493977, 82.1917808219178, 79.54545454545455, 65.38461538461539, 76.71232876712328, 64.28571428571429, 77.33333333333333, 61.97183098591549, 64.94845360824742, 67.81609195402298, 62.5, 59.78260869565217, 74.72527472527473, 62.65060240963856, 60.75949367088608, 61.25000000000001, 80.0, 75.32467532467533, 46.774193548387096, 67.77777777777779, 77.21518987341773]
30: [52.77777777777778, 46.666666666666664, 55.952380952380956, 69.23076923076923, 74.35897435897436, 68.18181818181817, 80.43478260869566, 79.22077922077922, 67.08860759493672, 75.90361445783132, 75.34246575342466, 77.27272727272727, 64.1025641025641, 64.38356164383562, 67.14285714285714, 69.33333333333334, 45.07042253521127, 70.10309278350515, 71.26436781609196, 72.5, 59.78260869565217, 73.62637362637363, 60.24096385542169, 63.29113924050633, 61.25000000000001, 74.11764705882354, 85.71428571428571, 54.83870967741935, 70.0, 72.15189873417721, 48.75]
31: [56.666666666666664, 50.0, 47.61904761904761, 66.66666666666666, 67.94871794871796, 56.060606060606055, 70.65217391304348, 76.62337662337663, 60.75949367088608, 83.13253012048193, 73.97260273972603, 73.86363636363636, 61.53846153846154, 72.6027397260274, 70.0, 74.66666666666667, 61.97183098591549, 64.94845360824742, 70.11494252873564, 61.25000000000001, 55.434782608695656, 83.51648351648352, 57.831325301204814, 58.22784810126582, 57.49999999999999, 74.11764705882354, 81.81818181818183, 54.83870967741935, 63.33333333333333, 75.9493670886076, 56.25, 58.333333333333336]
32: [61.66666666666667, 41.11111111111111, 44.047619047619044, 64.1025641025641, 67.94871794871796, 56.060606060606055, 73.91304347826086, 67.53246753246754, 68.35443037974683, 75.90361445783132, 80.82191780821918, 76.13636363636364, 67.94871794871796, 69.86301369863014, 65.71428571428571, 80.0, 56.33802816901409, 72.16494845360825, 72.41379310344827, 52.5, 61.95652173913043, 83.51648351648352, 61.44578313253012, 58.22784810126582, 61.25000000000001, 78.82352941176471, 76.62337662337663, 48.38709677419355, 64.44444444444444, 77.21518987341773, 50.0, 71.42857142857143, 60.97560975609756]
33: [62.22222222222222, 54.44444444444444, 47.61904761904761, 69.23076923076923, 62.82051282051282, 54.54545454545454, 80.43478260869566, 75.32467532467533, 59.49367088607595, 69.87951807228916, 76.71232876712328, 77.27272727272727, 67.94871794871796, 68.4931506849315, 70.0, 76.0, 60.56338028169014, 69.0721649484536, 77.01149425287356, 63.74999999999999, 58.69565217391305, 78.02197802197803, 61.44578313253012, 63.29113924050633, 55.00000000000001, 80.0, 77.92207792207793, 48.38709677419355, 68.88888888888889, 75.9493670886076, 57.49999999999999, 65.47619047619048, 60.97560975609756, 54.54545454545454]
34: [65.55555555555556, 47.77777777777778, 51.19047619047619, 66.66666666666666, 67.94871794871796, 50.0, 76.08695652173914, 71.42857142857143, 60.75949367088608, 80.72289156626506, 82.1917808219178, 67.04545454545455, 64.1025641025641, 73.97260273972603, 61.42857142857143, 76.0, 54.929577464788736, 64.94845360824742, 65.51724137931035, 56.25, 59.78260869565217, 82.41758241758241, 59.036144578313255, 45.56962025316456, 62.5, 78.82352941176471, 79.22077922077922, 45.16129032258064, 71.11111111111111, 73.41772151898735, 52.5, 63.095238095238095, 62.19512195121951, 62.5, 62.31884057971014]
35: [56.666666666666664, 48.888888888888886, 53.57142857142857, 64.1025641025641, 65.38461538461539, 53.03030303030303, 79.34782608695652, 72.72727272727273, 72.15189873417721, 69.87951807228916, 75.34246575342466, 71.5909090909091, 69.23076923076923, 72.6027397260274, 65.71428571428571, 72.0, 50.70422535211267, 67.0103092783505, 68.96551724137932, 65.0, 56.52173913043478, 74.72527472527473, 61.44578313253012, 68.35443037974683, 58.75, 75.29411764705883, 74.02597402597402, 51.61290322580645, 52.22222222222223, 78.48101265822784, 55.00000000000001, 66.66666666666666, 62.19512195121951, 45.45454545454545, 68.11594202898551, 51.24999999999999]
36: [55.55555555555556, 44.44444444444444, 44.047619047619044, 67.94871794871796, 57.692307692307686, 48.484848484848484, 61.95652173913043, 77.92207792207793, 73.41772151898735, 77.10843373493977, 76.71232876712328, 71.5909090909091, 65.38461538461539, 75.34246575342466, 64.28571428571429, 74.66666666666667, 47.88732394366197, 70.10309278350515, 73.5632183908046, 56.25, 58.69565217391305, 75.82417582417582, 54.21686746987952, 59.49367088607595, 60.0, 71.76470588235294, 74.02597402597402, 50.0, 66.66666666666666, 77.21518987341773, 57.49999999999999, 58.333333333333336, 59.756097560975604, 52.27272727272727, 66.66666666666666, 47.5, 36.708860759493675]
37: [57.22222222222222, 43.333333333333336, 44.047619047619044, 67.94871794871796, 62.82051282051282, 50.0, 67.3913043478261, 72.72727272727273, 65.82278481012658, 75.90361445783132, 79.45205479452055, 65.9090909090909, 64.1025641025641, 68.4931506849315, 65.71428571428571, 72.0, 54.929577464788736, 63.91752577319587, 63.2183908045977, 57.49999999999999, 63.04347826086957, 76.92307692307693, 59.036144578313255, 67.08860759493672, 55.00000000000001, 70.58823529411765, 75.32467532467533, 32.25806451612903, 57.77777777777777, 78.48101265822784, 55.00000000000001, 51.19047619047619, 63.41463414634146, 56.81818181818182, 68.11594202898551, 50.0, 48.10126582278481, 63.95348837209303]
38: [61.111111111111114, 41.11111111111111, 40.476190476190474, 64.1025641025641, 55.12820512820513, 45.45454545454545, 70.65217391304348, 66.23376623376623, 74.68354430379746, 73.49397590361446, 78.08219178082192, 60.22727272727273, 67.94871794871796, 72.6027397260274, 62.857142857142854, 81.33333333333333, 46.478873239436616, 57.73195876288659, 71.26436781609196, 65.0, 51.08695652173913, 76.92307692307693, 59.036144578313255, 68.35443037974683, 56.25, 75.29411764705883, 84.4155844155844, 45.16129032258064, 61.111111111111114, 78.48101265822784, 61.25000000000001, 55.952380952380956, 67.07317073170732, 54.54545454545454, 62.31884057971014, 57.49999999999999, 39.24050632911392, 67.44186046511628, 56.470588235294116]
39: [58.333333333333336, 42.22222222222222, 51.19047619047619, 64.1025641025641, 64.1025641025641, 45.45454545454545, 65.21739130434783, 64.93506493506493, 62.0253164556962, 74.69879518072288, 71.23287671232876, 73.86363636363636, 67.94871794871796, 64.38356164383562, 61.42857142857143, 69.33333333333334, 52.112676056338024, 70.10309278350515, 72.41379310344827, 57.49999999999999, 72.82608695652173, 73.62637362637363, 60.24096385542169, 56.9620253164557, 63.74999999999999, 67.05882352941175, 76.62337662337663, 48.38709677419355, 58.88888888888889, 69.62025316455697, 53.75, 65.47619047619048, 64.63414634146342, 47.72727272727273, 57.971014492753625, 53.75, 44.303797468354425, 62.7906976744186, 50.588235294117645, 56.62650602409639]
40: [55.00000000000001, 45.55555555555556, 45.23809523809524, 75.64102564102564, 58.97435897435898, 59.09090909090909, 69.56521739130434, 71.42857142857143, 64.55696202531645, 69.87951807228916, 75.34246575342466, 68.18181818181817, 60.256410256410255, 68.4931506849315, 65.71428571428571, 72.0, 56.33802816901409, 67.0103092783505, 57.47126436781609, 46.25, 56.52173913043478, 81.31868131868131, 63.85542168674698, 50.63291139240506, 48.75, 74.11764705882354, 71.42857142857143, 48.38709677419355, 70.0, 77.21518987341773, 53.75, 66.66666666666666, 62.19512195121951, 52.27272727272727, 53.62318840579711, 46.25, 37.9746835443038, 61.627906976744185, 61.1764705882353, 53.01204819277109, 60.9375]
41: [50.55555555555556, 44.44444444444444, 35.714285714285715, 64.1025641025641, 61.53846153846154, 53.03030303030303, 80.43478260869566, 70.12987012987013, 62.0253164556962, 73.49397590361446, 63.013698630136986, 75.0, 64.1025641025641, 61.64383561643836, 61.42857142857143, 70.66666666666667, 63.38028169014085, 64.94845360824742, 75.86206896551724, 50.0, 56.52173913043478, 81.31868131868131, 59.036144578313255, 54.43037974683544, 60.0, 76.47058823529412, 79.22077922077922, 41.935483870967744, 54.44444444444444, 75.9493670886076, 45.0, 58.333333333333336, 58.536585365853654, 51.13636363636363, 57.971014492753625, 47.5, 37.9746835443038, 65.11627906976744, 50.588235294117645, 50.602409638554214, 53.125, 67.56756756756756]
42: [52.77777777777778, 48.888888888888886, 46.42857142857143, 61.53846153846154, 60.256410256410255, 50.0, 71.73913043478261, 71.42857142857143, 58.22784810126582, 67.46987951807229, 69.86301369863014, 65.9090909090909, 67.94871794871796, 75.34246575342466, 64.28571428571429, 74.66666666666667, 54.929577464788736, 70.10309278350515, 73.5632183908046, 56.25, 65.21739130434783, 82.41758241758241, 56.62650602409639, 56.9620253164557, 56.25, 75.29411764705883, 79.22077922077922, 54.83870967741935, 62.22222222222222, 77.21518987341773, 51.24999999999999, 63.095238095238095, 53.65853658536586, 56.81818181818182, 63.76811594202898, 43.75, 41.77215189873418, 68.6046511627907, 57.647058823529406, 45.78313253012048, 39.0625, 59.45945945945946, 65.06024096385542]
43: [44.44444444444444, 46.666666666666664, 44.047619047619044, 67.94871794871796, 52.56410256410257, 53.03030303030303, 76.08695652173914, 70.12987012987013, 60.75949367088608, 63.85542168674698, 80.82191780821918, 65.9090909090909, 57.692307692307686, 72.6027397260274, 62.857142857142854, 74.66666666666667, 50.70422535211267, 62.88659793814433, 67.81609195402298, 60.0, 64.13043478260869, 74.72527472527473, 62.65060240963856, 56.9620253164557, 51.24999999999999, 69.41176470588235, 88.31168831168831, 43.54838709677419, 61.111111111111114, 77.21518987341773, 42.5, 63.095238095238095, 62.19512195121951, 57.95454545454546, 68.11594202898551, 46.25, 31.645569620253166, 66.27906976744185, 51.76470588235295, 50.602409638554214, 54.6875, 56.75675675675676, 60.24096385542169, 63.29113924050633]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 46.8085
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 55.8333
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 57.8947
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 68.8679
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 56.6038
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 59.7826
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 63.1148
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 79.2453
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 57.9439
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 56.2500
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 63.3663
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 72.8814
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 57.0093
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 71.0000
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 59.7938
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 73.7864
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 49.4949
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 62.9921
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 67.2414
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 51.8182
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 52.4590
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 75.4098
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 62.5000
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 59.2593
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 56.4815
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 63.1579
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 80.0000
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 47.1264
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 61.6667
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 72.2222
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 55.0459
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 62.8319
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 65.4545
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 57.7586
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 69.7917
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 47.7064
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 37.9630
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 71.5517
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 55.2632
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 57.1429
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 55.5556
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 69.6078
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 61.2613
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 62.6168

====================

f1_score(micro): 60.87044946105349
f1_score(macro): 57.391678406908575
Classification report:
              precision    recall  f1-score   support

           0       1.00      0.33      0.50         9
           1       0.88      0.78      0.82         9
           2       0.75      0.67      0.71         9
           3       0.50      0.40      0.44         5
           4       1.00      0.75      0.86         4
           5       1.00      0.80      0.89         5
           6       1.00      0.00      0.00         9
           7       0.86      0.67      0.75         9
           8       1.00      0.00      0.00         4
           9       0.00      0.00      0.00         4
          10       0.00      0.00      0.00         9
          11       1.00      0.44      0.62         9
          12       1.00      0.00      0.00         4
          13       1.00      0.89      0.94         9
          14       0.60      0.33      0.43         9
          15       1.00      0.22      0.36         9
          16       1.00      0.75      0.86         4
          17       0.50      0.11      0.18         9
          18       1.00      0.67      0.80         9
          19       1.00      0.75      0.86         4
          20       1.00      0.67      0.80         9
          21       1.00      0.00      0.00         9
          22       0.50      0.25      0.33         4
          23       1.00      0.50      0.67         4
          24       0.83      0.56      0.67         9
          25       1.00      1.00      1.00         5
          26       1.00      0.00      0.00         4
          27       0.00      0.00      0.00         4
          28       1.00      0.56      0.71         9
          29       1.00      0.44      0.62         9
          30       1.00      1.00      1.00         9
          31       0.90      1.00      0.95         9
          32       1.00      0.25      0.40         4
          33       1.00      0.40      0.57         5
          34       0.60      0.75      0.67         4
          35       0.43      0.33      0.38         9
          36       1.00      1.00      1.00         4
          37       0.67      0.44      0.53         9
          38       0.25      0.11      0.15         9
          39       1.00      0.75      0.86         4
          40       0.56      1.00      0.71         5
          41       0.73      0.89      0.80         9
          42       0.33      0.14      0.20         7
          43       0.83      0.56      0.67         9
          44       1.00      1.00      1.00         4
          45       1.00      0.50      0.67         4
          46       1.00      0.00      0.00         4
          47       1.00      1.00      1.00         5
          48       0.55      0.67      0.60         9
          49       0.75      0.75      0.75         4
          50       1.00      0.00      0.00         4
          51       0.00      0.00      0.00         4
          52       1.00      1.00      1.00         9
          53       0.50      0.25      0.33         4
          54       1.00      0.40      0.57         5
          55       1.00      0.67      0.80         9
          56       1.00      0.89      0.94         9
          57       0.64      1.00      0.78         9
          58       1.00      0.22      0.36         9
          59       1.00      1.00      1.00         4
          60       0.50      0.25      0.33         4
          61       1.00      0.00      0.00         4
          62       1.00      0.50      0.67         4
          63       1.00      0.60      0.75         5
          64       0.25      0.25      0.25         4
          65       1.00      0.00      0.00         4
          66       1.00      0.80      0.89         5
          67       1.00      0.80      0.89         5
          68       1.00      0.00      0.00         4
          69       0.75      0.75      0.75         4
          70       0.33      0.25      0.29         4
          71       0.64      1.00      0.78         9
          72       0.88      0.78      0.82         9
          73       1.00      0.00      0.00         4
          74       0.57      0.80      0.67         5
          75       0.67      0.50      0.57         4
          76       1.00      1.00      1.00         9
          77       0.00      0.00      0.00         4
          78       0.80      0.80      0.80         5
          79       0.75      0.75      0.75         4
          80       1.00      0.25      0.40         4
          81       1.00      0.50      0.67         4
          82       1.00      1.00      1.00         4
          83       0.50      0.25      0.33         4
          84       1.00      0.80      0.89         5
          85       1.00      0.50      0.67         4
          86       0.00      0.00      0.00         4
          87       0.89      0.89      0.89         9
          88       0.75      1.00      0.86         9
          89       0.50      0.50      0.50         4
          90       1.00      0.80      0.89         5
          91       0.88      0.78      0.82         9
          92       1.00      0.80      0.89         5
          93       1.00      0.60      0.75         5
          94       0.75      0.75      0.75         4
          95       0.50      0.50      0.50         4
          96       0.67      1.00      0.80         4
          97       1.00      0.00      0.00         4
          98       1.00      0.60      0.75         5
          99       0.00      0.00      0.00         4
         100       1.00      0.75      0.86         4
         101       0.75      0.75      0.75         4
         102       1.00      0.44      0.62         9
         103       1.00      0.60      0.75         5
         104       1.00      0.89      0.94         9
         105       0.75      0.75      0.75         4
         106       0.83      0.62      0.71         8
         107       1.00      0.89      0.94         9
         108       1.00      0.75      0.86         4
         109       0.00      0.00      0.00         4
         110       0.60      0.75      0.67         4
         111       1.00      0.50      0.67         4
         112       0.75      0.75      0.75         4
         113       0.00      0.00      0.00         9
         114       1.00      0.00      0.00         4
         115       0.50      0.75      0.60         4
         116       0.38      0.75      0.50         4
         117       0.75      0.75      0.75         4
         118       1.00      1.00      1.00         4
         119       0.36      1.00      0.53         4
         120       0.50      0.25      0.33         4
         121       0.80      1.00      0.89         4
         122       0.25      0.40      0.31         5
         123       0.20      0.25      0.22         4
         124       0.75      0.75      0.75         4
         125       1.00      0.75      0.86         4
         126       0.60      0.75      0.67         4
         127       1.00      0.50      0.67         4
         128       1.00      1.00      1.00         4
         129       1.00      0.40      0.57         5
         130       1.00      0.75      0.86         4
         131       0.75      0.75      0.75         4
         132       0.86      0.67      0.75         9
         133       0.11      0.11      0.11         9
         134       1.00      1.00      1.00         4
         135       0.86      0.67      0.75         9
         136       0.67      0.40      0.50         5
         137       1.00      0.25      0.40         4
         138       0.50      0.75      0.60         4
         139       1.00      0.78      0.88         9
         140       1.00      0.89      0.94         9
         141       0.50      1.00      0.67         4
         142       0.38      0.75      0.50         4
         143       0.00      0.00      0.00         4
         144       0.67      0.80      0.73         5
         145       1.00      1.00      1.00         9
         146       1.00      0.56      0.71         9
         147       0.67      0.50      0.57         4
         148       1.00      0.75      0.86         4
         149       0.33      0.25      0.29         4
         150       1.00      0.50      0.67         4
         151       0.20      0.11      0.14         9
         152       0.75      0.67      0.71         9
         153       0.50      0.67      0.57         9
         154       0.80      1.00      0.89         4
         155       1.00      0.60      0.75         5
         156       0.36      1.00      0.53         4
         157       0.67      0.44      0.53         9
         158       0.00      0.00      0.00         4
         159       0.80      0.89      0.84         9
         160       0.57      1.00      0.73         4
         161       0.60      0.75      0.67         4
         162       0.83      1.00      0.91         5
         163       0.83      1.00      0.91         5
         164       0.83      1.00      0.91         5
         165       1.00      1.00      1.00         4
         166       1.00      0.00      0.00         4
         167       0.75      1.00      0.86         9
         168       1.00      0.75      0.86         4
         169       0.71      1.00      0.83         5
         170       1.00      1.00      1.00         5
         171       0.60      0.75      0.67         4
         172       0.62      0.89      0.73         9
         173       0.50      0.50      0.50         4
         174       0.00      0.00      0.00         4
         175       0.33      0.25      0.29         4
         176       0.00      0.00      0.00         4
         177       0.80      1.00      0.89         4
         178       0.33      0.25      0.29         4
         179       0.88      0.78      0.82         9
         180       0.00      0.00      0.00         4
         181       0.88      0.78      0.82         9
         182       0.80      1.00      0.89         4
         183       0.62      1.00      0.77         5
         184       0.67      1.00      0.80         4
         185       0.80      1.00      0.89         4
         186       1.00      0.80      0.89         5
         187       0.54      0.78      0.64         9
         188       0.00      0.00      0.00         4
         189       0.00      0.00      0.00         4
         190       0.50      0.22      0.31         9
         191       0.80      1.00      0.89         4
         192       0.70      0.78      0.74         9
         193       1.00      0.25      0.40         4
         194       1.00      0.89      0.94         9
         195       1.00      0.33      0.50         9
         196       0.00      0.00      0.00         4
         197       1.00      1.00      1.00         5
         198       1.00      0.00      0.00         4
         199       1.00      0.50      0.67         4
         200       0.25      0.25      0.25         4
         201       0.50      0.50      0.50         4
         202       1.00      0.60      0.75         5
         203       0.60      0.60      0.60         5
         204       0.80      0.80      0.80         5
         205       0.71      0.56      0.63         9
         206       1.00      0.22      0.36         9
         207       0.00      0.00      0.00         4
         208       0.80      0.80      0.80         5
         209       1.00      0.50      0.67         4
         210       0.67      1.00      0.80         4
         211       0.32      0.89      0.47         9
         212       0.80      0.80      0.80         5
         213       0.75      0.60      0.67         5
         214       0.75      0.75      0.75         4
         215       1.00      0.75      0.86         4
         216       0.83      1.00      0.91         5
         217       0.60      0.75      0.67         4
         218       1.00      1.00      1.00         4
         219       1.00      0.50      0.67         4
         220       1.00      0.75      0.86         4
         221       0.50      0.22      0.31         9
         222       0.30      0.75      0.43         4
         223       0.43      0.75      0.55         4
         224       0.60      0.75      0.67         4
         225       1.00      1.00      1.00         4
         226       0.75      0.75      0.75         4
         227       0.50      0.44      0.47         9
         228       0.50      0.50      0.50         4
         229       1.00      1.00      1.00         4
         230       0.60      0.33      0.43         9
         231       0.50      0.25      0.33         4
         232       0.80      1.00      0.89         4
         233       0.83      0.56      0.67         9
         234       1.00      0.89      0.94         9
         235       0.70      0.78      0.74         9
         236       0.60      0.75      0.67         4
         237       0.00      0.00      0.00         4
         238       1.00      0.75      0.86         4
         239       0.70      0.78      0.74         9
         240       0.44      0.80      0.57         5
         241       0.67      0.80      0.73         5
         242       0.00      0.00      0.00         4
         243       0.00      0.00      0.00         4
         244       0.78      0.78      0.78         9
         245       0.80      0.80      0.80         5
         246       0.33      0.25      0.29         4
         247       0.47      1.00      0.64         9
         248       1.00      1.00      1.00         4
         249       0.45      1.00      0.62         5
         250       0.62      0.89      0.73         9
         251       0.83      0.62      0.71         8
         252       0.80      1.00      0.89         4
         253       0.60      0.75      0.67         4
         254       0.53      0.89      0.67         9
         255       0.60      0.75      0.67         4
         256       1.00      0.50      0.67         4
         257       0.88      0.78      0.82         9
         258       1.00      0.00      0.00         4
         259       0.50      0.78      0.61         9
         260       1.00      0.20      0.33         5
         261       0.45      0.56      0.50         9
         262       1.00      0.00      0.00         5
         263       0.75      0.75      0.75         4
         264       1.00      0.80      0.89         5
         265       0.00      0.00      0.00         4
         266       1.00      1.00      1.00         5
         267       1.00      1.00      1.00         4
         268       0.80      0.80      0.80         5
         269       0.75      0.75      0.75         4
         270       0.20      0.20      0.20         5
         271       0.33      0.20      0.25         5
         272       1.00      0.50      0.67         4
         273       1.00      0.25      0.40         4
         274       0.71      1.00      0.83         5
         275       1.00      0.50      0.67         4
         276       0.43      0.75      0.55         4
         277       0.67      0.57      0.62         7
         278       0.56      1.00      0.71         5
         279       1.00      0.75      0.86         4
         280       0.50      0.50      0.50         4
         281       0.53      0.89      0.67         9
         282       0.67      0.50      0.57         4
         283       1.00      1.00      1.00         4
         284       1.00      0.75      0.86         4
         285       0.00      0.00      0.00         4
         286       0.67      1.00      0.80         4
         287       0.43      0.75      0.55         4
         288       0.14      0.25      0.18         4
         289       0.75      0.67      0.71         9
         290       0.50      0.25      0.33         4
         291       0.82      1.00      0.90         9
         292       1.00      0.75      0.86         4
         293       0.75      0.75      0.75         4
         294       1.00      1.00      1.00         4
         295       0.70      0.78      0.74         9
         296       0.50      0.50      0.50         4
         297       0.33      0.25      0.29         4
         298       1.00      0.60      0.75         5
         299       0.67      0.80      0.73         5
         300       0.67      1.00      0.80         4
         301       1.00      0.25      0.40         4
         302       0.00      0.00      0.00         4
         303       0.00      0.00      0.00         4
         304       1.00      0.75      0.86         4
         305       1.00      0.75      0.86         4
         306       0.75      0.67      0.71         9
         307       0.50      0.75      0.60         4
         308       0.55      0.67      0.60         9
         309       1.00      0.50      0.67         4
         310       0.50      0.75      0.60         4
         311       0.36      1.00      0.53         4
         312       1.00      0.50      0.67         4
         313       1.00      0.00      0.00         4
         314       1.00      1.00      1.00         4
         315       0.50      0.50      0.50         4
         316       0.67      0.89      0.76         9
         317       1.00      0.75      0.86         4
         318       0.82      1.00      0.90         9
         319       1.00      1.00      1.00         4
         320       0.43      0.75      0.55         4
         321       1.00      0.80      0.89         5
         322       1.00      1.00      1.00         4
         323       1.00      0.00      0.00         4
         324       0.67      1.00      0.80         4
         325       0.82      1.00      0.90         9
         326       0.67      0.40      0.50         5
         327       1.00      1.00      1.00         4
         328       0.75      0.75      0.75         4
         329       0.50      0.25      0.33         4
         330       0.00      0.00      0.00         4
         331       0.00      0.00      0.00         4
         332       1.00      0.80      0.89         5
         333       0.89      0.89      0.89         9
         334       0.57      0.44      0.50         9
         335       1.00      0.00      0.00         4
         336       0.33      0.25      0.29         4
         337       1.00      0.50      0.67         4
         338       0.75      0.60      0.67         5
         339       0.00      0.00      0.00         9
         340       1.00      0.78      0.88         9
         341       0.80      1.00      0.89         4
         342       1.00      1.00      1.00         4
         343       1.00      0.00      0.00         4
         344       0.67      0.50      0.57         4
         345       1.00      0.80      0.89         5
         346       0.25      0.50      0.33         4
         347       1.00      0.25      0.40         4
         348       1.00      0.40      0.57         5
         349       0.00      0.00      0.00         4
         350       1.00      1.00      1.00         4
         351       0.25      0.25      0.25         4
         352       0.67      1.00      0.80         4
         353       1.00      0.80      0.89         5
         354       1.00      0.75      0.86         4
         355       0.80      0.89      0.84         9
         356       0.00      0.00      0.00         5
         357       1.00      0.00      0.00         4
         358       1.00      1.00      1.00         4
         359       0.57      0.44      0.50         9
         360       1.00      0.89      0.94         9
         361       1.00      0.67      0.80         9
         362       0.73      0.89      0.80         9
         363       1.00      0.00      0.00         4
         364       0.86      0.67      0.75         9
         365       1.00      0.00      0.00         4
         366       0.67      1.00      0.80         4
         367       0.10      0.25      0.14         4
         368       0.69      1.00      0.82         9
         369       1.00      0.44      0.62         9
         370       0.60      0.75      0.67         4
         371       0.43      0.60      0.50         5
         372       0.33      0.25      0.29         4
         373       1.00      0.89      0.94         9
         374       0.90      1.00      0.95         9
         375       1.00      0.50      0.67         4
         376       0.75      0.33      0.46         9
         377       0.90      1.00      0.95         9
         378       0.89      0.89      0.89         9
         379       1.00      0.80      0.89         5
         380       1.00      0.00      0.00         4
         381       0.80      1.00      0.89         4
         382       0.50      0.50      0.50         4
         383       0.89      0.89      0.89         9
         384       0.60      0.75      0.67         4
         385       0.40      0.50      0.44         4
         386       0.00      0.00      0.00         4
         387       0.75      0.75      0.75         4
         388       0.80      0.89      0.84         9
         389       0.00      0.00      0.00         4
         390       1.00      0.75      0.86         4
         391       0.67      0.50      0.57         4
         392       0.62      0.56      0.59         9
         393       1.00      0.75      0.86         4
         394       0.00      0.00      0.00         9
         395       0.06      0.25      0.10         4
         396       0.83      1.00      0.91         5
         397       1.00      0.00      0.00         4
         398       0.40      0.22      0.29         9
         399       0.00      0.00      0.00         4
         400       0.80      0.80      0.80         5
         401       0.75      0.75      0.75         4
         402       0.89      0.89      0.89         9
         403       1.00      1.00      1.00         4
         404       0.50      1.00      0.67         5
         405       0.53      0.89      0.67         9
         406       0.30      0.60      0.40         5
         407       0.50      0.50      0.50         4
         408       0.00      0.00      0.00         4
         409       1.00      0.75      0.86         4
         410       0.00      0.00      0.00         5
         411       0.00      0.00      0.00         4
         412       0.80      1.00      0.89         4
         413       0.71      0.56      0.63         9
         414       0.00      0.00      0.00         4
         415       0.60      0.67      0.63         9
         416       0.57      0.44      0.50         9
         417       0.00      0.00      0.00         4
         418       0.50      0.50      0.50         4
         419       0.33      0.50      0.40         4
         420       0.50      0.40      0.44         5
         421       1.00      0.25      0.40         4
         422       0.40      0.40      0.40         5
         423       0.13      0.44      0.20         9
         424       0.75      0.75      0.75         4
         425       0.25      0.22      0.24         9
         426       0.50      0.75      0.60         4
         427       0.50      0.75      0.60         4
         428       0.00      0.00      0.00         4
         429       0.83      0.56      0.67         9
         430       0.80      0.89      0.84         9
         431       1.00      0.89      0.94         9
         432       1.00      0.50      0.67         4
         433       0.78      0.78      0.78         9
         434       1.00      0.75      0.86         4
         435       0.80      0.44      0.57         9
         436       0.80      1.00      0.89         4
         437       0.00      0.00      0.00         4
         438       0.73      0.89      0.80         9
         439       1.00      1.00      1.00         4
         440       0.60      0.75      0.67         4
         441       1.00      1.00      1.00         9
         442       1.00      0.50      0.67         4
         443       0.53      1.00      0.69         9
         444       1.00      0.00      0.00         4
         445       1.00      0.40      0.57         5
         446       0.82      1.00      0.90         9
         447       0.80      0.89      0.84         9
         448       0.67      1.00      0.80         4
         449       1.00      0.60      0.75         5
         450       0.89      0.89      0.89         9
         451       0.69      1.00      0.82         9
         452       0.67      0.50      0.57         4
         453       0.50      0.25      0.33         4
         454       1.00      0.75      0.86         4
         455       1.00      0.00      0.00         4
         456       0.75      0.60      0.67         5
         457       1.00      0.00      0.00         4
         458       1.00      0.75      0.86         4
         459       0.80      1.00      0.89         4
         460       1.00      0.00      0.00         4
         461       0.07      0.25      0.11         4
         462       0.50      0.50      0.50         4
         463       0.90      1.00      0.95         9
         464       0.08      0.11      0.10         9
         465       0.43      0.60      0.50         5
         466       0.00      0.00      0.00         4
         467       0.75      0.75      0.75         4
         468       0.69      1.00      0.82         9
         469       0.57      0.44      0.50         9
         470       0.75      0.75      0.75         4
         471       0.67      1.00      0.80         4
         472       0.64      1.00      0.78         9
         473       1.00      1.00      1.00         9
         474       0.50      0.75      0.60         4
         475       0.25      0.25      0.25         4
         476       1.00      0.25      0.40         4
         477       1.00      1.00      1.00         4
         478       0.67      0.67      0.67         9
         479       0.83      0.56      0.67         9
         480       1.00      1.00      1.00         4
         481       1.00      0.60      0.75         5
         482       0.25      0.22      0.24         9
         483       0.75      0.75      0.75         4
         484       0.80      1.00      0.89         4
         485       1.00      0.75      0.86         4
         486       0.67      0.89      0.76         9
         487       0.67      0.40      0.50         5
         488       1.00      0.25      0.40         4
         489       1.00      1.00      1.00         4
         490       0.50      0.60      0.55         5
         491       0.00      0.00      0.00         4
         492       1.00      0.00      0.00         4
         493       1.00      0.78      0.88         9
         494       0.00      0.00      0.00         4
         495       0.60      0.75      0.67         4
         496       0.00      0.00      0.00         4
         497       0.75      0.75      0.75         4
         498       0.80      1.00      0.89         4
         499       0.71      0.56      0.63         9
         500       0.00      0.00      0.00         4
         501       1.00      0.44      0.62         9
         502       0.89      0.89      0.89         9
         503       0.88      0.78      0.82         9
         504       1.00      1.00      1.00         4
         505       1.00      0.60      0.75         5
         506       0.20      0.25      0.22         4
         507       0.67      1.00      0.80         4
         508       0.80      0.80      0.80         5
         509       1.00      0.00      0.00         4
         510       1.00      0.25      0.40         4
         511       0.75      0.60      0.67         5
         512       0.60      0.75      0.67         4
         513       0.67      0.44      0.53         9
         514       1.00      0.75      0.86         4
         515       0.50      0.50      0.50         4
         516       0.80      0.80      0.80         5
         517       0.50      0.40      0.44         5
         518       0.67      0.50      0.57         4
         519       0.67      0.67      0.67         9
         520       0.67      0.50      0.57         4
         521       1.00      0.00      0.00         4
         522       1.00      0.75      0.86         4
         523       0.80      0.89      0.84         9
         524       0.00      0.00      0.00         4
         525       0.57      0.44      0.50         9
         526       1.00      0.75      0.86         4
         527       0.00      0.00      0.00         4
         528       0.33      0.60      0.43         5
         529       0.90      1.00      0.95         9
         530       0.88      0.78      0.82         9
         531       0.67      0.50      0.57         4
         532       0.53      0.89      0.67         9
         533       0.67      0.80      0.73         5
         534       0.67      1.00      0.80         4
         535       0.90      1.00      0.95         9
         536       0.50      0.40      0.44         5
         537       1.00      0.75      0.86         4
         538       0.67      0.80      0.73         5
         539       0.64      1.00      0.78         9
         540       0.80      1.00      0.89         4
         541       0.80      0.80      0.80         5
         542       0.83      1.00      0.91         5
         543       1.00      1.00      1.00         4
         544       0.88      0.78      0.82         9
         545       0.55      0.67      0.60         9
         546       0.50      0.50      0.50         4
         547       1.00      0.75      0.86         4
         548       1.00      1.00      1.00         5
         549       0.17      1.00      0.29         4
         550       0.80      1.00      0.89         4
         551       0.00      0.00      0.00         4
         552       0.50      0.50      0.50         4
         553       0.33      0.75      0.46         4
         554       0.50      0.25      0.33         4
         555       0.75      0.60      0.67         5
         556       1.00      1.00      1.00         4
         557       0.20      0.25      0.22         4
         558       0.75      0.75      0.75         4
         559       0.50      0.44      0.47         9
         560       0.50      0.25      0.33         4
         561       0.00      0.00      0.00         4
         562       0.22      0.50      0.31         4
         563       1.00      0.75      0.86         4
         564       0.44      1.00      0.62         4
         565       1.00      1.00      1.00         4
         566       0.60      0.75      0.67         4
         567       0.00      0.00      0.00         4
         568       0.50      0.20      0.29         5
         569       1.00      0.75      0.86         4
         570       0.75      0.75      0.75         4
         571       1.00      0.25      0.40         4
         572       0.00      0.00      0.00         4
         573       1.00      0.00      0.00         4
         574       0.00      0.00      0.00         5
         575       0.69      1.00      0.82         9
         576       1.00      1.00      1.00         4
         577       0.50      0.56      0.53         9
         578       0.62      0.56      0.59         9
         579       0.57      1.00      0.73         4
         580       0.75      0.75      0.75         4
         581       1.00      1.00      1.00         5
         582       0.00      0.00      0.00         9
         583       1.00      1.00      1.00         9
         584       1.00      0.00      0.00         5
         585       0.86      0.67      0.75         9
         586       0.40      0.80      0.53         5
         587       1.00      0.60      0.75         5
         588       0.75      0.75      0.75         4
         589       0.31      0.44      0.36         9
         590       1.00      0.75      0.86         4
         591       0.12      0.25      0.17         4
         592       0.67      1.00      0.80         4
         593       1.00      0.50      0.67         4
         594       0.80      0.80      0.80         5
         595       1.00      1.00      1.00         5
         596       1.00      0.78      0.88         9
         597       0.83      1.00      0.91         5
         598       0.62      0.56      0.59         9
         599       1.00      0.80      0.89         5
         600       0.50      0.25      0.33         4
         601       1.00      1.00      1.00         4
         602       1.00      0.00      0.00         4
         603       1.00      0.75      0.86         4
         604       0.82      1.00      0.90         9
         605       0.33      0.25      0.29         4
         606       0.00      0.00      0.00         4
         607       0.00      0.00      0.00         4
         608       0.71      1.00      0.83         5
         609       0.57      0.80      0.67         5
         610       0.90      1.00      0.95         9
         611       1.00      0.75      0.86         4
         612       0.67      0.80      0.73         5
         613       0.62      1.00      0.77         5
         614       0.80      1.00      0.89         4
         615       0.90      1.00      0.95         9
         616       0.00      0.00      0.00         4
         617       1.00      1.00      1.00         5
         618       1.00      0.50      0.67         4
         619       0.00      0.00      0.00         4
         620       0.75      0.75      0.75         4
         621       1.00      0.00      0.00         4
         622       0.67      0.80      0.73         5
         623       1.00      1.00      1.00         4
         624       0.10      0.25      0.14         4
         625       1.00      0.00      0.00         4
         626       0.20      0.11      0.14         9
         627       0.33      0.60      0.43         5
         628       0.08      0.11      0.10         9
         629       0.67      0.89      0.76         9
         630       0.80      1.00      0.89         4
         631       0.67      0.40      0.50         5
         632       0.82      1.00      0.90         9
         633       0.00      0.00      0.00         4
         634       0.67      0.40      0.50         5
         635       0.60      0.60      0.60         5
         636       0.44      0.89      0.59         9
         637       0.50      1.00      0.67         9
         638       0.00      0.00      0.00         4
         639       0.80      1.00      0.89         4
         640       0.67      1.00      0.80         4
         641       0.90      1.00      0.95         9
         642       1.00      0.50      0.67         4
         643       0.12      0.22      0.16         9
         644       1.00      0.50      0.67         4
         645       0.50      0.75      0.60         4
         646       0.50      0.75      0.60         4
         647       0.00      0.00      0.00         4
         648       0.20      0.75      0.32         4
         649       0.80      0.80      0.80         5
         650       1.00      0.50      0.67         4
         651       0.75      0.33      0.46         9
         652       0.09      0.25      0.13         4
         653       0.54      0.78      0.64         9
         654       0.60      0.75      0.67         4
         655       0.60      0.67      0.63         9
         656       0.50      0.50      0.50         4
         657       0.57      1.00      0.73         4
         658       0.42      0.56      0.48         9
         659       0.33      0.25      0.29         4
         660       0.00      0.00      0.00         4
         661       1.00      0.67      0.80         9
         662       0.75      0.75      0.75         4
         663       0.90      1.00      0.95         9
         664       1.00      0.75      0.86         4
         665       1.00      0.75      0.86         4
         666       0.56      1.00      0.72         9
         667       0.36      0.56      0.43         9
         668       1.00      0.00      0.00         4
         669       0.60      0.75      0.67         4
         670       0.43      0.75      0.55         4
         671       0.60      0.75      0.67         4
         672       0.07      0.25      0.11         4
         673       0.33      0.75      0.46         4
         674       0.00      0.00      0.00         4
         675       0.67      0.67      0.67         9
         676       0.50      1.00      0.67         4
         677       0.00      0.00      0.00         4
         678       0.60      0.75      0.67         4
         679       0.78      0.78      0.78         9
         680       0.00      0.00      0.00         4
         681       0.20      0.25      0.22         4
         682       0.55      0.67      0.60         9
         683       0.00      0.00      0.00         4
         684       0.80      1.00      0.89         4
         685       0.50      0.78      0.61         9
         686       0.43      0.75      0.55         4
         687       0.50      0.60      0.55         5
         688       0.08      0.25      0.12         4
         689       0.00      0.00      0.00         9
         690       0.46      0.67      0.55         9
         691       0.80      1.00      0.89         4
         692       0.69      1.00      0.82         9
         693       0.75      0.75      0.75         4
         694       1.00      0.00      0.00         4
         695       0.75      0.75      0.75         4
         696       1.00      1.00      1.00         5
         697       0.83      1.00      0.91         5
         698       0.06      0.25      0.09         4
         699       0.60      0.60      0.60         5
         700       0.80      0.80      0.80         5
         701       0.60      0.75      0.67         4
         702       1.00      0.50      0.67         4
         703       0.57      1.00      0.73         4
         704       1.00      0.75      0.86         4
         705       0.50      0.80      0.62         5
         706       0.50      1.00      0.67         4
         707       0.67      1.00      0.80         4
         708       0.50      0.25      0.33         4
         709       1.00      0.40      0.57         5
         710       0.00      0.00      0.00         4
         711       1.00      1.00      1.00         9
         712       0.60      0.67      0.63         9
         713       1.00      1.00      1.00         4
         714       0.22      0.22      0.22         9
         715       0.38      0.75      0.50         4
         716       0.00      0.00      0.00         9
         717       0.15      0.50      0.24         4
         718       0.50      0.50      0.50         4
         719       0.80      0.80      0.80         5
         720       1.00      0.00      0.00         4
         721       0.50      0.89      0.64         9
         722       1.00      0.67      0.80         9
         723       0.00      0.00      0.00         4
         724       0.50      0.20      0.29         5
         725       0.00      0.00      0.00         4
         726       0.71      1.00      0.83         5
         727       0.00      0.00      0.00         5
         728       0.80      1.00      0.89         4
         729       1.00      0.25      0.40         4
         730       0.75      0.67      0.71         9
         731       0.75      0.75      0.75         4
         732       0.67      0.50      0.57         4
         733       1.00      0.75      0.86         4
         734       0.75      1.00      0.86         9
         735       0.25      0.11      0.15         9
         736       0.78      0.78      0.78         9
         737       1.00      0.25      0.40         4
         738       0.25      0.25      0.25         4
         739       0.00      0.00      0.00         9
         740       0.42      0.56      0.48         9
         741       1.00      0.40      0.57         5
         742       0.10      0.25      0.14         4
         743       1.00      1.00      1.00         4
         744       0.00      0.00      0.00         4
         745       1.00      0.25      0.40         4
         746       0.00      0.00      0.00         4
         747       0.00      0.00      0.00         4
         748       1.00      0.75      0.86         4
         749       0.80      1.00      0.89         4
         750       1.00      0.00      0.00         4
         751       0.29      0.50      0.36         4
         752       1.00      0.00      0.00         4
         753       0.00      0.00      0.00         6
         754       0.33      0.75      0.46         4
         755       0.44      1.00      0.62         4
         756       0.50      0.25      0.33         4
         757       0.43      0.67      0.52         9
         758       0.14      0.25      0.18         4
         759       1.00      0.67      0.80         9
         760       0.33      0.50      0.40         4
         761       1.00      0.50      0.67         4
         762       1.00      1.00      1.00         5
         763       0.83      1.00      0.91         5
         764       0.89      0.89      0.89         9
         765       0.67      0.50      0.57         4
         766       0.90      1.00      0.95         9
         767       0.80      0.80      0.80         5
         768       0.89      0.89      0.89         9
         769       0.75      0.60      0.67         5
         770       0.54      0.78      0.64         9
         771       0.80      0.80      0.80         5
         772       0.20      0.25      0.22         4
         773       0.33      0.40      0.36         5
         774       0.00      0.00      0.00         4
         775       0.62      0.89      0.73         9
         776       0.80      0.44      0.57         9
         777       0.67      0.50      0.57         4
         778       0.60      0.67      0.63         9
         779       0.00      0.00      0.00         4
         780       1.00      0.00      0.00         4
         781       0.60      0.60      0.60         5
         782       0.00      0.00      0.00         4
         783       1.00      0.56      0.71         9
         784       0.90      1.00      0.95         9
         785       0.43      0.60      0.50         5
         786       0.42      0.56      0.48         9
         787       1.00      0.50      0.67         4
         788       0.67      1.00      0.80         4
         789       0.33      0.25      0.29         4
         790       0.00      0.00      0.00         4
         791       1.00      0.80      0.89         5
         792       0.75      0.75      0.75         4
         793       1.00      0.80      0.89         5
         794       0.67      0.40      0.50         5
         795       1.00      1.00      1.00         4
         796       0.71      1.00      0.83         5
         797       0.00      0.00      0.00         4
         798       0.80      1.00      0.89         4
         799       1.00      1.00      1.00         4
         800       0.40      0.44      0.42         9
         801       0.67      0.50      0.57         4
         802       0.64      1.00      0.78         9
         803       1.00      0.75      0.86         4
         804       0.00      0.00      0.00         4
         805       1.00      0.75      0.86         4
         806       0.00      0.00      0.00         4
         807       0.89      0.89      0.89         9
         808       1.00      0.67      0.80         9
         809       0.83      0.56      0.67         9
         810       1.00      0.00      0.00         4
         811       0.33      0.25      0.29         4
         812       0.80      1.00      0.89         4
         813       1.00      0.00      0.00         9
         814       1.00      0.75      0.86         4
         815       0.67      0.50      0.57         4
         816       0.00      0.00      0.00         4
         817       0.00      0.00      0.00         4
         818       0.60      1.00      0.75         9
         819       0.50      0.40      0.44         5
         820       0.75      0.75      0.75         4
         821       0.60      0.75      0.67         4
         822       0.14      0.20      0.17         5
         823       0.80      1.00      0.89         4
         824       0.50      0.25      0.33         4
         825       0.50      0.80      0.62         5
         826       1.00      0.00      0.00         4
         827       1.00      0.00      0.00         4
         828       0.00      0.00      0.00         4
         829       0.80      1.00      0.89         4
         830       0.60      0.60      0.60         5
         831       0.75      0.75      0.75         4
         832       0.60      0.75      0.67         4
         833       0.71      1.00      0.83         5
         834       0.08      0.11      0.10         9
         835       0.75      0.86      0.80         7
         836       1.00      0.80      0.89         5
         837       1.00      0.75      0.86         4
         838       0.83      0.56      0.67         9
         839       0.50      0.75      0.60         4
         840       1.00      1.00      1.00         4
         841       1.00      1.00      1.00         4
         842       0.67      0.40      0.50         5
         843       1.00      0.00      0.00         4
         844       0.50      1.00      0.67         4
         845       0.83      1.00      0.91         5
         846       0.67      0.50      0.57         4
         847       1.00      0.50      0.67         4
         848       0.50      0.50      0.50         4
         849       1.00      0.75      0.86         4
         850       0.71      1.00      0.83         5
         851       0.64      1.00      0.78         9
         852       1.00      0.75      0.86         4
         853       1.00      1.00      1.00         4
         854       1.00      0.25      0.40         4
         855       0.00      0.00      0.00         4
         856       1.00      0.75      0.86         4
         857       1.00      0.75      0.86         4
         858       0.80      0.89      0.84         9
         859       0.60      0.33      0.43         9
         860       1.00      0.50      0.67         4
         861       1.00      0.50      0.67         4
         862       0.15      0.22      0.18         9
         863       0.60      0.75      0.67         4
         864       0.67      0.50      0.57         4
         865       0.60      0.67      0.63         9
         866       0.75      0.75      0.75         4
         867       0.75      0.75      0.75         4
         868       1.00      0.75      0.86         4
         869       1.00      0.89      0.94         9
         870       0.44      1.00      0.62         4
         871       0.55      0.75      0.63         8
         872       0.33      0.20      0.25         5
         873       0.71      1.00      0.83         5
         874       0.33      0.25      0.29         4
         875       0.44      0.78      0.56         9
         876       0.33      0.89      0.48         9
         877       1.00      0.80      0.89         5
         878       1.00      1.00      1.00         4
         879       0.82      1.00      0.90         9
         880       0.50      0.75      0.60         4
         881       0.04      0.11      0.06         9
         882       1.00      0.50      0.67         4
         883       0.67      1.00      0.80         4
         884       1.00      0.75      0.86         4
         885       0.75      0.75      0.75         4
         886       0.00      0.00      0.00         4
         887       0.50      0.50      0.50         4
         888       1.00      0.75      0.86         4
         889       0.60      0.75      0.67         4
         890       0.14      0.33      0.19         9
         891       0.50      0.75      0.60         4
         892       0.44      1.00      0.62         4
         893       0.00      0.00      0.00         5

    accuracy                           0.61      4917
   macro avg       0.67      0.59      0.57      4917
weighted avg       0.67      0.61      0.60      4917

