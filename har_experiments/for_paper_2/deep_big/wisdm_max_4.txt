CLASS_ORDER: [568, 635, 38, 549, 890, 835, 66, 797, 175, 802, 671, 683, 779, 159, 8, 510, 252, 221, 813, 250, 335, 7, 536, 60, 630, 795, 49, 94, 573, 106, 457, 21, 889, 249, 747, 13, 585, 717, 196, 464, 185, 591, 563, 791, 657, 627, 888, 691, 388, 182, 818, 842, 729, 611, 2, 359, 34, 551, 424, 218, 733, 800, 174, 43, 372, 296, 411, 699, 310, 367, 819, 309, 628, 731, 276, 82, 615, 298, 88, 450, 33, 742, 337, 71, 803, 3, 419, 121, 769, 550, 91, 578, 446, 89, 644, 726, 317, 366, 626, 234, 409, 137, 5, 514, 330, 771, 187, 521, 241, 444, 44, 17, 849, 546, 219, 680, 869, 641, 607, 758, 300, 40, 209, 614, 201, 240, 103, 647, 727, 855, 764, 382, 484, 173, 385, 131, 152, 261, 665, 397, 722, 698, 14, 312, 476, 316, 483, 526, 515, 673, 72, 204, 648, 165, 439, 718, 531, 387, 806, 31, 679, 636, 190, 629, 114, 827, 467, 604, 850, 143, 200, 844, 453, 23, 798, 864, 634, 78, 625, 290, 720, 593, 191, 537, 192, 858, 283, 490, 465, 343, 497, 154, 259, 368, 646, 213, 97, 887, 608, 613, 640, 235, 865, 246, 661, 50, 576, 207, 30, 405, 734, 820, 286, 853, 378, 84, 410, 456, 793, 552, 564, 430, 587, 612, 87, 436, 617, 857, 529, 195, 713, 225, 113, 32, 651, 663, 406, 876, 460, 690, 295, 144, 489, 845, 522, 728, 725, 817, 603, 743, 86, 247, 407, 528, 307, 470, 36, 253, 694, 708, 668, 274, 63, 332, 756, 474, 616, 494, 507, 80, 341, 588, 19, 534, 109, 393, 101, 812, 392, 874, 594, 524, 821, 618, 52, 828, 715, 222, 140, 395, 277, 168, 371, 749, 773, 716, 320, 449, 15, 62, 792, 571, 499, 93, 700, 189, 155, 151, 26, 558, 459, 125, 184, 415, 487, 752, 267, 327, 11, 893, 394, 511, 138, 867, 248, 417, 540, 369, 92, 61, 211, 139, 754, 413, 485, 68, 354, 596, 245, 255, 861, 172, 719, 724, 877, 495, 486, 478, 741, 493, 146, 482, 279, 830, 319, 847, 767, 169, 548, 67, 54, 142, 777, 621, 244, 884, 816, 166, 95, 569, 414, 475, 785, 532, 263, 57, 781, 400, 670, 766, 297, 687, 705, 656, 462, 160, 805, 620, 301, 16, 163, 262, 567, 390, 562, 454, 408, 365, 396, 654, 438, 505, 29, 572, 619, 321, 560, 746, 314, 437, 391, 119, 233, 841, 0, 693, 269, 150, 440, 631, 750, 681, 435, 289, 523, 340, 838, 565, 501, 356, 350, 326, 760, 315, 216, 775, 264, 547, 345, 236, 239, 153, 748, 35, 555, 291, 517, 609, 512, 744, 789, 642, 348, 530, 886, 401, 721, 12, 778, 70, 863, 740, 39, 73, 799, 180, 434, 883, 170, 770, 46, 351, 759, 231, 242, 871, 590, 451, 753, 882, 790, 85, 386, 839, 275, 675, 862, 768, 344, 879, 176, 161, 59, 10, 124, 304, 637, 328, 110, 846, 375, 633, 518, 584, 45, 834, 426, 815, 597, 373, 527, 251, 349, 763, 282, 822, 194, 574, 254, 308, 423, 22, 1, 193, 362, 606, 293, 730, 217, 674, 287, 632, 342, 538, 20, 682, 624, 561, 25, 299, 48, 706, 652, 355, 468, 660, 284, 206, 658, 136, 796, 305, 653, 24, 492, 649, 141, 272, 707, 422, 498, 602, 428, 491, 186, 238, 650, 714, 177, 108, 421, 376, 229, 81, 448, 598, 331, 271, 688, 377, 288, 516, 755, 128, 120, 751, 553, 333, 324, 156, 100, 881, 875, 677, 98, 383, 381, 739, 64, 294, 115, 543, 676, 662, 171, 117, 358, 504, 306, 412, 164, 582, 27, 509, 102, 586, 188, 542, 500, 723, 212, 757, 829, 51, 480, 761, 6, 313, 866, 445, 678, 135, 18, 28, 801, 96, 47, 56, 519, 458, 441, 579, 473, 804, 643, 353, 787, 227, 76, 203, 709, 145, 162, 357, 311, 111, 447, 278, 762, 339, 402, 481, 732, 452, 389, 692, 105, 336, 878, 503, 783, 416, 782, 559, 466, 873, 122, 610, 346, 266, 843, 669, 848, 148, 710, 638, 132, 600, 130, 370, 825, 599, 836, 689, 471, 104, 772, 325, 570, 814, 281, 318, 539, 461, 363, 508, 695, 179, 885, 557, 323, 210, 566, 592, 58, 852, 69, 496, 575, 554, 455, 433, 431, 260, 833, 79, 581, 704, 556, 230, 215, 374, 118, 429, 856, 840, 384, 228, 220, 199, 256, 133, 208, 469, 183, 860, 851, 432, 776, 541, 577, 418, 4, 639, 379, 535, 712, 9, 322, 112, 198, 223, 702, 158, 202, 237, 808, 74, 352, 664, 765, 285, 826, 780, 197, 55, 443, 205, 116, 685, 645, 167, 745, 329, 870, 810, 854, 364, 672, 601, 736, 181, 65, 666, 737, 347, 463, 147, 302, 273, 303, 807, 738, 427, 837, 360, 892, 41, 520, 622, 224, 824, 831, 404, 334, 533, 823, 107, 53, 711, 786, 134, 157, 880, 83, 226, 232, 268, 258, 442, 129, 545, 655, 544, 701, 868, 99, 90, 380, 623, 214, 257, 684, 891, 149, 696, 513, 859, 589, 506, 292, 265, 583, 477, 280, 243, 488, 127, 472, 686, 788, 403, 872, 502, 605, 425, 75, 37, 703, 659, 123, 697, 595, 580, 420, 832, 361, 126, 399, 809, 735, 479, 667, 784, 794, 270, 774, 338, 178, 42, 525, 398, 77, 811]
class_group: [(568, 635, 38, 549, 890, 835, 66, 797, 175, 802, 671, 683, 779, 159, 8, 510, 252, 221, 813, 250, 335, 7, 536, 60, 630, 795, 49, 94, 573, 106, 457, 21, 889, 249), (747, 13, 585, 717, 196, 464, 185, 591, 563, 791, 657, 627, 888, 691, 388, 182, 818, 842, 729, 611), (2, 359, 34, 551, 424, 218, 733, 800, 174, 43, 372, 296, 411, 699, 310, 367, 819, 309, 628, 731), (276, 82, 615, 298, 88, 450, 33, 742, 337, 71, 803, 3, 419, 121, 769, 550, 91, 578, 446, 89), (644, 726, 317, 366, 626, 234, 409, 137, 5, 514, 330, 771, 187, 521, 241, 444, 44, 17, 849, 546), (219, 680, 869, 641, 607, 758, 300, 40, 209, 614, 201, 240, 103, 647, 727, 855, 764, 382, 484, 173), (385, 131, 152, 261, 665, 397, 722, 698, 14, 312, 476, 316, 483, 526, 515, 673, 72, 204, 648, 165), (439, 718, 531, 387, 806, 31, 679, 636, 190, 629, 114, 827, 467, 604, 850, 143, 200, 844, 453, 23), (798, 864, 634, 78, 625, 290, 720, 593, 191, 537, 192, 858, 283, 490, 465, 343, 497, 154, 259, 368), (646, 213, 97, 887, 608, 613, 640, 235, 865, 246, 661, 50, 576, 207, 30, 405, 734, 820, 286, 853), (378, 84, 410, 456, 793, 552, 564, 430, 587, 612, 87, 436, 617, 857, 529, 195, 713, 225, 113, 32), (651, 663, 406, 876, 460, 690, 295, 144, 489, 845, 522, 728, 725, 817, 603, 743, 86, 247, 407, 528), (307, 470, 36, 253, 694, 708, 668, 274, 63, 332, 756, 474, 616, 494, 507, 80, 341, 588, 19, 534), (109, 393, 101, 812, 392, 874, 594, 524, 821, 618, 52, 828, 715, 222, 140, 395, 277, 168, 371, 749), (773, 716, 320, 449, 15, 62, 792, 571, 499, 93, 700, 189, 155, 151, 26, 558, 459, 125, 184, 415), (487, 752, 267, 327, 11, 893, 394, 511, 138, 867, 248, 417, 540, 369, 92, 61, 211, 139, 754, 413), (485, 68, 354, 596, 245, 255, 861, 172, 719, 724, 877, 495, 486, 478, 741, 493, 146, 482, 279, 830), (319, 847, 767, 169, 548, 67, 54, 142, 777, 621, 244, 884, 816, 166, 95, 569, 414, 475, 785, 532), (263, 57, 781, 400, 670, 766, 297, 687, 705, 656, 462, 160, 805, 620, 301, 16, 163, 262, 567, 390), (562, 454, 408, 365, 396, 654, 438, 505, 29, 572, 619, 321, 560, 746, 314, 437, 391, 119, 233, 841), (0, 693, 269, 150, 440, 631, 750, 681, 435, 289, 523, 340, 838, 565, 501, 356, 350, 326, 760, 315), (216, 775, 264, 547, 345, 236, 239, 153, 748, 35, 555, 291, 517, 609, 512, 744, 789, 642, 348, 530), (886, 401, 721, 12, 778, 70, 863, 740, 39, 73, 799, 180, 434, 883, 170, 770, 46, 351, 759, 231), (242, 871, 590, 451, 753, 882, 790, 85, 386, 839, 275, 675, 862, 768, 344, 879, 176, 161, 59, 10), (124, 304, 637, 328, 110, 846, 375, 633, 518, 584, 45, 834, 426, 815, 597, 373, 527, 251, 349, 763), (282, 822, 194, 574, 254, 308, 423, 22, 1, 193, 362, 606, 293, 730, 217, 674, 287, 632, 342, 538), (20, 682, 624, 561, 25, 299, 48, 706, 652, 355, 468, 660, 284, 206, 658, 136, 796, 305, 653, 24), (492, 649, 141, 272, 707, 422, 498, 602, 428, 491, 186, 238, 650, 714, 177, 108, 421, 376, 229, 81), (448, 598, 331, 271, 688, 377, 288, 516, 755, 128, 120, 751, 553, 333, 324, 156, 100, 881, 875, 677), (98, 383, 381, 739, 64, 294, 115, 543, 676, 662, 171, 117, 358, 504, 306, 412, 164, 582, 27, 509), (102, 586, 188, 542, 500, 723, 212, 757, 829, 51, 480, 761, 6, 313, 866, 445, 678, 135, 18, 28), (801, 96, 47, 56, 519, 458, 441, 579, 473, 804, 643, 353, 787, 227, 76, 203, 709, 145, 162, 357), (311, 111, 447, 278, 762, 339, 402, 481, 732, 452, 389, 692, 105, 336, 878, 503, 783, 416, 782, 559), (466, 873, 122, 610, 346, 266, 843, 669, 848, 148, 710, 638, 132, 600, 130, 370, 825, 599, 836, 689), (471, 104, 772, 325, 570, 814, 281, 318, 539, 461, 363, 508, 695, 179, 885, 557, 323, 210, 566, 592), (58, 852, 69, 496, 575, 554, 455, 433, 431, 260, 833, 79, 581, 704, 556, 230, 215, 374, 118, 429), (856, 840, 384, 228, 220, 199, 256, 133, 208, 469, 183, 860, 851, 432, 776, 541, 577, 418, 4, 639), (379, 535, 712, 9, 322, 112, 198, 223, 702, 158, 202, 237, 808, 74, 352, 664, 765, 285, 826, 780), (197, 55, 443, 205, 116, 685, 645, 167, 745, 329, 870, 810, 854, 364, 672, 601, 736, 181, 65, 666), (737, 347, 463, 147, 302, 273, 303, 807, 738, 427, 837, 360, 892, 41, 520, 622, 224, 824, 831, 404), (334, 533, 823, 107, 53, 711, 786, 134, 157, 880, 83, 226, 232, 268, 258, 442, 129, 545, 655, 544), (701, 868, 99, 90, 380, 623, 214, 257, 684, 891, 149, 696, 513, 859, 589, 506, 292, 265, 583, 477), (280, 243, 488, 127, 472, 686, 788, 403, 872, 502, 605, 425, 75, 37, 703, 659, 123, 697, 595, 580), (420, 832, 361, 126, 399, 809, 735, 479, 667, 784, 794, 270, 774, 338, 178, 42, 525, 398, 77, 811)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.9160	step1_train_accuracy: 64.7940
STEP-1	Epoch: 20/50	loss: 0.9374	step1_train_accuracy: 84.2697
STEP-1	Epoch: 30/50	loss: 0.5198	step1_train_accuracy: 93.8202
STEP-1	Epoch: 40/50	loss: 0.3317	step1_train_accuracy: 95.8801
STEP-1	Epoch: 50/50	loss: 0.2325	step1_train_accuracy: 97.7528
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 34, 1: 34, 2: 34, 3: 34, 4: 34, 5: 34, 6: 34, 7: 34, 8: 34, 9: 34, 10: 34, 11: 34, 12: 34, 13: 34, 14: 34, 15: 34, 16: 34, 17: 34, 18: 34, 19: 34, 20: 34, 21: 34, 22: 34, 23: 34, 24: 34, 25: 34, 26: 34, 27: 34, 28: 34, 29: 34, 30: 34, 31: 34, 32: 34, 33: 34})
STEP-2	Epoch: 20/200	classification_loss: 0.0748	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.0698	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.0658	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0621	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0597	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0581	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0568	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0554	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0545	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0540	gate_loss: 0.0000	step2_classification_accuracy: 97.4048	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 99.2481	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.7026	step1_train_accuracy: 63.7037
STEP-1	Epoch: 20/50	loss: 0.7695	step1_train_accuracy: 86.6667
STEP-1	Epoch: 30/50	loss: 0.4628	step1_train_accuracy: 91.3580
STEP-1	Epoch: 40/50	loss: 0.3043	step1_train_accuracy: 94.0741
STEP-1	Epoch: 50/50	loss: 0.2062	step1_train_accuracy: 97.2840
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.1051	gate_loss: 0.0760	step2_classification_accuracy: 96.1227	step_2_gate_accuracy: 97.8009
STEP-2	Epoch: 40/200	classification_loss: 0.0706	gate_loss: 0.0292	step2_classification_accuracy: 96.9907	step_2_gate_accuracy: 99.4213
STEP-2	Epoch: 60/200	classification_loss: 0.0635	gate_loss: 0.0233	step2_classification_accuracy: 97.0486	step_2_gate_accuracy: 99.4792
STEP-2	Epoch: 80/200	classification_loss: 0.0582	gate_loss: 0.0192	step2_classification_accuracy: 97.1644	step_2_gate_accuracy: 99.6528
STEP-2	Epoch: 100/200	classification_loss: 0.0616	gate_loss: 0.0164	step2_classification_accuracy: 96.9907	step_2_gate_accuracy: 99.5949
STEP-2	Epoch: 120/200	classification_loss: 0.0532	gate_loss: 0.0132	step2_classification_accuracy: 97.3958	step_2_gate_accuracy: 99.8264
STEP-2	Epoch: 140/200	classification_loss: 0.0502	gate_loss: 0.0100	step2_classification_accuracy: 97.3958	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0487	gate_loss: 0.0095	step2_classification_accuracy: 97.3958	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0763	gate_loss: 0.0134	step2_classification_accuracy: 97.1065	step_2_gate_accuracy: 99.7106
STEP-2	Epoch: 200/200	classification_loss: 0.0465	gate_loss: 0.0061	step2_classification_accuracy: 97.3958	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 95.4887	gate_accuracy: 96.9925
	Task-1	val_accuracy: 92.0792	gate_accuracy: 98.0198
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 97.4359


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 2.1176	step1_train_accuracy: 55.2288
STEP-1	Epoch: 20/50	loss: 0.8703	step1_train_accuracy: 81.6993
STEP-1	Epoch: 30/50	loss: 0.4886	step1_train_accuracy: 90.5229
STEP-1	Epoch: 40/50	loss: 0.3477	step1_train_accuracy: 93.4641
STEP-1	Epoch: 50/50	loss: 0.2668	step1_train_accuracy: 93.7908
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.0989	gate_loss: 0.0999	step2_classification_accuracy: 95.6926	step_2_gate_accuracy: 98.3953
STEP-2	Epoch: 40/200	classification_loss: 0.0824	gate_loss: 0.0392	step2_classification_accuracy: 96.1571	step_2_gate_accuracy: 99.2399
STEP-2	Epoch: 60/200	classification_loss: 0.0783	gate_loss: 0.0268	step2_classification_accuracy: 96.3260	step_2_gate_accuracy: 99.4088
STEP-2	Epoch: 80/200	classification_loss: 0.0682	gate_loss: 0.0204	step2_classification_accuracy: 96.4949	step_2_gate_accuracy: 99.7044
STEP-2	Epoch: 100/200	classification_loss: 0.0662	gate_loss: 0.0163	step2_classification_accuracy: 96.4949	step_2_gate_accuracy: 99.7044
STEP-2	Epoch: 120/200	classification_loss: 0.0661	gate_loss: 0.0152	step2_classification_accuracy: 96.3682	step_2_gate_accuracy: 99.7044
STEP-2	Epoch: 140/200	classification_loss: 0.0607	gate_loss: 0.0135	step2_classification_accuracy: 96.4527	step_2_gate_accuracy: 99.7466
STEP-2	Epoch: 160/200	classification_loss: 0.0655	gate_loss: 0.0127	step2_classification_accuracy: 96.2838	step_2_gate_accuracy: 99.7044
STEP-2	Epoch: 180/200	classification_loss: 0.0618	gate_loss: 0.0106	step2_classification_accuracy: 96.4527	step_2_gate_accuracy: 99.7888
STEP-2	Epoch: 200/200	classification_loss: 0.0624	gate_loss: 0.0107	step2_classification_accuracy: 96.4949	step_2_gate_accuracy: 99.7044
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 96.2406	gate_accuracy: 97.7444
	Task-1	val_accuracy: 91.0891	gate_accuracy: 95.0495
	Task-2	val_accuracy: 93.5065	gate_accuracy: 96.1039
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 96.4630


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 2.0997	step1_train_accuracy: 59.0278
STEP-1	Epoch: 20/50	loss: 0.9879	step1_train_accuracy: 77.4306
STEP-1	Epoch: 30/50	loss: 0.5556	step1_train_accuracy: 92.3611
STEP-1	Epoch: 40/50	loss: 0.3622	step1_train_accuracy: 95.1389
STEP-1	Epoch: 50/50	loss: 0.2574	step1_train_accuracy: 96.1806
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.1435	gate_loss: 0.1364	step2_classification_accuracy: 94.2908	step_2_gate_accuracy: 97.9787
STEP-2	Epoch: 40/200	classification_loss: 0.1120	gate_loss: 0.0441	step2_classification_accuracy: 94.8227	step_2_gate_accuracy: 99.1844
STEP-2	Epoch: 60/200	classification_loss: 0.1019	gate_loss: 0.0269	step2_classification_accuracy: 95.1418	step_2_gate_accuracy: 99.4681
STEP-2	Epoch: 80/200	classification_loss: 0.0977	gate_loss: 0.0195	step2_classification_accuracy: 95.1773	step_2_gate_accuracy: 99.6454
STEP-2	Epoch: 100/200	classification_loss: 0.0919	gate_loss: 0.0155	step2_classification_accuracy: 95.2837	step_2_gate_accuracy: 99.7518
STEP-2	Epoch: 120/200	classification_loss: 0.0939	gate_loss: 0.0138	step2_classification_accuracy: 95.2128	step_2_gate_accuracy: 99.7518
STEP-2	Epoch: 140/200	classification_loss: 0.1013	gate_loss: 0.0150	step2_classification_accuracy: 95.0709	step_2_gate_accuracy: 99.5745
STEP-2	Epoch: 160/200	classification_loss: 0.0869	gate_loss: 0.0114	step2_classification_accuracy: 95.3191	step_2_gate_accuracy: 99.7518
STEP-2	Epoch: 180/200	classification_loss: 0.0866	gate_loss: 0.0106	step2_classification_accuracy: 95.4255	step_2_gate_accuracy: 99.8227
STEP-2	Epoch: 200/200	classification_loss: 0.0834	gate_loss: 0.0092	step2_classification_accuracy: 95.4255	step_2_gate_accuracy: 99.7872
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 93.9850	gate_accuracy: 99.2481
	Task-1	val_accuracy: 93.0693	gate_accuracy: 98.0198
	Task-2	val_accuracy: 90.9091	gate_accuracy: 93.5065
	Task-3	val_accuracy: 88.8889	gate_accuracy: 94.4444
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 96.8668


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.2140	step1_train_accuracy: 62.6822
STEP-1	Epoch: 20/50	loss: 0.9550	step1_train_accuracy: 81.0496
STEP-1	Epoch: 30/50	loss: 0.4847	step1_train_accuracy: 93.0029
STEP-1	Epoch: 40/50	loss: 0.3185	step1_train_accuracy: 96.5015
STEP-1	Epoch: 50/50	loss: 0.2248	step1_train_accuracy: 97.6676
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.1628	gate_loss: 0.1723	step2_classification_accuracy: 93.8871	step_2_gate_accuracy: 96.1075
STEP-2	Epoch: 40/200	classification_loss: 0.1212	gate_loss: 0.0631	step2_classification_accuracy: 94.8739	step_2_gate_accuracy: 98.4923
STEP-2	Epoch: 60/200	classification_loss: 0.1068	gate_loss: 0.0397	step2_classification_accuracy: 95.3399	step_2_gate_accuracy: 98.9309
STEP-2	Epoch: 80/200	classification_loss: 0.1023	gate_loss: 0.0312	step2_classification_accuracy: 95.2029	step_2_gate_accuracy: 98.9583
STEP-2	Epoch: 100/200	classification_loss: 0.0945	gate_loss: 0.0257	step2_classification_accuracy: 95.4496	step_2_gate_accuracy: 99.1228
STEP-2	Epoch: 120/200	classification_loss: 0.0952	gate_loss: 0.0247	step2_classification_accuracy: 95.2577	step_2_gate_accuracy: 99.0406
STEP-2	Epoch: 140/200	classification_loss: 0.0931	gate_loss: 0.0232	step2_classification_accuracy: 95.2303	step_2_gate_accuracy: 99.0680
STEP-2	Epoch: 160/200	classification_loss: 0.0949	gate_loss: 0.0220	step2_classification_accuracy: 95.5318	step_2_gate_accuracy: 99.2050
STEP-2	Epoch: 180/200	classification_loss: 0.0863	gate_loss: 0.0191	step2_classification_accuracy: 95.5318	step_2_gate_accuracy: 99.3147
STEP-2	Epoch: 200/200	classification_loss: 0.0867	gate_loss: 0.0182	step2_classification_accuracy: 95.5866	step_2_gate_accuracy: 99.3147
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 95.4887	gate_accuracy: 98.4962
	Task-1	val_accuracy: 92.0792	gate_accuracy: 95.0495
	Task-2	val_accuracy: 85.7143	gate_accuracy: 88.3117
	Task-3	val_accuracy: 88.8889	gate_accuracy: 94.4444
	Task-4	val_accuracy: 81.3953	gate_accuracy: 81.3953
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 92.3241


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.5481	step1_train_accuracy: 53.6585
STEP-1	Epoch: 20/50	loss: 1.1359	step1_train_accuracy: 78.0488
STEP-1	Epoch: 30/50	loss: 0.6171	step1_train_accuracy: 86.7596
STEP-1	Epoch: 40/50	loss: 0.3841	step1_train_accuracy: 94.0767
STEP-1	Epoch: 50/50	loss: 0.2698	step1_train_accuracy: 97.5610
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.1877	gate_loss: 0.2178	step2_classification_accuracy: 93.0970	step_2_gate_accuracy: 95.1726
STEP-2	Epoch: 40/200	classification_loss: 0.1431	gate_loss: 0.0826	step2_classification_accuracy: 94.2864	step_2_gate_accuracy: 97.8312
STEP-2	Epoch: 60/200	classification_loss: 0.1303	gate_loss: 0.0568	step2_classification_accuracy: 94.7761	step_2_gate_accuracy: 98.2743
STEP-2	Epoch: 80/200	classification_loss: 0.1023	gate_loss: 0.0352	step2_classification_accuracy: 95.1959	step_2_gate_accuracy: 98.9039
STEP-2	Epoch: 100/200	classification_loss: 0.0979	gate_loss: 0.0296	step2_classification_accuracy: 95.2192	step_2_gate_accuracy: 99.0905
STEP-2	Epoch: 120/200	classification_loss: 0.0942	gate_loss: 0.0255	step2_classification_accuracy: 95.4058	step_2_gate_accuracy: 99.2304
STEP-2	Epoch: 140/200	classification_loss: 0.0899	gate_loss: 0.0216	step2_classification_accuracy: 95.4991	step_2_gate_accuracy: 99.4403
STEP-2	Epoch: 160/200	classification_loss: 0.0902	gate_loss: 0.0206	step2_classification_accuracy: 95.4991	step_2_gate_accuracy: 99.4636
STEP-2	Epoch: 180/200	classification_loss: 0.0940	gate_loss: 0.0220	step2_classification_accuracy: 95.2659	step_2_gate_accuracy: 99.2304
STEP-2	Epoch: 200/200	classification_loss: 0.0860	gate_loss: 0.0176	step2_classification_accuracy: 95.5690	step_2_gate_accuracy: 99.5569
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 90.2256	gate_accuracy: 91.7293
	Task-1	val_accuracy: 83.1683	gate_accuracy: 89.1089
	Task-2	val_accuracy: 88.3117	gate_accuracy: 90.9091
	Task-3	val_accuracy: 87.5000	gate_accuracy: 93.0556
	Task-4	val_accuracy: 83.7209	gate_accuracy: 87.2093
	Task-5	val_accuracy: 83.3333	gate_accuracy: 81.9444
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 89.2791


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 2.6300	step1_train_accuracy: 44.8560
STEP-1	Epoch: 20/50	loss: 1.2379	step1_train_accuracy: 80.2469
STEP-1	Epoch: 30/50	loss: 0.7056	step1_train_accuracy: 86.0082
STEP-1	Epoch: 40/50	loss: 0.4988	step1_train_accuracy: 89.7119
STEP-1	Epoch: 50/50	loss: 0.3798	step1_train_accuracy: 92.5926
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 25, 1: 25, 2: 25, 3: 25, 4: 25, 5: 25, 6: 25, 7: 25, 8: 25, 9: 25, 10: 25, 11: 25, 12: 25, 13: 25, 14: 25, 15: 25, 16: 25, 17: 25, 18: 25, 19: 25, 20: 25, 21: 25, 22: 25, 23: 25, 24: 25, 25: 25, 26: 25, 27: 25, 28: 25, 29: 25, 30: 25, 31: 25, 32: 25, 33: 25, 34: 25, 35: 25, 36: 25, 37: 25, 38: 25, 39: 25, 40: 25, 41: 25, 42: 25, 43: 25, 44: 25, 45: 25, 46: 25, 47: 25, 48: 25, 49: 25, 50: 25, 51: 25, 52: 25, 53: 25, 54: 25, 55: 25, 56: 25, 57: 25, 58: 25, 59: 25, 60: 25, 61: 25, 62: 25, 63: 25, 64: 25, 65: 25, 66: 25, 67: 25, 68: 25, 69: 25, 70: 25, 71: 25, 72: 25, 73: 25, 74: 25, 75: 25, 76: 25, 77: 25, 78: 25, 79: 25, 80: 25, 81: 25, 82: 25, 83: 25, 84: 25, 85: 25, 86: 25, 87: 25, 88: 25, 89: 25, 90: 25, 91: 25, 92: 25, 93: 25, 94: 25, 95: 25, 96: 25, 97: 25, 98: 25, 99: 25, 100: 25, 101: 25, 102: 25, 103: 25, 104: 25, 105: 25, 106: 25, 107: 25, 108: 25, 109: 25, 110: 25, 111: 25, 112: 25, 113: 25, 114: 25, 115: 25, 116: 25, 117: 25, 118: 25, 119: 25, 120: 25, 121: 25, 122: 25, 123: 25, 124: 25, 125: 25, 126: 25, 127: 25, 128: 25, 129: 25, 130: 25, 131: 25, 132: 25, 133: 25, 134: 25, 135: 25, 136: 25, 137: 25, 138: 25, 139: 25, 140: 25, 141: 25, 142: 25, 143: 25, 144: 25, 145: 25, 146: 25, 147: 25, 148: 25, 149: 25, 150: 25, 151: 25, 152: 25, 153: 25})
STEP-2	Epoch: 20/200	classification_loss: 0.2408	gate_loss: 0.3704	step2_classification_accuracy: 91.7922	step_2_gate_accuracy: 90.5195
STEP-2	Epoch: 40/200	classification_loss: 0.1667	gate_loss: 0.1315	step2_classification_accuracy: 93.7922	step_2_gate_accuracy: 96.7792
STEP-2	Epoch: 60/200	classification_loss: 0.1426	gate_loss: 0.0825	step2_classification_accuracy: 94.1558	step_2_gate_accuracy: 97.6364
STEP-2	Epoch: 80/200	classification_loss: 0.1306	gate_loss: 0.0611	step2_classification_accuracy: 94.4935	step_2_gate_accuracy: 98.3377
STEP-2	Epoch: 100/200	classification_loss: 0.1133	gate_loss: 0.0455	step2_classification_accuracy: 95.0130	step_2_gate_accuracy: 98.6753
STEP-2	Epoch: 120/200	classification_loss: 0.1096	gate_loss: 0.0397	step2_classification_accuracy: 94.9870	step_2_gate_accuracy: 98.9870
STEP-2	Epoch: 140/200	classification_loss: 0.1063	gate_loss: 0.0351	step2_classification_accuracy: 95.2208	step_2_gate_accuracy: 98.9091
STEP-2	Epoch: 160/200	classification_loss: 0.0975	gate_loss: 0.0297	step2_classification_accuracy: 95.2468	step_2_gate_accuracy: 99.1169
STEP-2	Epoch: 180/200	classification_loss: 0.0967	gate_loss: 0.0285	step2_classification_accuracy: 95.1429	step_2_gate_accuracy: 98.9870
STEP-2	Epoch: 200/200	classification_loss: 0.0937	gate_loss: 0.0266	step2_classification_accuracy: 95.2208	step_2_gate_accuracy: 99.1429
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 88.7218	gate_accuracy: 93.9850
	Task-1	val_accuracy: 85.1485	gate_accuracy: 92.0792
	Task-2	val_accuracy: 84.4156	gate_accuracy: 87.0130
	Task-3	val_accuracy: 87.5000	gate_accuracy: 93.0556
	Task-4	val_accuracy: 80.2326	gate_accuracy: 83.7209
	Task-5	val_accuracy: 83.3333	gate_accuracy: 86.1111
	Task-6	val_accuracy: 88.5246	gate_accuracy: 85.2459
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 89.3688


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 1.9968	step1_train_accuracy: 60.3659
STEP-1	Epoch: 20/50	loss: 0.9231	step1_train_accuracy: 78.9634
STEP-1	Epoch: 30/50	loss: 0.4831	step1_train_accuracy: 91.4634
STEP-1	Epoch: 40/50	loss: 0.3125	step1_train_accuracy: 95.7317
STEP-1	Epoch: 50/50	loss: 0.2139	step1_train_accuracy: 98.4756
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.2579	gate_loss: 0.3226	step2_classification_accuracy: 91.0424	step_2_gate_accuracy: 91.0622
STEP-2	Epoch: 40/200	classification_loss: 0.1773	gate_loss: 0.1246	step2_classification_accuracy: 93.6980	step_2_gate_accuracy: 96.5914
STEP-2	Epoch: 60/200	classification_loss: 0.1524	gate_loss: 0.0810	step2_classification_accuracy: 94.0349	step_2_gate_accuracy: 97.5822
STEP-2	Epoch: 80/200	classification_loss: 0.1245	gate_loss: 0.0561	step2_classification_accuracy: 94.7483	step_2_gate_accuracy: 98.3750
STEP-2	Epoch: 100/200	classification_loss: 0.1167	gate_loss: 0.0462	step2_classification_accuracy: 94.8078	step_2_gate_accuracy: 98.5335
STEP-2	Epoch: 120/200	classification_loss: 0.1140	gate_loss: 0.0421	step2_classification_accuracy: 95.0258	step_2_gate_accuracy: 98.5929
STEP-2	Epoch: 140/200	classification_loss: 0.1036	gate_loss: 0.0345	step2_classification_accuracy: 95.2636	step_2_gate_accuracy: 98.9497
STEP-2	Epoch: 160/200	classification_loss: 0.1011	gate_loss: 0.0316	step2_classification_accuracy: 95.1050	step_2_gate_accuracy: 99.0289
STEP-2	Epoch: 180/200	classification_loss: 0.1066	gate_loss: 0.0320	step2_classification_accuracy: 95.1249	step_2_gate_accuracy: 98.9893
STEP-2	Epoch: 200/200	classification_loss: 0.0931	gate_loss: 0.0256	step2_classification_accuracy: 95.4816	step_2_gate_accuracy: 99.2073
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 86.4662	gate_accuracy: 90.9774
	Task-1	val_accuracy: 82.1782	gate_accuracy: 88.1188
	Task-2	val_accuracy: 81.8182	gate_accuracy: 83.1169
	Task-3	val_accuracy: 88.8889	gate_accuracy: 91.6667
	Task-4	val_accuracy: 80.2326	gate_accuracy: 83.7209
	Task-5	val_accuracy: 77.7778	gate_accuracy: 79.1667
	Task-6	val_accuracy: 85.2459	gate_accuracy: 81.9672
	Task-7	val_accuracy: 90.2439	gate_accuracy: 92.6829
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 86.9883


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.1857	step1_train_accuracy: 63.2768
STEP-1	Epoch: 20/50	loss: 0.8552	step1_train_accuracy: 89.2655
STEP-1	Epoch: 30/50	loss: 0.4166	step1_train_accuracy: 94.6328
STEP-1	Epoch: 40/50	loss: 0.2451	step1_train_accuracy: 98.0226
STEP-1	Epoch: 50/50	loss: 0.1612	step1_train_accuracy: 98.8701
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.2792	gate_loss: 0.3063	step2_classification_accuracy: 90.9716	step_2_gate_accuracy: 91.2840
STEP-2	Epoch: 40/200	classification_loss: 0.1804	gate_loss: 0.1270	step2_classification_accuracy: 93.2833	step_2_gate_accuracy: 95.7982
STEP-2	Epoch: 60/200	classification_loss: 0.1627	gate_loss: 0.0892	step2_classification_accuracy: 93.6582	step_2_gate_accuracy: 96.8291
STEP-2	Epoch: 80/200	classification_loss: 0.1394	gate_loss: 0.0676	step2_classification_accuracy: 94.3924	step_2_gate_accuracy: 97.5476
STEP-2	Epoch: 100/200	classification_loss: 0.1423	gate_loss: 0.0646	step2_classification_accuracy: 94.4549	step_2_gate_accuracy: 97.7195
STEP-2	Epoch: 120/200	classification_loss: 0.1270	gate_loss: 0.0527	step2_classification_accuracy: 94.6579	step_2_gate_accuracy: 97.8913
STEP-2	Epoch: 140/200	classification_loss: 0.1136	gate_loss: 0.0424	step2_classification_accuracy: 94.7829	step_2_gate_accuracy: 98.4848
STEP-2	Epoch: 160/200	classification_loss: 0.1106	gate_loss: 0.0414	step2_classification_accuracy: 95.0484	step_2_gate_accuracy: 98.5473
STEP-2	Epoch: 180/200	classification_loss: 0.1039	gate_loss: 0.0360	step2_classification_accuracy: 95.2359	step_2_gate_accuracy: 98.7504
STEP-2	Epoch: 200/200	classification_loss: 0.1063	gate_loss: 0.0384	step2_classification_accuracy: 95.1109	step_2_gate_accuracy: 98.5005
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 88.7218	gate_accuracy: 93.2331
	Task-1	val_accuracy: 82.1782	gate_accuracy: 86.1386
	Task-2	val_accuracy: 85.7143	gate_accuracy: 89.6104
	Task-3	val_accuracy: 86.1111	gate_accuracy: 93.0556
	Task-4	val_accuracy: 77.9070	gate_accuracy: 81.3953
	Task-5	val_accuracy: 80.5556	gate_accuracy: 79.1667
	Task-6	val_accuracy: 85.2459	gate_accuracy: 86.8852
	Task-7	val_accuracy: 89.0244	gate_accuracy: 86.5854
	Task-8	val_accuracy: 82.9545	gate_accuracy: 84.0909
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 87.0466


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.2727	step1_train_accuracy: 52.9586
STEP-1	Epoch: 20/50	loss: 0.9135	step1_train_accuracy: 87.8698
STEP-1	Epoch: 30/50	loss: 0.4634	step1_train_accuracy: 96.1538
STEP-1	Epoch: 40/50	loss: 0.2982	step1_train_accuracy: 97.0414
STEP-1	Epoch: 50/50	loss: 0.2121	step1_train_accuracy: 97.0414
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.3218	gate_loss: 0.3684	step2_classification_accuracy: 89.3277	step_2_gate_accuracy: 89.4634
STEP-2	Epoch: 40/200	classification_loss: 0.2106	gate_loss: 0.1559	step2_classification_accuracy: 92.3576	step_2_gate_accuracy: 94.9352
STEP-2	Epoch: 60/200	classification_loss: 0.1758	gate_loss: 0.1085	step2_classification_accuracy: 93.2318	step_2_gate_accuracy: 96.3220
STEP-2	Epoch: 80/200	classification_loss: 0.1546	gate_loss: 0.0834	step2_classification_accuracy: 93.9252	step_2_gate_accuracy: 97.2566
STEP-2	Epoch: 100/200	classification_loss: 0.1377	gate_loss: 0.0697	step2_classification_accuracy: 94.4377	step_2_gate_accuracy: 97.2867
STEP-2	Epoch: 120/200	classification_loss: 0.1422	gate_loss: 0.0664	step2_classification_accuracy: 94.3624	step_2_gate_accuracy: 97.6033
STEP-2	Epoch: 140/200	classification_loss: 0.1207	gate_loss: 0.0519	step2_classification_accuracy: 94.8749	step_2_gate_accuracy: 98.0705
STEP-2	Epoch: 160/200	classification_loss: 0.1178	gate_loss: 0.0472	step2_classification_accuracy: 95.0106	step_2_gate_accuracy: 98.3569
STEP-2	Epoch: 180/200	classification_loss: 0.1103	gate_loss: 0.0436	step2_classification_accuracy: 95.0708	step_2_gate_accuracy: 98.3569
STEP-2	Epoch: 200/200	classification_loss: 0.1085	gate_loss: 0.0407	step2_classification_accuracy: 95.1161	step_2_gate_accuracy: 98.5981
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 87.2180	gate_accuracy: 94.7368
	Task-1	val_accuracy: 73.2673	gate_accuracy: 80.1980
	Task-2	val_accuracy: 83.1169	gate_accuracy: 85.7143
	Task-3	val_accuracy: 88.8889	gate_accuracy: 91.6667
	Task-4	val_accuracy: 76.7442	gate_accuracy: 75.5814
	Task-5	val_accuracy: 80.5556	gate_accuracy: 84.7222
	Task-6	val_accuracy: 83.6066	gate_accuracy: 81.9672
	Task-7	val_accuracy: 86.5854	gate_accuracy: 85.3659
	Task-8	val_accuracy: 78.4091	gate_accuracy: 82.9545
	Task-9	val_accuracy: 69.0476	gate_accuracy: 63.0952
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 83.0607


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.1511	step1_train_accuracy: 52.9781
STEP-1	Epoch: 20/50	loss: 0.8878	step1_train_accuracy: 85.5799
STEP-1	Epoch: 30/50	loss: 0.4828	step1_train_accuracy: 95.9248
STEP-1	Epoch: 40/50	loss: 0.3081	step1_train_accuracy: 97.8056
STEP-1	Epoch: 50/50	loss: 0.2136	step1_train_accuracy: 98.7461
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.3656	gate_loss: 0.4416	step2_classification_accuracy: 87.4966	step_2_gate_accuracy: 87.6068
STEP-2	Epoch: 40/200	classification_loss: 0.2445	gate_loss: 0.1995	step2_classification_accuracy: 91.1911	step_2_gate_accuracy: 93.2865
STEP-2	Epoch: 60/200	classification_loss: 0.1983	gate_loss: 0.1395	step2_classification_accuracy: 92.3077	step_2_gate_accuracy: 95.1751
STEP-2	Epoch: 80/200	classification_loss: 0.1685	gate_loss: 0.1105	step2_classification_accuracy: 93.1624	step_2_gate_accuracy: 96.2641
STEP-2	Epoch: 100/200	classification_loss: 0.1566	gate_loss: 0.0946	step2_classification_accuracy: 93.4795	step_2_gate_accuracy: 96.5674
STEP-2	Epoch: 120/200	classification_loss: 0.1455	gate_loss: 0.0842	step2_classification_accuracy: 93.9206	step_2_gate_accuracy: 96.9948
STEP-2	Epoch: 140/200	classification_loss: 0.1416	gate_loss: 0.0789	step2_classification_accuracy: 94.0585	step_2_gate_accuracy: 96.9120
STEP-2	Epoch: 160/200	classification_loss: 0.1306	gate_loss: 0.0712	step2_classification_accuracy: 94.1687	step_2_gate_accuracy: 97.2429
STEP-2	Epoch: 180/200	classification_loss: 0.1244	gate_loss: 0.0655	step2_classification_accuracy: 94.4031	step_2_gate_accuracy: 97.7805
STEP-2	Epoch: 200/200	classification_loss: 0.1174	gate_loss: 0.0593	step2_classification_accuracy: 94.5961	step_2_gate_accuracy: 97.7805
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 87.2180	gate_accuracy: 93.2331
	Task-1	val_accuracy: 75.2475	gate_accuracy: 82.1782
	Task-2	val_accuracy: 85.7143	gate_accuracy: 85.7143
	Task-3	val_accuracy: 86.1111	gate_accuracy: 91.6667
	Task-4	val_accuracy: 76.7442	gate_accuracy: 77.9070
	Task-5	val_accuracy: 84.7222	gate_accuracy: 83.3333
	Task-6	val_accuracy: 83.6066	gate_accuracy: 83.6066
	Task-7	val_accuracy: 90.2439	gate_accuracy: 91.4634
	Task-8	val_accuracy: 80.6818	gate_accuracy: 84.0909
	Task-9	val_accuracy: 67.8571	gate_accuracy: 65.4762
	Task-10	val_accuracy: 80.0000	gate_accuracy: 80.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 83.8675


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.1485	step1_train_accuracy: 62.7219
STEP-1	Epoch: 20/50	loss: 0.9207	step1_train_accuracy: 80.1775
STEP-1	Epoch: 30/50	loss: 0.4933	step1_train_accuracy: 94.9704
STEP-1	Epoch: 40/50	loss: 0.3124	step1_train_accuracy: 97.3373
STEP-1	Epoch: 50/50	loss: 0.2150	step1_train_accuracy: 98.5207
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.3731	gate_loss: 0.4546	step2_classification_accuracy: 87.6048	step_2_gate_accuracy: 86.8809
STEP-2	Epoch: 40/200	classification_loss: 0.2561	gate_loss: 0.2172	step2_classification_accuracy: 90.8687	step_2_gate_accuracy: 92.4689
STEP-2	Epoch: 60/200	classification_loss: 0.2070	gate_loss: 0.1505	step2_classification_accuracy: 92.4054	step_2_gate_accuracy: 94.8057
STEP-2	Epoch: 80/200	classification_loss: 0.1868	gate_loss: 0.1245	step2_classification_accuracy: 92.8118	step_2_gate_accuracy: 95.3137
STEP-2	Epoch: 100/200	classification_loss: 0.1645	gate_loss: 0.1050	step2_classification_accuracy: 93.5992	step_2_gate_accuracy: 95.9995
STEP-2	Epoch: 120/200	classification_loss: 0.1549	gate_loss: 0.0929	step2_classification_accuracy: 93.7135	step_2_gate_accuracy: 96.3932
STEP-2	Epoch: 140/200	classification_loss: 0.1404	gate_loss: 0.0813	step2_classification_accuracy: 94.1072	step_2_gate_accuracy: 96.9012
STEP-2	Epoch: 160/200	classification_loss: 0.1332	gate_loss: 0.0774	step2_classification_accuracy: 94.3866	step_2_gate_accuracy: 97.2187
STEP-2	Epoch: 180/200	classification_loss: 0.1341	gate_loss: 0.0738	step2_classification_accuracy: 94.2596	step_2_gate_accuracy: 97.1425
STEP-2	Epoch: 200/200	classification_loss: 0.1274	gate_loss: 0.0705	step2_classification_accuracy: 94.4120	step_2_gate_accuracy: 97.2568
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 87.2180	gate_accuracy: 88.7218
	Task-1	val_accuracy: 78.2178	gate_accuracy: 85.1485
	Task-2	val_accuracy: 85.7143	gate_accuracy: 89.6104
	Task-3	val_accuracy: 87.5000	gate_accuracy: 88.8889
	Task-4	val_accuracy: 76.7442	gate_accuracy: 79.0698
	Task-5	val_accuracy: 81.9444	gate_accuracy: 80.5556
	Task-6	val_accuracy: 78.6885	gate_accuracy: 75.4098
	Task-7	val_accuracy: 87.8049	gate_accuracy: 87.8049
	Task-8	val_accuracy: 75.0000	gate_accuracy: 78.4091
	Task-9	val_accuracy: 64.2857	gate_accuracy: 63.0952
	Task-10	val_accuracy: 80.0000	gate_accuracy: 82.5000
	Task-11	val_accuracy: 85.8824	gate_accuracy: 83.5294
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.2723


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.7750	step1_train_accuracy: 50.3205
STEP-1	Epoch: 20/50	loss: 1.1966	step1_train_accuracy: 77.5641
STEP-1	Epoch: 30/50	loss: 0.5106	step1_train_accuracy: 97.7564
STEP-1	Epoch: 40/50	loss: 0.2928	step1_train_accuracy: 97.7564
STEP-1	Epoch: 50/50	loss: 0.1976	step1_train_accuracy: 98.7179
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.4016	gate_loss: 0.4873	step2_classification_accuracy: 86.4257	step_2_gate_accuracy: 85.4015
STEP-2	Epoch: 40/200	classification_loss: 0.2798	gate_loss: 0.2342	step2_classification_accuracy: 90.0753	step_2_gate_accuracy: 92.1003
STEP-2	Epoch: 60/200	classification_loss: 0.2324	gate_loss: 0.1703	step2_classification_accuracy: 91.4763	step_2_gate_accuracy: 93.6190
STEP-2	Epoch: 80/200	classification_loss: 0.2017	gate_loss: 0.1355	step2_classification_accuracy: 92.0885	step_2_gate_accuracy: 94.9729
STEP-2	Epoch: 100/200	classification_loss: 0.1816	gate_loss: 0.1167	step2_classification_accuracy: 93.0421	step_2_gate_accuracy: 95.5027
STEP-2	Epoch: 120/200	classification_loss: 0.1687	gate_loss: 0.1058	step2_classification_accuracy: 93.2187	step_2_gate_accuracy: 96.1031
STEP-2	Epoch: 140/200	classification_loss: 0.1672	gate_loss: 0.0997	step2_classification_accuracy: 93.3365	step_2_gate_accuracy: 96.2326
STEP-2	Epoch: 160/200	classification_loss: 0.1538	gate_loss: 0.0896	step2_classification_accuracy: 93.7132	step_2_gate_accuracy: 96.5034
STEP-2	Epoch: 180/200	classification_loss: 0.1465	gate_loss: 0.0838	step2_classification_accuracy: 93.7956	step_2_gate_accuracy: 96.8566
STEP-2	Epoch: 200/200	classification_loss: 0.1419	gate_loss: 0.0784	step2_classification_accuracy: 93.9722	step_2_gate_accuracy: 97.0567
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 83.4586	gate_accuracy: 88.7218
	Task-1	val_accuracy: 77.2277	gate_accuracy: 79.2079
	Task-2	val_accuracy: 80.5195	gate_accuracy: 83.1169
	Task-3	val_accuracy: 87.5000	gate_accuracy: 91.6667
	Task-4	val_accuracy: 73.2558	gate_accuracy: 73.2558
	Task-5	val_accuracy: 70.8333	gate_accuracy: 73.6111
	Task-6	val_accuracy: 80.3279	gate_accuracy: 77.0492
	Task-7	val_accuracy: 87.8049	gate_accuracy: 89.0244
	Task-8	val_accuracy: 82.9545	gate_accuracy: 86.3636
	Task-9	val_accuracy: 71.4286	gate_accuracy: 71.4286
	Task-10	val_accuracy: 78.7500	gate_accuracy: 80.0000
	Task-11	val_accuracy: 84.7059	gate_accuracy: 81.1765
	Task-12	val_accuracy: 78.2051	gate_accuracy: 76.9231
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.2557


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 2.1728	step1_train_accuracy: 48.7805
STEP-1	Epoch: 20/50	loss: 0.8340	step1_train_accuracy: 86.8902
STEP-1	Epoch: 30/50	loss: 0.4062	step1_train_accuracy: 95.7317
STEP-1	Epoch: 40/50	loss: 0.2415	step1_train_accuracy: 98.1707
STEP-1	Epoch: 50/50	loss: 0.1661	step1_train_accuracy: 99.0854
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.4087	gate_loss: 0.5075	step2_classification_accuracy: 86.3946	step_2_gate_accuracy: 85.3507
STEP-2	Epoch: 40/200	classification_loss: 0.2738	gate_loss: 0.2341	step2_classification_accuracy: 90.7577	step_2_gate_accuracy: 92.2003
STEP-2	Epoch: 60/200	classification_loss: 0.2241	gate_loss: 0.1680	step2_classification_accuracy: 92.0009	step_2_gate_accuracy: 94.0300
STEP-2	Epoch: 80/200	classification_loss: 0.1900	gate_loss: 0.1343	step2_classification_accuracy: 92.8337	step_2_gate_accuracy: 94.9566
STEP-2	Epoch: 100/200	classification_loss: 0.1733	gate_loss: 0.1163	step2_classification_accuracy: 93.4319	step_2_gate_accuracy: 95.8128
STEP-2	Epoch: 120/200	classification_loss: 0.1604	gate_loss: 0.1039	step2_classification_accuracy: 93.7720	step_2_gate_accuracy: 95.9301
STEP-2	Epoch: 140/200	classification_loss: 0.1435	gate_loss: 0.0907	step2_classification_accuracy: 94.3702	step_2_gate_accuracy: 96.7159
STEP-2	Epoch: 160/200	classification_loss: 0.1442	gate_loss: 0.0883	step2_classification_accuracy: 94.2529	step_2_gate_accuracy: 96.5517
STEP-2	Epoch: 180/200	classification_loss: 0.1291	gate_loss: 0.0790	step2_classification_accuracy: 94.5578	step_2_gate_accuracy: 96.9036
STEP-2	Epoch: 200/200	classification_loss: 0.1266	gate_loss: 0.0764	step2_classification_accuracy: 94.7455	step_2_gate_accuracy: 97.0678
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 83.4586	gate_accuracy: 89.4737
	Task-1	val_accuracy: 71.2871	gate_accuracy: 78.2178
	Task-2	val_accuracy: 81.8182	gate_accuracy: 88.3117
	Task-3	val_accuracy: 84.7222	gate_accuracy: 88.8889
	Task-4	val_accuracy: 77.9070	gate_accuracy: 76.7442
	Task-5	val_accuracy: 77.7778	gate_accuracy: 79.1667
	Task-6	val_accuracy: 73.7705	gate_accuracy: 75.4098
	Task-7	val_accuracy: 89.0244	gate_accuracy: 89.0244
	Task-8	val_accuracy: 80.6818	gate_accuracy: 84.0909
	Task-9	val_accuracy: 70.2381	gate_accuracy: 70.2381
	Task-10	val_accuracy: 82.5000	gate_accuracy: 82.5000
	Task-11	val_accuracy: 74.1176	gate_accuracy: 70.5882
	Task-12	val_accuracy: 75.6410	gate_accuracy: 73.0769
	Task-13	val_accuracy: 81.7073	gate_accuracy: 82.9268
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.9483


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.4800	step1_train_accuracy: 51.2903
STEP-1	Epoch: 20/50	loss: 1.0112	step1_train_accuracy: 78.7097
STEP-1	Epoch: 30/50	loss: 0.5516	step1_train_accuracy: 92.2581
STEP-1	Epoch: 40/50	loss: 0.3557	step1_train_accuracy: 93.2258
STEP-1	Epoch: 50/50	loss: 0.2556	step1_train_accuracy: 95.4839
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.4295	gate_loss: 0.4748	step2_classification_accuracy: 86.1564	step_2_gate_accuracy: 85.2806
STEP-2	Epoch: 40/200	classification_loss: 0.3053	gate_loss: 0.2514	step2_classification_accuracy: 89.5900	step_2_gate_accuracy: 91.1724
STEP-2	Epoch: 60/200	classification_loss: 0.2506	gate_loss: 0.1828	step2_classification_accuracy: 91.0828	step_2_gate_accuracy: 93.2623
STEP-2	Epoch: 80/200	classification_loss: 0.2216	gate_loss: 0.1499	step2_classification_accuracy: 92.0581	step_2_gate_accuracy: 94.3173
STEP-2	Epoch: 100/200	classification_loss: 0.1948	gate_loss: 0.1277	step2_classification_accuracy: 92.6752	step_2_gate_accuracy: 95.1234
STEP-2	Epoch: 120/200	classification_loss: 0.1867	gate_loss: 0.1197	step2_classification_accuracy: 92.8244	step_2_gate_accuracy: 95.2130
STEP-2	Epoch: 140/200	classification_loss: 0.1689	gate_loss: 0.1058	step2_classification_accuracy: 93.5609	step_2_gate_accuracy: 95.8400
STEP-2	Epoch: 160/200	classification_loss: 0.1588	gate_loss: 0.0969	step2_classification_accuracy: 93.7002	step_2_gate_accuracy: 96.4072
STEP-2	Epoch: 180/200	classification_loss: 0.1579	gate_loss: 0.0941	step2_classification_accuracy: 93.8495	step_2_gate_accuracy: 96.3475
STEP-2	Epoch: 200/200	classification_loss: 0.1558	gate_loss: 0.0904	step2_classification_accuracy: 94.0983	step_2_gate_accuracy: 96.5167
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.6992	gate_accuracy: 87.2180
	Task-1	val_accuracy: 70.2970	gate_accuracy: 76.2376
	Task-2	val_accuracy: 79.2208	gate_accuracy: 87.0130
	Task-3	val_accuracy: 83.3333	gate_accuracy: 87.5000
	Task-4	val_accuracy: 76.7442	gate_accuracy: 76.7442
	Task-5	val_accuracy: 73.6111	gate_accuracy: 73.6111
	Task-6	val_accuracy: 77.0492	gate_accuracy: 72.1311
	Task-7	val_accuracy: 89.0244	gate_accuracy: 87.8049
	Task-8	val_accuracy: 79.5455	gate_accuracy: 81.8182
	Task-9	val_accuracy: 79.7619	gate_accuracy: 72.6190
	Task-10	val_accuracy: 78.7500	gate_accuracy: 81.2500
	Task-11	val_accuracy: 77.6471	gate_accuracy: 77.6471
	Task-12	val_accuracy: 78.2051	gate_accuracy: 73.0769
	Task-13	val_accuracy: 82.9268	gate_accuracy: 87.8049
	Task-14	val_accuracy: 75.3247	gate_accuracy: 76.6234
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.2862


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.5127	step1_train_accuracy: 50.6061
STEP-1	Epoch: 20/50	loss: 0.9728	step1_train_accuracy: 80.6061
STEP-1	Epoch: 30/50	loss: 0.4718	step1_train_accuracy: 95.1515
STEP-1	Epoch: 40/50	loss: 0.2904	step1_train_accuracy: 98.1818
STEP-1	Epoch: 50/50	loss: 0.2016	step1_train_accuracy: 98.4848
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.4928	gate_loss: 0.5507	step2_classification_accuracy: 84.2766	step_2_gate_accuracy: 82.9148
STEP-2	Epoch: 40/200	classification_loss: 0.3483	gate_loss: 0.2985	step2_classification_accuracy: 88.2944	step_2_gate_accuracy: 89.5403
STEP-2	Epoch: 60/200	classification_loss: 0.2892	gate_loss: 0.2258	step2_classification_accuracy: 89.8397	step_2_gate_accuracy: 91.5202
STEP-2	Epoch: 80/200	classification_loss: 0.2551	gate_loss: 0.1869	step2_classification_accuracy: 90.9310	step_2_gate_accuracy: 92.7178
STEP-2	Epoch: 100/200	classification_loss: 0.2288	gate_loss: 0.1618	step2_classification_accuracy: 91.4622	step_2_gate_accuracy: 93.8671
STEP-2	Epoch: 120/200	classification_loss: 0.2139	gate_loss: 0.1484	step2_classification_accuracy: 92.0127	step_2_gate_accuracy: 94.3983
STEP-2	Epoch: 140/200	classification_loss: 0.1972	gate_loss: 0.1325	step2_classification_accuracy: 92.7178	step_2_gate_accuracy: 94.9295
STEP-2	Epoch: 160/200	classification_loss: 0.1862	gate_loss: 0.1240	step2_classification_accuracy: 93.0172	step_2_gate_accuracy: 95.3738
STEP-2	Epoch: 180/200	classification_loss: 0.1767	gate_loss: 0.1166	step2_classification_accuracy: 93.2297	step_2_gate_accuracy: 95.6442
STEP-2	Epoch: 200/200	classification_loss: 0.1696	gate_loss: 0.1103	step2_classification_accuracy: 93.4035	step_2_gate_accuracy: 95.9050
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.6992	gate_accuracy: 87.9699
	Task-1	val_accuracy: 74.2574	gate_accuracy: 84.1584
	Task-2	val_accuracy: 77.9221	gate_accuracy: 84.4156
	Task-3	val_accuracy: 88.8889	gate_accuracy: 91.6667
	Task-4	val_accuracy: 72.0930	gate_accuracy: 70.9302
	Task-5	val_accuracy: 79.1667	gate_accuracy: 75.0000
	Task-6	val_accuracy: 80.3279	gate_accuracy: 80.3279
	Task-7	val_accuracy: 89.0244	gate_accuracy: 90.2439
	Task-8	val_accuracy: 78.4091	gate_accuracy: 81.8182
	Task-9	val_accuracy: 61.9048	gate_accuracy: 54.7619
	Task-10	val_accuracy: 83.7500	gate_accuracy: 83.7500
	Task-11	val_accuracy: 78.8235	gate_accuracy: 80.0000
	Task-12	val_accuracy: 75.6410	gate_accuracy: 73.0769
	Task-13	val_accuracy: 85.3659	gate_accuracy: 85.3659
	Task-14	val_accuracy: 77.9221	gate_accuracy: 72.7273
	Task-15	val_accuracy: 75.9036	gate_accuracy: 75.9036
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.7912


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.1225	step1_train_accuracy: 56.9231
STEP-1	Epoch: 20/50	loss: 0.8340	step1_train_accuracy: 88.9231
STEP-1	Epoch: 30/50	loss: 0.4536	step1_train_accuracy: 96.3077
STEP-1	Epoch: 40/50	loss: 0.2637	step1_train_accuracy: 98.4615
STEP-1	Epoch: 50/50	loss: 0.1726	step1_train_accuracy: 99.3846
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.4723	gate_loss: 0.5430	step2_classification_accuracy: 85.1649	step_2_gate_accuracy: 83.1693
STEP-2	Epoch: 40/200	classification_loss: 0.3225	gate_loss: 0.2822	step2_classification_accuracy: 89.2200	step_2_gate_accuracy: 90.3864
STEP-2	Epoch: 60/200	classification_loss: 0.2667	gate_loss: 0.2109	step2_classification_accuracy: 90.6780	step_2_gate_accuracy: 92.3000
STEP-2	Epoch: 80/200	classification_loss: 0.2281	gate_loss: 0.1700	step2_classification_accuracy: 91.5528	step_2_gate_accuracy: 93.6213
STEP-2	Epoch: 100/200	classification_loss: 0.2115	gate_loss: 0.1515	step2_classification_accuracy: 92.1086	step_2_gate_accuracy: 94.4414
STEP-2	Epoch: 120/200	classification_loss: 0.1908	gate_loss: 0.1340	step2_classification_accuracy: 92.7100	step_2_gate_accuracy: 94.8606
STEP-2	Epoch: 140/200	classification_loss: 0.1789	gate_loss: 0.1203	step2_classification_accuracy: 93.1839	step_2_gate_accuracy: 95.4802
STEP-2	Epoch: 160/200	classification_loss: 0.1696	gate_loss: 0.1131	step2_classification_accuracy: 93.2203	step_2_gate_accuracy: 95.5805
STEP-2	Epoch: 180/200	classification_loss: 0.1665	gate_loss: 0.1098	step2_classification_accuracy: 93.4208	step_2_gate_accuracy: 95.7992
STEP-2	Epoch: 200/200	classification_loss: 0.1566	gate_loss: 0.1011	step2_classification_accuracy: 93.8035	step_2_gate_accuracy: 96.1363
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 81.9549	gate_accuracy: 86.4662
	Task-1	val_accuracy: 73.2673	gate_accuracy: 77.2277
	Task-2	val_accuracy: 81.8182	gate_accuracy: 84.4156
	Task-3	val_accuracy: 86.1111	gate_accuracy: 90.2778
	Task-4	val_accuracy: 70.9302	gate_accuracy: 67.4419
	Task-5	val_accuracy: 79.1667	gate_accuracy: 75.0000
	Task-6	val_accuracy: 80.3279	gate_accuracy: 72.1311
	Task-7	val_accuracy: 89.0244	gate_accuracy: 85.3659
	Task-8	val_accuracy: 81.8182	gate_accuracy: 84.0909
	Task-9	val_accuracy: 72.6190	gate_accuracy: 66.6667
	Task-10	val_accuracy: 76.2500	gate_accuracy: 78.7500
	Task-11	val_accuracy: 82.3529	gate_accuracy: 77.6471
	Task-12	val_accuracy: 76.9231	gate_accuracy: 73.0769
	Task-13	val_accuracy: 81.7073	gate_accuracy: 84.1463
	Task-14	val_accuracy: 77.9221	gate_accuracy: 77.9221
	Task-15	val_accuracy: 75.9036	gate_accuracy: 72.2892
	Task-16	val_accuracy: 82.7160	gate_accuracy: 81.4815
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.7623


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 2.0185	step1_train_accuracy: 59.0116
STEP-1	Epoch: 20/50	loss: 0.7362	step1_train_accuracy: 86.6279
STEP-1	Epoch: 30/50	loss: 0.4481	step1_train_accuracy: 95.3488
STEP-1	Epoch: 40/50	loss: 0.2905	step1_train_accuracy: 97.9651
STEP-1	Epoch: 50/50	loss: 0.2111	step1_train_accuracy: 98.2558
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.5264	gate_loss: 0.6148	step2_classification_accuracy: 83.5608	step_2_gate_accuracy: 81.2558
STEP-2	Epoch: 40/200	classification_loss: 0.3440	gate_loss: 0.3011	step2_classification_accuracy: 88.3552	step_2_gate_accuracy: 89.8949
STEP-2	Epoch: 60/200	classification_loss: 0.2743	gate_loss: 0.2113	step2_classification_accuracy: 90.5956	step_2_gate_accuracy: 92.5042
STEP-2	Epoch: 80/200	classification_loss: 0.2418	gate_loss: 0.1750	step2_classification_accuracy: 91.6651	step_2_gate_accuracy: 93.7581
STEP-2	Epoch: 100/200	classification_loss: 0.2102	gate_loss: 0.1445	step2_classification_accuracy: 92.4488	step_2_gate_accuracy: 94.6247
STEP-2	Epoch: 120/200	classification_loss: 0.2007	gate_loss: 0.1320	step2_classification_accuracy: 92.8084	step_2_gate_accuracy: 95.1411
STEP-2	Epoch: 140/200	classification_loss: 0.1853	gate_loss: 0.1204	step2_classification_accuracy: 93.1495	step_2_gate_accuracy: 95.3900
STEP-2	Epoch: 160/200	classification_loss: 0.1790	gate_loss: 0.1148	step2_classification_accuracy: 93.4999	step_2_gate_accuracy: 95.8418
STEP-2	Epoch: 180/200	classification_loss: 0.1640	gate_loss: 0.1012	step2_classification_accuracy: 93.6843	step_2_gate_accuracy: 96.2751
STEP-2	Epoch: 200/200	classification_loss: 0.1563	gate_loss: 0.0952	step2_classification_accuracy: 93.9701	step_2_gate_accuracy: 96.4134
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.6842	gate_accuracy: 81.2030
	Task-1	val_accuracy: 70.2970	gate_accuracy: 82.1782
	Task-2	val_accuracy: 75.3247	gate_accuracy: 79.2208
	Task-3	val_accuracy: 87.5000	gate_accuracy: 93.0556
	Task-4	val_accuracy: 70.9302	gate_accuracy: 74.4186
	Task-5	val_accuracy: 72.2222	gate_accuracy: 72.2222
	Task-6	val_accuracy: 77.0492	gate_accuracy: 72.1311
	Task-7	val_accuracy: 85.3659	gate_accuracy: 85.3659
	Task-8	val_accuracy: 81.8182	gate_accuracy: 85.2273
	Task-9	val_accuracy: 65.4762	gate_accuracy: 63.0952
	Task-10	val_accuracy: 78.7500	gate_accuracy: 78.7500
	Task-11	val_accuracy: 74.1176	gate_accuracy: 70.5882
	Task-12	val_accuracy: 71.7949	gate_accuracy: 73.0769
	Task-13	val_accuracy: 81.7073	gate_accuracy: 82.9268
	Task-14	val_accuracy: 77.9221	gate_accuracy: 76.6234
	Task-15	val_accuracy: 73.4940	gate_accuracy: 75.9036
	Task-16	val_accuracy: 82.7160	gate_accuracy: 82.7160
	Task-17	val_accuracy: 81.3953	gate_accuracy: 81.3953
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.5146


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 2.2794	step1_train_accuracy: 50.5988
STEP-1	Epoch: 20/50	loss: 0.9851	step1_train_accuracy: 78.4431
STEP-1	Epoch: 30/50	loss: 0.6163	step1_train_accuracy: 87.1257
STEP-1	Epoch: 40/50	loss: 0.4600	step1_train_accuracy: 91.6168
STEP-1	Epoch: 50/50	loss: 0.3705	step1_train_accuracy: 91.3174
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.5345	gate_loss: 0.6245	step2_classification_accuracy: 83.0034	step_2_gate_accuracy: 80.6599
STEP-2	Epoch: 40/200	classification_loss: 0.3710	gate_loss: 0.3199	step2_classification_accuracy: 87.6565	step_2_gate_accuracy: 88.9594
STEP-2	Epoch: 60/200	classification_loss: 0.3002	gate_loss: 0.2364	step2_classification_accuracy: 89.4755	step_2_gate_accuracy: 91.4382
STEP-2	Epoch: 80/200	classification_loss: 0.2609	gate_loss: 0.1944	step2_classification_accuracy: 90.4907	step_2_gate_accuracy: 92.5465
STEP-2	Epoch: 100/200	classification_loss: 0.2340	gate_loss: 0.1679	step2_classification_accuracy: 91.1929	step_2_gate_accuracy: 93.5110
STEP-2	Epoch: 120/200	classification_loss: 0.2238	gate_loss: 0.1597	step2_classification_accuracy: 91.6413	step_2_gate_accuracy: 93.9425
STEP-2	Epoch: 140/200	classification_loss: 0.2122	gate_loss: 0.1446	step2_classification_accuracy: 91.8782	step_2_gate_accuracy: 94.3655
STEP-2	Epoch: 160/200	classification_loss: 0.1973	gate_loss: 0.1298	step2_classification_accuracy: 92.4619	step_2_gate_accuracy: 94.9069
STEP-2	Epoch: 180/200	classification_loss: 0.1916	gate_loss: 0.1253	step2_classification_accuracy: 92.4873	step_2_gate_accuracy: 95.0000
STEP-2	Epoch: 200/200	classification_loss: 0.1869	gate_loss: 0.1201	step2_classification_accuracy: 92.6734	step_2_gate_accuracy: 95.3722
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.9624	gate_accuracy: 87.9699
	Task-1	val_accuracy: 66.3366	gate_accuracy: 67.3267
	Task-2	val_accuracy: 77.9221	gate_accuracy: 80.5195
	Task-3	val_accuracy: 87.5000	gate_accuracy: 93.0556
	Task-4	val_accuracy: 73.2558	gate_accuracy: 74.4186
	Task-5	val_accuracy: 70.8333	gate_accuracy: 70.8333
	Task-6	val_accuracy: 78.6885	gate_accuracy: 80.3279
	Task-7	val_accuracy: 81.7073	gate_accuracy: 82.9268
	Task-8	val_accuracy: 79.5455	gate_accuracy: 80.6818
	Task-9	val_accuracy: 65.4762	gate_accuracy: 63.0952
	Task-10	val_accuracy: 82.5000	gate_accuracy: 83.7500
	Task-11	val_accuracy: 82.3529	gate_accuracy: 81.1765
	Task-12	val_accuracy: 71.7949	gate_accuracy: 67.9487
	Task-13	val_accuracy: 82.9268	gate_accuracy: 80.4878
	Task-14	val_accuracy: 76.6234	gate_accuracy: 75.3247
	Task-15	val_accuracy: 69.8795	gate_accuracy: 66.2651
	Task-16	val_accuracy: 85.1852	gate_accuracy: 82.7160
	Task-17	val_accuracy: 84.8837	gate_accuracy: 77.9070
	Task-18	val_accuracy: 63.0952	gate_accuracy: 54.7619
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.5075


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.7137	step1_train_accuracy: 56.9733
STEP-1	Epoch: 20/50	loss: 0.9158	step1_train_accuracy: 82.7893
STEP-1	Epoch: 30/50	loss: 0.4179	step1_train_accuracy: 92.5816
STEP-1	Epoch: 40/50	loss: 0.2823	step1_train_accuracy: 94.6588
STEP-1	Epoch: 50/50	loss: 0.2163	step1_train_accuracy: 95.5490
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.5632	gate_loss: 0.6061	step2_classification_accuracy: 81.9293	step_2_gate_accuracy: 80.7065
STEP-2	Epoch: 40/200	classification_loss: 0.4074	gate_loss: 0.3369	step2_classification_accuracy: 86.2017	step_2_gate_accuracy: 88.1869
STEP-2	Epoch: 60/200	classification_loss: 0.3313	gate_loss: 0.2485	step2_classification_accuracy: 88.3077	step_2_gate_accuracy: 91.0779
STEP-2	Epoch: 80/200	classification_loss: 0.2967	gate_loss: 0.2085	step2_classification_accuracy: 89.3418	step_2_gate_accuracy: 92.1422
STEP-2	Epoch: 100/200	classification_loss: 0.2680	gate_loss: 0.1788	step2_classification_accuracy: 90.5193	step_2_gate_accuracy: 93.5688
STEP-2	Epoch: 120/200	classification_loss: 0.2511	gate_loss: 0.1641	step2_classification_accuracy: 90.5873	step_2_gate_accuracy: 93.7198
STEP-2	Epoch: 140/200	classification_loss: 0.2330	gate_loss: 0.1472	step2_classification_accuracy: 91.1232	step_2_gate_accuracy: 94.3614
STEP-2	Epoch: 160/200	classification_loss: 0.2156	gate_loss: 0.1339	step2_classification_accuracy: 91.8931	step_2_gate_accuracy: 94.7766
STEP-2	Epoch: 180/200	classification_loss: 0.2104	gate_loss: 0.1284	step2_classification_accuracy: 91.8629	step_2_gate_accuracy: 95.0634
STEP-2	Epoch: 200/200	classification_loss: 0.1969	gate_loss: 0.1182	step2_classification_accuracy: 92.2932	step_2_gate_accuracy: 95.5314
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 78.1955	gate_accuracy: 85.7143
	Task-1	val_accuracy: 64.3564	gate_accuracy: 75.2475
	Task-2	val_accuracy: 83.1169	gate_accuracy: 85.7143
	Task-3	val_accuracy: 80.5556	gate_accuracy: 83.3333
	Task-4	val_accuracy: 67.4419	gate_accuracy: 67.4419
	Task-5	val_accuracy: 79.1667	gate_accuracy: 80.5556
	Task-6	val_accuracy: 72.1311	gate_accuracy: 73.7705
	Task-7	val_accuracy: 76.8293	gate_accuracy: 76.8293
	Task-8	val_accuracy: 75.0000	gate_accuracy: 76.1364
	Task-9	val_accuracy: 60.7143	gate_accuracy: 60.7143
	Task-10	val_accuracy: 77.5000	gate_accuracy: 78.7500
	Task-11	val_accuracy: 76.4706	gate_accuracy: 78.8235
	Task-12	val_accuracy: 74.3590	gate_accuracy: 70.5128
	Task-13	val_accuracy: 78.0488	gate_accuracy: 80.4878
	Task-14	val_accuracy: 76.6234	gate_accuracy: 74.0260
	Task-15	val_accuracy: 74.6988	gate_accuracy: 72.2892
	Task-16	val_accuracy: 80.2469	gate_accuracy: 79.0123
	Task-17	val_accuracy: 87.2093	gate_accuracy: 83.7209
	Task-18	val_accuracy: 60.7143	gate_accuracy: 63.0952
	Task-19	val_accuracy: 65.4762	gate_accuracy: 69.0476
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.9547


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.3309	step1_train_accuracy: 48.0597
STEP-1	Epoch: 20/50	loss: 1.0318	step1_train_accuracy: 86.8657
STEP-1	Epoch: 30/50	loss: 0.5547	step1_train_accuracy: 93.1343
STEP-1	Epoch: 40/50	loss: 0.3667	step1_train_accuracy: 94.6269
STEP-1	Epoch: 50/50	loss: 0.2770	step1_train_accuracy: 94.9254
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.5771	gate_loss: 0.6231	step2_classification_accuracy: 81.7675	step_2_gate_accuracy: 80.7418
STEP-2	Epoch: 40/200	classification_loss: 0.4069	gate_loss: 0.3402	step2_classification_accuracy: 86.7103	step_2_gate_accuracy: 88.3083
STEP-2	Epoch: 60/200	classification_loss: 0.3355	gate_loss: 0.2563	step2_classification_accuracy: 88.3826	step_2_gate_accuracy: 90.5010
STEP-2	Epoch: 80/200	classification_loss: 0.3001	gate_loss: 0.2164	step2_classification_accuracy: 89.5719	step_2_gate_accuracy: 92.0767
STEP-2	Epoch: 100/200	classification_loss: 0.2703	gate_loss: 0.1858	step2_classification_accuracy: 90.1814	step_2_gate_accuracy: 93.0355
STEP-2	Epoch: 120/200	classification_loss: 0.2449	gate_loss: 0.1655	step2_classification_accuracy: 90.9321	step_2_gate_accuracy: 93.8606
STEP-2	Epoch: 140/200	classification_loss: 0.2353	gate_loss: 0.1567	step2_classification_accuracy: 91.2517	step_2_gate_accuracy: 94.0018
STEP-2	Epoch: 160/200	classification_loss: 0.2274	gate_loss: 0.1470	step2_classification_accuracy: 91.6159	step_2_gate_accuracy: 94.3883
STEP-2	Epoch: 180/200	classification_loss: 0.2072	gate_loss: 0.1301	step2_classification_accuracy: 92.2254	step_2_gate_accuracy: 95.0647
STEP-2	Epoch: 200/200	classification_loss: 0.2021	gate_loss: 0.1237	step2_classification_accuracy: 92.4781	step_2_gate_accuracy: 95.4958
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.6992	gate_accuracy: 88.7218
	Task-1	val_accuracy: 69.3069	gate_accuracy: 74.2574
	Task-2	val_accuracy: 80.5195	gate_accuracy: 89.6104
	Task-3	val_accuracy: 81.9444	gate_accuracy: 86.1111
	Task-4	val_accuracy: 70.9302	gate_accuracy: 67.4419
	Task-5	val_accuracy: 75.0000	gate_accuracy: 73.6111
	Task-6	val_accuracy: 77.0492	gate_accuracy: 75.4098
	Task-7	val_accuracy: 78.0488	gate_accuracy: 80.4878
	Task-8	val_accuracy: 78.4091	gate_accuracy: 79.5455
	Task-9	val_accuracy: 66.6667	gate_accuracy: 63.0952
	Task-10	val_accuracy: 81.2500	gate_accuracy: 77.5000
	Task-11	val_accuracy: 77.6471	gate_accuracy: 72.9412
	Task-12	val_accuracy: 73.0769	gate_accuracy: 67.9487
	Task-13	val_accuracy: 82.9268	gate_accuracy: 89.0244
	Task-14	val_accuracy: 75.3247	gate_accuracy: 74.0260
	Task-15	val_accuracy: 77.1084	gate_accuracy: 68.6747
	Task-16	val_accuracy: 87.6543	gate_accuracy: 80.2469
	Task-17	val_accuracy: 79.0698	gate_accuracy: 77.9070
	Task-18	val_accuracy: 58.3333	gate_accuracy: 57.1429
	Task-19	val_accuracy: 65.4762	gate_accuracy: 65.4762
	Task-20	val_accuracy: 80.9524	gate_accuracy: 80.9524
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.9659


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.3356	step1_train_accuracy: 55.9375
STEP-1	Epoch: 20/50	loss: 0.8451	step1_train_accuracy: 85.0000
STEP-1	Epoch: 30/50	loss: 0.4828	step1_train_accuracy: 93.1250
STEP-1	Epoch: 40/50	loss: 0.3343	step1_train_accuracy: 93.7500
STEP-1	Epoch: 50/50	loss: 0.2501	step1_train_accuracy: 95.0000
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6308	gate_loss: 0.6747	step2_classification_accuracy: 80.7269	step_2_gate_accuracy: 78.9060
STEP-2	Epoch: 40/200	classification_loss: 0.4401	gate_loss: 0.3680	step2_classification_accuracy: 85.7269	step_2_gate_accuracy: 87.1953
STEP-2	Epoch: 60/200	classification_loss: 0.3633	gate_loss: 0.2730	step2_classification_accuracy: 88.0029	step_2_gate_accuracy: 90.2643
STEP-2	Epoch: 80/200	classification_loss: 0.3125	gate_loss: 0.2229	step2_classification_accuracy: 89.2878	step_2_gate_accuracy: 91.7915
STEP-2	Epoch: 100/200	classification_loss: 0.2854	gate_loss: 0.1958	step2_classification_accuracy: 89.9046	step_2_gate_accuracy: 92.9001
STEP-2	Epoch: 120/200	classification_loss: 0.2642	gate_loss: 0.1757	step2_classification_accuracy: 90.4479	step_2_gate_accuracy: 93.4214
STEP-2	Epoch: 140/200	classification_loss: 0.2502	gate_loss: 0.1611	step2_classification_accuracy: 91.0646	step_2_gate_accuracy: 93.9721
STEP-2	Epoch: 160/200	classification_loss: 0.2337	gate_loss: 0.1484	step2_classification_accuracy: 91.4391	step_2_gate_accuracy: 94.5007
STEP-2	Epoch: 180/200	classification_loss: 0.2261	gate_loss: 0.1409	step2_classification_accuracy: 91.6520	step_2_gate_accuracy: 94.7504
STEP-2	Epoch: 200/200	classification_loss: 0.2169	gate_loss: 0.1322	step2_classification_accuracy: 92.1806	step_2_gate_accuracy: 95.1615
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.6842	gate_accuracy: 80.4511
	Task-1	val_accuracy: 69.3069	gate_accuracy: 77.2277
	Task-2	val_accuracy: 85.7143	gate_accuracy: 87.0130
	Task-3	val_accuracy: 86.1111	gate_accuracy: 91.6667
	Task-4	val_accuracy: 67.4419	gate_accuracy: 69.7674
	Task-5	val_accuracy: 76.3889	gate_accuracy: 73.6111
	Task-6	val_accuracy: 68.8525	gate_accuracy: 70.4918
	Task-7	val_accuracy: 84.1463	gate_accuracy: 82.9268
	Task-8	val_accuracy: 76.1364	gate_accuracy: 77.2727
	Task-9	val_accuracy: 64.2857	gate_accuracy: 59.5238
	Task-10	val_accuracy: 77.5000	gate_accuracy: 80.0000
	Task-11	val_accuracy: 72.9412	gate_accuracy: 68.2353
	Task-12	val_accuracy: 73.0769	gate_accuracy: 66.6667
	Task-13	val_accuracy: 78.0488	gate_accuracy: 79.2683
	Task-14	val_accuracy: 71.4286	gate_accuracy: 71.4286
	Task-15	val_accuracy: 75.9036	gate_accuracy: 74.6988
	Task-16	val_accuracy: 81.4815	gate_accuracy: 77.7778
	Task-17	val_accuracy: 88.3721	gate_accuracy: 86.0465
	Task-18	val_accuracy: 60.7143	gate_accuracy: 60.7143
	Task-19	val_accuracy: 71.4286	gate_accuracy: 72.6190
	Task-20	val_accuracy: 78.5714	gate_accuracy: 69.0476
	Task-21	val_accuracy: 77.5000	gate_accuracy: 76.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 75.2174


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.9292	step1_train_accuracy: 54.5775
STEP-1	Epoch: 20/50	loss: 0.9741	step1_train_accuracy: 85.9155
STEP-1	Epoch: 30/50	loss: 0.4354	step1_train_accuracy: 96.8310
STEP-1	Epoch: 40/50	loss: 0.2669	step1_train_accuracy: 96.8310
STEP-1	Epoch: 50/50	loss: 0.1916	step1_train_accuracy: 98.5916
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6356	gate_loss: 0.6899	step2_classification_accuracy: 80.3516	step_2_gate_accuracy: 78.9381
STEP-2	Epoch: 40/200	classification_loss: 0.4527	gate_loss: 0.3809	step2_classification_accuracy: 85.5556	step_2_gate_accuracy: 87.0323
STEP-2	Epoch: 60/200	classification_loss: 0.3665	gate_loss: 0.2833	step2_classification_accuracy: 87.6442	step_2_gate_accuracy: 89.8523
STEP-2	Epoch: 80/200	classification_loss: 0.3295	gate_loss: 0.2400	step2_classification_accuracy: 88.8045	step_2_gate_accuracy: 91.1674
STEP-2	Epoch: 100/200	classification_loss: 0.2982	gate_loss: 0.2092	step2_classification_accuracy: 89.5710	step_2_gate_accuracy: 92.2996
STEP-2	Epoch: 120/200	classification_loss: 0.2804	gate_loss: 0.1911	step2_classification_accuracy: 90.1266	step_2_gate_accuracy: 92.8200
STEP-2	Epoch: 140/200	classification_loss: 0.2562	gate_loss: 0.1686	step2_classification_accuracy: 90.9845	step_2_gate_accuracy: 93.6709
STEP-2	Epoch: 160/200	classification_loss: 0.2428	gate_loss: 0.1573	step2_classification_accuracy: 91.1955	step_2_gate_accuracy: 94.1210
STEP-2	Epoch: 180/200	classification_loss: 0.2390	gate_loss: 0.1541	step2_classification_accuracy: 91.3572	step_2_gate_accuracy: 94.1561
STEP-2	Epoch: 200/200	classification_loss: 0.2234	gate_loss: 0.1411	step2_classification_accuracy: 91.7932	step_2_gate_accuracy: 94.7468
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.1880	gate_accuracy: 84.9624
	Task-1	val_accuracy: 68.3168	gate_accuracy: 76.2376
	Task-2	val_accuracy: 79.2208	gate_accuracy: 79.2208
	Task-3	val_accuracy: 81.9444	gate_accuracy: 86.1111
	Task-4	val_accuracy: 69.7674	gate_accuracy: 72.0930
	Task-5	val_accuracy: 69.4444	gate_accuracy: 70.8333
	Task-6	val_accuracy: 68.8525	gate_accuracy: 68.8525
	Task-7	val_accuracy: 76.8293	gate_accuracy: 76.8293
	Task-8	val_accuracy: 76.1364	gate_accuracy: 77.2727
	Task-9	val_accuracy: 58.3333	gate_accuracy: 54.7619
	Task-10	val_accuracy: 77.5000	gate_accuracy: 75.0000
	Task-11	val_accuracy: 76.4706	gate_accuracy: 74.1176
	Task-12	val_accuracy: 58.9744	gate_accuracy: 57.6923
	Task-13	val_accuracy: 78.0488	gate_accuracy: 79.2683
	Task-14	val_accuracy: 72.7273	gate_accuracy: 70.1299
	Task-15	val_accuracy: 75.9036	gate_accuracy: 71.0843
	Task-16	val_accuracy: 86.4198	gate_accuracy: 81.4815
	Task-17	val_accuracy: 76.7442	gate_accuracy: 70.9302
	Task-18	val_accuracy: 58.3333	gate_accuracy: 58.3333
	Task-19	val_accuracy: 57.1429	gate_accuracy: 61.9048
	Task-20	val_accuracy: 79.7619	gate_accuracy: 76.1905
	Task-21	val_accuracy: 76.2500	gate_accuracy: 75.0000
	Task-22	val_accuracy: 74.6479	gate_accuracy: 77.4648
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.1554


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 2.5879	step1_train_accuracy: 49.8516
STEP-1	Epoch: 20/50	loss: 0.9204	step1_train_accuracy: 80.1187
STEP-1	Epoch: 30/50	loss: 0.4434	step1_train_accuracy: 94.3620
STEP-1	Epoch: 40/50	loss: 0.2732	step1_train_accuracy: 96.1424
STEP-1	Epoch: 50/50	loss: 0.1909	step1_train_accuracy: 96.7359
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.6405	gate_loss: 0.7078	step2_classification_accuracy: 79.8942	step_2_gate_accuracy: 77.7654
STEP-2	Epoch: 40/200	classification_loss: 0.4559	gate_loss: 0.4010	step2_classification_accuracy: 85.3141	step_2_gate_accuracy: 86.3524
STEP-2	Epoch: 60/200	classification_loss: 0.3721	gate_loss: 0.2995	step2_classification_accuracy: 87.5996	step_2_gate_accuracy: 89.5129
STEP-2	Epoch: 80/200	classification_loss: 0.3252	gate_loss: 0.2497	step2_classification_accuracy: 88.6574	step_2_gate_accuracy: 90.6230
STEP-2	Epoch: 100/200	classification_loss: 0.2927	gate_loss: 0.2160	step2_classification_accuracy: 89.6631	step_2_gate_accuracy: 92.2228
STEP-2	Epoch: 120/200	classification_loss: 0.2709	gate_loss: 0.1951	step2_classification_accuracy: 90.1789	step_2_gate_accuracy: 92.8693
STEP-2	Epoch: 140/200	classification_loss: 0.2542	gate_loss: 0.1779	step2_classification_accuracy: 90.6883	step_2_gate_accuracy: 93.3917
STEP-2	Epoch: 160/200	classification_loss: 0.2438	gate_loss: 0.1666	step2_classification_accuracy: 91.0017	step_2_gate_accuracy: 93.9271
STEP-2	Epoch: 180/200	classification_loss: 0.2250	gate_loss: 0.1526	step2_classification_accuracy: 91.7853	step_2_gate_accuracy: 94.4822
STEP-2	Epoch: 200/200	classification_loss: 0.2239	gate_loss: 0.1483	step2_classification_accuracy: 91.8571	step_2_gate_accuracy: 94.5605
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.6842	gate_accuracy: 81.9549
	Task-1	val_accuracy: 67.3267	gate_accuracy: 75.2475
	Task-2	val_accuracy: 80.5195	gate_accuracy: 81.8182
	Task-3	val_accuracy: 81.9444	gate_accuracy: 86.1111
	Task-4	val_accuracy: 68.6047	gate_accuracy: 69.7674
	Task-5	val_accuracy: 76.3889	gate_accuracy: 73.6111
	Task-6	val_accuracy: 68.8525	gate_accuracy: 70.4918
	Task-7	val_accuracy: 80.4878	gate_accuracy: 76.8293
	Task-8	val_accuracy: 73.8636	gate_accuracy: 77.2727
	Task-9	val_accuracy: 59.5238	gate_accuracy: 58.3333
	Task-10	val_accuracy: 81.2500	gate_accuracy: 77.5000
	Task-11	val_accuracy: 71.7647	gate_accuracy: 68.2353
	Task-12	val_accuracy: 65.3846	gate_accuracy: 58.9744
	Task-13	val_accuracy: 84.1463	gate_accuracy: 84.1463
	Task-14	val_accuracy: 64.9351	gate_accuracy: 64.9351
	Task-15	val_accuracy: 71.0843	gate_accuracy: 65.0602
	Task-16	val_accuracy: 83.9506	gate_accuracy: 76.5432
	Task-17	val_accuracy: 84.8837	gate_accuracy: 83.7209
	Task-18	val_accuracy: 65.4762	gate_accuracy: 63.0952
	Task-19	val_accuracy: 63.0952	gate_accuracy: 65.4762
	Task-20	val_accuracy: 77.3810	gate_accuracy: 79.7619
	Task-21	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-22	val_accuracy: 74.6479	gate_accuracy: 76.0563
	Task-23	val_accuracy: 80.9524	gate_accuracy: 79.7619
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.9348


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.7404	step1_train_accuracy: 45.3125
STEP-1	Epoch: 20/50	loss: 1.1992	step1_train_accuracy: 67.1875
STEP-1	Epoch: 30/50	loss: 0.6075	step1_train_accuracy: 91.5625
STEP-1	Epoch: 40/50	loss: 0.3838	step1_train_accuracy: 96.2500
STEP-1	Epoch: 50/50	loss: 0.2704	step1_train_accuracy: 97.5000
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6688	gate_loss: 0.7348	step2_classification_accuracy: 79.6239	step_2_gate_accuracy: 77.1336
STEP-2	Epoch: 40/200	classification_loss: 0.4746	gate_loss: 0.4109	step2_classification_accuracy: 84.9935	step_2_gate_accuracy: 86.1219
STEP-2	Epoch: 60/200	classification_loss: 0.3886	gate_loss: 0.3073	step2_classification_accuracy: 87.5681	step_2_gate_accuracy: 89.2477
STEP-2	Epoch: 80/200	classification_loss: 0.3374	gate_loss: 0.2514	step2_classification_accuracy: 88.8911	step_2_gate_accuracy: 91.2451
STEP-2	Epoch: 100/200	classification_loss: 0.3084	gate_loss: 0.2255	step2_classification_accuracy: 89.4358	step_2_gate_accuracy: 92.1012
STEP-2	Epoch: 120/200	classification_loss: 0.2811	gate_loss: 0.1964	step2_classification_accuracy: 90.4150	step_2_gate_accuracy: 93.0610
STEP-2	Epoch: 140/200	classification_loss: 0.2680	gate_loss: 0.1853	step2_classification_accuracy: 90.8042	step_2_gate_accuracy: 93.2815
STEP-2	Epoch: 160/200	classification_loss: 0.2565	gate_loss: 0.1722	step2_classification_accuracy: 91.0182	step_2_gate_accuracy: 93.8132
STEP-2	Epoch: 180/200	classification_loss: 0.2387	gate_loss: 0.1590	step2_classification_accuracy: 91.3813	step_2_gate_accuracy: 94.2088
STEP-2	Epoch: 200/200	classification_loss: 0.2329	gate_loss: 0.1539	step2_classification_accuracy: 91.6991	step_2_gate_accuracy: 94.4293
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.6767	gate_accuracy: 78.9474
	Task-1	val_accuracy: 68.3168	gate_accuracy: 76.2376
	Task-2	val_accuracy: 75.3247	gate_accuracy: 77.9221
	Task-3	val_accuracy: 86.1111	gate_accuracy: 91.6667
	Task-4	val_accuracy: 67.4419	gate_accuracy: 69.7674
	Task-5	val_accuracy: 69.4444	gate_accuracy: 66.6667
	Task-6	val_accuracy: 75.4098	gate_accuracy: 67.2131
	Task-7	val_accuracy: 76.8293	gate_accuracy: 74.3902
	Task-8	val_accuracy: 76.1364	gate_accuracy: 79.5455
	Task-9	val_accuracy: 60.7143	gate_accuracy: 57.1429
	Task-10	val_accuracy: 76.2500	gate_accuracy: 75.0000
	Task-11	val_accuracy: 74.1176	gate_accuracy: 65.8824
	Task-12	val_accuracy: 70.5128	gate_accuracy: 66.6667
	Task-13	val_accuracy: 79.2683	gate_accuracy: 82.9268
	Task-14	val_accuracy: 71.4286	gate_accuracy: 71.4286
	Task-15	val_accuracy: 71.0843	gate_accuracy: 63.8554
	Task-16	val_accuracy: 79.0123	gate_accuracy: 76.5432
	Task-17	val_accuracy: 76.7442	gate_accuracy: 73.2558
	Task-18	val_accuracy: 63.0952	gate_accuracy: 71.4286
	Task-19	val_accuracy: 59.5238	gate_accuracy: 61.9048
	Task-20	val_accuracy: 78.5714	gate_accuracy: 83.3333
	Task-21	val_accuracy: 72.5000	gate_accuracy: 77.5000
	Task-22	val_accuracy: 76.0563	gate_accuracy: 76.0563
	Task-23	val_accuracy: 79.7619	gate_accuracy: 80.9524
	Task-24	val_accuracy: 71.2500	gate_accuracy: 72.5000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.6868


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 2.3372	step1_train_accuracy: 56.2874
STEP-1	Epoch: 20/50	loss: 0.9427	step1_train_accuracy: 79.6407
STEP-1	Epoch: 30/50	loss: 0.4733	step1_train_accuracy: 91.9162
STEP-1	Epoch: 40/50	loss: 0.3055	step1_train_accuracy: 93.1138
STEP-1	Epoch: 50/50	loss: 0.2213	step1_train_accuracy: 95.2096
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.6910	gate_loss: 0.7350	step2_classification_accuracy: 78.9779	step_2_gate_accuracy: 77.2562
STEP-2	Epoch: 40/200	classification_loss: 0.4898	gate_loss: 0.4225	step2_classification_accuracy: 84.5355	step_2_gate_accuracy: 85.7859
STEP-2	Epoch: 60/200	classification_loss: 0.3989	gate_loss: 0.3187	step2_classification_accuracy: 86.8310	step_2_gate_accuracy: 88.6916
STEP-2	Epoch: 80/200	classification_loss: 0.3462	gate_loss: 0.2615	step2_classification_accuracy: 88.2929	step_2_gate_accuracy: 90.6971
STEP-2	Epoch: 100/200	classification_loss: 0.3111	gate_loss: 0.2280	step2_classification_accuracy: 89.2473	step_2_gate_accuracy: 91.6274
STEP-2	Epoch: 120/200	classification_loss: 0.2887	gate_loss: 0.2064	step2_classification_accuracy: 89.9420	step_2_gate_accuracy: 92.4550
STEP-2	Epoch: 140/200	classification_loss: 0.2668	gate_loss: 0.1858	step2_classification_accuracy: 90.4072	step_2_gate_accuracy: 93.0289
STEP-2	Epoch: 160/200	classification_loss: 0.2561	gate_loss: 0.1739	step2_classification_accuracy: 90.7938	step_2_gate_accuracy: 93.7357
STEP-2	Epoch: 180/200	classification_loss: 0.2492	gate_loss: 0.1687	step2_classification_accuracy: 90.8663	step_2_gate_accuracy: 93.8444
STEP-2	Epoch: 200/200	classification_loss: 0.2394	gate_loss: 0.1581	step2_classification_accuracy: 91.2408	step_2_gate_accuracy: 94.2310
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.1805	gate_accuracy: 83.4586
	Task-1	val_accuracy: 64.3564	gate_accuracy: 72.2772
	Task-2	val_accuracy: 77.9221	gate_accuracy: 80.5195
	Task-3	val_accuracy: 81.9444	gate_accuracy: 81.9444
	Task-4	val_accuracy: 73.2558	gate_accuracy: 73.2558
	Task-5	val_accuracy: 72.2222	gate_accuracy: 70.8333
	Task-6	val_accuracy: 73.7705	gate_accuracy: 73.7705
	Task-7	val_accuracy: 79.2683	gate_accuracy: 80.4878
	Task-8	val_accuracy: 77.2727	gate_accuracy: 75.0000
	Task-9	val_accuracy: 61.9048	gate_accuracy: 57.1429
	Task-10	val_accuracy: 80.0000	gate_accuracy: 82.5000
	Task-11	val_accuracy: 77.6471	gate_accuracy: 75.2941
	Task-12	val_accuracy: 66.6667	gate_accuracy: 67.9487
	Task-13	val_accuracy: 82.9268	gate_accuracy: 84.1463
	Task-14	val_accuracy: 74.0260	gate_accuracy: 68.8312
	Task-15	val_accuracy: 77.1084	gate_accuracy: 68.6747
	Task-16	val_accuracy: 81.4815	gate_accuracy: 80.2469
	Task-17	val_accuracy: 87.2093	gate_accuracy: 88.3721
	Task-18	val_accuracy: 60.7143	gate_accuracy: 60.7143
	Task-19	val_accuracy: 59.5238	gate_accuracy: 61.9048
	Task-20	val_accuracy: 77.3810	gate_accuracy: 79.7619
	Task-21	val_accuracy: 72.5000	gate_accuracy: 72.5000
	Task-22	val_accuracy: 71.8310	gate_accuracy: 73.2394
	Task-23	val_accuracy: 70.2381	gate_accuracy: 69.0476
	Task-24	val_accuracy: 66.2500	gate_accuracy: 65.0000
	Task-25	val_accuracy: 64.2857	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.7842


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 2.6020	step1_train_accuracy: 49.5268
STEP-1	Epoch: 20/50	loss: 1.1467	step1_train_accuracy: 77.6025
STEP-1	Epoch: 30/50	loss: 0.6398	step1_train_accuracy: 85.8044
STEP-1	Epoch: 40/50	loss: 0.4552	step1_train_accuracy: 91.1672
STEP-1	Epoch: 50/50	loss: 0.3505	step1_train_accuracy: 91.1672
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30, 514: 30, 515: 30, 516: 30, 517: 30, 518: 30, 519: 30, 520: 30, 521: 30, 522: 30, 523: 30, 524: 30, 525: 30, 526: 30, 527: 30, 528: 30, 529: 30, 530: 30, 531: 30, 532: 30, 533: 30, 534: 30, 535: 30, 536: 30, 537: 30, 538: 30, 539: 30, 540: 30, 541: 30, 542: 30, 543: 30, 544: 30, 545: 30, 546: 30, 547: 30, 548: 30, 549: 30, 550: 30, 551: 30, 552: 30, 553: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.7008	gate_loss: 0.7467	step2_classification_accuracy: 78.7786	step_2_gate_accuracy: 76.9555
STEP-2	Epoch: 40/200	classification_loss: 0.4993	gate_loss: 0.4283	step2_classification_accuracy: 84.5668	step_2_gate_accuracy: 85.5295
STEP-2	Epoch: 60/200	classification_loss: 0.4045	gate_loss: 0.3186	step2_classification_accuracy: 86.8652	step_2_gate_accuracy: 88.7665
STEP-2	Epoch: 80/200	classification_loss: 0.3560	gate_loss: 0.2652	step2_classification_accuracy: 88.3153	step_2_gate_accuracy: 90.3791
STEP-2	Epoch: 100/200	classification_loss: 0.3265	gate_loss: 0.2354	step2_classification_accuracy: 89.1456	step_2_gate_accuracy: 91.4380
STEP-2	Epoch: 120/200	classification_loss: 0.2980	gate_loss: 0.2088	step2_classification_accuracy: 89.7413	step_2_gate_accuracy: 92.2082
STEP-2	Epoch: 140/200	classification_loss: 0.2870	gate_loss: 0.1972	step2_classification_accuracy: 90.0301	step_2_gate_accuracy: 92.6655
STEP-2	Epoch: 160/200	classification_loss: 0.2675	gate_loss: 0.1789	step2_classification_accuracy: 90.6619	step_2_gate_accuracy: 93.3454
STEP-2	Epoch: 180/200	classification_loss: 0.2524	gate_loss: 0.1672	step2_classification_accuracy: 91.0890	step_2_gate_accuracy: 93.6823
STEP-2	Epoch: 200/200	classification_loss: 0.2391	gate_loss: 0.1569	step2_classification_accuracy: 91.4260	step_2_gate_accuracy: 94.2298
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.4060	gate_accuracy: 71.4286
	Task-1	val_accuracy: 65.3465	gate_accuracy: 68.3168
	Task-2	val_accuracy: 74.0260	gate_accuracy: 77.9221
	Task-3	val_accuracy: 77.7778	gate_accuracy: 84.7222
	Task-4	val_accuracy: 66.2791	gate_accuracy: 67.4419
	Task-5	val_accuracy: 70.8333	gate_accuracy: 75.0000
	Task-6	val_accuracy: 72.1311	gate_accuracy: 65.5738
	Task-7	val_accuracy: 82.9268	gate_accuracy: 84.1463
	Task-8	val_accuracy: 72.7273	gate_accuracy: 75.0000
	Task-9	val_accuracy: 59.5238	gate_accuracy: 51.1905
	Task-10	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-11	val_accuracy: 70.5882	gate_accuracy: 65.8824
	Task-12	val_accuracy: 62.8205	gate_accuracy: 61.5385
	Task-13	val_accuracy: 80.4878	gate_accuracy: 78.0488
	Task-14	val_accuracy: 70.1299	gate_accuracy: 67.5325
	Task-15	val_accuracy: 68.6747	gate_accuracy: 67.4699
	Task-16	val_accuracy: 81.4815	gate_accuracy: 75.3086
	Task-17	val_accuracy: 87.2093	gate_accuracy: 81.3953
	Task-18	val_accuracy: 60.7143	gate_accuracy: 60.7143
	Task-19	val_accuracy: 72.6190	gate_accuracy: 77.3810
	Task-20	val_accuracy: 79.7619	gate_accuracy: 78.5714
	Task-21	val_accuracy: 76.2500	gate_accuracy: 75.0000
	Task-22	val_accuracy: 78.8732	gate_accuracy: 76.0563
	Task-23	val_accuracy: 75.0000	gate_accuracy: 77.3810
	Task-24	val_accuracy: 65.0000	gate_accuracy: 65.0000
	Task-25	val_accuracy: 66.6667	gate_accuracy: 65.4762
	Task-26	val_accuracy: 49.3671	gate_accuracy: 65.8228
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.5371


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.0557	step1_train_accuracy: 58.3799
STEP-1	Epoch: 20/50	loss: 0.8119	step1_train_accuracy: 88.8268
STEP-1	Epoch: 30/50	loss: 0.4109	step1_train_accuracy: 94.4134
STEP-1	Epoch: 40/50	loss: 0.2849	step1_train_accuracy: 94.6927
STEP-1	Epoch: 50/50	loss: 0.2217	step1_train_accuracy: 94.9721
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.6909	gate_loss: 0.7325	step2_classification_accuracy: 79.0886	step_2_gate_accuracy: 77.3628
STEP-2	Epoch: 40/200	classification_loss: 0.5060	gate_loss: 0.4365	step2_classification_accuracy: 84.0266	step_2_gate_accuracy: 85.0120
STEP-2	Epoch: 60/200	classification_loss: 0.4131	gate_loss: 0.3296	step2_classification_accuracy: 86.3567	step_2_gate_accuracy: 88.1098
STEP-2	Epoch: 80/200	classification_loss: 0.3671	gate_loss: 0.2797	step2_classification_accuracy: 87.3258	step_2_gate_accuracy: 89.7594
STEP-2	Epoch: 100/200	classification_loss: 0.3220	gate_loss: 0.2343	step2_classification_accuracy: 88.8883	step_2_gate_accuracy: 91.5832
STEP-2	Epoch: 120/200	classification_loss: 0.2973	gate_loss: 0.2104	step2_classification_accuracy: 89.5361	step_2_gate_accuracy: 92.1712
STEP-2	Epoch: 140/200	classification_loss: 0.2867	gate_loss: 0.1991	step2_classification_accuracy: 89.6233	step_2_gate_accuracy: 92.8299
STEP-2	Epoch: 160/200	classification_loss: 0.2706	gate_loss: 0.1850	step2_classification_accuracy: 90.2276	step_2_gate_accuracy: 93.1239
STEP-2	Epoch: 180/200	classification_loss: 0.2546	gate_loss: 0.1716	step2_classification_accuracy: 90.8047	step_2_gate_accuracy: 93.7010
STEP-2	Epoch: 200/200	classification_loss: 0.2490	gate_loss: 0.1650	step2_classification_accuracy: 90.8537	step_2_gate_accuracy: 93.8534
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.4286	gate_accuracy: 75.1880
	Task-1	val_accuracy: 66.3366	gate_accuracy: 71.2871
	Task-2	val_accuracy: 81.8182	gate_accuracy: 83.1169
	Task-3	val_accuracy: 76.3889	gate_accuracy: 83.3333
	Task-4	val_accuracy: 65.1163	gate_accuracy: 68.6047
	Task-5	val_accuracy: 72.2222	gate_accuracy: 75.0000
	Task-6	val_accuracy: 68.8525	gate_accuracy: 65.5738
	Task-7	val_accuracy: 79.2683	gate_accuracy: 78.0488
	Task-8	val_accuracy: 75.0000	gate_accuracy: 77.2727
	Task-9	val_accuracy: 66.6667	gate_accuracy: 64.2857
	Task-10	val_accuracy: 77.5000	gate_accuracy: 76.2500
	Task-11	val_accuracy: 71.7647	gate_accuracy: 68.2353
	Task-12	val_accuracy: 61.5385	gate_accuracy: 56.4103
	Task-13	val_accuracy: 79.2683	gate_accuracy: 79.2683
	Task-14	val_accuracy: 70.1299	gate_accuracy: 71.4286
	Task-15	val_accuracy: 71.0843	gate_accuracy: 63.8554
	Task-16	val_accuracy: 76.5432	gate_accuracy: 75.3086
	Task-17	val_accuracy: 80.2326	gate_accuracy: 75.5814
	Task-18	val_accuracy: 63.0952	gate_accuracy: 61.9048
	Task-19	val_accuracy: 66.6667	gate_accuracy: 60.7143
	Task-20	val_accuracy: 82.1429	gate_accuracy: 83.3333
	Task-21	val_accuracy: 73.7500	gate_accuracy: 75.0000
	Task-22	val_accuracy: 70.4225	gate_accuracy: 70.4225
	Task-23	val_accuracy: 80.9524	gate_accuracy: 79.7619
	Task-24	val_accuracy: 62.5000	gate_accuracy: 66.2500
	Task-25	val_accuracy: 65.4762	gate_accuracy: 66.6667
	Task-26	val_accuracy: 45.5696	gate_accuracy: 46.8354
	Task-27	val_accuracy: 67.7778	gate_accuracy: 75.5556
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.3488


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 2.9277	step1_train_accuracy: 47.1698
STEP-1	Epoch: 20/50	loss: 0.9254	step1_train_accuracy: 82.7044
STEP-1	Epoch: 30/50	loss: 0.4758	step1_train_accuracy: 96.8553
STEP-1	Epoch: 40/50	loss: 0.2980	step1_train_accuracy: 98.1132
STEP-1	Epoch: 50/50	loss: 0.2027	step1_train_accuracy: 98.4277
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29, 374: 29, 375: 29, 376: 29, 377: 29, 378: 29, 379: 29, 380: 29, 381: 29, 382: 29, 383: 29, 384: 29, 385: 29, 386: 29, 387: 29, 388: 29, 389: 29, 390: 29, 391: 29, 392: 29, 393: 29, 394: 29, 395: 29, 396: 29, 397: 29, 398: 29, 399: 29, 400: 29, 401: 29, 402: 29, 403: 29, 404: 29, 405: 29, 406: 29, 407: 29, 408: 29, 409: 29, 410: 29, 411: 29, 412: 29, 413: 29, 414: 29, 415: 29, 416: 29, 417: 29, 418: 29, 419: 29, 420: 29, 421: 29, 422: 29, 423: 29, 424: 29, 425: 29, 426: 29, 427: 29, 428: 29, 429: 29, 430: 29, 431: 29, 432: 29, 433: 29, 434: 29, 435: 29, 436: 29, 437: 29, 438: 29, 439: 29, 440: 29, 441: 29, 442: 29, 443: 29, 444: 29, 445: 29, 446: 29, 447: 29, 448: 29, 449: 29, 450: 29, 451: 29, 452: 29, 453: 29, 454: 29, 455: 29, 456: 29, 457: 29, 458: 29, 459: 29, 460: 29, 461: 29, 462: 29, 463: 29, 464: 29, 465: 29, 466: 29, 467: 29, 468: 29, 469: 29, 470: 29, 471: 29, 472: 29, 473: 29, 474: 29, 475: 29, 476: 29, 477: 29, 478: 29, 479: 29, 480: 29, 481: 29, 482: 29, 483: 29, 484: 29, 485: 29, 486: 29, 487: 29, 488: 29, 489: 29, 490: 29, 491: 29, 492: 29, 493: 29, 494: 29, 495: 29, 496: 29, 497: 29, 498: 29, 499: 29, 500: 29, 501: 29, 502: 29, 503: 29, 504: 29, 505: 29, 506: 29, 507: 29, 508: 29, 509: 29, 510: 29, 511: 29, 512: 29, 513: 29, 514: 29, 515: 29, 516: 29, 517: 29, 518: 29, 519: 29, 520: 29, 521: 29, 522: 29, 523: 29, 524: 29, 525: 29, 526: 29, 527: 29, 528: 29, 529: 29, 530: 29, 531: 29, 532: 29, 533: 29, 534: 29, 535: 29, 536: 29, 537: 29, 538: 29, 539: 29, 540: 29, 541: 29, 542: 29, 543: 29, 544: 29, 545: 29, 546: 29, 547: 29, 548: 29, 549: 29, 550: 29, 551: 29, 552: 29, 553: 29, 554: 29, 555: 29, 556: 29, 557: 29, 558: 29, 559: 29, 560: 29, 561: 29, 562: 29, 563: 29, 564: 29, 565: 29, 566: 29, 567: 29, 568: 29, 569: 29, 570: 29, 571: 29, 572: 29, 573: 29, 574: 29, 575: 29, 576: 29, 577: 29, 578: 29, 579: 29, 580: 29, 581: 29, 582: 29, 583: 29, 584: 29, 585: 29, 586: 29, 587: 29, 588: 29, 589: 29, 590: 29, 591: 29, 592: 29, 593: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.7188	gate_loss: 0.8096	step2_classification_accuracy: 77.8532	step_2_gate_accuracy: 75.3686
STEP-2	Epoch: 40/200	classification_loss: 0.5145	gate_loss: 0.4566	step2_classification_accuracy: 84.0300	step_2_gate_accuracy: 84.9414
STEP-2	Epoch: 60/200	classification_loss: 0.4267	gate_loss: 0.3481	step2_classification_accuracy: 86.0908	step_2_gate_accuracy: 87.8149
STEP-2	Epoch: 80/200	classification_loss: 0.3704	gate_loss: 0.2854	step2_classification_accuracy: 87.7627	step_2_gate_accuracy: 89.8177
STEP-2	Epoch: 100/200	classification_loss: 0.3373	gate_loss: 0.2506	step2_classification_accuracy: 88.7321	step_2_gate_accuracy: 91.1239
STEP-2	Epoch: 120/200	classification_loss: 0.3038	gate_loss: 0.2194	step2_classification_accuracy: 89.4636	step_2_gate_accuracy: 92.1166
STEP-2	Epoch: 140/200	classification_loss: 0.2888	gate_loss: 0.2029	step2_classification_accuracy: 90.0557	step_2_gate_accuracy: 92.6506
STEP-2	Epoch: 160/200	classification_loss: 0.2652	gate_loss: 0.1830	step2_classification_accuracy: 90.5201	step_2_gate_accuracy: 93.3415
STEP-2	Epoch: 180/200	classification_loss: 0.2616	gate_loss: 0.1769	step2_classification_accuracy: 90.8685	step_2_gate_accuracy: 93.6433
STEP-2	Epoch: 200/200	classification_loss: 0.2426	gate_loss: 0.1614	step2_classification_accuracy: 91.5941	step_2_gate_accuracy: 94.3167
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.9173	gate_accuracy: 76.6917
	Task-1	val_accuracy: 68.3168	gate_accuracy: 71.2871
	Task-2	val_accuracy: 80.5195	gate_accuracy: 84.4156
	Task-3	val_accuracy: 84.7222	gate_accuracy: 86.1111
	Task-4	val_accuracy: 66.2791	gate_accuracy: 61.6279
	Task-5	val_accuracy: 65.2778	gate_accuracy: 66.6667
	Task-6	val_accuracy: 77.0492	gate_accuracy: 73.7705
	Task-7	val_accuracy: 74.3902	gate_accuracy: 73.1707
	Task-8	val_accuracy: 73.8636	gate_accuracy: 72.7273
	Task-9	val_accuracy: 63.0952	gate_accuracy: 61.9048
	Task-10	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-11	val_accuracy: 74.1176	gate_accuracy: 68.2353
	Task-12	val_accuracy: 76.9231	gate_accuracy: 67.9487
	Task-13	val_accuracy: 79.2683	gate_accuracy: 71.9512
	Task-14	val_accuracy: 72.7273	gate_accuracy: 70.1299
	Task-15	val_accuracy: 73.4940	gate_accuracy: 73.4940
	Task-16	val_accuracy: 83.9506	gate_accuracy: 74.0741
	Task-17	val_accuracy: 80.2326	gate_accuracy: 74.4186
	Task-18	val_accuracy: 54.7619	gate_accuracy: 48.8095
	Task-19	val_accuracy: 64.2857	gate_accuracy: 67.8571
	Task-20	val_accuracy: 80.9524	gate_accuracy: 82.1429
	Task-21	val_accuracy: 71.2500	gate_accuracy: 71.2500
	Task-22	val_accuracy: 69.0141	gate_accuracy: 69.0141
	Task-23	val_accuracy: 73.8095	gate_accuracy: 71.4286
	Task-24	val_accuracy: 66.2500	gate_accuracy: 68.7500
	Task-25	val_accuracy: 63.0952	gate_accuracy: 63.0952
	Task-26	val_accuracy: 51.8987	gate_accuracy: 64.5570
	Task-27	val_accuracy: 70.0000	gate_accuracy: 70.0000
	Task-28	val_accuracy: 70.8861	gate_accuracy: 63.2911
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.6689


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.6426	step1_train_accuracy: 47.3373
STEP-1	Epoch: 20/50	loss: 1.0802	step1_train_accuracy: 85.7988
STEP-1	Epoch: 30/50	loss: 0.5411	step1_train_accuracy: 94.3787
STEP-1	Epoch: 40/50	loss: 0.3349	step1_train_accuracy: 96.4497
STEP-1	Epoch: 50/50	loss: 0.2375	step1_train_accuracy: 97.9290
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.7377	gate_loss: 0.7724	step2_classification_accuracy: 77.5448	step_2_gate_accuracy: 75.7736
STEP-2	Epoch: 40/200	classification_loss: 0.5365	gate_loss: 0.4607	step2_classification_accuracy: 83.0670	step_2_gate_accuracy: 84.3648
STEP-2	Epoch: 60/200	classification_loss: 0.4424	gate_loss: 0.3537	step2_classification_accuracy: 85.5761	step_2_gate_accuracy: 87.4949
STEP-2	Epoch: 80/200	classification_loss: 0.3936	gate_loss: 0.2990	step2_classification_accuracy: 86.8333	step_2_gate_accuracy: 89.2712
STEP-2	Epoch: 100/200	classification_loss: 0.3578	gate_loss: 0.2635	step2_classification_accuracy: 87.8919	step_2_gate_accuracy: 90.1415
STEP-2	Epoch: 120/200	classification_loss: 0.3258	gate_loss: 0.2333	step2_classification_accuracy: 88.6604	step_2_gate_accuracy: 91.4902
STEP-2	Epoch: 140/200	classification_loss: 0.3033	gate_loss: 0.2140	step2_classification_accuracy: 89.3781	step_2_gate_accuracy: 92.1773
STEP-2	Epoch: 160/200	classification_loss: 0.3025	gate_loss: 0.2099	step2_classification_accuracy: 89.5257	step_2_gate_accuracy: 92.2893
STEP-2	Epoch: 180/200	classification_loss: 0.2810	gate_loss: 0.1933	step2_classification_accuracy: 89.9990	step_2_gate_accuracy: 92.8288
STEP-2	Epoch: 200/200	classification_loss: 0.2646	gate_loss: 0.1788	step2_classification_accuracy: 90.5181	step_2_gate_accuracy: 93.5210
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.9248	gate_accuracy: 75.9398
	Task-1	val_accuracy: 64.3564	gate_accuracy: 72.2772
	Task-2	val_accuracy: 72.7273	gate_accuracy: 76.6234
	Task-3	val_accuracy: 76.3889	gate_accuracy: 83.3333
	Task-4	val_accuracy: 60.4651	gate_accuracy: 61.6279
	Task-5	val_accuracy: 72.2222	gate_accuracy: 73.6111
	Task-6	val_accuracy: 63.9344	gate_accuracy: 65.5738
	Task-7	val_accuracy: 78.0488	gate_accuracy: 70.7317
	Task-8	val_accuracy: 75.0000	gate_accuracy: 76.1364
	Task-9	val_accuracy: 60.7143	gate_accuracy: 58.3333
	Task-10	val_accuracy: 76.2500	gate_accuracy: 78.7500
	Task-11	val_accuracy: 74.1176	gate_accuracy: 69.4118
	Task-12	val_accuracy: 57.6923	gate_accuracy: 56.4103
	Task-13	val_accuracy: 81.7073	gate_accuracy: 81.7073
	Task-14	val_accuracy: 72.7273	gate_accuracy: 71.4286
	Task-15	val_accuracy: 75.9036	gate_accuracy: 74.6988
	Task-16	val_accuracy: 79.0123	gate_accuracy: 74.0741
	Task-17	val_accuracy: 84.8837	gate_accuracy: 79.0698
	Task-18	val_accuracy: 54.7619	gate_accuracy: 58.3333
	Task-19	val_accuracy: 70.2381	gate_accuracy: 73.8095
	Task-20	val_accuracy: 79.7619	gate_accuracy: 79.7619
	Task-21	val_accuracy: 71.2500	gate_accuracy: 72.5000
	Task-22	val_accuracy: 77.4648	gate_accuracy: 78.8732
	Task-23	val_accuracy: 75.0000	gate_accuracy: 70.2381
	Task-24	val_accuracy: 66.2500	gate_accuracy: 68.7500
	Task-25	val_accuracy: 73.8095	gate_accuracy: 71.4286
	Task-26	val_accuracy: 53.1646	gate_accuracy: 67.0886
	Task-27	val_accuracy: 72.2222	gate_accuracy: 75.5556
	Task-28	val_accuracy: 74.6835	gate_accuracy: 73.4177
	Task-29	val_accuracy: 64.2857	gate_accuracy: 67.8571
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.9791


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 2.3302	step1_train_accuracy: 59.6875
STEP-1	Epoch: 20/50	loss: 0.6696	step1_train_accuracy: 90.0000
STEP-1	Epoch: 30/50	loss: 0.3377	step1_train_accuracy: 98.4375
STEP-1	Epoch: 40/50	loss: 0.2140	step1_train_accuracy: 99.3750
STEP-1	Epoch: 50/50	loss: 0.1530	step1_train_accuracy: 100.0000
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.7606	gate_loss: 0.8109	step2_classification_accuracy: 77.3736	step_2_gate_accuracy: 74.9262
STEP-2	Epoch: 40/200	classification_loss: 0.5553	gate_loss: 0.4868	step2_classification_accuracy: 82.8127	step_2_gate_accuracy: 83.8811
STEP-2	Epoch: 60/200	classification_loss: 0.4578	gate_loss: 0.3768	step2_classification_accuracy: 85.3007	step_2_gate_accuracy: 86.7457
STEP-2	Epoch: 80/200	classification_loss: 0.4058	gate_loss: 0.3157	step2_classification_accuracy: 86.7457	step_2_gate_accuracy: 88.7249
STEP-2	Epoch: 100/200	classification_loss: 0.3694	gate_loss: 0.2814	step2_classification_accuracy: 87.5954	step_2_gate_accuracy: 89.9919
STEP-2	Epoch: 120/200	classification_loss: 0.3422	gate_loss: 0.2547	step2_classification_accuracy: 88.4756	step_2_gate_accuracy: 90.8212
STEP-2	Epoch: 140/200	classification_loss: 0.3206	gate_loss: 0.2332	step2_classification_accuracy: 88.9743	step_2_gate_accuracy: 91.4267
STEP-2	Epoch: 160/200	classification_loss: 0.3008	gate_loss: 0.2156	step2_classification_accuracy: 89.5950	step_2_gate_accuracy: 92.2204
STEP-2	Epoch: 180/200	classification_loss: 0.2911	gate_loss: 0.2057	step2_classification_accuracy: 89.7425	step_2_gate_accuracy: 92.3273
STEP-2	Epoch: 200/200	classification_loss: 0.2805	gate_loss: 0.1965	step2_classification_accuracy: 90.2005	step_2_gate_accuracy: 92.7343
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.9248	gate_accuracy: 79.6992
	Task-1	val_accuracy: 71.2871	gate_accuracy: 74.2574
	Task-2	val_accuracy: 72.7273	gate_accuracy: 74.0260
	Task-3	val_accuracy: 81.9444	gate_accuracy: 87.5000
	Task-4	val_accuracy: 62.7907	gate_accuracy: 66.2791
	Task-5	val_accuracy: 72.2222	gate_accuracy: 68.0556
	Task-6	val_accuracy: 62.2951	gate_accuracy: 62.2951
	Task-7	val_accuracy: 75.6098	gate_accuracy: 74.3902
	Task-8	val_accuracy: 72.7273	gate_accuracy: 73.8636
	Task-9	val_accuracy: 53.5714	gate_accuracy: 48.8095
	Task-10	val_accuracy: 76.2500	gate_accuracy: 72.5000
	Task-11	val_accuracy: 68.2353	gate_accuracy: 61.1765
	Task-12	val_accuracy: 58.9744	gate_accuracy: 50.0000
	Task-13	val_accuracy: 79.2683	gate_accuracy: 74.3902
	Task-14	val_accuracy: 67.5325	gate_accuracy: 67.5325
	Task-15	val_accuracy: 79.5181	gate_accuracy: 73.4940
	Task-16	val_accuracy: 85.1852	gate_accuracy: 77.7778
	Task-17	val_accuracy: 80.2326	gate_accuracy: 70.9302
	Task-18	val_accuracy: 63.0952	gate_accuracy: 60.7143
	Task-19	val_accuracy: 71.4286	gate_accuracy: 73.8095
	Task-20	val_accuracy: 77.3810	gate_accuracy: 73.8095
	Task-21	val_accuracy: 73.7500	gate_accuracy: 75.0000
	Task-22	val_accuracy: 66.1972	gate_accuracy: 63.3803
	Task-23	val_accuracy: 72.6190	gate_accuracy: 63.0952
	Task-24	val_accuracy: 65.0000	gate_accuracy: 62.5000
	Task-25	val_accuracy: 73.8095	gate_accuracy: 75.0000
	Task-26	val_accuracy: 50.6329	gate_accuracy: 51.8987
	Task-27	val_accuracy: 66.6667	gate_accuracy: 72.2222
	Task-28	val_accuracy: 74.6835	gate_accuracy: 77.2152
	Task-29	val_accuracy: 66.6667	gate_accuracy: 71.4286
	Task-30	val_accuracy: 61.2500	gate_accuracy: 61.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.2727


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 2.2062	step1_train_accuracy: 60.7046
STEP-1	Epoch: 20/50	loss: 0.8942	step1_train_accuracy: 85.3659
STEP-1	Epoch: 30/50	loss: 0.4314	step1_train_accuracy: 95.1219
STEP-1	Epoch: 40/50	loss: 0.2406	step1_train_accuracy: 97.5610
STEP-1	Epoch: 50/50	loss: 0.1675	step1_train_accuracy: 98.1030
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 34, 1: 34, 2: 34, 3: 34, 4: 34, 5: 34, 6: 34, 7: 34, 8: 34, 9: 34, 10: 34, 11: 34, 12: 34, 13: 34, 14: 34, 15: 34, 16: 34, 17: 34, 18: 34, 19: 34, 20: 34, 21: 34, 22: 34, 23: 34, 24: 34, 25: 34, 26: 34, 27: 34, 28: 34, 29: 34, 30: 34, 31: 34, 32: 34, 33: 34, 34: 34, 35: 34, 36: 34, 37: 34, 38: 34, 39: 34, 40: 34, 41: 34, 42: 34, 43: 34, 44: 34, 45: 34, 46: 34, 47: 34, 48: 34, 49: 34, 50: 34, 51: 34, 52: 34, 53: 34, 54: 34, 55: 34, 56: 34, 57: 34, 58: 34, 59: 34, 60: 34, 61: 34, 62: 34, 63: 34, 64: 34, 65: 34, 66: 34, 67: 34, 68: 34, 69: 34, 70: 34, 71: 34, 72: 34, 73: 34, 74: 34, 75: 34, 76: 34, 77: 34, 78: 34, 79: 34, 80: 34, 81: 34, 82: 34, 83: 34, 84: 34, 85: 34, 86: 34, 87: 34, 88: 34, 89: 34, 90: 34, 91: 34, 92: 34, 93: 34, 94: 34, 95: 34, 96: 34, 97: 34, 98: 34, 99: 34, 100: 34, 101: 34, 102: 34, 103: 34, 104: 34, 105: 34, 106: 34, 107: 34, 108: 34, 109: 34, 110: 34, 111: 34, 112: 34, 113: 34, 114: 34, 115: 34, 116: 34, 117: 34, 118: 34, 119: 34, 120: 34, 121: 34, 122: 34, 123: 34, 124: 34, 125: 34, 126: 34, 127: 34, 128: 34, 129: 34, 130: 34, 131: 34, 132: 34, 133: 34, 134: 34, 135: 34, 136: 34, 137: 34, 138: 34, 139: 34, 140: 34, 141: 34, 142: 34, 143: 34, 144: 34, 145: 34, 146: 34, 147: 34, 148: 34, 149: 34, 150: 34, 151: 34, 152: 34, 153: 34, 154: 34, 155: 34, 156: 34, 157: 34, 158: 34, 159: 34, 160: 34, 161: 34, 162: 34, 163: 34, 164: 34, 165: 34, 166: 34, 167: 34, 168: 34, 169: 34, 170: 34, 171: 34, 172: 34, 173: 34, 174: 34, 175: 34, 176: 34, 177: 34, 178: 34, 179: 34, 180: 34, 181: 34, 182: 34, 183: 34, 184: 34, 185: 34, 186: 34, 187: 34, 188: 34, 189: 34, 190: 34, 191: 34, 192: 34, 193: 34, 194: 34, 195: 34, 196: 34, 197: 34, 198: 34, 199: 34, 200: 34, 201: 34, 202: 34, 203: 34, 204: 34, 205: 34, 206: 34, 207: 34, 208: 34, 209: 34, 210: 34, 211: 34, 212: 34, 213: 34, 214: 34, 215: 34, 216: 34, 217: 34, 218: 34, 219: 34, 220: 34, 221: 34, 222: 34, 223: 34, 224: 34, 225: 34, 226: 34, 227: 34, 228: 34, 229: 34, 230: 34, 231: 34, 232: 34, 233: 34, 234: 34, 235: 34, 236: 34, 237: 34, 238: 34, 239: 34, 240: 34, 241: 34, 242: 34, 243: 34, 244: 34, 245: 34, 246: 34, 247: 34, 248: 34, 249: 34, 250: 34, 251: 34, 252: 34, 253: 34, 254: 34, 255: 34, 256: 34, 257: 34, 258: 34, 259: 34, 260: 34, 261: 34, 262: 34, 263: 34, 264: 34, 265: 34, 266: 34, 267: 34, 268: 34, 269: 34, 270: 34, 271: 34, 272: 34, 273: 34, 274: 34, 275: 34, 276: 34, 277: 34, 278: 34, 279: 34, 280: 34, 281: 34, 282: 34, 283: 34, 284: 34, 285: 34, 286: 34, 287: 34, 288: 34, 289: 34, 290: 34, 291: 34, 292: 34, 293: 34, 294: 34, 295: 34, 296: 34, 297: 34, 298: 34, 299: 34, 300: 34, 301: 34, 302: 34, 303: 34, 304: 34, 305: 34, 306: 34, 307: 34, 308: 34, 309: 34, 310: 34, 311: 34, 312: 34, 313: 34, 314: 34, 315: 34, 316: 34, 317: 34, 318: 34, 319: 34, 320: 34, 321: 34, 322: 34, 323: 34, 324: 34, 325: 34, 326: 34, 327: 34, 328: 34, 329: 34, 330: 34, 331: 34, 332: 34, 333: 34, 334: 34, 335: 34, 336: 34, 337: 34, 338: 34, 339: 34, 340: 34, 341: 34, 342: 34, 343: 34, 344: 34, 345: 34, 346: 34, 347: 34, 348: 34, 349: 34, 350: 34, 351: 34, 352: 34, 353: 34, 354: 34, 355: 34, 356: 34, 357: 34, 358: 34, 359: 34, 360: 34, 361: 34, 362: 34, 363: 34, 364: 34, 365: 34, 366: 34, 367: 34, 368: 34, 369: 34, 370: 34, 371: 34, 372: 34, 373: 34, 374: 34, 375: 34, 376: 34, 377: 34, 378: 34, 379: 34, 380: 34, 381: 34, 382: 34, 383: 34, 384: 34, 385: 34, 386: 34, 387: 34, 388: 34, 389: 34, 390: 34, 391: 34, 392: 34, 393: 34, 394: 34, 395: 34, 396: 34, 397: 34, 398: 34, 399: 34, 400: 34, 401: 34, 402: 34, 403: 34, 404: 34, 405: 34, 406: 34, 407: 34, 408: 34, 409: 34, 410: 34, 411: 34, 412: 34, 413: 34, 414: 34, 415: 34, 416: 34, 417: 34, 418: 34, 419: 34, 420: 34, 421: 34, 422: 34, 423: 34, 424: 34, 425: 34, 426: 34, 427: 34, 428: 34, 429: 34, 430: 34, 431: 34, 432: 34, 433: 34, 434: 34, 435: 34, 436: 34, 437: 34, 438: 34, 439: 34, 440: 34, 441: 34, 442: 34, 443: 34, 444: 34, 445: 34, 446: 34, 447: 34, 448: 34, 449: 34, 450: 34, 451: 34, 452: 34, 453: 34, 454: 34, 455: 34, 456: 34, 457: 34, 458: 34, 459: 34, 460: 34, 461: 34, 462: 34, 463: 34, 464: 34, 465: 34, 466: 34, 467: 34, 468: 34, 469: 34, 470: 34, 471: 34, 472: 34, 473: 34, 474: 34, 475: 34, 476: 34, 477: 34, 478: 34, 479: 34, 480: 34, 481: 34, 482: 34, 483: 34, 484: 34, 485: 34, 486: 34, 487: 34, 488: 34, 489: 34, 490: 34, 491: 34, 492: 34, 493: 34, 494: 34, 495: 34, 496: 34, 497: 34, 498: 34, 499: 34, 500: 34, 501: 34, 502: 34, 503: 34, 504: 34, 505: 34, 506: 34, 507: 34, 508: 34, 509: 34, 510: 34, 511: 34, 512: 34, 513: 34, 514: 34, 515: 34, 516: 34, 517: 34, 518: 34, 519: 34, 520: 34, 521: 34, 522: 34, 523: 34, 524: 34, 525: 34, 526: 34, 527: 34, 528: 34, 529: 34, 530: 34, 531: 34, 532: 34, 533: 34, 534: 34, 535: 34, 536: 34, 537: 34, 538: 34, 539: 34, 540: 34, 541: 34, 542: 34, 543: 34, 544: 34, 545: 34, 546: 34, 547: 34, 548: 34, 549: 34, 550: 34, 551: 34, 552: 34, 553: 34, 554: 34, 555: 34, 556: 34, 557: 34, 558: 34, 559: 34, 560: 34, 561: 34, 562: 34, 563: 34, 564: 34, 565: 34, 566: 34, 567: 34, 568: 34, 569: 34, 570: 34, 571: 34, 572: 34, 573: 34, 574: 34, 575: 34, 576: 34, 577: 34, 578: 34, 579: 34, 580: 34, 581: 34, 582: 34, 583: 34, 584: 34, 585: 34, 586: 34, 587: 34, 588: 34, 589: 34, 590: 34, 591: 34, 592: 34, 593: 34, 594: 34, 595: 34, 596: 34, 597: 34, 598: 34, 599: 34, 600: 34, 601: 34, 602: 34, 603: 34, 604: 34, 605: 34, 606: 34, 607: 34, 608: 34, 609: 34, 610: 34, 611: 34, 612: 34, 613: 34, 614: 34, 615: 34, 616: 34, 617: 34, 618: 34, 619: 34, 620: 34, 621: 34, 622: 34, 623: 34, 624: 34, 625: 34, 626: 34, 627: 34, 628: 34, 629: 34, 630: 34, 631: 34, 632: 34, 633: 34, 634: 34, 635: 34, 636: 34, 637: 34, 638: 34, 639: 34, 640: 34, 641: 34, 642: 34, 643: 34, 644: 34, 645: 34, 646: 34, 647: 34, 648: 34, 649: 34, 650: 34, 651: 34, 652: 34, 653: 34})
STEP-2	Epoch: 20/200	classification_loss: 0.7784	gate_loss: 0.7950	step2_classification_accuracy: 76.8798	step_2_gate_accuracy: 75.0495
STEP-2	Epoch: 40/200	classification_loss: 0.5778	gate_loss: 0.4979	step2_classification_accuracy: 82.0831	step_2_gate_accuracy: 82.9376
STEP-2	Epoch: 60/200	classification_loss: 0.4772	gate_loss: 0.3870	step2_classification_accuracy: 84.8534	step_2_gate_accuracy: 86.4544
STEP-2	Epoch: 80/200	classification_loss: 0.4189	gate_loss: 0.3260	step2_classification_accuracy: 86.3600	step_2_gate_accuracy: 88.4377
STEP-2	Epoch: 100/200	classification_loss: 0.3865	gate_loss: 0.2960	step2_classification_accuracy: 87.1919	step_2_gate_accuracy: 89.3371
STEP-2	Epoch: 120/200	classification_loss: 0.3589	gate_loss: 0.2658	step2_classification_accuracy: 87.8665	step_2_gate_accuracy: 90.4794
STEP-2	Epoch: 140/200	classification_loss: 0.3376	gate_loss: 0.2471	step2_classification_accuracy: 88.2263	step_2_gate_accuracy: 90.8482
STEP-2	Epoch: 160/200	classification_loss: 0.3281	gate_loss: 0.2374	step2_classification_accuracy: 88.7885	step_2_gate_accuracy: 91.2619
STEP-2	Epoch: 180/200	classification_loss: 0.3116	gate_loss: 0.2229	step2_classification_accuracy: 89.2022	step_2_gate_accuracy: 91.8645
STEP-2	Epoch: 200/200	classification_loss: 0.2987	gate_loss: 0.2098	step2_classification_accuracy: 89.5575	step_2_gate_accuracy: 92.1524
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.4361	gate_accuracy: 81.9549
	Task-1	val_accuracy: 66.3366	gate_accuracy: 68.3168
	Task-2	val_accuracy: 81.8182	gate_accuracy: 81.8182
	Task-3	val_accuracy: 79.1667	gate_accuracy: 83.3333
	Task-4	val_accuracy: 65.1163	gate_accuracy: 62.7907
	Task-5	val_accuracy: 73.6111	gate_accuracy: 68.0556
	Task-6	val_accuracy: 75.4098	gate_accuracy: 70.4918
	Task-7	val_accuracy: 79.2683	gate_accuracy: 78.0488
	Task-8	val_accuracy: 75.0000	gate_accuracy: 72.7273
	Task-9	val_accuracy: 58.3333	gate_accuracy: 55.9524
	Task-10	val_accuracy: 75.0000	gate_accuracy: 77.5000
	Task-11	val_accuracy: 63.5294	gate_accuracy: 58.8235
	Task-12	val_accuracy: 65.3846	gate_accuracy: 58.9744
	Task-13	val_accuracy: 76.8293	gate_accuracy: 75.6098
	Task-14	val_accuracy: 72.7273	gate_accuracy: 72.7273
	Task-15	val_accuracy: 65.0602	gate_accuracy: 62.6506
	Task-16	val_accuracy: 81.4815	gate_accuracy: 75.3086
	Task-17	val_accuracy: 86.0465	gate_accuracy: 80.2326
	Task-18	val_accuracy: 55.9524	gate_accuracy: 60.7143
	Task-19	val_accuracy: 66.6667	gate_accuracy: 69.0476
	Task-20	val_accuracy: 77.3810	gate_accuracy: 78.5714
	Task-21	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-22	val_accuracy: 74.6479	gate_accuracy: 74.6479
	Task-23	val_accuracy: 76.1905	gate_accuracy: 79.7619
	Task-24	val_accuracy: 67.5000	gate_accuracy: 67.5000
	Task-25	val_accuracy: 65.4762	gate_accuracy: 60.7143
	Task-26	val_accuracy: 48.1013	gate_accuracy: 54.4304
	Task-27	val_accuracy: 68.8889	gate_accuracy: 80.0000
	Task-28	val_accuracy: 81.0127	gate_accuracy: 79.7468
	Task-29	val_accuracy: 65.4762	gate_accuracy: 71.4286
	Task-30	val_accuracy: 68.7500	gate_accuracy: 75.0000
	Task-31	val_accuracy: 55.4348	gate_accuracy: 58.6957
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.0101


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 3.4524	step1_train_accuracy: 37.7163
STEP-1	Epoch: 20/50	loss: 1.2938	step1_train_accuracy: 76.8166
STEP-1	Epoch: 30/50	loss: 0.7049	step1_train_accuracy: 94.1176
STEP-1	Epoch: 40/50	loss: 0.4743	step1_train_accuracy: 96.8858
STEP-1	Epoch: 50/50	loss: 0.3346	step1_train_accuracy: 97.2318
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.8038	gate_loss: 0.8388	step2_classification_accuracy: 76.4621	step_2_gate_accuracy: 73.7580
STEP-2	Epoch: 40/200	classification_loss: 0.5890	gate_loss: 0.5216	step2_classification_accuracy: 81.7364	step_2_gate_accuracy: 82.4256
STEP-2	Epoch: 60/200	classification_loss: 0.4925	gate_loss: 0.4091	step2_classification_accuracy: 84.0863	step_2_gate_accuracy: 85.7615
STEP-2	Epoch: 80/200	classification_loss: 0.4387	gate_loss: 0.3494	step2_classification_accuracy: 85.6992	step_2_gate_accuracy: 87.5515
STEP-2	Epoch: 100/200	classification_loss: 0.3962	gate_loss: 0.3101	step2_classification_accuracy: 86.8862	step_2_gate_accuracy: 88.9968
STEP-2	Epoch: 120/200	classification_loss: 0.3664	gate_loss: 0.2771	step2_classification_accuracy: 87.6567	step_2_gate_accuracy: 90.0019
STEP-2	Epoch: 140/200	classification_loss: 0.3448	gate_loss: 0.2584	step2_classification_accuracy: 88.4512	step_2_gate_accuracy: 90.6385
STEP-2	Epoch: 160/200	classification_loss: 0.3267	gate_loss: 0.2439	step2_classification_accuracy: 88.8533	step_2_gate_accuracy: 91.2128
STEP-2	Epoch: 180/200	classification_loss: 0.3099	gate_loss: 0.2267	step2_classification_accuracy: 89.4180	step_2_gate_accuracy: 91.8206
STEP-2	Epoch: 200/200	classification_loss: 0.3123	gate_loss: 0.2252	step2_classification_accuracy: 89.2409	step_2_gate_accuracy: 91.8924
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.6917	gate_accuracy: 83.4586
	Task-1	val_accuracy: 65.3465	gate_accuracy: 66.3366
	Task-2	val_accuracy: 77.9221	gate_accuracy: 77.9221
	Task-3	val_accuracy: 81.9444	gate_accuracy: 88.8889
	Task-4	val_accuracy: 68.6047	gate_accuracy: 67.4419
	Task-5	val_accuracy: 70.8333	gate_accuracy: 72.2222
	Task-6	val_accuracy: 72.1311	gate_accuracy: 67.2131
	Task-7	val_accuracy: 80.4878	gate_accuracy: 74.3902
	Task-8	val_accuracy: 75.0000	gate_accuracy: 73.8636
	Task-9	val_accuracy: 57.1429	gate_accuracy: 51.1905
	Task-10	val_accuracy: 73.7500	gate_accuracy: 71.2500
	Task-11	val_accuracy: 70.5882	gate_accuracy: 65.8824
	Task-12	val_accuracy: 69.2308	gate_accuracy: 62.8205
	Task-13	val_accuracy: 74.3902	gate_accuracy: 73.1707
	Task-14	val_accuracy: 68.8312	gate_accuracy: 64.9351
	Task-15	val_accuracy: 72.2892	gate_accuracy: 65.0602
	Task-16	val_accuracy: 82.7160	gate_accuracy: 77.7778
	Task-17	val_accuracy: 77.9070	gate_accuracy: 70.9302
	Task-18	val_accuracy: 50.0000	gate_accuracy: 51.1905
	Task-19	val_accuracy: 55.9524	gate_accuracy: 60.7143
	Task-20	val_accuracy: 78.5714	gate_accuracy: 71.4286
	Task-21	val_accuracy: 68.7500	gate_accuracy: 70.0000
	Task-22	val_accuracy: 77.4648	gate_accuracy: 74.6479
	Task-23	val_accuracy: 73.8095	gate_accuracy: 72.6190
	Task-24	val_accuracy: 66.2500	gate_accuracy: 61.2500
	Task-25	val_accuracy: 63.0952	gate_accuracy: 64.2857
	Task-26	val_accuracy: 49.3671	gate_accuracy: 53.1646
	Task-27	val_accuracy: 67.7778	gate_accuracy: 71.1111
	Task-28	val_accuracy: 72.1519	gate_accuracy: 70.8861
	Task-29	val_accuracy: 61.9048	gate_accuracy: 70.2381
	Task-30	val_accuracy: 68.7500	gate_accuracy: 71.2500
	Task-31	val_accuracy: 52.1739	gate_accuracy: 55.4348
	Task-32	val_accuracy: 68.0556	gate_accuracy: 63.8889
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.5192


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.2917	step1_train_accuracy: 57.3864
STEP-1	Epoch: 20/50	loss: 0.9189	step1_train_accuracy: 81.8182
STEP-1	Epoch: 30/50	loss: 0.4559	step1_train_accuracy: 93.1818
STEP-1	Epoch: 40/50	loss: 0.2934	step1_train_accuracy: 94.6023
STEP-1	Epoch: 50/50	loss: 0.2175	step1_train_accuracy: 95.4545
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.8230	gate_loss: 0.8861	step2_classification_accuracy: 75.7181	step_2_gate_accuracy: 72.9153
STEP-2	Epoch: 40/200	classification_loss: 0.6125	gate_loss: 0.5462	step2_classification_accuracy: 81.1100	step_2_gate_accuracy: 81.8769
STEP-2	Epoch: 60/200	classification_loss: 0.4991	gate_loss: 0.4186	step2_classification_accuracy: 84.3776	step_2_gate_accuracy: 85.7302
STEP-2	Epoch: 80/200	classification_loss: 0.4417	gate_loss: 0.3554	step2_classification_accuracy: 85.7488	step_2_gate_accuracy: 87.2920
STEP-2	Epoch: 100/200	classification_loss: 0.3985	gate_loss: 0.3128	step2_classification_accuracy: 86.7249	step_2_gate_accuracy: 88.9607
STEP-2	Epoch: 120/200	classification_loss: 0.3623	gate_loss: 0.2795	step2_classification_accuracy: 87.8498	step_2_gate_accuracy: 90.0669
STEP-2	Epoch: 140/200	classification_loss: 0.3495	gate_loss: 0.2625	step2_classification_accuracy: 88.1891	step_2_gate_accuracy: 90.5689
STEP-2	Epoch: 160/200	classification_loss: 0.3387	gate_loss: 0.2516	step2_classification_accuracy: 88.7190	step_2_gate_accuracy: 90.8989
STEP-2	Epoch: 180/200	classification_loss: 0.3158	gate_loss: 0.2330	step2_classification_accuracy: 89.3000	step_2_gate_accuracy: 91.6334
STEP-2	Epoch: 200/200	classification_loss: 0.3050	gate_loss: 0.2235	step2_classification_accuracy: 89.5324	step_2_gate_accuracy: 92.0145
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.9248	gate_accuracy: 80.4511
	Task-1	val_accuracy: 64.3564	gate_accuracy: 71.2871
	Task-2	val_accuracy: 71.4286	gate_accuracy: 74.0260
	Task-3	val_accuracy: 83.3333	gate_accuracy: 86.1111
	Task-4	val_accuracy: 60.4651	gate_accuracy: 50.0000
	Task-5	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-6	val_accuracy: 72.1311	gate_accuracy: 65.5738
	Task-7	val_accuracy: 81.7073	gate_accuracy: 80.4878
	Task-8	val_accuracy: 73.8636	gate_accuracy: 72.7273
	Task-9	val_accuracy: 61.9048	gate_accuracy: 60.7143
	Task-10	val_accuracy: 76.2500	gate_accuracy: 76.2500
	Task-11	val_accuracy: 64.7059	gate_accuracy: 56.4706
	Task-12	val_accuracy: 66.6667	gate_accuracy: 60.2564
	Task-13	val_accuracy: 78.0488	gate_accuracy: 74.3902
	Task-14	val_accuracy: 71.4286	gate_accuracy: 71.4286
	Task-15	val_accuracy: 72.2892	gate_accuracy: 61.4458
	Task-16	val_accuracy: 83.9506	gate_accuracy: 80.2469
	Task-17	val_accuracy: 83.7209	gate_accuracy: 79.0698
	Task-18	val_accuracy: 50.0000	gate_accuracy: 47.6190
	Task-19	val_accuracy: 67.8571	gate_accuracy: 70.2381
	Task-20	val_accuracy: 82.1429	gate_accuracy: 82.1429
	Task-21	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-22	val_accuracy: 61.9718	gate_accuracy: 60.5634
	Task-23	val_accuracy: 72.6190	gate_accuracy: 65.4762
	Task-24	val_accuracy: 63.7500	gate_accuracy: 65.0000
	Task-25	val_accuracy: 67.8571	gate_accuracy: 67.8571
	Task-26	val_accuracy: 45.5696	gate_accuracy: 53.1646
	Task-27	val_accuracy: 64.4444	gate_accuracy: 70.0000
	Task-28	val_accuracy: 74.6835	gate_accuracy: 74.6835
	Task-29	val_accuracy: 65.4762	gate_accuracy: 67.8571
	Task-30	val_accuracy: 61.2500	gate_accuracy: 58.7500
	Task-31	val_accuracy: 57.6087	gate_accuracy: 67.3913
	Task-32	val_accuracy: 72.2222	gate_accuracy: 72.2222
	Task-33	val_accuracy: 72.7273	gate_accuracy: 76.1364
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.2526


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 2.6459	step1_train_accuracy: 50.3145
STEP-1	Epoch: 20/50	loss: 0.9391	step1_train_accuracy: 84.2767
STEP-1	Epoch: 30/50	loss: 0.4513	step1_train_accuracy: 96.8553
STEP-1	Epoch: 40/50	loss: 0.2741	step1_train_accuracy: 98.4277
STEP-1	Epoch: 50/50	loss: 0.1867	step1_train_accuracy: 99.3711
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.8274	gate_loss: 0.8644	step2_classification_accuracy: 75.5454	step_2_gate_accuracy: 73.4997
STEP-2	Epoch: 40/200	classification_loss: 0.6412	gate_loss: 0.5618	step2_classification_accuracy: 80.4983	step_2_gate_accuracy: 81.1943
STEP-2	Epoch: 60/200	classification_loss: 0.5268	gate_loss: 0.4386	step2_classification_accuracy: 83.3758	step_2_gate_accuracy: 84.8103
STEP-2	Epoch: 80/200	classification_loss: 0.4774	gate_loss: 0.3841	step2_classification_accuracy: 84.7678	step_2_gate_accuracy: 86.7371
STEP-2	Epoch: 100/200	classification_loss: 0.4317	gate_loss: 0.3397	step2_classification_accuracy: 85.9562	step_2_gate_accuracy: 87.8703
STEP-2	Epoch: 120/200	classification_loss: 0.4050	gate_loss: 0.3127	step2_classification_accuracy: 86.7838	step_2_gate_accuracy: 88.8040
STEP-2	Epoch: 140/200	classification_loss: 0.3869	gate_loss: 0.2948	step2_classification_accuracy: 87.0597	step_2_gate_accuracy: 89.1945
STEP-2	Epoch: 160/200	classification_loss: 0.3650	gate_loss: 0.2746	step2_classification_accuracy: 87.8321	step_2_gate_accuracy: 89.8820
STEP-2	Epoch: 180/200	classification_loss: 0.3533	gate_loss: 0.2672	step2_classification_accuracy: 88.1419	step_2_gate_accuracy: 90.2767
STEP-2	Epoch: 200/200	classification_loss: 0.3353	gate_loss: 0.2485	step2_classification_accuracy: 88.5281	step_2_gate_accuracy: 90.9133
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.4135	gate_accuracy: 73.6842
	Task-1	val_accuracy: 70.2970	gate_accuracy: 73.2673
	Task-2	val_accuracy: 77.9221	gate_accuracy: 75.3247
	Task-3	val_accuracy: 79.1667	gate_accuracy: 86.1111
	Task-4	val_accuracy: 65.1163	gate_accuracy: 65.1163
	Task-5	val_accuracy: 72.2222	gate_accuracy: 72.2222
	Task-6	val_accuracy: 68.8525	gate_accuracy: 59.0164
	Task-7	val_accuracy: 69.5122	gate_accuracy: 67.0732
	Task-8	val_accuracy: 77.2727	gate_accuracy: 69.3182
	Task-9	val_accuracy: 61.9048	gate_accuracy: 52.3810
	Task-10	val_accuracy: 76.2500	gate_accuracy: 75.0000
	Task-11	val_accuracy: 61.1765	gate_accuracy: 56.4706
	Task-12	val_accuracy: 60.2564	gate_accuracy: 56.4103
	Task-13	val_accuracy: 78.0488	gate_accuracy: 80.4878
	Task-14	val_accuracy: 66.2338	gate_accuracy: 67.5325
	Task-15	val_accuracy: 67.4699	gate_accuracy: 57.8313
	Task-16	val_accuracy: 82.7160	gate_accuracy: 75.3086
	Task-17	val_accuracy: 84.8837	gate_accuracy: 81.3953
	Task-18	val_accuracy: 51.1905	gate_accuracy: 51.1905
	Task-19	val_accuracy: 63.0952	gate_accuracy: 63.0952
	Task-20	val_accuracy: 77.3810	gate_accuracy: 76.1905
	Task-21	val_accuracy: 76.2500	gate_accuracy: 72.5000
	Task-22	val_accuracy: 73.2394	gate_accuracy: 70.4225
	Task-23	val_accuracy: 78.5714	gate_accuracy: 71.4286
	Task-24	val_accuracy: 58.7500	gate_accuracy: 60.0000
	Task-25	val_accuracy: 67.8571	gate_accuracy: 70.2381
	Task-26	val_accuracy: 51.8987	gate_accuracy: 58.2278
	Task-27	val_accuracy: 61.1111	gate_accuracy: 71.1111
	Task-28	val_accuracy: 79.7468	gate_accuracy: 77.2152
	Task-29	val_accuracy: 64.2857	gate_accuracy: 67.8571
	Task-30	val_accuracy: 68.7500	gate_accuracy: 71.2500
	Task-31	val_accuracy: 60.8696	gate_accuracy: 65.2174
	Task-32	val_accuracy: 69.4444	gate_accuracy: 68.0556
	Task-33	val_accuracy: 68.1818	gate_accuracy: 73.8636
	Task-34	val_accuracy: 65.8228	gate_accuracy: 67.0886
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.6423


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.7381	step1_train_accuracy: 38.0783
STEP-1	Epoch: 20/50	loss: 1.0382	step1_train_accuracy: 74.0214
STEP-1	Epoch: 30/50	loss: 0.5886	step1_train_accuracy: 84.6975
STEP-1	Epoch: 40/50	loss: 0.4247	step1_train_accuracy: 90.7473
STEP-1	Epoch: 50/50	loss: 0.3296	step1_train_accuracy: 94.3061
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.8657	gate_loss: 0.9194	step2_classification_accuracy: 75.1428	step_2_gate_accuracy: 71.8511
STEP-2	Epoch: 40/200	classification_loss: 0.6457	gate_loss: 0.5761	step2_classification_accuracy: 80.7023	step_2_gate_accuracy: 80.7638
STEP-2	Epoch: 60/200	classification_loss: 0.5428	gate_loss: 0.4547	step2_classification_accuracy: 83.3040	step_2_gate_accuracy: 84.2709
STEP-2	Epoch: 80/200	classification_loss: 0.4796	gate_loss: 0.3886	step2_classification_accuracy: 84.7675	step_2_gate_accuracy: 86.6177
STEP-2	Epoch: 100/200	classification_loss: 0.4396	gate_loss: 0.3469	step2_classification_accuracy: 85.8399	step_2_gate_accuracy: 87.6769
STEP-2	Epoch: 120/200	classification_loss: 0.4060	gate_loss: 0.3155	step2_classification_accuracy: 86.6441	step_2_gate_accuracy: 88.6833
STEP-2	Epoch: 140/200	classification_loss: 0.3803	gate_loss: 0.2891	step2_classification_accuracy: 87.6022	step_2_gate_accuracy: 89.7337
STEP-2	Epoch: 160/200	classification_loss: 0.3675	gate_loss: 0.2766	step2_classification_accuracy: 87.9713	step_2_gate_accuracy: 90.0369
STEP-2	Epoch: 180/200	classification_loss: 0.3574	gate_loss: 0.2658	step2_classification_accuracy: 88.1559	step_2_gate_accuracy: 90.4061
STEP-2	Epoch: 200/200	classification_loss: 0.3474	gate_loss: 0.2577	step2_classification_accuracy: 88.4592	step_2_gate_accuracy: 90.6786
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.1880	gate_accuracy: 83.4586
	Task-1	val_accuracy: 65.3465	gate_accuracy: 70.2970
	Task-2	val_accuracy: 76.6234	gate_accuracy: 80.5195
	Task-3	val_accuracy: 83.3333	gate_accuracy: 84.7222
	Task-4	val_accuracy: 65.1163	gate_accuracy: 62.7907
	Task-5	val_accuracy: 65.2778	gate_accuracy: 63.8889
	Task-6	val_accuracy: 72.1311	gate_accuracy: 65.5738
	Task-7	val_accuracy: 80.4878	gate_accuracy: 73.1707
	Task-8	val_accuracy: 69.3182	gate_accuracy: 69.3182
	Task-9	val_accuracy: 55.9524	gate_accuracy: 53.5714
	Task-10	val_accuracy: 76.2500	gate_accuracy: 77.5000
	Task-11	val_accuracy: 78.8235	gate_accuracy: 71.7647
	Task-12	val_accuracy: 61.5385	gate_accuracy: 56.4103
	Task-13	val_accuracy: 79.2683	gate_accuracy: 75.6098
	Task-14	val_accuracy: 68.8312	gate_accuracy: 66.2338
	Task-15	val_accuracy: 68.6747	gate_accuracy: 61.4458
	Task-16	val_accuracy: 81.4815	gate_accuracy: 75.3086
	Task-17	val_accuracy: 82.5581	gate_accuracy: 80.2326
	Task-18	val_accuracy: 51.1905	gate_accuracy: 51.1905
	Task-19	val_accuracy: 69.0476	gate_accuracy: 72.6190
	Task-20	val_accuracy: 70.2381	gate_accuracy: 66.6667
	Task-21	val_accuracy: 66.2500	gate_accuracy: 67.5000
	Task-22	val_accuracy: 67.6056	gate_accuracy: 70.4225
	Task-23	val_accuracy: 71.4286	gate_accuracy: 70.2381
	Task-24	val_accuracy: 60.0000	gate_accuracy: 61.2500
	Task-25	val_accuracy: 64.2857	gate_accuracy: 63.0952
	Task-26	val_accuracy: 50.6329	gate_accuracy: 53.1646
	Task-27	val_accuracy: 68.8889	gate_accuracy: 76.6667
	Task-28	val_accuracy: 75.9494	gate_accuracy: 70.8861
	Task-29	val_accuracy: 57.1429	gate_accuracy: 63.0952
	Task-30	val_accuracy: 60.0000	gate_accuracy: 61.2500
	Task-31	val_accuracy: 51.0870	gate_accuracy: 56.5217
	Task-32	val_accuracy: 69.4444	gate_accuracy: 75.0000
	Task-33	val_accuracy: 71.5909	gate_accuracy: 72.7273
	Task-34	val_accuracy: 54.4304	gate_accuracy: 59.4937
	Task-35	val_accuracy: 50.0000	gate_accuracy: 57.1429
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.0686


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 3.3982	step1_train_accuracy: 31.3589
STEP-1	Epoch: 20/50	loss: 1.3014	step1_train_accuracy: 75.9582
STEP-1	Epoch: 30/50	loss: 0.7179	step1_train_accuracy: 87.8049
STEP-1	Epoch: 40/50	loss: 0.4935	step1_train_accuracy: 90.9408
STEP-1	Epoch: 50/50	loss: 0.3553	step1_train_accuracy: 91.9861
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30, 514: 30, 515: 30, 516: 30, 517: 30, 518: 30, 519: 30, 520: 30, 521: 30, 522: 30, 523: 30, 524: 30, 525: 30, 526: 30, 527: 30, 528: 30, 529: 30, 530: 30, 531: 30, 532: 30, 533: 30, 534: 30, 535: 30, 536: 30, 537: 30, 538: 30, 539: 30, 540: 30, 541: 30, 542: 30, 543: 30, 544: 30, 545: 30, 546: 30, 547: 30, 548: 30, 549: 30, 550: 30, 551: 30, 552: 30, 553: 30, 554: 30, 555: 30, 556: 30, 557: 30, 558: 30, 559: 30, 560: 30, 561: 30, 562: 30, 563: 30, 564: 30, 565: 30, 566: 30, 567: 30, 568: 30, 569: 30, 570: 30, 571: 30, 572: 30, 573: 30, 574: 30, 575: 30, 576: 30, 577: 30, 578: 30, 579: 30, 580: 30, 581: 30, 582: 30, 583: 30, 584: 30, 585: 30, 586: 30, 587: 30, 588: 30, 589: 30, 590: 30, 591: 30, 592: 30, 593: 30, 594: 30, 595: 30, 596: 30, 597: 30, 598: 30, 599: 30, 600: 30, 601: 30, 602: 30, 603: 30, 604: 30, 605: 30, 606: 30, 607: 30, 608: 30, 609: 30, 610: 30, 611: 30, 612: 30, 613: 30, 614: 30, 615: 30, 616: 30, 617: 30, 618: 30, 619: 30, 620: 30, 621: 30, 622: 30, 623: 30, 624: 30, 625: 30, 626: 30, 627: 30, 628: 30, 629: 30, 630: 30, 631: 30, 632: 30, 633: 30, 634: 30, 635: 30, 636: 30, 637: 30, 638: 30, 639: 30, 640: 30, 641: 30, 642: 30, 643: 30, 644: 30, 645: 30, 646: 30, 647: 30, 648: 30, 649: 30, 650: 30, 651: 30, 652: 30, 653: 30, 654: 30, 655: 30, 656: 30, 657: 30, 658: 30, 659: 30, 660: 30, 661: 30, 662: 30, 663: 30, 664: 30, 665: 30, 666: 30, 667: 30, 668: 30, 669: 30, 670: 30, 671: 30, 672: 30, 673: 30, 674: 30, 675: 30, 676: 30, 677: 30, 678: 30, 679: 30, 680: 30, 681: 30, 682: 30, 683: 30, 684: 30, 685: 30, 686: 30, 687: 30, 688: 30, 689: 30, 690: 30, 691: 30, 692: 30, 693: 30, 694: 30, 695: 30, 696: 30, 697: 30, 698: 30, 699: 30, 700: 30, 701: 30, 702: 30, 703: 30, 704: 30, 705: 30, 706: 30, 707: 30, 708: 30, 709: 30, 710: 30, 711: 30, 712: 30, 713: 30, 714: 30, 715: 30, 716: 30, 717: 30, 718: 30, 719: 30, 720: 30, 721: 30, 722: 30, 723: 30, 724: 30, 725: 30, 726: 30, 727: 30, 728: 30, 729: 30, 730: 30, 731: 30, 732: 30, 733: 30, 734: 30, 735: 30, 736: 30, 737: 30, 738: 30, 739: 30, 740: 30, 741: 30, 742: 30, 743: 30, 744: 30, 745: 30, 746: 30, 747: 30, 748: 30, 749: 30, 750: 30, 751: 30, 752: 30, 753: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.9211	gate_loss: 0.9624	step2_classification_accuracy: 73.5853	step_2_gate_accuracy: 70.8400
STEP-2	Epoch: 40/200	classification_loss: 0.6872	gate_loss: 0.6061	step2_classification_accuracy: 79.4120	step_2_gate_accuracy: 79.8939
STEP-2	Epoch: 60/200	classification_loss: 0.5824	gate_loss: 0.4813	step2_classification_accuracy: 82.1397	step_2_gate_accuracy: 83.6649
STEP-2	Epoch: 80/200	classification_loss: 0.5173	gate_loss: 0.4150	step2_classification_accuracy: 83.7975	step_2_gate_accuracy: 85.4465
STEP-2	Epoch: 100/200	classification_loss: 0.4712	gate_loss: 0.3691	step2_classification_accuracy: 85.0221	step_2_gate_accuracy: 86.7949
STEP-2	Epoch: 120/200	classification_loss: 0.4395	gate_loss: 0.3386	step2_classification_accuracy: 85.8134	step_2_gate_accuracy: 87.8912
STEP-2	Epoch: 140/200	classification_loss: 0.4286	gate_loss: 0.3250	step2_classification_accuracy: 85.9903	step_2_gate_accuracy: 88.2405
STEP-2	Epoch: 160/200	classification_loss: 0.4059	gate_loss: 0.3043	step2_classification_accuracy: 86.7639	step_2_gate_accuracy: 88.9965
STEP-2	Epoch: 180/200	classification_loss: 0.3815	gate_loss: 0.2808	step2_classification_accuracy: 87.1662	step_2_gate_accuracy: 89.7524
STEP-2	Epoch: 200/200	classification_loss: 0.3713	gate_loss: 0.2742	step2_classification_accuracy: 87.5332	step_2_gate_accuracy: 90.0531
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.9323	gate_accuracy: 76.6917
	Task-1	val_accuracy: 59.4059	gate_accuracy: 64.3564
	Task-2	val_accuracy: 76.6234	gate_accuracy: 77.9221
	Task-3	val_accuracy: 84.7222	gate_accuracy: 87.5000
	Task-4	val_accuracy: 61.6279	gate_accuracy: 60.4651
	Task-5	val_accuracy: 66.6667	gate_accuracy: 56.9444
	Task-6	val_accuracy: 67.2131	gate_accuracy: 60.6557
	Task-7	val_accuracy: 73.1707	gate_accuracy: 68.2927
	Task-8	val_accuracy: 77.2727	gate_accuracy: 70.4545
	Task-9	val_accuracy: 60.7143	gate_accuracy: 59.5238
	Task-10	val_accuracy: 66.2500	gate_accuracy: 66.2500
	Task-11	val_accuracy: 69.4118	gate_accuracy: 62.3529
	Task-12	val_accuracy: 69.2308	gate_accuracy: 61.5385
	Task-13	val_accuracy: 71.9512	gate_accuracy: 70.7317
	Task-14	val_accuracy: 64.9351	gate_accuracy: 68.8312
	Task-15	val_accuracy: 69.8795	gate_accuracy: 63.8554
	Task-16	val_accuracy: 83.9506	gate_accuracy: 72.8395
	Task-17	val_accuracy: 79.0698	gate_accuracy: 75.5814
	Task-18	val_accuracy: 53.5714	gate_accuracy: 52.3810
	Task-19	val_accuracy: 64.2857	gate_accuracy: 64.2857
	Task-20	val_accuracy: 79.7619	gate_accuracy: 72.6190
	Task-21	val_accuracy: 68.7500	gate_accuracy: 67.5000
	Task-22	val_accuracy: 78.8732	gate_accuracy: 73.2394
	Task-23	val_accuracy: 76.1905	gate_accuracy: 67.8571
	Task-24	val_accuracy: 58.7500	gate_accuracy: 60.0000
	Task-25	val_accuracy: 69.0476	gate_accuracy: 70.2381
	Task-26	val_accuracy: 50.6329	gate_accuracy: 60.7595
	Task-27	val_accuracy: 66.6667	gate_accuracy: 71.1111
	Task-28	val_accuracy: 81.0127	gate_accuracy: 73.4177
	Task-29	val_accuracy: 60.7143	gate_accuracy: 67.8571
	Task-30	val_accuracy: 61.2500	gate_accuracy: 65.0000
	Task-31	val_accuracy: 50.0000	gate_accuracy: 51.0870
	Task-32	val_accuracy: 70.8333	gate_accuracy: 70.8333
	Task-33	val_accuracy: 69.3182	gate_accuracy: 68.1818
	Task-34	val_accuracy: 65.8228	gate_accuracy: 70.8861
	Task-35	val_accuracy: 48.5714	gate_accuracy: 57.1429
	Task-36	val_accuracy: 56.9444	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.0171


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.8341	step1_train_accuracy: 48.5050
STEP-1	Epoch: 20/50	loss: 0.8075	step1_train_accuracy: 88.7043
STEP-1	Epoch: 30/50	loss: 0.3571	step1_train_accuracy: 97.6744
STEP-1	Epoch: 40/50	loss: 0.2327	step1_train_accuracy: 97.6744
STEP-1	Epoch: 50/50	loss: 0.1767	step1_train_accuracy: 97.6744
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33, 714: 33, 715: 33, 716: 33, 717: 33, 718: 33, 719: 33, 720: 33, 721: 33, 722: 33, 723: 33, 724: 33, 725: 33, 726: 33, 727: 33, 728: 33, 729: 33, 730: 33, 731: 33, 732: 33, 733: 33, 734: 33, 735: 33, 736: 33, 737: 33, 738: 33, 739: 33, 740: 33, 741: 33, 742: 33, 743: 33, 744: 33, 745: 33, 746: 33, 747: 33, 748: 33, 749: 33, 750: 33, 751: 33, 752: 33, 753: 33, 754: 33, 755: 33, 756: 33, 757: 33, 758: 33, 759: 33, 760: 33, 761: 33, 762: 33, 763: 33, 764: 33, 765: 33, 766: 33, 767: 33, 768: 33, 769: 33, 770: 33, 771: 33, 772: 33, 773: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.8860	gate_loss: 0.9009	step2_classification_accuracy: 74.5556	step_2_gate_accuracy: 72.2731
STEP-2	Epoch: 40/200	classification_loss: 0.6801	gate_loss: 0.5974	step2_classification_accuracy: 79.7314	step_2_gate_accuracy: 80.4205
STEP-2	Epoch: 60/200	classification_loss: 0.5756	gate_loss: 0.4813	step2_classification_accuracy: 82.4133	step_2_gate_accuracy: 83.5369
STEP-2	Epoch: 80/200	classification_loss: 0.5214	gate_loss: 0.4244	step2_classification_accuracy: 83.6583	step_2_gate_accuracy: 85.0168
STEP-2	Epoch: 100/200	classification_loss: 0.4802	gate_loss: 0.3801	step2_classification_accuracy: 84.8093	step_2_gate_accuracy: 86.7043
STEP-2	Epoch: 120/200	classification_loss: 0.4460	gate_loss: 0.3482	step2_classification_accuracy: 85.6002	step_2_gate_accuracy: 87.4168
STEP-2	Epoch: 140/200	classification_loss: 0.4255	gate_loss: 0.3284	step2_classification_accuracy: 86.3010	step_2_gate_accuracy: 88.0902
STEP-2	Epoch: 160/200	classification_loss: 0.4051	gate_loss: 0.3109	step2_classification_accuracy: 86.6416	step_2_gate_accuracy: 88.7714
STEP-2	Epoch: 180/200	classification_loss: 0.3924	gate_loss: 0.2959	step2_classification_accuracy: 87.2602	step_2_gate_accuracy: 89.3156
STEP-2	Epoch: 200/200	classification_loss: 0.3749	gate_loss: 0.2827	step2_classification_accuracy: 87.4403	step_2_gate_accuracy: 89.7150
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 67.6692	gate_accuracy: 72.9323
	Task-1	val_accuracy: 67.3267	gate_accuracy: 73.2673
	Task-2	val_accuracy: 79.2208	gate_accuracy: 80.5195
	Task-3	val_accuracy: 80.5556	gate_accuracy: 87.5000
	Task-4	val_accuracy: 72.0930	gate_accuracy: 70.9302
	Task-5	val_accuracy: 69.4444	gate_accuracy: 63.8889
	Task-6	val_accuracy: 65.5738	gate_accuracy: 65.5738
	Task-7	val_accuracy: 80.4878	gate_accuracy: 75.6098
	Task-8	val_accuracy: 77.2727	gate_accuracy: 76.1364
	Task-9	val_accuracy: 59.5238	gate_accuracy: 51.1905
	Task-10	val_accuracy: 77.5000	gate_accuracy: 73.7500
	Task-11	val_accuracy: 69.4118	gate_accuracy: 58.8235
	Task-12	val_accuracy: 66.6667	gate_accuracy: 61.5385
	Task-13	val_accuracy: 69.5122	gate_accuracy: 65.8537
	Task-14	val_accuracy: 67.5325	gate_accuracy: 67.5325
	Task-15	val_accuracy: 69.8795	gate_accuracy: 62.6506
	Task-16	val_accuracy: 80.2469	gate_accuracy: 67.9012
	Task-17	val_accuracy: 82.5581	gate_accuracy: 68.6047
	Task-18	val_accuracy: 59.5238	gate_accuracy: 59.5238
	Task-19	val_accuracy: 65.4762	gate_accuracy: 66.6667
	Task-20	val_accuracy: 78.5714	gate_accuracy: 70.2381
	Task-21	val_accuracy: 70.0000	gate_accuracy: 68.7500
	Task-22	val_accuracy: 59.1549	gate_accuracy: 66.1972
	Task-23	val_accuracy: 72.6190	gate_accuracy: 72.6190
	Task-24	val_accuracy: 61.2500	gate_accuracy: 58.7500
	Task-25	val_accuracy: 69.0476	gate_accuracy: 67.8571
	Task-26	val_accuracy: 53.1646	gate_accuracy: 75.9494
	Task-27	val_accuracy: 64.4444	gate_accuracy: 66.6667
	Task-28	val_accuracy: 73.4177	gate_accuracy: 65.8228
	Task-29	val_accuracy: 59.5238	gate_accuracy: 63.0952
	Task-30	val_accuracy: 61.2500	gate_accuracy: 61.2500
	Task-31	val_accuracy: 52.1739	gate_accuracy: 61.9565
	Task-32	val_accuracy: 69.4444	gate_accuracy: 68.0556
	Task-33	val_accuracy: 69.3182	gate_accuracy: 71.5909
	Task-34	val_accuracy: 63.2911	gate_accuracy: 68.3544
	Task-35	val_accuracy: 42.8571	gate_accuracy: 51.4286
	Task-36	val_accuracy: 56.9444	gate_accuracy: 66.6667
	Task-37	val_accuracy: 58.6667	gate_accuracy: 64.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.4896


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 3.0856	step1_train_accuracy: 38.4615
STEP-1	Epoch: 20/50	loss: 1.0919	step1_train_accuracy: 76.0355
STEP-1	Epoch: 30/50	loss: 0.6030	step1_train_accuracy: 91.7160
STEP-1	Epoch: 40/50	loss: 0.3964	step1_train_accuracy: 95.2663
STEP-1	Epoch: 50/50	loss: 0.2867	step1_train_accuracy: 96.1538
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9500	gate_loss: 0.9855	step2_classification_accuracy: 72.6050	step_2_gate_accuracy: 69.5417
STEP-2	Epoch: 40/200	classification_loss: 0.7215	gate_loss: 0.6372	step2_classification_accuracy: 78.6788	step_2_gate_accuracy: 79.1948
STEP-2	Epoch: 60/200	classification_loss: 0.6106	gate_loss: 0.5150	step2_classification_accuracy: 81.4293	step_2_gate_accuracy: 82.5384
STEP-2	Epoch: 80/200	classification_loss: 0.5438	gate_loss: 0.4394	step2_classification_accuracy: 83.2209	step_2_gate_accuracy: 84.7566
STEP-2	Epoch: 100/200	classification_loss: 0.4989	gate_loss: 0.3947	step2_classification_accuracy: 84.3991	step_2_gate_accuracy: 86.3452
STEP-2	Epoch: 120/200	classification_loss: 0.4654	gate_loss: 0.3634	step2_classification_accuracy: 85.0776	step_2_gate_accuracy: 87.3202
STEP-2	Epoch: 140/200	classification_loss: 0.4364	gate_loss: 0.3366	step2_classification_accuracy: 85.8373	step_2_gate_accuracy: 88.1084
STEP-2	Epoch: 160/200	classification_loss: 0.4226	gate_loss: 0.3210	step2_classification_accuracy: 86.1014	step_2_gate_accuracy: 88.7056
STEP-2	Epoch: 180/200	classification_loss: 0.4020	gate_loss: 0.3044	step2_classification_accuracy: 86.6783	step_2_gate_accuracy: 89.2947
STEP-2	Epoch: 200/200	classification_loss: 0.3899	gate_loss: 0.2936	step2_classification_accuracy: 87.0115	step_2_gate_accuracy: 89.4775
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.1579	gate_accuracy: 71.4286
	Task-1	val_accuracy: 61.3861	gate_accuracy: 65.3465
	Task-2	val_accuracy: 77.9221	gate_accuracy: 79.2208
	Task-3	val_accuracy: 76.3889	gate_accuracy: 80.5556
	Task-4	val_accuracy: 67.4419	gate_accuracy: 66.2791
	Task-5	val_accuracy: 65.2778	gate_accuracy: 65.2778
	Task-6	val_accuracy: 63.9344	gate_accuracy: 65.5738
	Task-7	val_accuracy: 74.3902	gate_accuracy: 64.6341
	Task-8	val_accuracy: 73.8636	gate_accuracy: 70.4545
	Task-9	val_accuracy: 55.9524	gate_accuracy: 54.7619
	Task-10	val_accuracy: 76.2500	gate_accuracy: 70.0000
	Task-11	val_accuracy: 75.2941	gate_accuracy: 65.8824
	Task-12	val_accuracy: 64.1026	gate_accuracy: 51.2821
	Task-13	val_accuracy: 81.7073	gate_accuracy: 76.8293
	Task-14	val_accuracy: 63.6364	gate_accuracy: 61.0390
	Task-15	val_accuracy: 72.2892	gate_accuracy: 65.0602
	Task-16	val_accuracy: 79.0123	gate_accuracy: 72.8395
	Task-17	val_accuracy: 77.9070	gate_accuracy: 72.0930
	Task-18	val_accuracy: 53.5714	gate_accuracy: 54.7619
	Task-19	val_accuracy: 63.0952	gate_accuracy: 64.2857
	Task-20	val_accuracy: 83.3333	gate_accuracy: 77.3810
	Task-21	val_accuracy: 68.7500	gate_accuracy: 65.0000
	Task-22	val_accuracy: 69.0141	gate_accuracy: 66.1972
	Task-23	val_accuracy: 77.3810	gate_accuracy: 75.0000
	Task-24	val_accuracy: 60.0000	gate_accuracy: 61.2500
	Task-25	val_accuracy: 67.8571	gate_accuracy: 73.8095
	Task-26	val_accuracy: 48.1013	gate_accuracy: 67.0886
	Task-27	val_accuracy: 62.2222	gate_accuracy: 65.5556
	Task-28	val_accuracy: 72.1519	gate_accuracy: 70.8861
	Task-29	val_accuracy: 70.2381	gate_accuracy: 75.0000
	Task-30	val_accuracy: 61.2500	gate_accuracy: 61.2500
	Task-31	val_accuracy: 57.6087	gate_accuracy: 59.7826
	Task-32	val_accuracy: 63.8889	gate_accuracy: 68.0556
	Task-33	val_accuracy: 62.5000	gate_accuracy: 61.3636
	Task-34	val_accuracy: 56.9620	gate_accuracy: 54.4304
	Task-35	val_accuracy: 41.4286	gate_accuracy: 42.8571
	Task-36	val_accuracy: 59.7222	gate_accuracy: 68.0556
	Task-37	val_accuracy: 60.0000	gate_accuracy: 69.3333
	Task-38	val_accuracy: 70.5882	gate_accuracy: 72.9412
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.6042


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 2.6263	step1_train_accuracy: 54.4910
STEP-1	Epoch: 20/50	loss: 0.8606	step1_train_accuracy: 86.2275
STEP-1	Epoch: 30/50	loss: 0.3837	step1_train_accuracy: 94.9102
STEP-1	Epoch: 40/50	loss: 0.2401	step1_train_accuracy: 97.0060
STEP-1	Epoch: 50/50	loss: 0.1752	step1_train_accuracy: 97.6048
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32, 734: 32, 735: 32, 736: 32, 737: 32, 738: 32, 739: 32, 740: 32, 741: 32, 742: 32, 743: 32, 744: 32, 745: 32, 746: 32, 747: 32, 748: 32, 749: 32, 750: 32, 751: 32, 752: 32, 753: 32, 754: 32, 755: 32, 756: 32, 757: 32, 758: 32, 759: 32, 760: 32, 761: 32, 762: 32, 763: 32, 764: 32, 765: 32, 766: 32, 767: 32, 768: 32, 769: 32, 770: 32, 771: 32, 772: 32, 773: 32, 774: 32, 775: 32, 776: 32, 777: 32, 778: 32, 779: 32, 780: 32, 781: 32, 782: 32, 783: 32, 784: 32, 785: 32, 786: 32, 787: 32, 788: 32, 789: 32, 790: 32, 791: 32, 792: 32, 793: 32, 794: 32, 795: 32, 796: 32, 797: 32, 798: 32, 799: 32, 800: 32, 801: 32, 802: 32, 803: 32, 804: 32, 805: 32, 806: 32, 807: 32, 808: 32, 809: 32, 810: 32, 811: 32, 812: 32, 813: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.9343	gate_loss: 0.9710	step2_classification_accuracy: 73.3454	step_2_gate_accuracy: 70.4469
STEP-2	Epoch: 40/200	classification_loss: 0.7198	gate_loss: 0.6375	step2_classification_accuracy: 78.8890	step_2_gate_accuracy: 79.0963
STEP-2	Epoch: 60/200	classification_loss: 0.6207	gate_loss: 0.5210	step2_classification_accuracy: 81.3997	step_2_gate_accuracy: 82.6206
STEP-2	Epoch: 80/200	classification_loss: 0.5589	gate_loss: 0.4567	step2_classification_accuracy: 82.7703	step_2_gate_accuracy: 84.1638
STEP-2	Epoch: 100/200	classification_loss: 0.5140	gate_loss: 0.4092	step2_classification_accuracy: 84.1293	step_2_gate_accuracy: 85.6611
STEP-2	Epoch: 120/200	classification_loss: 0.4839	gate_loss: 0.3807	step2_classification_accuracy: 84.7781	step_2_gate_accuracy: 86.6746
STEP-2	Epoch: 140/200	classification_loss: 0.4528	gate_loss: 0.3501	step2_classification_accuracy: 85.6572	step_2_gate_accuracy: 87.5921
STEP-2	Epoch: 160/200	classification_loss: 0.4352	gate_loss: 0.3336	step2_classification_accuracy: 86.1832	step_2_gate_accuracy: 88.1987
STEP-2	Epoch: 180/200	classification_loss: 0.4220	gate_loss: 0.3212	step2_classification_accuracy: 86.3521	step_2_gate_accuracy: 88.5365
STEP-2	Epoch: 200/200	classification_loss: 0.3972	gate_loss: 0.3009	step2_classification_accuracy: 87.0854	step_2_gate_accuracy: 89.3466
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.1729	gate_accuracy: 75.9398
	Task-1	val_accuracy: 65.3465	gate_accuracy: 65.3465
	Task-2	val_accuracy: 76.6234	gate_accuracy: 83.1169
	Task-3	val_accuracy: 80.5556	gate_accuracy: 79.1667
	Task-4	val_accuracy: 60.4651	gate_accuracy: 56.9767
	Task-5	val_accuracy: 68.0556	gate_accuracy: 65.2778
	Task-6	val_accuracy: 67.2131	gate_accuracy: 62.2951
	Task-7	val_accuracy: 73.1707	gate_accuracy: 62.1951
	Task-8	val_accuracy: 70.4545	gate_accuracy: 70.4545
	Task-9	val_accuracy: 57.1429	gate_accuracy: 50.0000
	Task-10	val_accuracy: 76.2500	gate_accuracy: 71.2500
	Task-11	val_accuracy: 76.4706	gate_accuracy: 65.8824
	Task-12	val_accuracy: 64.1026	gate_accuracy: 61.5385
	Task-13	val_accuracy: 71.9512	gate_accuracy: 67.0732
	Task-14	val_accuracy: 61.0390	gate_accuracy: 57.1429
	Task-15	val_accuracy: 73.4940	gate_accuracy: 54.2169
	Task-16	val_accuracy: 85.1852	gate_accuracy: 80.2469
	Task-17	val_accuracy: 81.3953	gate_accuracy: 76.7442
	Task-18	val_accuracy: 53.5714	gate_accuracy: 51.1905
	Task-19	val_accuracy: 60.7143	gate_accuracy: 55.9524
	Task-20	val_accuracy: 73.8095	gate_accuracy: 63.0952
	Task-21	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-22	val_accuracy: 66.1972	gate_accuracy: 61.9718
	Task-23	val_accuracy: 78.5714	gate_accuracy: 78.5714
	Task-24	val_accuracy: 63.7500	gate_accuracy: 61.2500
	Task-25	val_accuracy: 67.8571	gate_accuracy: 63.0952
	Task-26	val_accuracy: 46.8354	gate_accuracy: 56.9620
	Task-27	val_accuracy: 72.2222	gate_accuracy: 77.7778
	Task-28	val_accuracy: 77.2152	gate_accuracy: 74.6835
	Task-29	val_accuracy: 65.4762	gate_accuracy: 65.4762
	Task-30	val_accuracy: 70.0000	gate_accuracy: 67.5000
	Task-31	val_accuracy: 56.5217	gate_accuracy: 58.6957
	Task-32	val_accuracy: 75.0000	gate_accuracy: 72.2222
	Task-33	val_accuracy: 73.8636	gate_accuracy: 76.1364
	Task-34	val_accuracy: 60.7595	gate_accuracy: 59.4937
	Task-35	val_accuracy: 44.2857	gate_accuracy: 48.5714
	Task-36	val_accuracy: 58.3333	gate_accuracy: 76.3889
	Task-37	val_accuracy: 64.0000	gate_accuracy: 64.0000
	Task-38	val_accuracy: 68.2353	gate_accuracy: 71.7647
	Task-39	val_accuracy: 58.3333	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.4538


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 2.8758	step1_train_accuracy: 48.9028
STEP-1	Epoch: 20/50	loss: 1.2162	step1_train_accuracy: 70.5329
STEP-1	Epoch: 30/50	loss: 0.5887	step1_train_accuracy: 92.1630
STEP-1	Epoch: 40/50	loss: 0.3560	step1_train_accuracy: 94.9843
STEP-1	Epoch: 50/50	loss: 0.2463	step1_train_accuracy: 97.4922
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32, 734: 32, 735: 32, 736: 32, 737: 32, 738: 32, 739: 32, 740: 32, 741: 32, 742: 32, 743: 32, 744: 32, 745: 32, 746: 32, 747: 32, 748: 32, 749: 32, 750: 32, 751: 32, 752: 32, 753: 32, 754: 32, 755: 32, 756: 32, 757: 32, 758: 32, 759: 32, 760: 32, 761: 32, 762: 32, 763: 32, 764: 32, 765: 32, 766: 32, 767: 32, 768: 32, 769: 32, 770: 32, 771: 32, 772: 32, 773: 32, 774: 32, 775: 32, 776: 32, 777: 32, 778: 32, 779: 32, 780: 32, 781: 32, 782: 32, 783: 32, 784: 32, 785: 32, 786: 32, 787: 32, 788: 32, 789: 32, 790: 32, 791: 32, 792: 32, 793: 32, 794: 32, 795: 32, 796: 32, 797: 32, 798: 32, 799: 32, 800: 32, 801: 32, 802: 32, 803: 32, 804: 32, 805: 32, 806: 32, 807: 32, 808: 32, 809: 32, 810: 32, 811: 32, 812: 32, 813: 32, 814: 32, 815: 32, 816: 32, 817: 32, 818: 32, 819: 32, 820: 32, 821: 32, 822: 32, 823: 32, 824: 32, 825: 32, 826: 32, 827: 32, 828: 32, 829: 32, 830: 32, 831: 32, 832: 32, 833: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.9409	gate_loss: 0.9723	step2_classification_accuracy: 73.0890	step_2_gate_accuracy: 69.8441
STEP-2	Epoch: 40/200	classification_loss: 0.7279	gate_loss: 0.6445	step2_classification_accuracy: 78.6121	step_2_gate_accuracy: 78.5709
STEP-2	Epoch: 60/200	classification_loss: 0.6225	gate_loss: 0.5257	step2_classification_accuracy: 81.1076	step_2_gate_accuracy: 82.1643
STEP-2	Epoch: 80/200	classification_loss: 0.5539	gate_loss: 0.4531	step2_classification_accuracy: 82.9399	step_2_gate_accuracy: 84.0977
STEP-2	Epoch: 100/200	classification_loss: 0.5176	gate_loss: 0.4115	step2_classification_accuracy: 83.9216	step_2_gate_accuracy: 85.6153
STEP-2	Epoch: 120/200	classification_loss: 0.4970	gate_loss: 0.3878	step2_classification_accuracy: 84.6897	step_2_gate_accuracy: 86.4359
STEP-2	Epoch: 140/200	classification_loss: 0.4600	gate_loss: 0.3581	step2_classification_accuracy: 85.4391	step_2_gate_accuracy: 87.2639
STEP-2	Epoch: 160/200	classification_loss: 0.4373	gate_loss: 0.3365	step2_classification_accuracy: 85.9525	step_2_gate_accuracy: 88.1820
STEP-2	Epoch: 180/200	classification_loss: 0.4254	gate_loss: 0.3240	step2_classification_accuracy: 86.5033	step_2_gate_accuracy: 88.3431
STEP-2	Epoch: 200/200	classification_loss: 0.4138	gate_loss: 0.3123	step2_classification_accuracy: 86.6906	step_2_gate_accuracy: 88.8452
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.1805	gate_accuracy: 81.2030
	Task-1	val_accuracy: 62.3762	gate_accuracy: 66.3366
	Task-2	val_accuracy: 71.4286	gate_accuracy: 74.0260
	Task-3	val_accuracy: 80.5556	gate_accuracy: 84.7222
	Task-4	val_accuracy: 65.1163	gate_accuracy: 58.1395
	Task-5	val_accuracy: 63.8889	gate_accuracy: 63.8889
	Task-6	val_accuracy: 62.2951	gate_accuracy: 55.7377
	Task-7	val_accuracy: 75.6098	gate_accuracy: 71.9512
	Task-8	val_accuracy: 80.6818	gate_accuracy: 72.7273
	Task-9	val_accuracy: 54.7619	gate_accuracy: 48.8095
	Task-10	val_accuracy: 73.7500	gate_accuracy: 67.5000
	Task-11	val_accuracy: 64.7059	gate_accuracy: 55.2941
	Task-12	val_accuracy: 61.5385	gate_accuracy: 60.2564
	Task-13	val_accuracy: 70.7317	gate_accuracy: 64.6341
	Task-14	val_accuracy: 64.9351	gate_accuracy: 66.2338
	Task-15	val_accuracy: 69.8795	gate_accuracy: 59.0361
	Task-16	val_accuracy: 71.6049	gate_accuracy: 61.7284
	Task-17	val_accuracy: 76.7442	gate_accuracy: 70.9302
	Task-18	val_accuracy: 53.5714	gate_accuracy: 48.8095
	Task-19	val_accuracy: 58.3333	gate_accuracy: 54.7619
	Task-20	val_accuracy: 79.7619	gate_accuracy: 73.8095
	Task-21	val_accuracy: 68.7500	gate_accuracy: 68.7500
	Task-22	val_accuracy: 64.7887	gate_accuracy: 63.3803
	Task-23	val_accuracy: 67.8571	gate_accuracy: 69.0476
	Task-24	val_accuracy: 66.2500	gate_accuracy: 63.7500
	Task-25	val_accuracy: 67.8571	gate_accuracy: 75.0000
	Task-26	val_accuracy: 50.6329	gate_accuracy: 48.1013
	Task-27	val_accuracy: 67.7778	gate_accuracy: 67.7778
	Task-28	val_accuracy: 72.1519	gate_accuracy: 67.0886
	Task-29	val_accuracy: 61.9048	gate_accuracy: 63.0952
	Task-30	val_accuracy: 70.0000	gate_accuracy: 71.2500
	Task-31	val_accuracy: 53.2609	gate_accuracy: 58.6957
	Task-32	val_accuracy: 70.8333	gate_accuracy: 72.2222
	Task-33	val_accuracy: 69.3182	gate_accuracy: 69.3182
	Task-34	val_accuracy: 58.2278	gate_accuracy: 63.2911
	Task-35	val_accuracy: 47.1429	gate_accuracy: 52.8571
	Task-36	val_accuracy: 52.7778	gate_accuracy: 62.5000
	Task-37	val_accuracy: 64.0000	gate_accuracy: 66.6667
	Task-38	val_accuracy: 75.2941	gate_accuracy: 74.1176
	Task-39	val_accuracy: 46.4286	gate_accuracy: 57.1429
	Task-40	val_accuracy: 51.2500	gate_accuracy: 66.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.1722


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 2.6982	step1_train_accuracy: 55.3571
STEP-1	Epoch: 20/50	loss: 0.8297	step1_train_accuracy: 85.7143
STEP-1	Epoch: 30/50	loss: 0.4153	step1_train_accuracy: 92.8571
STEP-1	Epoch: 40/50	loss: 0.2861	step1_train_accuracy: 94.3452
STEP-1	Epoch: 50/50	loss: 0.2184	step1_train_accuracy: 95.5357
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29, 374: 29, 375: 29, 376: 29, 377: 29, 378: 29, 379: 29, 380: 29, 381: 29, 382: 29, 383: 29, 384: 29, 385: 29, 386: 29, 387: 29, 388: 29, 389: 29, 390: 29, 391: 29, 392: 29, 393: 29, 394: 29, 395: 29, 396: 29, 397: 29, 398: 29, 399: 29, 400: 29, 401: 29, 402: 29, 403: 29, 404: 29, 405: 29, 406: 29, 407: 29, 408: 29, 409: 29, 410: 29, 411: 29, 412: 29, 413: 29, 414: 29, 415: 29, 416: 29, 417: 29, 418: 29, 419: 29, 420: 29, 421: 29, 422: 29, 423: 29, 424: 29, 425: 29, 426: 29, 427: 29, 428: 29, 429: 29, 430: 29, 431: 29, 432: 29, 433: 29, 434: 29, 435: 29, 436: 29, 437: 29, 438: 29, 439: 29, 440: 29, 441: 29, 442: 29, 443: 29, 444: 29, 445: 29, 446: 29, 447: 29, 448: 29, 449: 29, 450: 29, 451: 29, 452: 29, 453: 29, 454: 29, 455: 29, 456: 29, 457: 29, 458: 29, 459: 29, 460: 29, 461: 29, 462: 29, 463: 29, 464: 29, 465: 29, 466: 29, 467: 29, 468: 29, 469: 29, 470: 29, 471: 29, 472: 29, 473: 29, 474: 29, 475: 29, 476: 29, 477: 29, 478: 29, 479: 29, 480: 29, 481: 29, 482: 29, 483: 29, 484: 29, 485: 29, 486: 29, 487: 29, 488: 29, 489: 29, 490: 29, 491: 29, 492: 29, 493: 29, 494: 29, 495: 29, 496: 29, 497: 29, 498: 29, 499: 29, 500: 29, 501: 29, 502: 29, 503: 29, 504: 29, 505: 29, 506: 29, 507: 29, 508: 29, 509: 29, 510: 29, 511: 29, 512: 29, 513: 29, 514: 29, 515: 29, 516: 29, 517: 29, 518: 29, 519: 29, 520: 29, 521: 29, 522: 29, 523: 29, 524: 29, 525: 29, 526: 29, 527: 29, 528: 29, 529: 29, 530: 29, 531: 29, 532: 29, 533: 29, 534: 29, 535: 29, 536: 29, 537: 29, 538: 29, 539: 29, 540: 29, 541: 29, 542: 29, 543: 29, 544: 29, 545: 29, 546: 29, 547: 29, 548: 29, 549: 29, 550: 29, 551: 29, 552: 29, 553: 29, 554: 29, 555: 29, 556: 29, 557: 29, 558: 29, 559: 29, 560: 29, 561: 29, 562: 29, 563: 29, 564: 29, 565: 29, 566: 29, 567: 29, 568: 29, 569: 29, 570: 29, 571: 29, 572: 29, 573: 29, 574: 29, 575: 29, 576: 29, 577: 29, 578: 29, 579: 29, 580: 29, 581: 29, 582: 29, 583: 29, 584: 29, 585: 29, 586: 29, 587: 29, 588: 29, 589: 29, 590: 29, 591: 29, 592: 29, 593: 29, 594: 29, 595: 29, 596: 29, 597: 29, 598: 29, 599: 29, 600: 29, 601: 29, 602: 29, 603: 29, 604: 29, 605: 29, 606: 29, 607: 29, 608: 29, 609: 29, 610: 29, 611: 29, 612: 29, 613: 29, 614: 29, 615: 29, 616: 29, 617: 29, 618: 29, 619: 29, 620: 29, 621: 29, 622: 29, 623: 29, 624: 29, 625: 29, 626: 29, 627: 29, 628: 29, 629: 29, 630: 29, 631: 29, 632: 29, 633: 29, 634: 29, 635: 29, 636: 29, 637: 29, 638: 29, 639: 29, 640: 29, 641: 29, 642: 29, 643: 29, 644: 29, 645: 29, 646: 29, 647: 29, 648: 29, 649: 29, 650: 29, 651: 29, 652: 29, 653: 29, 654: 29, 655: 29, 656: 29, 657: 29, 658: 29, 659: 29, 660: 29, 661: 29, 662: 29, 663: 29, 664: 29, 665: 29, 666: 29, 667: 29, 668: 29, 669: 29, 670: 29, 671: 29, 672: 29, 673: 29, 674: 29, 675: 29, 676: 29, 677: 29, 678: 29, 679: 29, 680: 29, 681: 29, 682: 29, 683: 29, 684: 29, 685: 29, 686: 29, 687: 29, 688: 29, 689: 29, 690: 29, 691: 29, 692: 29, 693: 29, 694: 29, 695: 29, 696: 29, 697: 29, 698: 29, 699: 29, 700: 29, 701: 29, 702: 29, 703: 29, 704: 29, 705: 29, 706: 29, 707: 29, 708: 29, 709: 29, 710: 29, 711: 29, 712: 29, 713: 29, 714: 29, 715: 29, 716: 29, 717: 29, 718: 29, 719: 29, 720: 29, 721: 29, 722: 29, 723: 29, 724: 29, 725: 29, 726: 29, 727: 29, 728: 29, 729: 29, 730: 29, 731: 29, 732: 29, 733: 29, 734: 29, 735: 29, 736: 29, 737: 29, 738: 29, 739: 29, 740: 29, 741: 29, 742: 29, 743: 29, 744: 29, 745: 29, 746: 29, 747: 29, 748: 29, 749: 29, 750: 29, 751: 29, 752: 29, 753: 29, 754: 29, 755: 29, 756: 29, 757: 29, 758: 29, 759: 29, 760: 29, 761: 29, 762: 29, 763: 29, 764: 29, 765: 29, 766: 29, 767: 29, 768: 29, 769: 29, 770: 29, 771: 29, 772: 29, 773: 29, 774: 29, 775: 29, 776: 29, 777: 29, 778: 29, 779: 29, 780: 29, 781: 29, 782: 29, 783: 29, 784: 29, 785: 29, 786: 29, 787: 29, 788: 29, 789: 29, 790: 29, 791: 29, 792: 29, 793: 29, 794: 29, 795: 29, 796: 29, 797: 29, 798: 29, 799: 29, 800: 29, 801: 29, 802: 29, 803: 29, 804: 29, 805: 29, 806: 29, 807: 29, 808: 29, 809: 29, 810: 29, 811: 29, 812: 29, 813: 29, 814: 29, 815: 29, 816: 29, 817: 29, 818: 29, 819: 29, 820: 29, 821: 29, 822: 29, 823: 29, 824: 29, 825: 29, 826: 29, 827: 29, 828: 29, 829: 29, 830: 29, 831: 29, 832: 29, 833: 29, 834: 29, 835: 29, 836: 29, 837: 29, 838: 29, 839: 29, 840: 29, 841: 29, 842: 29, 843: 29, 844: 29, 845: 29, 846: 29, 847: 29, 848: 29, 849: 29, 850: 29, 851: 29, 852: 29, 853: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.9657	gate_loss: 1.0232	step2_classification_accuracy: 72.7086	step_2_gate_accuracy: 69.1755
STEP-2	Epoch: 40/200	classification_loss: 0.7430	gate_loss: 0.6623	step2_classification_accuracy: 78.4099	step_2_gate_accuracy: 78.5472
STEP-2	Epoch: 60/200	classification_loss: 0.6323	gate_loss: 0.5358	step2_classification_accuracy: 81.1718	step_2_gate_accuracy: 81.8945
STEP-2	Epoch: 80/200	classification_loss: 0.5704	gate_loss: 0.4680	step2_classification_accuracy: 82.4800	step_2_gate_accuracy: 83.9982
STEP-2	Epoch: 100/200	classification_loss: 0.5167	gate_loss: 0.4150	step2_classification_accuracy: 83.9215	step_2_gate_accuracy: 85.4801
STEP-2	Epoch: 120/200	classification_loss: 0.4858	gate_loss: 0.3852	step2_classification_accuracy: 84.9633	step_2_gate_accuracy: 86.5663
STEP-2	Epoch: 140/200	classification_loss: 0.4613	gate_loss: 0.3598	step2_classification_accuracy: 85.5124	step_2_gate_accuracy: 87.3375
STEP-2	Epoch: 160/200	classification_loss: 0.4413	gate_loss: 0.3405	step2_classification_accuracy: 86.0090	step_2_gate_accuracy: 87.8947
STEP-2	Epoch: 180/200	classification_loss: 0.4254	gate_loss: 0.3236	step2_classification_accuracy: 86.4613	step_2_gate_accuracy: 88.5448
STEP-2	Epoch: 200/200	classification_loss: 0.4067	gate_loss: 0.3060	step2_classification_accuracy: 87.0185	step_2_gate_accuracy: 89.0293
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.6767	gate_accuracy: 81.2030
	Task-1	val_accuracy: 72.2772	gate_accuracy: 71.2871
	Task-2	val_accuracy: 68.8312	gate_accuracy: 68.8312
	Task-3	val_accuracy: 75.0000	gate_accuracy: 77.7778
	Task-4	val_accuracy: 61.6279	gate_accuracy: 56.9767
	Task-5	val_accuracy: 61.1111	gate_accuracy: 59.7222
	Task-6	val_accuracy: 73.7705	gate_accuracy: 67.2131
	Task-7	val_accuracy: 80.4878	gate_accuracy: 68.2927
	Task-8	val_accuracy: 75.0000	gate_accuracy: 69.3182
	Task-9	val_accuracy: 52.3810	gate_accuracy: 44.0476
	Task-10	val_accuracy: 71.2500	gate_accuracy: 67.5000
	Task-11	val_accuracy: 68.2353	gate_accuracy: 48.2353
	Task-12	val_accuracy: 65.3846	gate_accuracy: 60.2564
	Task-13	val_accuracy: 75.6098	gate_accuracy: 73.1707
	Task-14	val_accuracy: 64.9351	gate_accuracy: 67.5325
	Task-15	val_accuracy: 69.8795	gate_accuracy: 61.4458
	Task-16	val_accuracy: 74.0741	gate_accuracy: 64.1975
	Task-17	val_accuracy: 70.9302	gate_accuracy: 61.6279
	Task-18	val_accuracy: 57.1429	gate_accuracy: 58.3333
	Task-19	val_accuracy: 55.9524	gate_accuracy: 54.7619
	Task-20	val_accuracy: 78.5714	gate_accuracy: 76.1905
	Task-21	val_accuracy: 70.0000	gate_accuracy: 67.5000
	Task-22	val_accuracy: 60.5634	gate_accuracy: 57.7465
	Task-23	val_accuracy: 63.0952	gate_accuracy: 61.9048
	Task-24	val_accuracy: 60.0000	gate_accuracy: 62.5000
	Task-25	val_accuracy: 59.5238	gate_accuracy: 59.5238
	Task-26	val_accuracy: 50.6329	gate_accuracy: 51.8987
	Task-27	val_accuracy: 67.7778	gate_accuracy: 70.0000
	Task-28	val_accuracy: 77.2152	gate_accuracy: 77.2152
	Task-29	val_accuracy: 63.0952	gate_accuracy: 65.4762
	Task-30	val_accuracy: 66.2500	gate_accuracy: 67.5000
	Task-31	val_accuracy: 57.6087	gate_accuracy: 63.0435
	Task-32	val_accuracy: 63.8889	gate_accuracy: 65.2778
	Task-33	val_accuracy: 71.5909	gate_accuracy: 71.5909
	Task-34	val_accuracy: 58.2278	gate_accuracy: 58.2278
	Task-35	val_accuracy: 44.2857	gate_accuracy: 54.2857
	Task-36	val_accuracy: 40.2778	gate_accuracy: 45.8333
	Task-37	val_accuracy: 68.0000	gate_accuracy: 69.3333
	Task-38	val_accuracy: 65.8824	gate_accuracy: 65.8824
	Task-39	val_accuracy: 51.1905	gate_accuracy: 58.3333
	Task-40	val_accuracy: 52.5000	gate_accuracy: 67.5000
	Task-41	val_accuracy: 67.8571	gate_accuracy: 71.4286
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.3685


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 2.6061	step1_train_accuracy: 56.8365
STEP-1	Epoch: 20/50	loss: 0.7940	step1_train_accuracy: 87.1314
STEP-1	Epoch: 30/50	loss: 0.3566	step1_train_accuracy: 95.9786
STEP-1	Epoch: 40/50	loss: 0.2230	step1_train_accuracy: 97.8552
STEP-1	Epoch: 50/50	loss: 0.1580	step1_train_accuracy: 98.1233
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31, 794: 31, 795: 31, 796: 31, 797: 31, 798: 31, 799: 31, 800: 31, 801: 31, 802: 31, 803: 31, 804: 31, 805: 31, 806: 31, 807: 31, 808: 31, 809: 31, 810: 31, 811: 31, 812: 31, 813: 31, 814: 31, 815: 31, 816: 31, 817: 31, 818: 31, 819: 31, 820: 31, 821: 31, 822: 31, 823: 31, 824: 31, 825: 31, 826: 31, 827: 31, 828: 31, 829: 31, 830: 31, 831: 31, 832: 31, 833: 31, 834: 31, 835: 31, 836: 31, 837: 31, 838: 31, 839: 31, 840: 31, 841: 31, 842: 31, 843: 31, 844: 31, 845: 31, 846: 31, 847: 31, 848: 31, 849: 31, 850: 31, 851: 31, 852: 31, 853: 31, 854: 31, 855: 31, 856: 31, 857: 31, 858: 31, 859: 31, 860: 31, 861: 31, 862: 31, 863: 31, 864: 31, 865: 31, 866: 31, 867: 31, 868: 31, 869: 31, 870: 31, 871: 31, 872: 31, 873: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9768	gate_loss: 1.0370	step2_classification_accuracy: 72.0713	step_2_gate_accuracy: 68.3915
STEP-2	Epoch: 40/200	classification_loss: 0.7543	gate_loss: 0.6877	step2_classification_accuracy: 77.9619	step_2_gate_accuracy: 77.1093
STEP-2	Epoch: 60/200	classification_loss: 0.6414	gate_loss: 0.5529	step2_classification_accuracy: 80.7633	step_2_gate_accuracy: 81.4165
STEP-2	Epoch: 80/200	classification_loss: 0.5811	gate_loss: 0.4890	step2_classification_accuracy: 82.1326	step_2_gate_accuracy: 83.0479
STEP-2	Epoch: 100/200	classification_loss: 0.5377	gate_loss: 0.4428	step2_classification_accuracy: 83.3912	step_2_gate_accuracy: 84.6866
STEP-2	Epoch: 120/200	classification_loss: 0.5014	gate_loss: 0.4069	step2_classification_accuracy: 84.2622	step_2_gate_accuracy: 85.5909
STEP-2	Epoch: 140/200	classification_loss: 0.4785	gate_loss: 0.3848	step2_classification_accuracy: 84.8786	step_2_gate_accuracy: 86.4177
STEP-2	Epoch: 160/200	classification_loss: 0.4616	gate_loss: 0.3663	step2_classification_accuracy: 85.3104	step_2_gate_accuracy: 87.1632
STEP-2	Epoch: 180/200	classification_loss: 0.4413	gate_loss: 0.3472	step2_classification_accuracy: 85.8013	step_2_gate_accuracy: 87.6984
STEP-2	Epoch: 200/200	classification_loss: 0.4241	gate_loss: 0.3317	step2_classification_accuracy: 86.3032	step_2_gate_accuracy: 88.0675
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.3985	gate_accuracy: 68.4211
	Task-1	val_accuracy: 68.3168	gate_accuracy: 75.2475
	Task-2	val_accuracy: 76.6234	gate_accuracy: 77.9221
	Task-3	val_accuracy: 75.0000	gate_accuracy: 80.5556
	Task-4	val_accuracy: 65.1163	gate_accuracy: 60.4651
	Task-5	val_accuracy: 66.6667	gate_accuracy: 66.6667
	Task-6	val_accuracy: 67.2131	gate_accuracy: 55.7377
	Task-7	val_accuracy: 79.2683	gate_accuracy: 69.5122
	Task-8	val_accuracy: 72.7273	gate_accuracy: 71.5909
	Task-9	val_accuracy: 55.9524	gate_accuracy: 50.0000
	Task-10	val_accuracy: 73.7500	gate_accuracy: 67.5000
	Task-11	val_accuracy: 61.1765	gate_accuracy: 50.5882
	Task-12	val_accuracy: 65.3846	gate_accuracy: 50.0000
	Task-13	val_accuracy: 71.9512	gate_accuracy: 68.2927
	Task-14	val_accuracy: 71.4286	gate_accuracy: 68.8312
	Task-15	val_accuracy: 61.4458	gate_accuracy: 59.0361
	Task-16	val_accuracy: 80.2469	gate_accuracy: 65.4321
	Task-17	val_accuracy: 77.9070	gate_accuracy: 70.9302
	Task-18	val_accuracy: 54.7619	gate_accuracy: 51.1905
	Task-19	val_accuracy: 57.1429	gate_accuracy: 58.3333
	Task-20	val_accuracy: 72.6190	gate_accuracy: 63.0952
	Task-21	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-22	val_accuracy: 71.8310	gate_accuracy: 69.0141
	Task-23	val_accuracy: 73.8095	gate_accuracy: 66.6667
	Task-24	val_accuracy: 62.5000	gate_accuracy: 58.7500
	Task-25	val_accuracy: 73.8095	gate_accuracy: 67.8571
	Task-26	val_accuracy: 49.3671	gate_accuracy: 56.9620
	Task-27	val_accuracy: 64.4444	gate_accuracy: 70.0000
	Task-28	val_accuracy: 73.4177	gate_accuracy: 67.0886
	Task-29	val_accuracy: 61.9048	gate_accuracy: 64.2857
	Task-30	val_accuracy: 67.5000	gate_accuracy: 67.5000
	Task-31	val_accuracy: 55.4348	gate_accuracy: 57.6087
	Task-32	val_accuracy: 69.4444	gate_accuracy: 70.8333
	Task-33	val_accuracy: 67.0455	gate_accuracy: 69.3182
	Task-34	val_accuracy: 65.8228	gate_accuracy: 72.1519
	Task-35	val_accuracy: 47.1429	gate_accuracy: 51.4286
	Task-36	val_accuracy: 55.5556	gate_accuracy: 66.6667
	Task-37	val_accuracy: 69.3333	gate_accuracy: 69.3333
	Task-38	val_accuracy: 70.5882	gate_accuracy: 72.9412
	Task-39	val_accuracy: 52.3810	gate_accuracy: 60.7143
	Task-40	val_accuracy: 50.0000	gate_accuracy: 55.0000
	Task-41	val_accuracy: 69.0476	gate_accuracy: 71.4286
	Task-42	val_accuracy: 53.7634	gate_accuracy: 61.2903
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.9929


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 2.1915	step1_train_accuracy: 59.4203
STEP-1	Epoch: 20/50	loss: 0.7227	step1_train_accuracy: 86.0870
STEP-1	Epoch: 30/50	loss: 0.3549	step1_train_accuracy: 96.2319
STEP-1	Epoch: 40/50	loss: 0.2231	step1_train_accuracy: 97.6812
STEP-1	Epoch: 50/50	loss: 0.1508	step1_train_accuracy: 98.5507
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31, 794: 31, 795: 31, 796: 31, 797: 31, 798: 31, 799: 31, 800: 31, 801: 31, 802: 31, 803: 31, 804: 31, 805: 31, 806: 31, 807: 31, 808: 31, 809: 31, 810: 31, 811: 31, 812: 31, 813: 31, 814: 31, 815: 31, 816: 31, 817: 31, 818: 31, 819: 31, 820: 31, 821: 31, 822: 31, 823: 31, 824: 31, 825: 31, 826: 31, 827: 31, 828: 31, 829: 31, 830: 31, 831: 31, 832: 31, 833: 31, 834: 31, 835: 31, 836: 31, 837: 31, 838: 31, 839: 31, 840: 31, 841: 31, 842: 31, 843: 31, 844: 31, 845: 31, 846: 31, 847: 31, 848: 31, 849: 31, 850: 31, 851: 31, 852: 31, 853: 31, 854: 31, 855: 31, 856: 31, 857: 31, 858: 31, 859: 31, 860: 31, 861: 31, 862: 31, 863: 31, 864: 31, 865: 31, 866: 31, 867: 31, 868: 31, 869: 31, 870: 31, 871: 31, 872: 31, 873: 31, 874: 31, 875: 31, 876: 31, 877: 31, 878: 31, 879: 31, 880: 31, 881: 31, 882: 31, 883: 31, 884: 31, 885: 31, 886: 31, 887: 31, 888: 31, 889: 31, 890: 31, 891: 31, 892: 31, 893: 31})
STEP-2	Epoch: 20/200	classification_loss: 1.0092	gate_loss: 1.0468	step2_classification_accuracy: 71.5198	step_2_gate_accuracy: 68.1858
STEP-2	Epoch: 40/200	classification_loss: 0.7727	gate_loss: 0.6917	step2_classification_accuracy: 77.6467	step_2_gate_accuracy: 77.5384
STEP-2	Epoch: 60/200	classification_loss: 0.6597	gate_loss: 0.5651	step2_classification_accuracy: 80.4647	step_2_gate_accuracy: 80.9230
STEP-2	Epoch: 80/200	classification_loss: 0.5916	gate_loss: 0.4895	step2_classification_accuracy: 82.2112	step_2_gate_accuracy: 83.0555
STEP-2	Epoch: 100/200	classification_loss: 0.5444	gate_loss: 0.4416	step2_classification_accuracy: 83.3730	step_2_gate_accuracy: 84.7370
STEP-2	Epoch: 120/200	classification_loss: 0.5177	gate_loss: 0.4111	step2_classification_accuracy: 83.8782	step_2_gate_accuracy: 85.6462
STEP-2	Epoch: 140/200	classification_loss: 0.4790	gate_loss: 0.3786	step2_classification_accuracy: 85.0870	step_2_gate_accuracy: 86.6457
STEP-2	Epoch: 160/200	classification_loss: 0.4684	gate_loss: 0.3668	step2_classification_accuracy: 85.2890	step_2_gate_accuracy: 86.7901
STEP-2	Epoch: 180/200	classification_loss: 0.4454	gate_loss: 0.3484	step2_classification_accuracy: 86.0107	step_2_gate_accuracy: 87.6164
STEP-2	Epoch: 200/200	classification_loss: 0.4375	gate_loss: 0.3377	step2_classification_accuracy: 86.0720	step_2_gate_accuracy: 88.0891
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 61.6541	gate_accuracy: 67.6692
	Task-1	val_accuracy: 65.3465	gate_accuracy: 70.2970
	Task-2	val_accuracy: 79.2208	gate_accuracy: 76.6234
	Task-3	val_accuracy: 76.3889	gate_accuracy: 80.5556
	Task-4	val_accuracy: 61.6279	gate_accuracy: 54.6512
	Task-5	val_accuracy: 61.1111	gate_accuracy: 61.1111
	Task-6	val_accuracy: 70.4918	gate_accuracy: 60.6557
	Task-7	val_accuracy: 80.4878	gate_accuracy: 74.3902
	Task-8	val_accuracy: 75.0000	gate_accuracy: 71.5909
	Task-9	val_accuracy: 54.7619	gate_accuracy: 50.0000
	Task-10	val_accuracy: 70.0000	gate_accuracy: 61.2500
	Task-11	val_accuracy: 70.5882	gate_accuracy: 63.5294
	Task-12	val_accuracy: 66.6667	gate_accuracy: 60.2564
	Task-13	val_accuracy: 75.6098	gate_accuracy: 69.5122
	Task-14	val_accuracy: 67.5325	gate_accuracy: 67.5325
	Task-15	val_accuracy: 69.8795	gate_accuracy: 55.4217
	Task-16	val_accuracy: 80.2469	gate_accuracy: 72.8395
	Task-17	val_accuracy: 81.3953	gate_accuracy: 75.5814
	Task-18	val_accuracy: 60.7143	gate_accuracy: 55.9524
	Task-19	val_accuracy: 55.9524	gate_accuracy: 59.5238
	Task-20	val_accuracy: 75.0000	gate_accuracy: 76.1905
	Task-21	val_accuracy: 68.7500	gate_accuracy: 63.7500
	Task-22	val_accuracy: 77.4648	gate_accuracy: 73.2394
	Task-23	val_accuracy: 76.1905	gate_accuracy: 73.8095
	Task-24	val_accuracy: 62.5000	gate_accuracy: 60.0000
	Task-25	val_accuracy: 63.0952	gate_accuracy: 61.9048
	Task-26	val_accuracy: 49.3671	gate_accuracy: 51.8987
	Task-27	val_accuracy: 63.3333	gate_accuracy: 64.4444
	Task-28	val_accuracy: 69.6203	gate_accuracy: 68.3544
	Task-29	val_accuracy: 64.2857	gate_accuracy: 71.4286
	Task-30	val_accuracy: 66.2500	gate_accuracy: 71.2500
	Task-31	val_accuracy: 59.7826	gate_accuracy: 64.1304
	Task-32	val_accuracy: 62.5000	gate_accuracy: 62.5000
	Task-33	val_accuracy: 60.2273	gate_accuracy: 60.2273
	Task-34	val_accuracy: 64.5570	gate_accuracy: 64.5570
	Task-35	val_accuracy: 48.5714	gate_accuracy: 60.0000
	Task-36	val_accuracy: 55.5556	gate_accuracy: 55.5556
	Task-37	val_accuracy: 62.6667	gate_accuracy: 64.0000
	Task-38	val_accuracy: 67.0588	gate_accuracy: 70.5882
	Task-39	val_accuracy: 48.8095	gate_accuracy: 60.7143
	Task-40	val_accuracy: 46.2500	gate_accuracy: 57.5000
	Task-41	val_accuracy: 70.2381	gate_accuracy: 80.9524
	Task-42	val_accuracy: 48.3871	gate_accuracy: 52.6882
	Task-43	val_accuracy: 79.0698	gate_accuracy: 83.7209
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.5742


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 20:40:27	Wall time: 19:50:58
CPU time: 74427.049902218	Wall time: 71458.18579053879
FAA: 72.96810025955077
FF: 25.516352480591237

TRAINER.METRIC.ACCURACY
0: [99.24812030075188]
1: [95.48872180451127, 92.07920792079209]
2: [96.2406015037594, 91.0891089108911, 93.5064935064935]
3: [93.98496240601504, 93.06930693069307, 90.9090909090909, 88.88888888888889]
4: [95.48872180451127, 92.07920792079209, 85.71428571428571, 88.88888888888889, 81.3953488372093]
5: [90.22556390977444, 83.16831683168317, 88.31168831168831, 87.5, 83.72093023255815, 83.33333333333334]
6: [88.7218045112782, 85.14851485148515, 84.4155844155844, 87.5, 80.23255813953489, 83.33333333333334, 88.52459016393442]
7: [86.46616541353383, 82.17821782178217, 81.81818181818183, 88.88888888888889, 80.23255813953489, 77.77777777777779, 85.24590163934425, 90.2439024390244]
8: [88.7218045112782, 82.17821782178217, 85.71428571428571, 86.11111111111111, 77.90697674418605, 80.55555555555556, 85.24590163934425, 89.02439024390245, 82.95454545454545]
9: [87.21804511278195, 73.26732673267327, 83.11688311688312, 88.88888888888889, 76.74418604651163, 80.55555555555556, 83.60655737704919, 86.58536585365853, 78.4090909090909, 69.04761904761905]
10: [87.21804511278195, 75.24752475247524, 85.71428571428571, 86.11111111111111, 76.74418604651163, 84.72222222222221, 83.60655737704919, 90.2439024390244, 80.68181818181817, 67.85714285714286, 80.0]
11: [87.21804511278195, 78.21782178217822, 85.71428571428571, 87.5, 76.74418604651163, 81.94444444444444, 78.68852459016394, 87.8048780487805, 75.0, 64.28571428571429, 80.0, 85.88235294117646]
12: [83.45864661654136, 77.22772277227723, 80.51948051948052, 87.5, 73.25581395348837, 70.83333333333334, 80.32786885245902, 87.8048780487805, 82.95454545454545, 71.42857142857143, 78.75, 84.70588235294117, 78.2051282051282]
13: [83.45864661654136, 71.28712871287128, 81.81818181818183, 84.72222222222221, 77.90697674418605, 77.77777777777779, 73.77049180327869, 89.02439024390245, 80.68181818181817, 70.23809523809523, 82.5, 74.11764705882354, 75.64102564102564, 81.70731707317073]
14: [79.69924812030075, 70.29702970297029, 79.22077922077922, 83.33333333333334, 76.74418604651163, 73.61111111111111, 77.04918032786885, 89.02439024390245, 79.54545454545455, 79.76190476190477, 78.75, 77.64705882352942, 78.2051282051282, 82.92682926829268, 75.32467532467533]
15: [79.69924812030075, 74.25742574257426, 77.92207792207793, 88.88888888888889, 72.09302325581395, 79.16666666666666, 80.32786885245902, 89.02439024390245, 78.4090909090909, 61.904761904761905, 83.75, 78.82352941176471, 75.64102564102564, 85.36585365853658, 77.92207792207793, 75.90361445783132]
16: [81.95488721804512, 73.26732673267327, 81.81818181818183, 86.11111111111111, 70.93023255813954, 79.16666666666666, 80.32786885245902, 89.02439024390245, 81.81818181818183, 72.61904761904762, 76.25, 82.35294117647058, 76.92307692307693, 81.70731707317073, 77.92207792207793, 75.90361445783132, 82.71604938271605]
17: [73.68421052631578, 70.29702970297029, 75.32467532467533, 87.5, 70.93023255813954, 72.22222222222221, 77.04918032786885, 85.36585365853658, 81.81818181818183, 65.47619047619048, 78.75, 74.11764705882354, 71.7948717948718, 81.70731707317073, 77.92207792207793, 73.49397590361446, 82.71604938271605, 81.3953488372093]
18: [84.9624060150376, 66.33663366336634, 77.92207792207793, 87.5, 73.25581395348837, 70.83333333333334, 78.68852459016394, 81.70731707317073, 79.54545454545455, 65.47619047619048, 82.5, 82.35294117647058, 71.7948717948718, 82.92682926829268, 76.62337662337663, 69.87951807228916, 85.18518518518519, 84.88372093023256, 63.095238095238095]
19: [78.19548872180451, 64.35643564356435, 83.11688311688312, 80.55555555555556, 67.44186046511628, 79.16666666666666, 72.1311475409836, 76.82926829268293, 75.0, 60.71428571428571, 77.5, 76.47058823529412, 74.35897435897436, 78.04878048780488, 76.62337662337663, 74.69879518072288, 80.24691358024691, 87.20930232558139, 60.71428571428571, 65.47619047619048]
20: [79.69924812030075, 69.3069306930693, 80.51948051948052, 81.94444444444444, 70.93023255813954, 75.0, 77.04918032786885, 78.04878048780488, 78.4090909090909, 66.66666666666666, 81.25, 77.64705882352942, 73.07692307692307, 82.92682926829268, 75.32467532467533, 77.10843373493977, 87.65432098765432, 79.06976744186046, 58.333333333333336, 65.47619047619048, 80.95238095238095]
21: [73.68421052631578, 69.3069306930693, 85.71428571428571, 86.11111111111111, 67.44186046511628, 76.38888888888889, 68.85245901639344, 84.14634146341463, 76.13636363636364, 64.28571428571429, 77.5, 72.94117647058823, 73.07692307692307, 78.04878048780488, 71.42857142857143, 75.90361445783132, 81.48148148148148, 88.37209302325581, 60.71428571428571, 71.42857142857143, 78.57142857142857, 77.5]
22: [75.18796992481202, 68.31683168316832, 79.22077922077922, 81.94444444444444, 69.76744186046511, 69.44444444444444, 68.85245901639344, 76.82926829268293, 76.13636363636364, 58.333333333333336, 77.5, 76.47058823529412, 58.97435897435898, 78.04878048780488, 72.72727272727273, 75.90361445783132, 86.41975308641975, 76.74418604651163, 58.333333333333336, 57.14285714285714, 79.76190476190477, 76.25, 74.64788732394366]
23: [73.68421052631578, 67.32673267326733, 80.51948051948052, 81.94444444444444, 68.6046511627907, 76.38888888888889, 68.85245901639344, 80.48780487804879, 73.86363636363636, 59.523809523809526, 81.25, 71.76470588235294, 65.38461538461539, 84.14634146341463, 64.93506493506493, 71.08433734939759, 83.9506172839506, 84.88372093023256, 65.47619047619048, 63.095238095238095, 77.38095238095238, 75.0, 74.64788732394366, 80.95238095238095]
24: [70.67669172932331, 68.31683168316832, 75.32467532467533, 86.11111111111111, 67.44186046511628, 69.44444444444444, 75.40983606557377, 76.82926829268293, 76.13636363636364, 60.71428571428571, 76.25, 74.11764705882354, 70.51282051282051, 79.26829268292683, 71.42857142857143, 71.08433734939759, 79.01234567901234, 76.74418604651163, 63.095238095238095, 59.523809523809526, 78.57142857142857, 72.5, 76.05633802816901, 79.76190476190477, 71.25]
25: [72.18045112781954, 64.35643564356435, 77.92207792207793, 81.94444444444444, 73.25581395348837, 72.22222222222221, 73.77049180327869, 79.26829268292683, 77.27272727272727, 61.904761904761905, 80.0, 77.64705882352942, 66.66666666666666, 82.92682926829268, 74.02597402597402, 77.10843373493977, 81.48148148148148, 87.20930232558139, 60.71428571428571, 59.523809523809526, 77.38095238095238, 72.5, 71.83098591549296, 70.23809523809523, 66.25, 64.28571428571429]
26: [62.40601503759399, 65.34653465346535, 74.02597402597402, 77.77777777777779, 66.27906976744185, 70.83333333333334, 72.1311475409836, 82.92682926829268, 72.72727272727273, 59.523809523809526, 75.0, 70.58823529411765, 62.82051282051282, 80.48780487804879, 70.12987012987013, 68.67469879518072, 81.48148148148148, 87.20930232558139, 60.71428571428571, 72.61904761904762, 79.76190476190477, 76.25, 78.87323943661971, 75.0, 65.0, 66.66666666666666, 49.36708860759494]
27: [71.42857142857143, 66.33663366336634, 81.81818181818183, 76.38888888888889, 65.11627906976744, 72.22222222222221, 68.85245901639344, 79.26829268292683, 75.0, 66.66666666666666, 77.5, 71.76470588235294, 61.53846153846154, 79.26829268292683, 70.12987012987013, 71.08433734939759, 76.5432098765432, 80.23255813953489, 63.095238095238095, 66.66666666666666, 82.14285714285714, 73.75, 70.4225352112676, 80.95238095238095, 62.5, 65.47619047619048, 45.56962025316456, 67.77777777777779]
28: [66.9172932330827, 68.31683168316832, 80.51948051948052, 84.72222222222221, 66.27906976744185, 65.27777777777779, 77.04918032786885, 74.39024390243902, 73.86363636363636, 63.095238095238095, 80.0, 74.11764705882354, 76.92307692307693, 79.26829268292683, 72.72727272727273, 73.49397590361446, 83.9506172839506, 80.23255813953489, 54.761904761904766, 64.28571428571429, 80.95238095238095, 71.25, 69.01408450704226, 73.80952380952381, 66.25, 63.095238095238095, 51.89873417721519, 70.0, 70.88607594936708]
29: [69.92481203007519, 64.35643564356435, 72.72727272727273, 76.38888888888889, 60.46511627906976, 72.22222222222221, 63.934426229508205, 78.04878048780488, 75.0, 60.71428571428571, 76.25, 74.11764705882354, 57.692307692307686, 81.70731707317073, 72.72727272727273, 75.90361445783132, 79.01234567901234, 84.88372093023256, 54.761904761904766, 70.23809523809523, 79.76190476190477, 71.25, 77.46478873239437, 75.0, 66.25, 73.80952380952381, 53.16455696202531, 72.22222222222221, 74.68354430379746, 64.28571428571429]
30: [69.92481203007519, 71.28712871287128, 72.72727272727273, 81.94444444444444, 62.7906976744186, 72.22222222222221, 62.295081967213115, 75.60975609756098, 72.72727272727273, 53.57142857142857, 76.25, 68.23529411764706, 58.97435897435898, 79.26829268292683, 67.53246753246754, 79.51807228915662, 85.18518518518519, 80.23255813953489, 63.095238095238095, 71.42857142857143, 77.38095238095238, 73.75, 66.19718309859155, 72.61904761904762, 65.0, 73.80952380952381, 50.63291139240506, 66.66666666666666, 74.68354430379746, 66.66666666666666, 61.25000000000001]
31: [74.43609022556392, 66.33663366336634, 81.81818181818183, 79.16666666666666, 65.11627906976744, 73.61111111111111, 75.40983606557377, 79.26829268292683, 75.0, 58.333333333333336, 75.0, 63.52941176470588, 65.38461538461539, 76.82926829268293, 72.72727272727273, 65.06024096385542, 81.48148148148148, 86.04651162790698, 55.952380952380956, 66.66666666666666, 77.38095238095238, 75.0, 74.64788732394366, 76.19047619047619, 67.5, 65.47619047619048, 48.10126582278481, 68.88888888888889, 81.0126582278481, 65.47619047619048, 68.75, 55.434782608695656]
32: [76.69172932330827, 65.34653465346535, 77.92207792207793, 81.94444444444444, 68.6046511627907, 70.83333333333334, 72.1311475409836, 80.48780487804879, 75.0, 57.14285714285714, 73.75, 70.58823529411765, 69.23076923076923, 74.39024390243902, 68.83116883116884, 72.28915662650603, 82.71604938271605, 77.90697674418605, 50.0, 55.952380952380956, 78.57142857142857, 68.75, 77.46478873239437, 73.80952380952381, 66.25, 63.095238095238095, 49.36708860759494, 67.77777777777779, 72.15189873417721, 61.904761904761905, 68.75, 52.17391304347826, 68.05555555555556]
33: [69.92481203007519, 64.35643564356435, 71.42857142857143, 83.33333333333334, 60.46511627906976, 75.0, 72.1311475409836, 81.70731707317073, 73.86363636363636, 61.904761904761905, 76.25, 64.70588235294117, 66.66666666666666, 78.04878048780488, 71.42857142857143, 72.28915662650603, 83.9506172839506, 83.72093023255815, 50.0, 67.85714285714286, 82.14285714285714, 75.0, 61.97183098591549, 72.61904761904762, 63.74999999999999, 67.85714285714286, 45.56962025316456, 64.44444444444444, 74.68354430379746, 65.47619047619048, 61.25000000000001, 57.608695652173914, 72.22222222222221, 72.72727272727273]
34: [65.41353383458647, 70.29702970297029, 77.92207792207793, 79.16666666666666, 65.11627906976744, 72.22222222222221, 68.85245901639344, 69.51219512195121, 77.27272727272727, 61.904761904761905, 76.25, 61.1764705882353, 60.256410256410255, 78.04878048780488, 66.23376623376623, 67.46987951807229, 82.71604938271605, 84.88372093023256, 51.19047619047619, 63.095238095238095, 77.38095238095238, 76.25, 73.23943661971832, 78.57142857142857, 58.75, 67.85714285714286, 51.89873417721519, 61.111111111111114, 79.74683544303798, 64.28571428571429, 68.75, 60.86956521739131, 69.44444444444444, 68.18181818181817, 65.82278481012658]
35: [75.18796992481202, 65.34653465346535, 76.62337662337663, 83.33333333333334, 65.11627906976744, 65.27777777777779, 72.1311475409836, 80.48780487804879, 69.31818181818183, 55.952380952380956, 76.25, 78.82352941176471, 61.53846153846154, 79.26829268292683, 68.83116883116884, 68.67469879518072, 81.48148148148148, 82.55813953488372, 51.19047619047619, 69.04761904761905, 70.23809523809523, 66.25, 67.6056338028169, 71.42857142857143, 60.0, 64.28571428571429, 50.63291139240506, 68.88888888888889, 75.9493670886076, 57.14285714285714, 60.0, 51.08695652173913, 69.44444444444444, 71.5909090909091, 54.43037974683544, 50.0]
36: [72.93233082706767, 59.4059405940594, 76.62337662337663, 84.72222222222221, 61.627906976744185, 66.66666666666666, 67.21311475409836, 73.17073170731707, 77.27272727272727, 60.71428571428571, 66.25, 69.41176470588235, 69.23076923076923, 71.95121951219512, 64.93506493506493, 69.87951807228916, 83.9506172839506, 79.06976744186046, 53.57142857142857, 64.28571428571429, 79.76190476190477, 68.75, 78.87323943661971, 76.19047619047619, 58.75, 69.04761904761905, 50.63291139240506, 66.66666666666666, 81.0126582278481, 60.71428571428571, 61.25000000000001, 50.0, 70.83333333333334, 69.31818181818183, 65.82278481012658, 48.57142857142857, 56.94444444444444]
37: [67.66917293233082, 67.32673267326733, 79.22077922077922, 80.55555555555556, 72.09302325581395, 69.44444444444444, 65.57377049180327, 80.48780487804879, 77.27272727272727, 59.523809523809526, 77.5, 69.41176470588235, 66.66666666666666, 69.51219512195121, 67.53246753246754, 69.87951807228916, 80.24691358024691, 82.55813953488372, 59.523809523809526, 65.47619047619048, 78.57142857142857, 70.0, 59.154929577464785, 72.61904761904762, 61.25000000000001, 69.04761904761905, 53.16455696202531, 64.44444444444444, 73.41772151898735, 59.523809523809526, 61.25000000000001, 52.17391304347826, 69.44444444444444, 69.31818181818183, 63.29113924050633, 42.857142857142854, 56.94444444444444, 58.666666666666664]
38: [63.1578947368421, 61.386138613861384, 77.92207792207793, 76.38888888888889, 67.44186046511628, 65.27777777777779, 63.934426229508205, 74.39024390243902, 73.86363636363636, 55.952380952380956, 76.25, 75.29411764705883, 64.1025641025641, 81.70731707317073, 63.63636363636363, 72.28915662650603, 79.01234567901234, 77.90697674418605, 53.57142857142857, 63.095238095238095, 83.33333333333334, 68.75, 69.01408450704226, 77.38095238095238, 60.0, 67.85714285714286, 48.10126582278481, 62.22222222222222, 72.15189873417721, 70.23809523809523, 61.25000000000001, 57.608695652173914, 63.888888888888886, 62.5, 56.9620253164557, 41.42857142857143, 59.72222222222222, 60.0, 70.58823529411765]
39: [69.17293233082707, 65.34653465346535, 76.62337662337663, 80.55555555555556, 60.46511627906976, 68.05555555555556, 67.21311475409836, 73.17073170731707, 70.45454545454545, 57.14285714285714, 76.25, 76.47058823529412, 64.1025641025641, 71.95121951219512, 61.038961038961034, 73.49397590361446, 85.18518518518519, 81.3953488372093, 53.57142857142857, 60.71428571428571, 73.80952380952381, 75.0, 66.19718309859155, 78.57142857142857, 63.74999999999999, 67.85714285714286, 46.835443037974684, 72.22222222222221, 77.21518987341773, 65.47619047619048, 70.0, 56.52173913043478, 75.0, 73.86363636363636, 60.75949367088608, 44.285714285714285, 58.333333333333336, 64.0, 68.23529411764706, 58.333333333333336]
40: [72.18045112781954, 62.37623762376238, 71.42857142857143, 80.55555555555556, 65.11627906976744, 63.888888888888886, 62.295081967213115, 75.60975609756098, 80.68181818181817, 54.761904761904766, 73.75, 64.70588235294117, 61.53846153846154, 70.73170731707317, 64.93506493506493, 69.87951807228916, 71.60493827160494, 76.74418604651163, 53.57142857142857, 58.333333333333336, 79.76190476190477, 68.75, 64.7887323943662, 67.85714285714286, 66.25, 67.85714285714286, 50.63291139240506, 67.77777777777779, 72.15189873417721, 61.904761904761905, 70.0, 53.2608695652174, 70.83333333333334, 69.31818181818183, 58.22784810126582, 47.14285714285714, 52.77777777777778, 64.0, 75.29411764705883, 46.42857142857143, 51.24999999999999]
41: [70.67669172932331, 72.27722772277228, 68.83116883116884, 75.0, 61.627906976744185, 61.111111111111114, 73.77049180327869, 80.48780487804879, 75.0, 52.38095238095239, 71.25, 68.23529411764706, 65.38461538461539, 75.60975609756098, 64.93506493506493, 69.87951807228916, 74.07407407407408, 70.93023255813954, 57.14285714285714, 55.952380952380956, 78.57142857142857, 70.0, 60.56338028169014, 63.095238095238095, 60.0, 59.523809523809526, 50.63291139240506, 67.77777777777779, 77.21518987341773, 63.095238095238095, 66.25, 57.608695652173914, 63.888888888888886, 71.5909090909091, 58.22784810126582, 44.285714285714285, 40.27777777777778, 68.0, 65.88235294117646, 51.19047619047619, 52.5, 67.85714285714286]
42: [59.3984962406015, 68.31683168316832, 76.62337662337663, 75.0, 65.11627906976744, 66.66666666666666, 67.21311475409836, 79.26829268292683, 72.72727272727273, 55.952380952380956, 73.75, 61.1764705882353, 65.38461538461539, 71.95121951219512, 71.42857142857143, 61.44578313253012, 80.24691358024691, 77.90697674418605, 54.761904761904766, 57.14285714285714, 72.61904761904762, 75.0, 71.83098591549296, 73.80952380952381, 62.5, 73.80952380952381, 49.36708860759494, 64.44444444444444, 73.41772151898735, 61.904761904761905, 67.5, 55.434782608695656, 69.44444444444444, 67.04545454545455, 65.82278481012658, 47.14285714285714, 55.55555555555556, 69.33333333333334, 70.58823529411765, 52.38095238095239, 50.0, 69.04761904761905, 53.76344086021505]
43: [61.65413533834586, 65.34653465346535, 79.22077922077922, 76.38888888888889, 61.627906976744185, 61.111111111111114, 70.49180327868852, 80.48780487804879, 75.0, 54.761904761904766, 70.0, 70.58823529411765, 66.66666666666666, 75.60975609756098, 67.53246753246754, 69.87951807228916, 80.24691358024691, 81.3953488372093, 60.71428571428571, 55.952380952380956, 75.0, 68.75, 77.46478873239437, 76.19047619047619, 62.5, 63.095238095238095, 49.36708860759494, 63.33333333333333, 69.62025316455697, 64.28571428571429, 66.25, 59.78260869565217, 62.5, 60.22727272727273, 64.55696202531645, 48.57142857142857, 55.55555555555556, 62.66666666666667, 67.05882352941175, 48.80952380952381, 46.25, 70.23809523809523, 48.38709677419355, 79.06976744186046]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 59.8901
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 69.4030
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 65.7143
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 84.0000
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 71.3043
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 58.5859
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 55.1724
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 78.3784
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 80.5085
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 65.7895
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 63.3028
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 68.4211
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 66.0377
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 78.3784
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 72.3810
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 60.3604
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 81.8182
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 79.3103
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 58.0357
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 63.7168
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 75.2212
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 62.3853
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 70.4082
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 72.5664
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 64.2202
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 62.8319
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 55.5556
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 72.5000
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 75.9259
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 61.0619
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 70.6422
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 63.4146
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 55.5556
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 60.6838
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 62.0370
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 57.7320
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 60.6061
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 77.6699
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 61.9469
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 64.9123
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 53.7037
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 69.9115
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 56.0976
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 76.5217

====================

f1_score(micro): 66.93105552165954
f1_score(macro): 63.3997858999224
Classification report:
              precision    recall  f1-score   support

           0       0.33      0.50      0.40         4
           1       0.50      0.44      0.47         9
           2       1.00      0.75      0.86         4
           3       0.50      0.25      0.33         4
           4       1.00      0.80      0.89         5
           5       1.00      1.00      1.00         9
           6       0.67      1.00      0.80         4
           7       0.80      0.80      0.80         5
           8       0.00      0.00      0.00         4
           9       1.00      1.00      1.00         4
          10       0.33      0.25      0.29         4
          11       1.00      0.75      0.86         4
          12       1.00      0.75      0.86         4
          13       1.00      0.00      0.00         4
          14       1.00      1.00      1.00         9
          15       0.00      0.00      0.00         4
          16       1.00      0.75      0.86         4
          17       0.75      0.75      0.75         4
          18       1.00      0.33      0.50         9
          19       0.80      1.00      0.89         4
          20       0.40      0.50      0.44         4
          21       0.83      1.00      0.91         5
          22       0.80      0.44      0.57         9
          23       0.00      0.00      0.00         4
          24       1.00      0.25      0.40         4
          25       1.00      1.00      1.00         5
          26       0.88      0.78      0.82         9
          27       0.70      0.78      0.74         9
          28       1.00      0.75      0.86         4
          29       1.00      0.80      0.89         5
          30       1.00      0.00      0.00         4
          31       0.67      0.50      0.57         4
          32       0.50      0.11      0.18         9
          33       1.00      0.80      0.89         5
          34       1.00      1.00      1.00         5
          35       0.00      0.00      0.00         4
          36       0.73      0.89      0.80         9
          37       0.67      1.00      0.80         4
          38       1.00      1.00      1.00         5
          39       0.25      0.50      0.33         4
          40       0.67      0.50      0.57         4
          41       0.80      0.80      0.80         5
          42       0.00      0.00      0.00         5
          43       0.75      1.00      0.86         9
          44       0.18      0.22      0.20         9
          45       0.69      1.00      0.82         9
          46       0.80      0.89      0.84         9
          47       0.67      0.44      0.53         9
          48       0.60      1.00      0.75         9
          49       0.89      0.89      0.89         9
          50       0.73      1.00      0.84         8
          51       1.00      0.40      0.57         5
          52       1.00      0.75      0.86         4
          53       0.17      0.11      0.13         9
          54       1.00      0.50      0.67         4
          55       0.00      0.00      0.00         4
          56       0.90      1.00      0.95         9
          57       1.00      0.75      0.86         4
          58       0.67      0.67      0.67         6
          59       1.00      0.60      0.75         5
          60       0.80      1.00      0.89         4
          61       0.67      1.00      0.80         4
          62       1.00      0.78      0.88         9
          63       0.69      1.00      0.82         9
          64       1.00      0.00      0.00         4
          65       1.00      0.89      0.94         9
          66       1.00      0.00      0.00         4
          67       0.67      0.40      0.50         5
          68       1.00      0.00      0.00         4
          69       0.80      0.80      0.80         5
          70       0.33      0.50      0.40         4
          71       0.00      0.00      0.00         4
          72       0.57      1.00      0.73         4
          73       1.00      1.00      1.00         4
          74       1.00      0.75      0.86         4
          75       1.00      0.75      0.86         4
          76       1.00      0.60      0.75         5
          77       1.00      0.80      0.89         5
          78       1.00      1.00      1.00         4
          79       0.40      1.00      0.57         4
          80       1.00      1.00      1.00         4
          81       0.80      0.80      0.80         5
          82       0.67      0.89      0.76         9
          83       1.00      0.75      0.86         4
          84       1.00      0.75      0.86         4
          85       0.83      1.00      0.91         5
          86       0.40      0.50      0.44         4
          87       1.00      1.00      1.00         4
          88       1.00      1.00      1.00         4
          89       1.00      1.00      1.00         5
          90       0.82      1.00      0.90         9
          91       0.75      0.75      0.75         4
          92       1.00      0.00      0.00         4
          93       0.90      1.00      0.95         9
          94       1.00      1.00      1.00         9
          95       1.00      0.75      0.86         4
          96       0.67      0.40      0.50         5
          97       0.80      0.89      0.84         9
          98       0.50      0.75      0.60         4
          99       0.80      1.00      0.89         4
         100       1.00      0.00      0.00         4
         101       1.00      1.00      1.00         4
         102       0.80      0.80      0.80         5
         103       0.57      0.44      0.50         9
         104       1.00      0.50      0.67         4
         105       1.00      0.75      0.86         4
         106       1.00      0.25      0.40         4
         107       0.89      0.89      0.89         9
         108       0.60      0.33      0.43         9
         109       0.80      0.80      0.80         5
         110       0.78      0.78      0.78         9
         111       0.80      1.00      0.89         4
         112       0.80      0.80      0.80         5
         113       1.00      1.00      1.00         5
         114       1.00      0.75      0.86         4
         115       0.75      0.75      0.75         4
         116       0.75      0.75      0.75         4
         117       0.43      0.60      0.50         5
         118       0.00      0.00      0.00         9
         119       0.00      0.00      0.00         4
         120       1.00      0.40      0.57         5
         121       0.82      1.00      0.90         9
         122       0.14      0.25      0.18         4
         123       0.33      0.25      0.29         4
         124       1.00      0.80      0.89         5
         125       1.00      0.89      0.94         9
         126       0.75      0.75      0.75         4
         127       0.50      0.50      0.50         4
         128       1.00      0.25      0.40         4
         129       0.00      0.00      0.00         4
         130       0.80      0.80      0.80         5
         131       1.00      1.00      1.00         4
         132       0.57      1.00      0.73         4
         133       1.00      0.75      0.86         4
         134       0.75      0.75      0.75         4
         135       1.00      0.25      0.40         4
         136       1.00      0.50      0.67         4
         137       0.09      0.50      0.15         4
         138       0.40      0.50      0.44         4
         139       0.33      0.25      0.29         4
         140       0.60      0.75      0.67         4
         141       0.44      1.00      0.62         4
         142       0.00      0.00      0.00         4
         143       0.00      0.00      0.00         4
         144       0.25      0.25      0.25         4
         145       0.67      0.89      0.76         9
         146       0.67      0.50      0.57         4
         147       1.00      0.00      0.00         4
         148       0.57      0.80      0.67         5
         149       0.57      1.00      0.73         4
         150       0.75      0.75      0.75         4
         151       0.38      0.75      0.50         4
         152       0.57      0.80      0.67         5
         153       0.33      0.25      0.29         4
         154       0.80      0.89      0.84         9
         155       1.00      1.00      1.00         4
         156       0.67      0.50      0.57         4
         157       0.86      0.67      0.75         9
         158       0.08      0.25      0.12         4
         159       1.00      1.00      1.00         4
         160       0.67      1.00      0.80         4
         161       1.00      1.00      1.00         9
         162       1.00      1.00      1.00         9
         163       0.64      0.78      0.70         9
         164       0.00      0.00      0.00         4
         165       0.57      1.00      0.73         4
         166       1.00      0.50      0.67         4
         167       1.00      0.60      0.75         5
         168       0.80      1.00      0.89         4
         169       0.90      1.00      0.95         9
         170       0.25      0.25      0.25         4
         171       0.57      1.00      0.73         4
         172       0.75      0.75      0.75         4
         173       1.00      0.75      0.86         4
         174       1.00      0.75      0.86         4
         175       0.50      0.60      0.55         5
         176       0.60      0.75      0.67         4
         177       1.00      1.00      1.00         4
         178       0.75      0.75      0.75         4
         179       1.00      0.89      0.94         9
         180       0.73      0.89      0.80         9
         181       1.00      1.00      1.00         5
         182       0.67      0.89      0.76         9
         183       1.00      0.89      0.94         9
         184       0.89      0.89      0.89         9
         185       0.00      0.00      0.00         4
         186       1.00      1.00      1.00         4
         187       0.80      0.89      0.84         9
         188       0.80      0.80      0.80         5
         189       0.67      0.80      0.73         5
         190       1.00      0.88      0.93         8
         191       1.00      1.00      1.00         4
         192       1.00      0.00      0.00         4
         193       1.00      0.75      0.86         4
         194       0.33      0.20      0.25         5
         195       0.00      0.00      0.00         4
         196       1.00      1.00      1.00         5
         197       1.00      0.44      0.62         9
         198       0.09      0.22      0.13         9
         199       1.00      0.60      0.75         5
         200       1.00      1.00      1.00         9
         201       1.00      0.25      0.40         4
         202       0.60      0.75      0.67         4
         203       0.83      1.00      0.91         5
         204       0.12      0.56      0.20         9
         205       1.00      1.00      1.00         4
         206       0.60      0.75      0.67         4
         207       1.00      0.75      0.86         4
         208       0.75      0.75      0.75         4
         209       0.60      0.75      0.67         4
         210       0.73      0.89      0.80         9
         211       0.86      0.67      0.75         9
         212       1.00      0.75      0.86         4
         213       0.80      1.00      0.89         4
         214       0.67      0.50      0.57         4
         215       1.00      0.25      0.40         4
         216       1.00      0.00      0.00         4
         217       0.00      0.00      0.00         4
         218       0.70      0.78      0.74         9
         219       0.75      0.60      0.67         5
         220       1.00      0.75      0.86         4
         221       1.00      0.00      0.00         4
         222       0.75      1.00      0.86         9
         223       1.00      0.50      0.67         4
         224       1.00      1.00      1.00         4
         225       1.00      0.75      0.86         4
         226       1.00      0.75      0.86         4
         227       0.00      0.00      0.00         4
         228       1.00      1.00      1.00         9
         229       1.00      0.78      0.88         9
         230       1.00      0.57      0.73         7
         231       1.00      1.00      1.00         4
         232       1.00      0.00      0.00         4
         233       0.89      0.89      0.89         9
         234       1.00      1.00      1.00         4
         235       1.00      0.75      0.86         4
         236       1.00      0.50      0.67         4
         237       0.60      0.75      0.67         4
         238       0.00      0.00      0.00         4
         239       0.50      0.60      0.55         5
         240       0.80      0.80      0.80         5
         241       0.56      1.00      0.71         5
         242       0.22      0.22      0.22         9
         243       0.56      1.00      0.71         5
         244       0.53      1.00      0.69         9
         245       0.83      0.56      0.67         9
         246       0.67      1.00      0.80         4
         247       0.44      1.00      0.62         4
         248       0.80      0.44      0.57         9
         249       0.89      0.89      0.89         9
         250       0.50      0.25      0.33         4
         251       0.80      0.89      0.84         9
         252       0.00      0.00      0.00         4
         253       0.80      1.00      0.89         4
         254       1.00      0.00      0.00         4
         255       0.80      1.00      0.89         4
         256       0.80      1.00      0.89         4
         257       0.71      1.00      0.83         5
         258       0.75      0.75      0.75         4
         259       0.14      0.11      0.12         9
         260       0.50      0.75      0.60         4
         261       0.70      0.78      0.74         9
         262       0.00      0.00      0.00         4
         263       1.00      0.50      0.67         4
         264       1.00      0.00      0.00         4
         265       0.67      1.00      0.80         4
         266       0.33      0.25      0.29         4
         267       1.00      1.00      1.00         9
         268       0.17      0.25      0.20         4
         269       0.80      1.00      0.89         4
         270       0.67      0.89      0.76         9
         271       0.54      0.78      0.64         9
         272       1.00      1.00      1.00         4
         273       0.50      0.75      0.60         4
         274       0.00      0.00      0.00         4
         275       0.44      0.80      0.57         5
         276       0.43      0.75      0.55         4
         277       0.67      0.80      0.73         5
         278       0.86      0.67      0.75         9
         279       0.67      1.00      0.80         4
         280       0.89      0.89      0.89         9
         281       1.00      0.25      0.40         4
         282       1.00      0.89      0.94         9
         283       0.80      1.00      0.89         4
         284       0.67      1.00      0.80         4
         285       0.50      0.50      0.50         4
         286       1.00      1.00      1.00         4
         287       0.89      0.89      0.89         9
         288       0.69      1.00      0.82         9
         289       1.00      0.80      0.89         5
         290       0.60      0.75      0.67         4
         291       1.00      0.80      0.89         5
         292       0.33      0.60      0.43         5
         293       1.00      0.80      0.89         5
         294       0.67      1.00      0.80         4
         295       0.57      1.00      0.73         4
         296       0.50      0.75      0.60         4
         297       0.83      1.00      0.91         5
         298       1.00      0.00      0.00         4
         299       0.00      0.00      0.00         4
         300       1.00      1.00      1.00         5
         301       0.67      0.89      0.76         9
         302       0.75      0.75      0.75         4
         303       0.75      1.00      0.86         9
         304       0.80      1.00      0.89         4
         305       1.00      0.78      0.88         9
         306       0.83      1.00      0.91         5
         307       1.00      0.40      0.57         5
         308       0.82      1.00      0.90         9
         309       0.00      0.00      0.00         4
         310       1.00      0.00      0.00         4
         311       0.50      0.25      0.33         4
         312       0.50      0.75      0.60         4
         313       0.33      0.80      0.47         5
         314       1.00      0.89      0.94         9
         315       1.00      0.50      0.67         4
         316       1.00      0.50      0.67         4
         317       1.00      1.00      1.00         4
         318       0.00      0.00      0.00         4
         319       1.00      0.56      0.71         9
         320       0.53      0.89      0.67         9
         321       0.00      0.00      0.00         4
         322       0.57      1.00      0.73         4
         323       0.60      0.75      0.67         4
         324       1.00      1.00      1.00         4
         325       0.44      0.80      0.57         5
         326       0.50      0.44      0.47         9
         327       0.57      1.00      0.73         4
         328       0.75      1.00      0.86         9
         329       0.00      0.00      0.00         4
         330       0.00      0.00      0.00         4
         331       0.86      0.67      0.75         9
         332       1.00      0.00      0.00         4
         333       0.00      0.00      0.00         4
         334       1.00      0.78      0.88         9
         335       0.80      1.00      0.89         4
         336       1.00      0.20      0.33         5
         337       0.83      1.00      0.91         5
         338       0.89      0.89      0.89         9
         339       0.60      0.60      0.60         5
         340       0.57      1.00      0.73         4
         341       0.89      0.89      0.89         9
         342       0.50      1.00      0.67         4
         343       0.60      0.75      0.67         4
         344       0.60      0.75      0.67         4
         345       0.80      1.00      0.89         4
         346       0.88      0.78      0.82         9
         347       1.00      0.75      0.86         4
         348       0.50      0.78      0.61         9
         349       1.00      1.00      1.00         5
         350       0.83      1.00      0.91         5
         351       0.40      0.50      0.44         4
         352       0.60      0.75      0.67         4
         353       1.00      1.00      1.00         4
         354       0.89      0.89      0.89         9
         355       1.00      0.60      0.75         5
         356       0.00      0.00      0.00         4
         357       1.00      0.75      0.86         4
         358       1.00      0.80      0.89         5
         359       1.00      1.00      1.00         5
         360       0.67      1.00      0.80         4
         361       0.67      0.67      0.67         9
         362       0.67      1.00      0.80         4
         363       0.89      0.89      0.89         9
         364       0.67      0.80      0.73         5
         365       0.73      0.89      0.80         9
         366       1.00      1.00      1.00         9
         367       1.00      0.80      0.89         5
         368       0.80      0.80      0.80         5
         369       0.75      0.75      0.75         4
         370       0.75      0.75      0.75         4
         371       0.43      0.75      0.55         4
         372       1.00      1.00      1.00         9
         373       0.00      0.00      0.00         4
         374       0.00      0.00      0.00         4
         375       1.00      1.00      1.00         5
         376       1.00      0.75      0.86         4
         377       0.71      1.00      0.83         5
         378       0.64      0.78      0.70         9
         379       0.90      1.00      0.95         9
         380       1.00      1.00      1.00         4
         381       0.70      0.78      0.74         9
         382       0.09      0.22      0.13         9
         383       0.00      0.00      0.00         9
         384       0.57      1.00      0.73         4
         385       0.05      0.25      0.09         4
         386       0.00      0.00      0.00         4
         387       0.57      1.00      0.73         4
         388       1.00      0.75      0.86         4
         389       0.00      0.00      0.00         4
         390       1.00      0.00      0.00         4
         391       0.00      0.00      0.00         4
         392       0.43      0.75      0.55         4
         393       0.73      0.89      0.80         9
         394       1.00      0.50      0.67         4
         395       0.50      0.50      0.50         4
         396       0.00      0.00      0.00         4
         397       0.80      0.80      0.80         5
         398       0.86      0.67      0.75         9
         399       1.00      1.00      1.00         4
         400       0.73      0.89      0.80         9
         401       0.00      0.00      0.00         4
         402       0.75      0.75      0.75         4
         403       0.67      0.57      0.62         7
         404       0.50      0.86      0.63         7
         405       0.67      0.50      0.57         4
         406       1.00      0.00      0.00         4
         407       1.00      0.00      0.00         4
         408       0.67      1.00      0.80         4
         409       0.67      0.67      0.67         9
         410       0.73      0.89      0.80         9
         411       0.67      0.40      0.50         5
         412       0.67      0.50      0.57         4
         413       0.90      1.00      0.95         9
         414       1.00      1.00      1.00         4
         415       0.78      0.78      0.78         9
         416       0.57      1.00      0.73         4
         417       0.50      0.75      0.60         4
         418       0.42      0.56      0.48         9
         419       0.75      0.75      0.75         4
         420       0.67      1.00      0.80         4
         421       1.00      0.75      0.86         4
         422       0.57      1.00      0.73         4
         423       0.90      1.00      0.95         9
         424       0.64      1.00      0.78         9
         425       1.00      1.00      1.00         9
         426       0.50      0.78      0.61         9
         427       0.80      0.80      0.80         5
         428       1.00      0.50      0.67         4
         429       1.00      0.25      0.40         4
         430       1.00      0.40      0.57         5
         431       1.00      0.50      0.67         4
         432       0.00      0.00      0.00         4
         433       0.60      0.60      0.60         5
         434       1.00      0.40      0.57         5
         435       1.00      1.00      1.00         4
         436       0.00      0.00      0.00         4
         437       1.00      0.25      0.40         4
         438       0.83      1.00      0.91         5
         439       1.00      0.75      0.86         4
         440       0.78      0.78      0.78         9
         441       0.67      0.80      0.73         5
         442       0.25      0.25      0.25         4
         443       1.00      1.00      1.00         4
         444       0.25      0.25      0.25         4
         445       0.57      0.89      0.70         9
         446       1.00      1.00      1.00         4
         447       0.00      0.00      0.00         9
         448       0.50      0.75      0.60         4
         449       0.60      0.75      0.67         4
         450       1.00      0.78      0.88         9
         451       0.86      0.67      0.75         9
         452       0.67      0.40      0.50         5
         453       0.75      0.75      0.75         4
         454       0.73      0.89      0.80         9
         455       1.00      0.50      0.67         4
         456       1.00      0.75      0.86         4
         457       0.00      0.00      0.00         4
         458       0.50      1.00      0.67         4
         459       0.50      0.75      0.60         4
         460       0.67      0.67      0.67         9
         461       0.75      0.60      0.67         5
         462       0.75      0.75      0.75         4
         463       1.00      1.00      1.00         4
         464       0.80      0.80      0.80         5
         465       0.00      0.00      0.00         4
         466       0.33      0.50      0.40         4
         467       0.80      1.00      0.89         4
         468       0.80      1.00      0.89         4
         469       1.00      0.89      0.94         9
         470       1.00      0.80      0.89         5
         471       0.36      1.00      0.53         4
         472       0.07      0.25      0.11         4
         473       0.50      0.50      0.50         4
         474       0.55      0.67      0.60         9
         475       1.00      0.89      0.94         9
         476       0.67      0.89      0.76         9
         477       0.71      1.00      0.83         5
         478       1.00      0.50      0.67         4
         479       0.75      0.75      0.75         4
         480       0.75      0.60      0.67         5
         481       1.00      0.75      0.86         4
         482       0.60      0.75      0.67         4
         483       0.54      0.78      0.64         9
         484       0.57      1.00      0.73         4
         485       0.50      0.75      0.60         4
         486       0.62      1.00      0.77         5
         487       0.69      1.00      0.82         9
         488       1.00      0.78      0.88         9
         489       0.75      0.75      0.75         4
         490       1.00      0.75      0.86         4
         491       0.00      0.00      0.00         4
         492       0.00      0.00      0.00         4
         493       0.00      0.00      0.00         4
         494       1.00      1.00      1.00         4
         495       0.50      0.80      0.62         5
         496       1.00      0.56      0.71         9
         497       0.67      0.50      0.57         4
         498       0.00      0.00      0.00         4
         499       0.67      0.40      0.50         5
         500       0.50      0.75      0.60         4
         501       0.75      0.75      0.75         4
         502       0.75      0.75      0.75         4
         503       0.00      0.00      0.00         4
         504       0.90      1.00      0.95         9
         505       1.00      1.00      1.00         9
         506       0.43      0.75      0.55         4
         507       0.40      0.50      0.44         4
         508       1.00      0.00      0.00         4
         509       0.64      0.78      0.70         9
         510       0.17      0.25      0.20         4
         511       0.62      1.00      0.77         5
         512       0.75      0.67      0.71         9
         513       0.67      0.40      0.50         5
         514       1.00      0.75      0.86         4
         515       1.00      0.50      0.67         4
         516       1.00      0.60      0.75         5
         517       0.67      0.50      0.57         4
         518       1.00      0.25      0.40         4
         519       0.00      0.00      0.00         4
         520       0.73      0.89      0.80         9
         521       1.00      1.00      1.00         4
         522       0.83      1.00      0.91         5
         523       0.58      0.78      0.67         9
         524       0.06      0.25      0.10         4
         525       0.08      0.11      0.10         9
         526       0.50      0.60      0.55         5
         527       0.50      0.50      0.50         4
         528       0.64      1.00      0.78         9
         529       1.00      0.75      0.86         4
         530       1.00      1.00      1.00         4
         531       1.00      0.75      0.86         4
         532       0.57      0.44      0.50         9
         533       0.86      0.67      0.75         9
         534       1.00      1.00      1.00         5
         535       0.60      0.75      0.67         4
         536       1.00      1.00      1.00         4
         537       0.00      0.00      0.00         4
         538       1.00      1.00      1.00         4
         539       1.00      0.50      0.67         4
         540       1.00      1.00      1.00         5
         541       0.06      0.11      0.08         9
         542       0.67      0.40      0.50         5
         543       1.00      0.75      0.86         4
         544       0.50      0.25      0.33         4
         545       0.00      0.00      0.00         9
         546       1.00      0.25      0.40         4
         547       0.75      0.75      0.75         4
         548       1.00      0.00      0.00         9
         549       0.50      0.25      0.33         4
         550       0.80      1.00      0.89         4
         551       0.57      1.00      0.73         4
         552       0.82      1.00      0.90         9
         553       0.89      0.89      0.89         9
         554       1.00      1.00      1.00         9
         555       0.50      0.50      0.50         4
         556       0.89      0.89      0.89         9
         557       0.88      0.78      0.82         9
         558       0.00      0.00      0.00         9
         559       1.00      0.00      0.00         4
         560       0.71      1.00      0.83         5
         561       0.80      0.80      0.80         5
         562       0.50      0.25      0.33         4
         563       0.29      0.80      0.42         5
         564       1.00      1.00      1.00         4
         565       1.00      1.00      1.00         9
         566       0.20      0.20      0.20         5
         567       1.00      0.80      0.89         5
         568       0.80      1.00      0.89         4
         569       0.80      1.00      0.89         4
         570       0.57      0.89      0.70         9
         571       0.67      0.50      0.57         4
         572       1.00      1.00      1.00         4
         573       0.78      0.78      0.78         9
         574       0.64      0.78      0.70         9
         575       0.71      1.00      0.83         5
         576       0.33      0.25      0.29         4
         577       0.44      1.00      0.62         4
         578       0.20      0.11      0.14         9
         579       0.75      0.75      0.75         4
         580       0.89      0.89      0.89         9
         581       1.00      1.00      1.00         4
         582       0.07      0.25      0.11         4
         583       1.00      0.75      0.86         4
         584       0.50      1.00      0.67         4
         585       1.00      0.80      0.89         5
         586       0.89      0.89      0.89         9
         587       1.00      1.00      1.00         4
         588       0.89      0.89      0.89         9
         589       1.00      1.00      1.00         4
         590       0.62      1.00      0.77         5
         591       0.50      0.50      0.50         4
         592       1.00      0.75      0.86         4
         593       1.00      0.75      0.86         4
         594       1.00      0.78      0.88         9
         595       0.60      0.75      0.67         4
         596       0.90      1.00      0.95         9
         597       0.78      0.78      0.78         9
         598       1.00      0.00      0.00         4
         599       0.67      0.67      0.67         9
         600       0.00      0.00      0.00         4
         601       1.00      1.00      1.00         5
         602       0.88      0.78      0.82         9
         603       0.71      1.00      0.83         5
         604       1.00      0.00      0.00         4
         605       0.75      0.60      0.67         5
         606       1.00      0.00      0.00         4
         607       1.00      1.00      1.00         4
         608       1.00      0.50      0.67         4
         609       0.09      0.25      0.13         4
         610       0.00      0.00      0.00         4
         611       0.50      0.50      0.50         4
         612       1.00      0.89      0.94         9
         613       1.00      0.00      0.00         4
         614       1.00      0.60      0.75         5
         615       0.80      0.89      0.84         9
         616       0.86      0.67      0.75         9
         617       0.67      0.44      0.53         9
         618       0.83      1.00      0.91         5
         619       1.00      1.00      1.00         4
         620       1.00      0.00      0.00         4
         621       1.00      0.00      0.00         4
         622       1.00      1.00      1.00         4
         623       0.67      0.80      0.73         5
         624       1.00      0.75      0.86         4
         625       1.00      1.00      1.00         4
         626       1.00      1.00      1.00         4
         627       1.00      0.00      0.00         4
         628       0.67      0.50      0.57         4
         629       0.56      0.56      0.56         9
         630       0.82      1.00      0.90         9
         631       1.00      0.75      0.86         4
         632       1.00      1.00      1.00         5
         633       1.00      1.00      1.00         4
         634       0.67      0.80      0.73         5
         635       0.90      1.00      0.95         9
         636       0.89      0.89      0.89         9
         637       1.00      1.00      1.00         4
         638       0.33      0.25      0.29         4
         639       1.00      0.00      0.00         4
         640       0.88      0.78      0.82         9
         641       0.80      0.44      0.57         9
         642       0.67      1.00      0.80         4
         643       1.00      0.00      0.00         4
         644       1.00      0.60      0.75         5
         645       1.00      0.50      0.67         4
         646       1.00      0.78      0.88         9
         647       1.00      1.00      1.00         4
         648       0.90      1.00      0.95         9
         649       0.67      0.80      0.73         5
         650       0.00      0.00      0.00         9
         651       0.89      0.89      0.89         9
         652       1.00      0.00      0.00         4
         653       1.00      0.00      0.00         4
         654       1.00      0.00      0.00         4
         655       0.00      0.00      0.00         4
         656       1.00      0.80      0.89         5
         657       1.00      1.00      1.00         4
         658       1.00      0.60      0.75         5
         659       0.82      1.00      0.90         9
         660       1.00      0.80      0.89         5
         661       0.50      0.25      0.33         4
         662       0.75      0.75      0.75         4
         663       0.00      0.00      0.00         4
         664       0.83      0.56      0.67         9
         665       0.33      0.40      0.36         5
         666       0.75      0.75      0.75         4
         667       0.75      0.75      0.75         4
         668       0.67      0.50      0.57         4
         669       1.00      1.00      1.00         4
         670       0.80      1.00      0.89         4
         671       0.00      0.00      0.00         9
         672       1.00      1.00      1.00         4
         673       1.00      0.00      0.00         4
         674       1.00      0.75      0.86         4
         675       1.00      1.00      1.00         4
         676       1.00      1.00      1.00         4
         677       0.00      0.00      0.00         9
         678       0.86      0.67      0.75         9
         679       1.00      0.20      0.33         5
         680       1.00      0.89      0.94         9
         681       1.00      0.50      0.67         4
         682       0.50      0.25      0.33         4
         683       1.00      1.00      1.00         4
         684       0.00      0.00      0.00         9
         685       1.00      1.00      1.00         9
         686       1.00      0.50      0.67         4
         687       0.50      1.00      0.67         5
         688       1.00      0.25      0.40         4
         689       1.00      0.00      0.00         4
         690       1.00      0.75      0.86         4
         691       1.00      1.00      1.00         4
         692       0.89      0.89      0.89         9
         693       0.86      0.67      0.75         9
         694       0.82      1.00      0.90         9
         695       0.67      0.80      0.73         5
         696       1.00      0.50      0.67         4
         697       0.67      0.50      0.57         4
         698       0.57      1.00      0.73         4
         699       1.00      1.00      1.00         5
         700       1.00      0.75      0.86         4
         701       0.75      0.75      0.75         4
         702       0.60      0.33      0.43         9
         703       0.00      0.00      0.00         4
         704       0.00      0.00      0.00         4
         705       0.00      0.00      0.00         4
         706       0.75      0.60      0.67         5
         707       0.25      0.50      0.33         4
         708       1.00      1.00      1.00         9
         709       0.00      0.00      0.00         4
         710       0.89      0.89      0.89         9
         711       0.00      0.00      0.00         4
         712       1.00      0.75      0.86         4
         713       0.88      0.78      0.82         9
         714       0.82      1.00      0.90         9
         715       1.00      0.25      0.40         4
         716       0.83      1.00      0.91         5
         717       1.00      0.80      0.89         5
         718       1.00      0.50      0.67         4
         719       0.67      0.50      0.57         4
         720       0.00      0.00      0.00         4
         721       0.40      0.50      0.44         4
         722       1.00      0.78      0.88         9
         723       1.00      0.00      0.00         4
         724       1.00      1.00      1.00         4
         725       0.75      0.75      0.75         4
         726       0.50      0.75      0.60         4
         727       0.00      0.00      0.00         9
         728       0.00      0.00      0.00         4
         729       0.67      0.50      0.57         4
         730       1.00      0.75      0.86         4
         731       1.00      0.75      0.86         4
         732       1.00      0.75      0.86         4
         733       0.75      0.75      0.75         4
         734       1.00      0.00      0.00         4
         735       1.00      0.20      0.33         5
         736       1.00      0.75      0.86         4
         737       1.00      0.75      0.86         4
         738       0.78      0.78      0.78         9
         739       1.00      1.00      1.00         5
         740       0.75      0.75      0.75         4
         741       0.62      0.89      0.73         9
         742       0.00      0.00      0.00         4
         743       0.67      0.50      0.57         4
         744       1.00      1.00      1.00         4
         745       0.00      0.00      0.00         4
         746       0.62      1.00      0.77         5
         747       1.00      0.75      0.86         4
         748       1.00      0.50      0.67         4
         749       0.25      0.20      0.22         5
         750       1.00      0.50      0.67         4
         751       1.00      0.75      0.86         4
         752       1.00      1.00      1.00         4
         753       0.44      0.44      0.44         9
         754       0.50      0.50      0.50         4
         755       0.58      0.78      0.67         9
         756       1.00      0.80      0.89         5
         757       1.00      1.00      1.00         4
         758       1.00      1.00      1.00         9
         759       0.00      0.00      0.00         4
         760       1.00      1.00      1.00         4
         761       0.67      0.50      0.57         4
         762       1.00      1.00      1.00         9
         763       1.00      0.00      0.00         4
         764       1.00      0.50      0.67         4
         765       1.00      1.00      1.00         4
         766       0.00      0.00      0.00         4
         767       1.00      1.00      1.00         9
         768       0.83      1.00      0.91         5
         769       0.80      0.80      0.80         5
         770       0.67      1.00      0.80         4
         771       1.00      1.00      1.00         4
         772       1.00      0.75      0.86         4
         773       1.00      1.00      1.00         4
         774       0.89      0.89      0.89         9
         775       1.00      1.00      1.00         5
         776       1.00      1.00      1.00         9
         777       1.00      0.60      0.75         5
         778       1.00      1.00      1.00         4
         779       0.88      0.78      0.82         9
         780       1.00      1.00      1.00         4
         781       0.33      0.11      0.17         9
         782       0.62      1.00      0.77         5
         783       0.50      0.25      0.33         4
         784       0.50      0.67      0.57         9
         785       1.00      0.00      0.00         4
         786       0.00      0.00      0.00         4
         787       0.67      0.50      0.57         4
         788       1.00      0.75      0.86         4
         789       1.00      0.50      0.67         4
         790       0.44      0.44      0.44         9
         791       1.00      1.00      1.00         4
         792       0.00      0.00      0.00         4
         793       0.67      0.50      0.57         4
         794       0.73      0.89      0.80         9
         795       1.00      0.75      0.86         4
         796       0.25      0.20      0.22         5
         797       0.89      0.89      0.89         9
         798       0.83      1.00      0.91         5
         799       1.00      0.25      0.40         4
         800       1.00      1.00      1.00         4
         801       1.00      0.00      0.00         4
         802       0.83      0.56      0.67         9
         803       1.00      0.50      0.67         4
         804       1.00      1.00      1.00         9
         805       0.00      0.00      0.00         4
         806       1.00      0.60      0.75         5
         807       1.00      0.89      0.94         9
         808       0.00      0.00      0.00         4
         809       0.67      0.50      0.57         4
         810       0.86      0.67      0.75         9
         811       1.00      0.50      0.67         4
         812       1.00      1.00      1.00         4
         813       1.00      0.60      0.75         5
         814       1.00      1.00      1.00         4
         815       1.00      0.25      0.40         4
         816       1.00      0.50      0.67         4
         817       0.75      0.75      0.75         4
         818       1.00      0.40      0.57         5
         819       1.00      0.50      0.67         4
         820       0.90      1.00      0.95         9
         821       1.00      0.75      0.86         4
         822       0.25      0.25      0.25         4
         823       0.80      1.00      0.89         4
         824       0.88      0.78      0.82         9
         825       0.75      0.75      0.75         4
         826       0.44      0.44      0.44         9
         827       0.33      0.20      0.25         5
         828       1.00      0.00      0.00         4
         829       1.00      0.00      0.00         5
         830       1.00      0.25      0.40         4
         831       1.00      1.00      1.00         4
         832       0.00      0.00      0.00         9
         833       0.78      0.78      0.78         9
         834       0.56      1.00      0.71         5
         835       0.64      0.78      0.70         9
         836       1.00      1.00      1.00         4
         837       1.00      1.00      1.00         9
         838       1.00      0.75      0.86         4
         839       0.80      1.00      0.89         4
         840       0.08      0.25      0.12         4
         841       0.17      0.25      0.20         4
         842       0.57      0.44      0.50         9
         843       0.64      0.78      0.70         9
         844       1.00      1.00      1.00         5
         845       0.67      1.00      0.80         4
         846       1.00      0.60      0.75         5
         847       0.00      0.00      0.00         4
         848       1.00      0.56      0.71         9
         849       0.00      0.00      0.00         4
         850       0.82      1.00      0.90         9
         851       0.67      1.00      0.80         4
         852       0.50      0.25      0.33         4
         853       0.75      0.75      0.75         4
         854       0.67      0.50      0.57         4
         855       0.89      0.89      0.89         9
         856       0.62      0.56      0.59         9
         857       1.00      1.00      1.00         4
         858       0.00      0.00      0.00         5
         859       0.67      0.67      0.67         9
         860       1.00      0.67      0.80         9
         861       1.00      0.50      0.67         4
         862       1.00      1.00      1.00         4
         863       1.00      0.20      0.33         5
         864       1.00      0.00      0.00         9
         865       0.75      0.75      0.75         4
         866       0.70      0.78      0.74         9
         867       1.00      0.50      0.67         4
         868       1.00      0.75      0.86         4
         869       0.00      0.00      0.00         9
         870       1.00      0.89      0.94         9
         871       1.00      0.40      0.57         5
         872       1.00      0.75      0.86         4
         873       0.60      0.75      0.67         4
         874       0.33      0.50      0.40         4
         875       1.00      0.50      0.67         4
         876       0.00      0.00      0.00         4
         877       0.75      1.00      0.86         9
         878       1.00      0.88      0.93         8
         879       0.00      0.00      0.00         4
         880       0.78      0.78      0.78         9
         881       0.00      0.00      0.00         4
         882       0.67      0.50      0.57         4
         883       0.89      0.89      0.89         9
         884       0.80      1.00      0.89         4
         885       1.00      1.00      1.00         9
         886       1.00      0.75      0.86         4
         887       1.00      0.78      0.88         9
         888       0.67      1.00      0.80         4
         889       0.90      1.00      0.95         9
         890       1.00      1.00      1.00         4
         891       1.00      0.60      0.75         5
         892       1.00      1.00      1.00         4
         893       1.00      1.00      1.00         4

    accuracy                           0.67      4917
   macro avg       0.72      0.64      0.63      4917
weighted avg       0.73      0.67      0.66      4917

