CLASS_ORDER: [338, 679, 54, 718, 457, 855, 640, 402, 65, 140, 334, 398, 865, 318, 844, 244, 299, 460, 577, 389, 191, 277, 239, 665, 560, 748, 44, 697, 193, 368, 714, 489, 716, 367, 468, 613, 320, 605, 838, 253, 109, 452, 615, 131, 376, 35, 710, 453, 808, 482, 268, 530, 872, 199, 155, 863, 763, 317, 563, 557, 884, 137, 781, 680, 75, 24, 183, 351, 811, 165, 741, 559, 521, 857, 270, 399, 705, 790, 302, 702, 424, 840, 525, 378, 508, 272, 663, 456, 591, 13, 212, 473, 494, 417, 554, 27, 373, 9, 647, 156, 732, 449, 316, 648, 690, 185, 249, 1, 333, 490, 779, 475, 587, 326, 392, 832, 495, 328, 694, 257, 407, 203, 491, 539, 669, 676, 699, 411, 682, 867, 650, 413, 240, 875, 330, 780, 390, 666, 314, 331, 778, 459, 93, 180, 572, 833, 540, 795, 639, 187, 836, 824, 845, 159, 698, 273, 285, 493, 595, 409, 266, 742, 469, 130, 358, 583, 444, 121, 327, 654, 546, 135, 731, 668, 48, 450, 772, 87, 888, 313, 34, 623, 2, 62, 20, 84, 308, 788, 636, 184, 217, 94, 675, 186, 104, 221, 510, 889, 160, 260, 241, 82, 332, 777, 243, 86, 443, 656, 282, 512, 813, 147, 597, 661, 66, 49, 601, 509, 197, 210, 223, 254, 255, 860, 831, 805, 804, 124, 98, 400, 819, 70, 231, 164, 644, 598, 532, 771, 198, 517, 825, 8, 371, 522, 102, 289, 531, 403, 36, 794, 16, 604, 5, 653, 802, 846, 114, 301, 479, 352, 712, 12, 21, 182, 205, 436, 63, 651, 4, 766, 535, 91, 238, 637, 810, 360, 78, 818, 33, 745, 174, 792, 464, 296, 703, 841, 362, 827, 879, 606, 283, 775, 204, 6, 592, 99, 46, 856, 380, 890, 295, 325, 526, 379, 288, 350, 100, 704, 562, 880, 686, 887, 881, 419, 139, 589, 622, 213, 71, 377, 814, 69, 95, 747, 14, 396, 202, 406, 365, 537, 418, 276, 108, 67, 765, 151, 891, 812, 339, 556, 854, 759, 610, 752, 224, 807, 73, 536, 848, 176, 784, 429, 0, 655, 324, 477, 753, 415, 252, 206, 520, 222, 81, 23, 426, 172, 315, 553, 15, 672, 470, 658, 113, 125, 820, 630, 228, 41, 304, 435, 303, 207, 754, 438, 463, 695, 141, 751, 412, 566, 600, 115, 720, 584, 281, 59, 497, 738, 800, 503, 486, 220, 776, 743, 674, 117, 849, 767, 877, 476, 423, 196, 629, 727, 708, 428, 862, 773, 132, 670, 235, 237, 265, 434, 74, 681, 724, 43, 516, 634, 499, 504, 527, 472, 336, 416, 55, 279, 602, 236, 688, 565, 481, 728, 439, 246, 599, 80, 116, 523, 642, 53, 349, 284, 822, 829, 791, 394, 133, 170, 441, 375, 382, 72, 30, 627, 219, 624, 321, 561, 361, 692, 195, 574, 821, 374, 579, 664, 768, 518, 609, 242, 594, 263, 667, 492, 873, 892, 706, 127, 649, 370, 878, 56, 723, 171, 798, 678, 229, 292, 545, 227, 128, 621, 216, 782, 480, 22, 234, 570, 498, 300, 839, 214, 179, 77, 47, 737, 177, 200, 467, 138, 505, 541, 719, 817, 146, 10, 142, 519, 143, 218, 166, 785, 372, 749, 425, 256, 430, 631, 152, 474, 548, 466, 169, 659, 25, 286, 635, 538, 42, 816, 259, 267, 383, 739, 826, 580, 422, 871, 344, 750, 882, 215, 404, 725, 410, 92, 110, 786, 564, 153, 542, 607, 226, 366, 305, 567, 297, 445, 461, 167, 118, 269, 387, 154, 107, 458, 789, 136, 823, 52, 64, 391, 145, 837, 230, 111, 384, 514, 549, 322, 809, 660, 803, 440, 575, 478, 693, 828, 405, 496, 853, 251, 190, 354, 876, 582, 484, 347, 588, 677, 662, 657, 442, 88, 626, 511, 462, 851, 161, 730, 329, 618, 381, 446, 806, 774, 364, 893, 471, 590, 173, 641, 685, 17, 258, 528, 717, 51, 150, 126, 192, 451, 89, 726, 97, 850, 578, 673, 60, 581, 342, 50, 632, 487, 485, 733, 843, 543, 616, 830, 859, 858, 290, 103, 261, 544, 32, 388, 346, 19, 348, 432, 356, 709, 534, 201, 700, 883, 501, 148, 431, 61, 625, 181, 357, 507, 593, 29, 162, 112, 274, 796, 3, 353, 864, 683, 420, 757, 225, 189, 885, 701, 612, 886, 340, 68, 96, 760, 232, 633, 801, 287, 211, 157, 401, 311, 455, 105, 106, 513, 652, 783, 250, 79, 168, 764, 293, 691, 585, 465, 397, 488, 248, 533, 671, 40, 797, 524, 735, 761, 620, 770, 319, 547, 586, 835, 294, 502, 734, 323, 608, 144, 264, 278, 178, 866, 385, 744, 571, 363, 852, 335, 506, 421, 85, 689, 386, 309, 76, 628, 847, 90, 57, 310, 696, 729, 45, 393, 762, 312, 437, 175, 874, 619, 515, 568, 247, 713, 414, 740, 645, 756, 262, 551, 280, 842, 684, 188, 529, 569, 758, 58, 337, 869, 769, 715, 861, 687, 558, 643, 868, 31, 291, 483, 793, 208, 799, 736, 163, 158, 298, 721, 271, 37, 448, 834, 28, 83, 500, 369, 722, 611, 408, 870, 122, 707, 123, 26, 454, 120, 359, 447, 129, 746, 711, 343, 194, 573, 617, 18, 101, 755, 787, 119, 233, 815, 341, 433, 245, 307, 427, 38, 550, 603, 576, 638, 275, 306, 555, 39, 395, 596, 134, 209, 646, 355, 345, 11, 7, 614, 149, 552]
class_group: [(338, 679, 54, 718, 457, 855, 640, 402, 65, 140, 334, 398, 865, 318, 844, 244, 299, 460, 577, 389, 191, 277, 239, 665, 560, 748, 44, 697, 193, 368, 714, 489, 716, 367), (468, 613, 320, 605, 838, 253, 109, 452, 615, 131, 376, 35, 710, 453, 808, 482, 268, 530, 872, 199), (155, 863, 763, 317, 563, 557, 884, 137, 781, 680, 75, 24, 183, 351, 811, 165, 741, 559, 521, 857), (270, 399, 705, 790, 302, 702, 424, 840, 525, 378, 508, 272, 663, 456, 591, 13, 212, 473, 494, 417), (554, 27, 373, 9, 647, 156, 732, 449, 316, 648, 690, 185, 249, 1, 333, 490, 779, 475, 587, 326), (392, 832, 495, 328, 694, 257, 407, 203, 491, 539, 669, 676, 699, 411, 682, 867, 650, 413, 240, 875), (330, 780, 390, 666, 314, 331, 778, 459, 93, 180, 572, 833, 540, 795, 639, 187, 836, 824, 845, 159), (698, 273, 285, 493, 595, 409, 266, 742, 469, 130, 358, 583, 444, 121, 327, 654, 546, 135, 731, 668), (48, 450, 772, 87, 888, 313, 34, 623, 2, 62, 20, 84, 308, 788, 636, 184, 217, 94, 675, 186), (104, 221, 510, 889, 160, 260, 241, 82, 332, 777, 243, 86, 443, 656, 282, 512, 813, 147, 597, 661), (66, 49, 601, 509, 197, 210, 223, 254, 255, 860, 831, 805, 804, 124, 98, 400, 819, 70, 231, 164), (644, 598, 532, 771, 198, 517, 825, 8, 371, 522, 102, 289, 531, 403, 36, 794, 16, 604, 5, 653), (802, 846, 114, 301, 479, 352, 712, 12, 21, 182, 205, 436, 63, 651, 4, 766, 535, 91, 238, 637), (810, 360, 78, 818, 33, 745, 174, 792, 464, 296, 703, 841, 362, 827, 879, 606, 283, 775, 204, 6), (592, 99, 46, 856, 380, 890, 295, 325, 526, 379, 288, 350, 100, 704, 562, 880, 686, 887, 881, 419), (139, 589, 622, 213, 71, 377, 814, 69, 95, 747, 14, 396, 202, 406, 365, 537, 418, 276, 108, 67), (765, 151, 891, 812, 339, 556, 854, 759, 610, 752, 224, 807, 73, 536, 848, 176, 784, 429, 0, 655), (324, 477, 753, 415, 252, 206, 520, 222, 81, 23, 426, 172, 315, 553, 15, 672, 470, 658, 113, 125), (820, 630, 228, 41, 304, 435, 303, 207, 754, 438, 463, 695, 141, 751, 412, 566, 600, 115, 720, 584), (281, 59, 497, 738, 800, 503, 486, 220, 776, 743, 674, 117, 849, 767, 877, 476, 423, 196, 629, 727), (708, 428, 862, 773, 132, 670, 235, 237, 265, 434, 74, 681, 724, 43, 516, 634, 499, 504, 527, 472), (336, 416, 55, 279, 602, 236, 688, 565, 481, 728, 439, 246, 599, 80, 116, 523, 642, 53, 349, 284), (822, 829, 791, 394, 133, 170, 441, 375, 382, 72, 30, 627, 219, 624, 321, 561, 361, 692, 195, 574), (821, 374, 579, 664, 768, 518, 609, 242, 594, 263, 667, 492, 873, 892, 706, 127, 649, 370, 878, 56), (723, 171, 798, 678, 229, 292, 545, 227, 128, 621, 216, 782, 480, 22, 234, 570, 498, 300, 839, 214), (179, 77, 47, 737, 177, 200, 467, 138, 505, 541, 719, 817, 146, 10, 142, 519, 143, 218, 166, 785), (372, 749, 425, 256, 430, 631, 152, 474, 548, 466, 169, 659, 25, 286, 635, 538, 42, 816, 259, 267), (383, 739, 826, 580, 422, 871, 344, 750, 882, 215, 404, 725, 410, 92, 110, 786, 564, 153, 542, 607), (226, 366, 305, 567, 297, 445, 461, 167, 118, 269, 387, 154, 107, 458, 789, 136, 823, 52, 64, 391), (145, 837, 230, 111, 384, 514, 549, 322, 809, 660, 803, 440, 575, 478, 693, 828, 405, 496, 853, 251), (190, 354, 876, 582, 484, 347, 588, 677, 662, 657, 442, 88, 626, 511, 462, 851, 161, 730, 329, 618), (381, 446, 806, 774, 364, 893, 471, 590, 173, 641, 685, 17, 258, 528, 717, 51, 150, 126, 192, 451), (89, 726, 97, 850, 578, 673, 60, 581, 342, 50, 632, 487, 485, 733, 843, 543, 616, 830, 859, 858), (290, 103, 261, 544, 32, 388, 346, 19, 348, 432, 356, 709, 534, 201, 700, 883, 501, 148, 431, 61), (625, 181, 357, 507, 593, 29, 162, 112, 274, 796, 3, 353, 864, 683, 420, 757, 225, 189, 885, 701), (612, 886, 340, 68, 96, 760, 232, 633, 801, 287, 211, 157, 401, 311, 455, 105, 106, 513, 652, 783), (250, 79, 168, 764, 293, 691, 585, 465, 397, 488, 248, 533, 671, 40, 797, 524, 735, 761, 620, 770), (319, 547, 586, 835, 294, 502, 734, 323, 608, 144, 264, 278, 178, 866, 385, 744, 571, 363, 852, 335), (506, 421, 85, 689, 386, 309, 76, 628, 847, 90, 57, 310, 696, 729, 45, 393, 762, 312, 437, 175), (874, 619, 515, 568, 247, 713, 414, 740, 645, 756, 262, 551, 280, 842, 684, 188, 529, 569, 758, 58), (337, 869, 769, 715, 861, 687, 558, 643, 868, 31, 291, 483, 793, 208, 799, 736, 163, 158, 298, 721), (271, 37, 448, 834, 28, 83, 500, 369, 722, 611, 408, 870, 122, 707, 123, 26, 454, 120, 359, 447), (129, 746, 711, 343, 194, 573, 617, 18, 101, 755, 787, 119, 233, 815, 341, 433, 245, 307, 427, 38), (550, 603, 576, 638, 275, 306, 555, 39, 395, 596, 134, 209, 646, 355, 345, 11, 7, 614, 149, 552)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.5955	step1_train_accuracy: 68.4211
STEP-1	Epoch: 20/50	loss: 0.7775	step1_train_accuracy: 93.8294
STEP-1	Epoch: 30/50	loss: 0.4227	step1_train_accuracy: 96.7332
STEP-1	Epoch: 40/50	loss: 0.2691	step1_train_accuracy: 97.0962
STEP-1	Epoch: 50/50	loss: 0.1962	step1_train_accuracy: 97.0962
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.0811	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.0775	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.0752	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0733	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0718	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0705	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0693	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0684	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0675	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0667	gate_loss: 0.0000	step2_classification_accuracy: 97.3529	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 95.6522	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 2.0286	step1_train_accuracy: 62.1528
STEP-1	Epoch: 20/50	loss: 0.9664	step1_train_accuracy: 79.5139
STEP-1	Epoch: 30/50	loss: 0.6212	step1_train_accuracy: 83.6806
STEP-1	Epoch: 40/50	loss: 0.4668	step1_train_accuracy: 91.6667
STEP-1	Epoch: 50/50	loss: 0.3681	step1_train_accuracy: 93.7500
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.1668	gate_loss: 0.3937	step2_classification_accuracy: 93.1481	step_2_gate_accuracy: 90.5556
STEP-2	Epoch: 40/200	classification_loss: 0.1320	gate_loss: 0.1555	step2_classification_accuracy: 94.2593	step_2_gate_accuracy: 98.3333
STEP-2	Epoch: 60/200	classification_loss: 0.1511	gate_loss: 0.0839	step2_classification_accuracy: 94.2593	step_2_gate_accuracy: 99.0741
STEP-2	Epoch: 80/200	classification_loss: 0.1051	gate_loss: 0.0518	step2_classification_accuracy: 95.1852	step_2_gate_accuracy: 99.6296
STEP-2	Epoch: 100/200	classification_loss: 0.1013	gate_loss: 0.0376	step2_classification_accuracy: 94.8148	step_2_gate_accuracy: 99.4444
STEP-2	Epoch: 120/200	classification_loss: 0.0903	gate_loss: 0.0288	step2_classification_accuracy: 95.1852	step_2_gate_accuracy: 99.8148
STEP-2	Epoch: 140/200	classification_loss: 0.0849	gate_loss: 0.0224	step2_classification_accuracy: 95.3704	step_2_gate_accuracy: 99.8148
STEP-2	Epoch: 160/200	classification_loss: 0.2122	gate_loss: 0.0478	step2_classification_accuracy: 94.0741	step_2_gate_accuracy: 98.3333
STEP-2	Epoch: 180/200	classification_loss: 0.0814	gate_loss: 0.0161	step2_classification_accuracy: 95.3704	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0781	gate_loss: 0.0140	step2_classification_accuracy: 95.3704	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 93.4783	gate_accuracy: 97.1014
	Task-1	val_accuracy: 87.5000	gate_accuracy: 95.8333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 96.6667


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 2.2869	step1_train_accuracy: 48.2353
STEP-1	Epoch: 20/50	loss: 1.0029	step1_train_accuracy: 78.2353
STEP-1	Epoch: 30/50	loss: 0.5046	step1_train_accuracy: 93.5294
STEP-1	Epoch: 40/50	loss: 0.3227	step1_train_accuracy: 95.2941
STEP-1	Epoch: 50/50	loss: 0.2389	step1_train_accuracy: 95.2941
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2530	gate_loss: 0.6795	step2_classification_accuracy: 89.4595	step_2_gate_accuracy: 77.2973
STEP-2	Epoch: 40/200	classification_loss: 0.2035	gate_loss: 0.3102	step2_classification_accuracy: 91.2162	step_2_gate_accuracy: 93.6487
STEP-2	Epoch: 60/200	classification_loss: 0.1997	gate_loss: 0.1649	step2_classification_accuracy: 91.7568	step_2_gate_accuracy: 96.4865
STEP-2	Epoch: 80/200	classification_loss: 0.1791	gate_loss: 0.1113	step2_classification_accuracy: 92.0270	step_2_gate_accuracy: 97.2973
STEP-2	Epoch: 100/200	classification_loss: 0.1510	gate_loss: 0.0850	step2_classification_accuracy: 93.1081	step_2_gate_accuracy: 97.4324
STEP-2	Epoch: 120/200	classification_loss: 0.1403	gate_loss: 0.0687	step2_classification_accuracy: 93.2432	step_2_gate_accuracy: 98.1081
STEP-2	Epoch: 140/200	classification_loss: 0.1446	gate_loss: 0.0637	step2_classification_accuracy: 92.5676	step_2_gate_accuracy: 97.8378
STEP-2	Epoch: 160/200	classification_loss: 0.1391	gate_loss: 0.0581	step2_classification_accuracy: 92.7027	step_2_gate_accuracy: 97.5676
STEP-2	Epoch: 180/200	classification_loss: 0.1388	gate_loss: 0.0513	step2_classification_accuracy: 92.8378	step_2_gate_accuracy: 97.9730
STEP-2	Epoch: 200/200	classification_loss: 0.1237	gate_loss: 0.0456	step2_classification_accuracy: 93.7838	step_2_gate_accuracy: 99.1892
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 92.0290	gate_accuracy: 95.6522
	Task-1	val_accuracy: 87.5000	gate_accuracy: 95.8333
	Task-2	val_accuracy: 87.0588	gate_accuracy: 85.8824
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 92.8814


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 1.8353	step1_train_accuracy: 58.1395
STEP-1	Epoch: 20/50	loss: 0.8693	step1_train_accuracy: 85.1744
STEP-1	Epoch: 30/50	loss: 0.4559	step1_train_accuracy: 95.0581
STEP-1	Epoch: 40/50	loss: 0.2785	step1_train_accuracy: 97.0930
STEP-1	Epoch: 50/50	loss: 0.1943	step1_train_accuracy: 99.1279
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2968	gate_loss: 0.9100	step2_classification_accuracy: 89.5745	step_2_gate_accuracy: 67.8723
STEP-2	Epoch: 40/200	classification_loss: 0.1967	gate_loss: 0.4081	step2_classification_accuracy: 91.9149	step_2_gate_accuracy: 89.7872
STEP-2	Epoch: 60/200	classification_loss: 0.1687	gate_loss: 0.2147	step2_classification_accuracy: 92.4468	step_2_gate_accuracy: 95.0000
STEP-2	Epoch: 80/200	classification_loss: 0.1601	gate_loss: 0.1496	step2_classification_accuracy: 92.4468	step_2_gate_accuracy: 95.7447
STEP-2	Epoch: 100/200	classification_loss: 0.1435	gate_loss: 0.1166	step2_classification_accuracy: 92.9787	step_2_gate_accuracy: 96.7021
STEP-2	Epoch: 120/200	classification_loss: 0.1332	gate_loss: 0.0949	step2_classification_accuracy: 93.4043	step_2_gate_accuracy: 97.0213
STEP-2	Epoch: 140/200	classification_loss: 0.1279	gate_loss: 0.0847	step2_classification_accuracy: 94.0426	step_2_gate_accuracy: 97.4468
STEP-2	Epoch: 160/200	classification_loss: 0.1281	gate_loss: 0.0757	step2_classification_accuracy: 94.0426	step_2_gate_accuracy: 97.7660
STEP-2	Epoch: 180/200	classification_loss: 0.1255	gate_loss: 0.0710	step2_classification_accuracy: 93.7234	step_2_gate_accuracy: 97.6596
STEP-2	Epoch: 200/200	classification_loss: 0.1221	gate_loss: 0.0655	step2_classification_accuracy: 94.0426	step_2_gate_accuracy: 97.9787
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 92.0290	gate_accuracy: 91.3043
	Task-1	val_accuracy: 79.1667	gate_accuracy: 90.2778
	Task-2	val_accuracy: 83.5294	gate_accuracy: 82.3529
	Task-3	val_accuracy: 86.0465	gate_accuracy: 82.5581
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 87.1391


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.2670	step1_train_accuracy: 59.9379
STEP-1	Epoch: 20/50	loss: 0.8378	step1_train_accuracy: 85.7143
STEP-1	Epoch: 30/50	loss: 0.4532	step1_train_accuracy: 95.9627
STEP-1	Epoch: 40/50	loss: 0.2915	step1_train_accuracy: 97.5155
STEP-1	Epoch: 50/50	loss: 0.2100	step1_train_accuracy: 97.8261
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.2511	gate_loss: 1.0182	step2_classification_accuracy: 91.4035	step_2_gate_accuracy: 66.2281
STEP-2	Epoch: 40/200	classification_loss: 0.1787	gate_loss: 0.4383	step2_classification_accuracy: 93.2456	step_2_gate_accuracy: 91.7544
STEP-2	Epoch: 60/200	classification_loss: 0.1526	gate_loss: 0.2155	step2_classification_accuracy: 93.5965	step_2_gate_accuracy: 95.5263
STEP-2	Epoch: 80/200	classification_loss: 0.1443	gate_loss: 0.1355	step2_classification_accuracy: 94.0351	step_2_gate_accuracy: 96.9298
STEP-2	Epoch: 100/200	classification_loss: 0.1333	gate_loss: 0.1030	step2_classification_accuracy: 93.8596	step_2_gate_accuracy: 97.5439
STEP-2	Epoch: 120/200	classification_loss: 0.1229	gate_loss: 0.0838	step2_classification_accuracy: 94.2105	step_2_gate_accuracy: 97.8070
STEP-2	Epoch: 140/200	classification_loss: 0.1153	gate_loss: 0.0731	step2_classification_accuracy: 94.9123	step_2_gate_accuracy: 97.8070
STEP-2	Epoch: 160/200	classification_loss: 0.1069	gate_loss: 0.0639	step2_classification_accuracy: 95.1754	step_2_gate_accuracy: 98.1579
STEP-2	Epoch: 180/200	classification_loss: 0.1081	gate_loss: 0.0588	step2_classification_accuracy: 95.0877	step_2_gate_accuracy: 98.3333
STEP-2	Epoch: 200/200	classification_loss: 0.0980	gate_loss: 0.0518	step2_classification_accuracy: 95.2632	step_2_gate_accuracy: 98.5088
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 89.1304	gate_accuracy: 92.0290
	Task-1	val_accuracy: 79.1667	gate_accuracy: 90.2778
	Task-2	val_accuracy: 85.8824	gate_accuracy: 90.5882
	Task-3	val_accuracy: 83.7209	gate_accuracy: 82.5581
	Task-4	val_accuracy: 88.7500	gate_accuracy: 88.7500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 89.1540


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.2139	step1_train_accuracy: 45.5738
STEP-1	Epoch: 20/50	loss: 1.0555	step1_train_accuracy: 78.0328
STEP-1	Epoch: 30/50	loss: 0.6814	step1_train_accuracy: 85.2459
STEP-1	Epoch: 40/50	loss: 0.4858	step1_train_accuracy: 87.2131
STEP-1	Epoch: 50/50	loss: 0.3966	step1_train_accuracy: 87.5410
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3355	gate_loss: 1.1386	step2_classification_accuracy: 88.0597	step_2_gate_accuracy: 66.1194
STEP-2	Epoch: 40/200	classification_loss: 0.2158	gate_loss: 0.4694	step2_classification_accuracy: 90.9701	step_2_gate_accuracy: 89.7015
STEP-2	Epoch: 60/200	classification_loss: 0.2037	gate_loss: 0.2391	step2_classification_accuracy: 91.8657	step_2_gate_accuracy: 94.9254
STEP-2	Epoch: 80/200	classification_loss: 0.1671	gate_loss: 0.1445	step2_classification_accuracy: 92.7612	step_2_gate_accuracy: 96.4179
STEP-2	Epoch: 100/200	classification_loss: 0.1632	gate_loss: 0.1076	step2_classification_accuracy: 92.3134	step_2_gate_accuracy: 97.3134
STEP-2	Epoch: 120/200	classification_loss: 0.1491	gate_loss: 0.0858	step2_classification_accuracy: 93.2836	step_2_gate_accuracy: 98.1343
STEP-2	Epoch: 140/200	classification_loss: 0.1393	gate_loss: 0.0696	step2_classification_accuracy: 93.2090	step_2_gate_accuracy: 97.9851
STEP-2	Epoch: 160/200	classification_loss: 0.1363	gate_loss: 0.0616	step2_classification_accuracy: 93.8806	step_2_gate_accuracy: 98.4328
STEP-2	Epoch: 180/200	classification_loss: 0.1336	gate_loss: 0.0561	step2_classification_accuracy: 93.2836	step_2_gate_accuracy: 98.5075
STEP-2	Epoch: 200/200	classification_loss: 0.1283	gate_loss: 0.0503	step2_classification_accuracy: 93.8806	step_2_gate_accuracy: 98.6567
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 88.4058	gate_accuracy: 92.7536
	Task-1	val_accuracy: 73.6111	gate_accuracy: 84.7222
	Task-2	val_accuracy: 80.0000	gate_accuracy: 83.5294
	Task-3	val_accuracy: 82.5581	gate_accuracy: 80.2326
	Task-4	val_accuracy: 91.2500	gate_accuracy: 92.5000
	Task-5	val_accuracy: 75.0000	gate_accuracy: 82.8947
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 86.7784


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 2.0141	step1_train_accuracy: 71.5596
STEP-1	Epoch: 20/50	loss: 0.8524	step1_train_accuracy: 87.1560
STEP-1	Epoch: 30/50	loss: 0.4293	step1_train_accuracy: 92.9664
STEP-1	Epoch: 40/50	loss: 0.2815	step1_train_accuracy: 96.3303
STEP-1	Epoch: 50/50	loss: 0.2147	step1_train_accuracy: 97.8593
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3781	gate_loss: 1.2832	step2_classification_accuracy: 86.1688	step_2_gate_accuracy: 60.7792
STEP-2	Epoch: 40/200	classification_loss: 0.2762	gate_loss: 0.5256	step2_classification_accuracy: 90.0000	step_2_gate_accuracy: 88.3766
STEP-2	Epoch: 60/200	classification_loss: 0.2335	gate_loss: 0.2587	step2_classification_accuracy: 90.7792	step_2_gate_accuracy: 93.4416
STEP-2	Epoch: 80/200	classification_loss: 0.1983	gate_loss: 0.1692	step2_classification_accuracy: 91.6883	step_2_gate_accuracy: 95.7143
STEP-2	Epoch: 100/200	classification_loss: 0.1875	gate_loss: 0.1275	step2_classification_accuracy: 92.0130	step_2_gate_accuracy: 96.8831
STEP-2	Epoch: 120/200	classification_loss: 0.1811	gate_loss: 0.1078	step2_classification_accuracy: 92.0130	step_2_gate_accuracy: 96.8831
STEP-2	Epoch: 140/200	classification_loss: 0.1688	gate_loss: 0.0891	step2_classification_accuracy: 92.5325	step_2_gate_accuracy: 97.4675
STEP-2	Epoch: 160/200	classification_loss: 0.1569	gate_loss: 0.0783	step2_classification_accuracy: 93.1169	step_2_gate_accuracy: 97.7922
STEP-2	Epoch: 180/200	classification_loss: 0.1474	gate_loss: 0.0664	step2_classification_accuracy: 93.1169	step_2_gate_accuracy: 97.9221
STEP-2	Epoch: 200/200	classification_loss: 0.1487	gate_loss: 0.0636	step2_classification_accuracy: 93.5065	step_2_gate_accuracy: 98.4416
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 82.6087	gate_accuracy: 88.4058
	Task-1	val_accuracy: 70.8333	gate_accuracy: 81.9444
	Task-2	val_accuracy: 78.8235	gate_accuracy: 77.6471
	Task-3	val_accuracy: 81.3953	gate_accuracy: 82.5581
	Task-4	val_accuracy: 85.0000	gate_accuracy: 85.0000
	Task-5	val_accuracy: 76.3158	gate_accuracy: 85.5263
	Task-6	val_accuracy: 92.6829	gate_accuracy: 91.4634
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 84.9758


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 3.1043	step1_train_accuracy: 38.9831
STEP-1	Epoch: 20/50	loss: 1.2236	step1_train_accuracy: 86.4407
STEP-1	Epoch: 30/50	loss: 0.6402	step1_train_accuracy: 90.6780
STEP-1	Epoch: 40/50	loss: 0.4258	step1_train_accuracy: 93.6441
STEP-1	Epoch: 50/50	loss: 0.3139	step1_train_accuracy: 95.3390
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.3931	gate_loss: 1.3016	step2_classification_accuracy: 86.3218	step_2_gate_accuracy: 64.2529
STEP-2	Epoch: 40/200	classification_loss: 0.2669	gate_loss: 0.4983	step2_classification_accuracy: 90.1724	step_2_gate_accuracy: 88.9080
STEP-2	Epoch: 60/200	classification_loss: 0.2255	gate_loss: 0.2446	step2_classification_accuracy: 91.7241	step_2_gate_accuracy: 94.6552
STEP-2	Epoch: 80/200	classification_loss: 0.1990	gate_loss: 0.1572	step2_classification_accuracy: 92.0690	step_2_gate_accuracy: 96.5517
STEP-2	Epoch: 100/200	classification_loss: 0.1900	gate_loss: 0.1236	step2_classification_accuracy: 92.1839	step_2_gate_accuracy: 97.2414
STEP-2	Epoch: 120/200	classification_loss: 0.1815	gate_loss: 0.1000	step2_classification_accuracy: 92.6437	step_2_gate_accuracy: 97.7011
STEP-2	Epoch: 140/200	classification_loss: 0.1783	gate_loss: 0.0884	step2_classification_accuracy: 92.4713	step_2_gate_accuracy: 97.5862
STEP-2	Epoch: 160/200	classification_loss: 0.1692	gate_loss: 0.0804	step2_classification_accuracy: 92.5862	step_2_gate_accuracy: 97.5287
STEP-2	Epoch: 180/200	classification_loss: 0.1693	gate_loss: 0.0747	step2_classification_accuracy: 92.6437	step_2_gate_accuracy: 97.5287
STEP-2	Epoch: 200/200	classification_loss: 0.1609	gate_loss: 0.0682	step2_classification_accuracy: 93.2184	step_2_gate_accuracy: 97.9310
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 86.2319	gate_accuracy: 88.4058
	Task-1	val_accuracy: 77.7778	gate_accuracy: 90.2778
	Task-2	val_accuracy: 80.0000	gate_accuracy: 85.8824
	Task-3	val_accuracy: 81.3953	gate_accuracy: 79.0698
	Task-4	val_accuracy: 88.7500	gate_accuracy: 91.2500
	Task-5	val_accuracy: 73.6842	gate_accuracy: 84.2105
	Task-6	val_accuracy: 89.0244	gate_accuracy: 80.4878
	Task-7	val_accuracy: 71.1864	gate_accuracy: 72.8814
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 84.6608


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.1457	step1_train_accuracy: 49.4012
STEP-1	Epoch: 20/50	loss: 1.0468	step1_train_accuracy: 77.2455
STEP-1	Epoch: 30/50	loss: 0.5818	step1_train_accuracy: 88.9222
STEP-1	Epoch: 40/50	loss: 0.3696	step1_train_accuracy: 91.9162
STEP-1	Epoch: 50/50	loss: 0.2682	step1_train_accuracy: 95.5090
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4373	gate_loss: 1.4868	step2_classification_accuracy: 84.0722	step_2_gate_accuracy: 55.4124
STEP-2	Epoch: 40/200	classification_loss: 0.2923	gate_loss: 0.5974	step2_classification_accuracy: 89.1237	step_2_gate_accuracy: 84.6907
STEP-2	Epoch: 60/200	classification_loss: 0.2378	gate_loss: 0.2860	step2_classification_accuracy: 91.0825	step_2_gate_accuracy: 93.8660
STEP-2	Epoch: 80/200	classification_loss: 0.2289	gate_loss: 0.1905	step2_classification_accuracy: 91.2887	step_2_gate_accuracy: 95.2062
STEP-2	Epoch: 100/200	classification_loss: 0.1944	gate_loss: 0.1334	step2_classification_accuracy: 92.2165	step_2_gate_accuracy: 96.8041
STEP-2	Epoch: 120/200	classification_loss: 0.1920	gate_loss: 0.1128	step2_classification_accuracy: 91.9072	step_2_gate_accuracy: 96.3402
STEP-2	Epoch: 140/200	classification_loss: 0.1778	gate_loss: 0.0926	step2_classification_accuracy: 92.4742	step_2_gate_accuracy: 97.4227
STEP-2	Epoch: 160/200	classification_loss: 0.1780	gate_loss: 0.0833	step2_classification_accuracy: 92.5258	step_2_gate_accuracy: 97.5258
STEP-2	Epoch: 180/200	classification_loss: 0.1640	gate_loss: 0.0728	step2_classification_accuracy: 92.9897	step_2_gate_accuracy: 97.6289
STEP-2	Epoch: 200/200	classification_loss: 0.1523	gate_loss: 0.0618	step2_classification_accuracy: 93.2474	step_2_gate_accuracy: 98.1959
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 82.6087	gate_accuracy: 89.1304
	Task-1	val_accuracy: 68.0556	gate_accuracy: 76.3889
	Task-2	val_accuracy: 82.3529	gate_accuracy: 87.0588
	Task-3	val_accuracy: 75.5814	gate_accuracy: 75.5814
	Task-4	val_accuracy: 85.0000	gate_accuracy: 82.5000
	Task-5	val_accuracy: 77.6316	gate_accuracy: 86.8421
	Task-6	val_accuracy: 86.5854	gate_accuracy: 82.9268
	Task-7	val_accuracy: 69.4915	gate_accuracy: 72.8814
	Task-8	val_accuracy: 73.4940	gate_accuracy: 74.6988
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.7346


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.0545	step1_train_accuracy: 51.5069
STEP-1	Epoch: 20/50	loss: 0.9246	step1_train_accuracy: 80.2740
STEP-1	Epoch: 30/50	loss: 0.5665	step1_train_accuracy: 87.9452
STEP-1	Epoch: 40/50	loss: 0.3920	step1_train_accuracy: 90.6849
STEP-1	Epoch: 50/50	loss: 0.3243	step1_train_accuracy: 90.1370
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.4607	gate_loss: 1.5119	step2_classification_accuracy: 84.2056	step_2_gate_accuracy: 55.3738
STEP-2	Epoch: 40/200	classification_loss: 0.3433	gate_loss: 0.6205	step2_classification_accuracy: 86.8692	step_2_gate_accuracy: 83.5514
STEP-2	Epoch: 60/200	classification_loss: 0.2817	gate_loss: 0.3092	step2_classification_accuracy: 89.7664	step_2_gate_accuracy: 92.1495
STEP-2	Epoch: 80/200	classification_loss: 0.2538	gate_loss: 0.2047	step2_classification_accuracy: 89.8131	step_2_gate_accuracy: 94.1122
STEP-2	Epoch: 100/200	classification_loss: 0.2289	gate_loss: 0.1478	step2_classification_accuracy: 90.4673	step_2_gate_accuracy: 95.7009
STEP-2	Epoch: 120/200	classification_loss: 0.2060	gate_loss: 0.1169	step2_classification_accuracy: 91.4019	step_2_gate_accuracy: 97.0561
STEP-2	Epoch: 140/200	classification_loss: 0.2017	gate_loss: 0.1010	step2_classification_accuracy: 91.1682	step_2_gate_accuracy: 97.0561
STEP-2	Epoch: 160/200	classification_loss: 0.1969	gate_loss: 0.0932	step2_classification_accuracy: 91.4486	step_2_gate_accuracy: 97.1963
STEP-2	Epoch: 180/200	classification_loss: 0.1841	gate_loss: 0.0791	step2_classification_accuracy: 92.1028	step_2_gate_accuracy: 97.5234
STEP-2	Epoch: 200/200	classification_loss: 0.1742	gate_loss: 0.0704	step2_classification_accuracy: 92.1963	step_2_gate_accuracy: 97.8037
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.4348	gate_accuracy: 86.2319
	Task-1	val_accuracy: 73.6111	gate_accuracy: 90.2778
	Task-2	val_accuracy: 72.9412	gate_accuracy: 81.1765
	Task-3	val_accuracy: 76.7442	gate_accuracy: 79.0698
	Task-4	val_accuracy: 87.5000	gate_accuracy: 91.2500
	Task-5	val_accuracy: 75.0000	gate_accuracy: 80.2632
	Task-6	val_accuracy: 79.2683	gate_accuracy: 79.2683
	Task-7	val_accuracy: 72.8814	gate_accuracy: 72.8814
	Task-8	val_accuracy: 85.5422	gate_accuracy: 86.7470
	Task-9	val_accuracy: 82.4176	gate_accuracy: 78.0220
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.8638


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.4476	step1_train_accuracy: 48.5816
STEP-1	Epoch: 20/50	loss: 1.0420	step1_train_accuracy: 82.2695
STEP-1	Epoch: 30/50	loss: 0.5836	step1_train_accuracy: 89.0071
STEP-1	Epoch: 40/50	loss: 0.3885	step1_train_accuracy: 94.3262
STEP-1	Epoch: 50/50	loss: 0.2817	step1_train_accuracy: 94.6809
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5304	gate_loss: 1.6893	step2_classification_accuracy: 80.8974	step_2_gate_accuracy: 49.2308
STEP-2	Epoch: 40/200	classification_loss: 0.3885	gate_loss: 0.7246	step2_classification_accuracy: 85.8547	step_2_gate_accuracy: 80.5983
STEP-2	Epoch: 60/200	classification_loss: 0.3112	gate_loss: 0.3644	step2_classification_accuracy: 87.6068	step_2_gate_accuracy: 90.4701
STEP-2	Epoch: 80/200	classification_loss: 0.2705	gate_loss: 0.2379	step2_classification_accuracy: 88.8889	step_2_gate_accuracy: 93.6752
STEP-2	Epoch: 100/200	classification_loss: 0.2458	gate_loss: 0.1783	step2_classification_accuracy: 89.2735	step_2_gate_accuracy: 95.2991
STEP-2	Epoch: 120/200	classification_loss: 0.2255	gate_loss: 0.1424	step2_classification_accuracy: 90.2137	step_2_gate_accuracy: 95.6838
STEP-2	Epoch: 140/200	classification_loss: 0.2205	gate_loss: 0.1259	step2_classification_accuracy: 90.1709	step_2_gate_accuracy: 96.1966
STEP-2	Epoch: 160/200	classification_loss: 0.2082	gate_loss: 0.1105	step2_classification_accuracy: 90.7692	step_2_gate_accuracy: 96.2393
STEP-2	Epoch: 180/200	classification_loss: 0.2061	gate_loss: 0.1041	step2_classification_accuracy: 90.8547	step_2_gate_accuracy: 96.6239
STEP-2	Epoch: 200/200	classification_loss: 0.2016	gate_loss: 0.0955	step2_classification_accuracy: 90.7692	step_2_gate_accuracy: 96.6667
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.4348	gate_accuracy: 88.4058
	Task-1	val_accuracy: 75.0000	gate_accuracy: 87.5000
	Task-2	val_accuracy: 77.6471	gate_accuracy: 80.0000
	Task-3	val_accuracy: 63.9535	gate_accuracy: 72.0930
	Task-4	val_accuracy: 87.5000	gate_accuracy: 86.2500
	Task-5	val_accuracy: 73.6842	gate_accuracy: 80.2632
	Task-6	val_accuracy: 80.4878	gate_accuracy: 80.4878
	Task-7	val_accuracy: 71.1864	gate_accuracy: 72.8814
	Task-8	val_accuracy: 73.4940	gate_accuracy: 77.1084
	Task-9	val_accuracy: 81.3187	gate_accuracy: 78.0220
	Task-10	val_accuracy: 71.8310	gate_accuracy: 70.4225
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.0650


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.6093	step1_train_accuracy: 46.2500
STEP-1	Epoch: 20/50	loss: 1.0034	step1_train_accuracy: 83.7500
STEP-1	Epoch: 30/50	loss: 0.5376	step1_train_accuracy: 93.4375
STEP-1	Epoch: 40/50	loss: 0.3389	step1_train_accuracy: 95.3125
STEP-1	Epoch: 50/50	loss: 0.2415	step1_train_accuracy: 96.5625
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5226	gate_loss: 1.7230	step2_classification_accuracy: 81.2205	step_2_gate_accuracy: 46.7717
STEP-2	Epoch: 40/200	classification_loss: 0.3665	gate_loss: 0.7412	step2_classification_accuracy: 85.9843	step_2_gate_accuracy: 79.8032
STEP-2	Epoch: 60/200	classification_loss: 0.3062	gate_loss: 0.3795	step2_classification_accuracy: 88.6614	step_2_gate_accuracy: 89.9606
STEP-2	Epoch: 80/200	classification_loss: 0.2687	gate_loss: 0.2536	step2_classification_accuracy: 89.4488	step_2_gate_accuracy: 92.5197
STEP-2	Epoch: 100/200	classification_loss: 0.2322	gate_loss: 0.1821	step2_classification_accuracy: 90.0394	step_2_gate_accuracy: 94.7638
STEP-2	Epoch: 120/200	classification_loss: 0.2223	gate_loss: 0.1504	step2_classification_accuracy: 91.0236	step_2_gate_accuracy: 95.7480
STEP-2	Epoch: 140/200	classification_loss: 0.2064	gate_loss: 0.1287	step2_classification_accuracy: 90.9843	step_2_gate_accuracy: 95.7874
STEP-2	Epoch: 160/200	classification_loss: 0.2028	gate_loss: 0.1157	step2_classification_accuracy: 91.2992	step_2_gate_accuracy: 96.0630
STEP-2	Epoch: 180/200	classification_loss: 0.1968	gate_loss: 0.1032	step2_classification_accuracy: 91.5354	step_2_gate_accuracy: 96.4173
STEP-2	Epoch: 200/200	classification_loss: 0.1896	gate_loss: 0.0933	step2_classification_accuracy: 91.6142	step_2_gate_accuracy: 96.6535
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.5362	gate_accuracy: 84.7826
	Task-1	val_accuracy: 69.4444	gate_accuracy: 83.3333
	Task-2	val_accuracy: 74.1176	gate_accuracy: 78.8235
	Task-3	val_accuracy: 68.6047	gate_accuracy: 68.6047
	Task-4	val_accuracy: 87.5000	gate_accuracy: 87.5000
	Task-5	val_accuracy: 71.0526	gate_accuracy: 82.8947
	Task-6	val_accuracy: 82.9268	gate_accuracy: 80.4878
	Task-7	val_accuracy: 66.1017	gate_accuracy: 76.2712
	Task-8	val_accuracy: 85.5422	gate_accuracy: 90.3614
	Task-9	val_accuracy: 79.1209	gate_accuracy: 74.7253
	Task-10	val_accuracy: 80.2817	gate_accuracy: 84.5070
	Task-11	val_accuracy: 83.7500	gate_accuracy: 80.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.1565


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 2.1018	step1_train_accuracy: 54.5723
STEP-1	Epoch: 20/50	loss: 0.9247	step1_train_accuracy: 85.8407
STEP-1	Epoch: 30/50	loss: 0.4759	step1_train_accuracy: 94.1003
STEP-1	Epoch: 40/50	loss: 0.3128	step1_train_accuracy: 94.6903
STEP-1	Epoch: 50/50	loss: 0.2414	step1_train_accuracy: 94.6903
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5535	gate_loss: 1.7794	step2_classification_accuracy: 80.2190	step_2_gate_accuracy: 45.1460
STEP-2	Epoch: 40/200	classification_loss: 0.4059	gate_loss: 0.7717	step2_classification_accuracy: 85.3650	step_2_gate_accuracy: 77.4453
STEP-2	Epoch: 60/200	classification_loss: 0.3190	gate_loss: 0.4025	step2_classification_accuracy: 87.6277	step_2_gate_accuracy: 88.7591
STEP-2	Epoch: 80/200	classification_loss: 0.2832	gate_loss: 0.2687	step2_classification_accuracy: 89.5255	step_2_gate_accuracy: 93.2117
STEP-2	Epoch: 100/200	classification_loss: 0.2466	gate_loss: 0.1999	step2_classification_accuracy: 90.2555	step_2_gate_accuracy: 94.1971
STEP-2	Epoch: 120/200	classification_loss: 0.2232	gate_loss: 0.1589	step2_classification_accuracy: 90.8759	step_2_gate_accuracy: 95.3650
STEP-2	Epoch: 140/200	classification_loss: 0.2062	gate_loss: 0.1344	step2_classification_accuracy: 91.2774	step_2_gate_accuracy: 95.7664
STEP-2	Epoch: 160/200	classification_loss: 0.2010	gate_loss: 0.1252	step2_classification_accuracy: 91.5693	step_2_gate_accuracy: 95.8394
STEP-2	Epoch: 180/200	classification_loss: 0.1923	gate_loss: 0.1106	step2_classification_accuracy: 91.7153	step_2_gate_accuracy: 96.5693
STEP-2	Epoch: 200/200	classification_loss: 0.1874	gate_loss: 0.1036	step2_classification_accuracy: 91.8248	step_2_gate_accuracy: 96.5328
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.7101	gate_accuracy: 87.6812
	Task-1	val_accuracy: 63.8889	gate_accuracy: 77.7778
	Task-2	val_accuracy: 68.2353	gate_accuracy: 75.2941
	Task-3	val_accuracy: 61.6279	gate_accuracy: 65.1163
	Task-4	val_accuracy: 85.0000	gate_accuracy: 85.0000
	Task-5	val_accuracy: 75.0000	gate_accuracy: 84.2105
	Task-6	val_accuracy: 84.1463	gate_accuracy: 91.4634
	Task-7	val_accuracy: 61.0169	gate_accuracy: 69.4915
	Task-8	val_accuracy: 79.5181	gate_accuracy: 78.3133
	Task-9	val_accuracy: 78.0220	gate_accuracy: 76.9231
	Task-10	val_accuracy: 73.2394	gate_accuracy: 80.2817
	Task-11	val_accuracy: 83.7500	gate_accuracy: 80.0000
	Task-12	val_accuracy: 82.3529	gate_accuracy: 77.6471
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.6875


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 2.5879	step1_train_accuracy: 49.5177
STEP-1	Epoch: 20/50	loss: 1.0525	step1_train_accuracy: 81.3505
STEP-1	Epoch: 30/50	loss: 0.5298	step1_train_accuracy: 91.9614
STEP-1	Epoch: 40/50	loss: 0.3397	step1_train_accuracy: 94.2122
STEP-1	Epoch: 50/50	loss: 0.2490	step1_train_accuracy: 95.4984
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.5625	gate_loss: 1.8276	step2_classification_accuracy: 79.9660	step_2_gate_accuracy: 43.0272
STEP-2	Epoch: 40/200	classification_loss: 0.4341	gate_loss: 0.7928	step2_classification_accuracy: 84.6939	step_2_gate_accuracy: 77.1088
STEP-2	Epoch: 60/200	classification_loss: 0.3532	gate_loss: 0.4200	step2_classification_accuracy: 86.5986	step_2_gate_accuracy: 88.5034
STEP-2	Epoch: 80/200	classification_loss: 0.3077	gate_loss: 0.2804	step2_classification_accuracy: 88.2653	step_2_gate_accuracy: 92.0068
STEP-2	Epoch: 100/200	classification_loss: 0.2724	gate_loss: 0.2145	step2_classification_accuracy: 89.3878	step_2_gate_accuracy: 93.8435
STEP-2	Epoch: 120/200	classification_loss: 0.2468	gate_loss: 0.1722	step2_classification_accuracy: 90.3061	step_2_gate_accuracy: 94.5238
STEP-2	Epoch: 140/200	classification_loss: 0.2403	gate_loss: 0.1557	step2_classification_accuracy: 90.0680	step_2_gate_accuracy: 95.0680
STEP-2	Epoch: 160/200	classification_loss: 0.2140	gate_loss: 0.1282	step2_classification_accuracy: 91.2585	step_2_gate_accuracy: 96.0884
STEP-2	Epoch: 180/200	classification_loss: 0.2114	gate_loss: 0.1216	step2_classification_accuracy: 90.7483	step_2_gate_accuracy: 95.6463
STEP-2	Epoch: 200/200	classification_loss: 0.1946	gate_loss: 0.1060	step2_classification_accuracy: 91.6327	step_2_gate_accuracy: 96.4286
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 78.9855	gate_accuracy: 86.9565
	Task-1	val_accuracy: 58.3333	gate_accuracy: 76.3889
	Task-2	val_accuracy: 69.4118	gate_accuracy: 69.4118
	Task-3	val_accuracy: 68.6047	gate_accuracy: 72.0930
	Task-4	val_accuracy: 77.5000	gate_accuracy: 81.2500
	Task-5	val_accuracy: 72.3684	gate_accuracy: 78.9474
	Task-6	val_accuracy: 82.9268	gate_accuracy: 82.9268
	Task-7	val_accuracy: 67.7966	gate_accuracy: 76.2712
	Task-8	val_accuracy: 78.3133	gate_accuracy: 81.9277
	Task-9	val_accuracy: 78.0220	gate_accuracy: 83.5165
	Task-10	val_accuracy: 71.8310	gate_accuracy: 70.4225
	Task-11	val_accuracy: 82.5000	gate_accuracy: 76.2500
	Task-12	val_accuracy: 82.3529	gate_accuracy: 80.0000
	Task-13	val_accuracy: 71.7949	gate_accuracy: 73.0769
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.3876


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.2481	step1_train_accuracy: 52.6480
STEP-1	Epoch: 20/50	loss: 0.8647	step1_train_accuracy: 81.6199
STEP-1	Epoch: 30/50	loss: 0.4857	step1_train_accuracy: 91.9003
STEP-1	Epoch: 40/50	loss: 0.3485	step1_train_accuracy: 92.5234
STEP-1	Epoch: 50/50	loss: 0.2537	step1_train_accuracy: 96.8847
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6200	gate_loss: 1.9505	step2_classification_accuracy: 78.5032	step_2_gate_accuracy: 41.1783
STEP-2	Epoch: 40/200	classification_loss: 0.4636	gate_loss: 0.8344	step2_classification_accuracy: 83.4713	step_2_gate_accuracy: 77.0701
STEP-2	Epoch: 60/200	classification_loss: 0.3611	gate_loss: 0.4291	step2_classification_accuracy: 86.9427	step_2_gate_accuracy: 87.4841
STEP-2	Epoch: 80/200	classification_loss: 0.3104	gate_loss: 0.2840	step2_classification_accuracy: 88.3121	step_2_gate_accuracy: 91.9427
STEP-2	Epoch: 100/200	classification_loss: 0.2721	gate_loss: 0.2157	step2_classification_accuracy: 89.3312	step_2_gate_accuracy: 93.5987
STEP-2	Epoch: 120/200	classification_loss: 0.2519	gate_loss: 0.1741	step2_classification_accuracy: 90.2866	step_2_gate_accuracy: 94.6497
STEP-2	Epoch: 140/200	classification_loss: 0.2380	gate_loss: 0.1498	step2_classification_accuracy: 90.7325	step_2_gate_accuracy: 95.5096
STEP-2	Epoch: 160/200	classification_loss: 0.2235	gate_loss: 0.1292	step2_classification_accuracy: 91.4650	step_2_gate_accuracy: 95.9873
STEP-2	Epoch: 180/200	classification_loss: 0.2171	gate_loss: 0.1187	step2_classification_accuracy: 91.0828	step_2_gate_accuracy: 95.7643
STEP-2	Epoch: 200/200	classification_loss: 0.2121	gate_loss: 0.1120	step2_classification_accuracy: 91.1146	step_2_gate_accuracy: 96.3376
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.5362	gate_accuracy: 81.8841
	Task-1	val_accuracy: 50.0000	gate_accuracy: 63.8889
	Task-2	val_accuracy: 76.4706	gate_accuracy: 82.3529
	Task-3	val_accuracy: 72.0930	gate_accuracy: 76.7442
	Task-4	val_accuracy: 76.2500	gate_accuracy: 78.7500
	Task-5	val_accuracy: 71.0526	gate_accuracy: 82.8947
	Task-6	val_accuracy: 79.2683	gate_accuracy: 82.9268
	Task-7	val_accuracy: 59.3220	gate_accuracy: 61.0169
	Task-8	val_accuracy: 75.9036	gate_accuracy: 77.1084
	Task-9	val_accuracy: 78.0220	gate_accuracy: 78.0220
	Task-10	val_accuracy: 74.6479	gate_accuracy: 76.0563
	Task-11	val_accuracy: 80.0000	gate_accuracy: 71.2500
	Task-12	val_accuracy: 80.0000	gate_accuracy: 72.9412
	Task-13	val_accuracy: 70.5128	gate_accuracy: 71.7949
	Task-14	val_accuracy: 81.2500	gate_accuracy: 80.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 76.4847


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.7033	step1_train_accuracy: 48.5437
STEP-1	Epoch: 20/50	loss: 0.9159	step1_train_accuracy: 82.8479
STEP-1	Epoch: 30/50	loss: 0.4853	step1_train_accuracy: 94.4984
STEP-1	Epoch: 40/50	loss: 0.2987	step1_train_accuracy: 96.4401
STEP-1	Epoch: 50/50	loss: 0.2009	step1_train_accuracy: 99.0291
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6163	gate_loss: 1.9257	step2_classification_accuracy: 78.6826	step_2_gate_accuracy: 42.2754
STEP-2	Epoch: 40/200	classification_loss: 0.4574	gate_loss: 0.7959	step2_classification_accuracy: 83.4132	step_2_gate_accuracy: 77.6946
STEP-2	Epoch: 60/200	classification_loss: 0.3661	gate_loss: 0.4364	step2_classification_accuracy: 86.8862	step_2_gate_accuracy: 87.5749
STEP-2	Epoch: 80/200	classification_loss: 0.3076	gate_loss: 0.2927	step2_classification_accuracy: 88.6527	step_2_gate_accuracy: 91.9461
STEP-2	Epoch: 100/200	classification_loss: 0.2823	gate_loss: 0.2309	step2_classification_accuracy: 89.5509	step_2_gate_accuracy: 93.2036
STEP-2	Epoch: 120/200	classification_loss: 0.2519	gate_loss: 0.1822	step2_classification_accuracy: 90.2994	step_2_gate_accuracy: 94.2814
STEP-2	Epoch: 140/200	classification_loss: 0.2353	gate_loss: 0.1593	step2_classification_accuracy: 90.6886	step_2_gate_accuracy: 94.9401
STEP-2	Epoch: 160/200	classification_loss: 0.2239	gate_loss: 0.1416	step2_classification_accuracy: 90.8982	step_2_gate_accuracy: 95.4491
STEP-2	Epoch: 180/200	classification_loss: 0.2175	gate_loss: 0.1311	step2_classification_accuracy: 91.2575	step_2_gate_accuracy: 95.5389
STEP-2	Epoch: 200/200	classification_loss: 0.1978	gate_loss: 0.1127	step2_classification_accuracy: 91.8563	step_2_gate_accuracy: 96.5269
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.4638	gate_accuracy: 80.4348
	Task-1	val_accuracy: 61.1111	gate_accuracy: 73.6111
	Task-2	val_accuracy: 71.7647	gate_accuracy: 72.9412
	Task-3	val_accuracy: 74.4186	gate_accuracy: 77.9070
	Task-4	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-5	val_accuracy: 60.5263	gate_accuracy: 67.1053
	Task-6	val_accuracy: 75.6098	gate_accuracy: 70.7317
	Task-7	val_accuracy: 74.5763	gate_accuracy: 71.1864
	Task-8	val_accuracy: 78.3133	gate_accuracy: 78.3133
	Task-9	val_accuracy: 74.7253	gate_accuracy: 71.4286
	Task-10	val_accuracy: 70.4225	gate_accuracy: 74.6479
	Task-11	val_accuracy: 80.0000	gate_accuracy: 77.5000
	Task-12	val_accuracy: 75.2941	gate_accuracy: 68.2353
	Task-13	val_accuracy: 70.5128	gate_accuracy: 74.3590
	Task-14	val_accuracy: 73.7500	gate_accuracy: 72.5000
	Task-15	val_accuracy: 83.1169	gate_accuracy: 77.9221
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.6032


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 1.9670	step1_train_accuracy: 52.2857
STEP-1	Epoch: 20/50	loss: 0.7598	step1_train_accuracy: 88.5714
STEP-1	Epoch: 30/50	loss: 0.3900	step1_train_accuracy: 95.1429
STEP-1	Epoch: 40/50	loss: 0.2501	step1_train_accuracy: 95.4286
STEP-1	Epoch: 50/50	loss: 0.1840	step1_train_accuracy: 96.0000
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6899	gate_loss: 2.0832	step2_classification_accuracy: 75.7345	step_2_gate_accuracy: 36.8927
STEP-2	Epoch: 40/200	classification_loss: 0.5161	gate_loss: 0.8980	step2_classification_accuracy: 82.1186	step_2_gate_accuracy: 74.6328
STEP-2	Epoch: 60/200	classification_loss: 0.4081	gate_loss: 0.4811	step2_classification_accuracy: 85.3955	step_2_gate_accuracy: 86.1864
STEP-2	Epoch: 80/200	classification_loss: 0.3492	gate_loss: 0.3285	step2_classification_accuracy: 87.4011	step_2_gate_accuracy: 90.9040
STEP-2	Epoch: 100/200	classification_loss: 0.3065	gate_loss: 0.2532	step2_classification_accuracy: 88.5593	step_2_gate_accuracy: 92.3729
STEP-2	Epoch: 120/200	classification_loss: 0.2826	gate_loss: 0.2071	step2_classification_accuracy: 89.4068	step_2_gate_accuracy: 93.7006
STEP-2	Epoch: 140/200	classification_loss: 0.2700	gate_loss: 0.1839	step2_classification_accuracy: 89.4633	step_2_gate_accuracy: 94.4633
STEP-2	Epoch: 160/200	classification_loss: 0.2467	gate_loss: 0.1566	step2_classification_accuracy: 90.1412	step_2_gate_accuracy: 95.3672
STEP-2	Epoch: 180/200	classification_loss: 0.2345	gate_loss: 0.1428	step2_classification_accuracy: 90.9040	step_2_gate_accuracy: 95.5650
STEP-2	Epoch: 200/200	classification_loss: 0.2195	gate_loss: 0.1273	step2_classification_accuracy: 91.0452	step_2_gate_accuracy: 96.0734
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.9130	gate_accuracy: 81.1594
	Task-1	val_accuracy: 45.8333	gate_accuracy: 61.1111
	Task-2	val_accuracy: 68.2353	gate_accuracy: 70.5882
	Task-3	val_accuracy: 67.4419	gate_accuracy: 68.6047
	Task-4	val_accuracy: 73.7500	gate_accuracy: 75.0000
	Task-5	val_accuracy: 69.7368	gate_accuracy: 77.6316
	Task-6	val_accuracy: 82.9268	gate_accuracy: 86.5854
	Task-7	val_accuracy: 64.4068	gate_accuracy: 74.5763
	Task-8	val_accuracy: 73.4940	gate_accuracy: 75.9036
	Task-9	val_accuracy: 64.8352	gate_accuracy: 64.8352
	Task-10	val_accuracy: 66.1972	gate_accuracy: 71.8310
	Task-11	val_accuracy: 78.7500	gate_accuracy: 73.7500
	Task-12	val_accuracy: 77.6471	gate_accuracy: 70.5882
	Task-13	val_accuracy: 67.9487	gate_accuracy: 74.3590
	Task-14	val_accuracy: 67.5000	gate_accuracy: 67.5000
	Task-15	val_accuracy: 80.5195	gate_accuracy: 77.9221
	Task-16	val_accuracy: 64.3678	gate_accuracy: 59.7701
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.6950


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 2.2977	step1_train_accuracy: 55.7576
STEP-1	Epoch: 20/50	loss: 0.8618	step1_train_accuracy: 83.6364
STEP-1	Epoch: 30/50	loss: 0.4061	step1_train_accuracy: 93.9394
STEP-1	Epoch: 40/50	loss: 0.2511	step1_train_accuracy: 97.5758
STEP-1	Epoch: 50/50	loss: 0.1742	step1_train_accuracy: 97.8788
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.6806	gate_loss: 2.0819	step2_classification_accuracy: 76.6845	step_2_gate_accuracy: 37.8610
STEP-2	Epoch: 40/200	classification_loss: 0.5016	gate_loss: 0.8688	step2_classification_accuracy: 82.5936	step_2_gate_accuracy: 75.8021
STEP-2	Epoch: 60/200	classification_loss: 0.4104	gate_loss: 0.4726	step2_classification_accuracy: 85.8021	step_2_gate_accuracy: 86.8449
STEP-2	Epoch: 80/200	classification_loss: 0.3432	gate_loss: 0.3256	step2_classification_accuracy: 87.5668	step_2_gate_accuracy: 90.3743
STEP-2	Epoch: 100/200	classification_loss: 0.3059	gate_loss: 0.2537	step2_classification_accuracy: 88.9305	step_2_gate_accuracy: 92.3262
STEP-2	Epoch: 120/200	classification_loss: 0.2818	gate_loss: 0.2096	step2_classification_accuracy: 89.7861	step_2_gate_accuracy: 93.4225
STEP-2	Epoch: 140/200	classification_loss: 0.2640	gate_loss: 0.1838	step2_classification_accuracy: 90.1604	step_2_gate_accuracy: 94.3316
STEP-2	Epoch: 160/200	classification_loss: 0.2421	gate_loss: 0.1592	step2_classification_accuracy: 90.8556	step_2_gate_accuracy: 94.6524
STEP-2	Epoch: 180/200	classification_loss: 0.2240	gate_loss: 0.1446	step2_classification_accuracy: 91.0428	step_2_gate_accuracy: 95.3743
STEP-2	Epoch: 200/200	classification_loss: 0.2149	gate_loss: 0.1316	step2_classification_accuracy: 91.1765	step_2_gate_accuracy: 95.6417
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.1884	gate_accuracy: 81.8841
	Task-1	val_accuracy: 52.7778	gate_accuracy: 66.6667
	Task-2	val_accuracy: 64.7059	gate_accuracy: 71.7647
	Task-3	val_accuracy: 60.4651	gate_accuracy: 66.2791
	Task-4	val_accuracy: 78.7500	gate_accuracy: 80.0000
	Task-5	val_accuracy: 64.4737	gate_accuracy: 71.0526
	Task-6	val_accuracy: 84.1463	gate_accuracy: 84.1463
	Task-7	val_accuracy: 67.7966	gate_accuracy: 71.1864
	Task-8	val_accuracy: 72.2892	gate_accuracy: 77.1084
	Task-9	val_accuracy: 70.3297	gate_accuracy: 70.3297
	Task-10	val_accuracy: 61.9718	gate_accuracy: 67.6056
	Task-11	val_accuracy: 85.0000	gate_accuracy: 82.5000
	Task-12	val_accuracy: 75.2941	gate_accuracy: 64.7059
	Task-13	val_accuracy: 70.5128	gate_accuracy: 76.9231
	Task-14	val_accuracy: 78.7500	gate_accuracy: 72.5000
	Task-15	val_accuracy: 87.0130	gate_accuracy: 83.1169
	Task-16	val_accuracy: 62.0690	gate_accuracy: 66.6667
	Task-17	val_accuracy: 71.0843	gate_accuracy: 63.8554
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.5432


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 2.5760	step1_train_accuracy: 50.9259
STEP-1	Epoch: 20/50	loss: 1.0305	step1_train_accuracy: 81.4815
STEP-1	Epoch: 30/50	loss: 0.5850	step1_train_accuracy: 88.2716
STEP-1	Epoch: 40/50	loss: 0.3982	step1_train_accuracy: 93.8272
STEP-1	Epoch: 50/50	loss: 0.2992	step1_train_accuracy: 95.9877
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7248	gate_loss: 2.1069	step2_classification_accuracy: 75.4569	step_2_gate_accuracy: 35.5330
STEP-2	Epoch: 40/200	classification_loss: 0.5440	gate_loss: 0.9136	step2_classification_accuracy: 81.8274	step_2_gate_accuracy: 73.1980
STEP-2	Epoch: 60/200	classification_loss: 0.4413	gate_loss: 0.5058	step2_classification_accuracy: 84.5178	step_2_gate_accuracy: 85.3299
STEP-2	Epoch: 80/200	classification_loss: 0.3518	gate_loss: 0.3418	step2_classification_accuracy: 87.3604	step_2_gate_accuracy: 89.7208
STEP-2	Epoch: 100/200	classification_loss: 0.3183	gate_loss: 0.2674	step2_classification_accuracy: 88.3503	step_2_gate_accuracy: 91.9797
STEP-2	Epoch: 120/200	classification_loss: 0.2847	gate_loss: 0.2169	step2_classification_accuracy: 89.5939	step_2_gate_accuracy: 93.4772
STEP-2	Epoch: 140/200	classification_loss: 0.2660	gate_loss: 0.1852	step2_classification_accuracy: 90.1523	step_2_gate_accuracy: 93.9594
STEP-2	Epoch: 160/200	classification_loss: 0.2392	gate_loss: 0.1582	step2_classification_accuracy: 90.8883	step_2_gate_accuracy: 95.4061
STEP-2	Epoch: 180/200	classification_loss: 0.2211	gate_loss: 0.1430	step2_classification_accuracy: 91.1929	step_2_gate_accuracy: 95.3299
STEP-2	Epoch: 200/200	classification_loss: 0.2112	gate_loss: 0.1262	step2_classification_accuracy: 91.7766	step_2_gate_accuracy: 96.2944
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.2174	gate_accuracy: 77.5362
	Task-1	val_accuracy: 59.7222	gate_accuracy: 75.0000
	Task-2	val_accuracy: 61.1765	gate_accuracy: 65.8824
	Task-3	val_accuracy: 60.4651	gate_accuracy: 62.7907
	Task-4	val_accuracy: 77.5000	gate_accuracy: 76.2500
	Task-5	val_accuracy: 59.2105	gate_accuracy: 63.1579
	Task-6	val_accuracy: 80.4878	gate_accuracy: 82.9268
	Task-7	val_accuracy: 59.3220	gate_accuracy: 67.7966
	Task-8	val_accuracy: 74.6988	gate_accuracy: 75.9036
	Task-9	val_accuracy: 72.5275	gate_accuracy: 70.3297
	Task-10	val_accuracy: 66.1972	gate_accuracy: 73.2394
	Task-11	val_accuracy: 77.5000	gate_accuracy: 78.7500
	Task-12	val_accuracy: 69.4118	gate_accuracy: 63.5294
	Task-13	val_accuracy: 69.2308	gate_accuracy: 78.2051
	Task-14	val_accuracy: 71.2500	gate_accuracy: 71.2500
	Task-15	val_accuracy: 83.1169	gate_accuracy: 77.9221
	Task-16	val_accuracy: 58.6207	gate_accuracy: 59.7701
	Task-17	val_accuracy: 60.2410	gate_accuracy: 54.2169
	Task-18	val_accuracy: 74.0741	gate_accuracy: 71.6049
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.9657


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.5272	step1_train_accuracy: 59.7143
STEP-1	Epoch: 20/50	loss: 1.0431	step1_train_accuracy: 85.1429
STEP-1	Epoch: 30/50	loss: 0.5489	step1_train_accuracy: 89.7143
STEP-1	Epoch: 40/50	loss: 0.3493	step1_train_accuracy: 95.7143
STEP-1	Epoch: 50/50	loss: 0.2515	step1_train_accuracy: 97.1429
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7256	gate_loss: 2.1608	step2_classification_accuracy: 76.2319	step_2_gate_accuracy: 36.9324
STEP-2	Epoch: 40/200	classification_loss: 0.5662	gate_loss: 0.9055	step2_classification_accuracy: 81.3285	step_2_gate_accuracy: 74.4203
STEP-2	Epoch: 60/200	classification_loss: 0.4363	gate_loss: 0.4951	step2_classification_accuracy: 85.2657	step_2_gate_accuracy: 86.2802
STEP-2	Epoch: 80/200	classification_loss: 0.3652	gate_loss: 0.3412	step2_classification_accuracy: 87.2705	step_2_gate_accuracy: 90.3382
STEP-2	Epoch: 100/200	classification_loss: 0.3044	gate_loss: 0.2575	step2_classification_accuracy: 89.0580	step_2_gate_accuracy: 92.7053
STEP-2	Epoch: 120/200	classification_loss: 0.2847	gate_loss: 0.2112	step2_classification_accuracy: 89.6135	step_2_gate_accuracy: 93.9372
STEP-2	Epoch: 140/200	classification_loss: 0.2703	gate_loss: 0.1825	step2_classification_accuracy: 90.4589	step_2_gate_accuracy: 94.6135
STEP-2	Epoch: 160/200	classification_loss: 0.2488	gate_loss: 0.1616	step2_classification_accuracy: 90.7488	step_2_gate_accuracy: 95.1449
STEP-2	Epoch: 180/200	classification_loss: 0.2326	gate_loss: 0.1427	step2_classification_accuracy: 91.1836	step_2_gate_accuracy: 95.6522
STEP-2	Epoch: 200/200	classification_loss: 0.2136	gate_loss: 0.1271	step2_classification_accuracy: 91.6425	step_2_gate_accuracy: 96.0870
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.6377	gate_accuracy: 89.1304
	Task-1	val_accuracy: 56.9444	gate_accuracy: 69.4444
	Task-2	val_accuracy: 62.3529	gate_accuracy: 63.5294
	Task-3	val_accuracy: 65.1163	gate_accuracy: 66.2791
	Task-4	val_accuracy: 72.5000	gate_accuracy: 71.2500
	Task-5	val_accuracy: 68.4211	gate_accuracy: 75.0000
	Task-6	val_accuracy: 75.6098	gate_accuracy: 73.1707
	Task-7	val_accuracy: 50.8475	gate_accuracy: 59.3220
	Task-8	val_accuracy: 78.3133	gate_accuracy: 79.5181
	Task-9	val_accuracy: 70.3297	gate_accuracy: 70.3297
	Task-10	val_accuracy: 70.4225	gate_accuracy: 74.6479
	Task-11	val_accuracy: 81.2500	gate_accuracy: 80.0000
	Task-12	val_accuracy: 68.2353	gate_accuracy: 64.7059
	Task-13	val_accuracy: 64.1026	gate_accuracy: 70.5128
	Task-14	val_accuracy: 71.2500	gate_accuracy: 68.7500
	Task-15	val_accuracy: 79.2208	gate_accuracy: 76.6234
	Task-16	val_accuracy: 58.6207	gate_accuracy: 60.9195
	Task-17	val_accuracy: 67.4699	gate_accuracy: 63.8554
	Task-18	val_accuracy: 76.5432	gate_accuracy: 76.5432
	Task-19	val_accuracy: 68.9655	gate_accuracy: 64.3678
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.5232


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.8112	step1_train_accuracy: 59.0604
STEP-1	Epoch: 20/50	loss: 0.9991	step1_train_accuracy: 82.8859
STEP-1	Epoch: 30/50	loss: 0.5188	step1_train_accuracy: 92.9530
STEP-1	Epoch: 40/50	loss: 0.3273	step1_train_accuracy: 93.9597
STEP-1	Epoch: 50/50	loss: 0.2393	step1_train_accuracy: 94.2953
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7350	gate_loss: 2.1826	step2_classification_accuracy: 75.7604	step_2_gate_accuracy: 33.9401
STEP-2	Epoch: 40/200	classification_loss: 0.5633	gate_loss: 0.9107	step2_classification_accuracy: 81.4747	step_2_gate_accuracy: 74.1475
STEP-2	Epoch: 60/200	classification_loss: 0.4301	gate_loss: 0.4908	step2_classification_accuracy: 85.4608	step_2_gate_accuracy: 85.9908
STEP-2	Epoch: 80/200	classification_loss: 0.3632	gate_loss: 0.3366	step2_classification_accuracy: 87.6959	step_2_gate_accuracy: 90.3917
STEP-2	Epoch: 100/200	classification_loss: 0.3221	gate_loss: 0.2620	step2_classification_accuracy: 88.7327	step_2_gate_accuracy: 92.3733
STEP-2	Epoch: 120/200	classification_loss: 0.2931	gate_loss: 0.2197	step2_classification_accuracy: 89.5392	step_2_gate_accuracy: 92.7880
STEP-2	Epoch: 140/200	classification_loss: 0.2742	gate_loss: 0.1919	step2_classification_accuracy: 89.9309	step_2_gate_accuracy: 93.6636
STEP-2	Epoch: 160/200	classification_loss: 0.2453	gate_loss: 0.1617	step2_classification_accuracy: 90.8756	step_2_gate_accuracy: 95.1152
STEP-2	Epoch: 180/200	classification_loss: 0.2391	gate_loss: 0.1508	step2_classification_accuracy: 90.8525	step_2_gate_accuracy: 95.0461
STEP-2	Epoch: 200/200	classification_loss: 0.2200	gate_loss: 0.1378	step2_classification_accuracy: 91.3825	step_2_gate_accuracy: 95.5991
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 64.4928	gate_accuracy: 74.6377
	Task-1	val_accuracy: 52.7778	gate_accuracy: 69.4444
	Task-2	val_accuracy: 70.5882	gate_accuracy: 72.9412
	Task-3	val_accuracy: 63.9535	gate_accuracy: 66.2791
	Task-4	val_accuracy: 86.2500	gate_accuracy: 87.5000
	Task-5	val_accuracy: 64.4737	gate_accuracy: 71.0526
	Task-6	val_accuracy: 71.9512	gate_accuracy: 73.1707
	Task-7	val_accuracy: 57.6271	gate_accuracy: 66.1017
	Task-8	val_accuracy: 73.4940	gate_accuracy: 77.1084
	Task-9	val_accuracy: 68.1319	gate_accuracy: 67.0330
	Task-10	val_accuracy: 67.6056	gate_accuracy: 71.8310
	Task-11	val_accuracy: 81.2500	gate_accuracy: 82.5000
	Task-12	val_accuracy: 72.9412	gate_accuracy: 68.2353
	Task-13	val_accuracy: 62.8205	gate_accuracy: 65.3846
	Task-14	val_accuracy: 77.5000	gate_accuracy: 75.0000
	Task-15	val_accuracy: 80.5195	gate_accuracy: 83.1169
	Task-16	val_accuracy: 55.1724	gate_accuracy: 58.6207
	Task-17	val_accuracy: 69.8795	gate_accuracy: 67.4699
	Task-18	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-19	val_accuracy: 67.8161	gate_accuracy: 66.6667
	Task-20	val_accuracy: 64.8649	gate_accuracy: 64.8649
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.4697


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.2744	step1_train_accuracy: 53.3333
STEP-1	Epoch: 20/50	loss: 0.9616	step1_train_accuracy: 80.8333
STEP-1	Epoch: 30/50	loss: 0.5680	step1_train_accuracy: 91.6667
STEP-1	Epoch: 40/50	loss: 0.3803	step1_train_accuracy: 96.1111
STEP-1	Epoch: 50/50	loss: 0.2688	step1_train_accuracy: 97.7778
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7295	gate_loss: 2.3036	step2_classification_accuracy: 76.2335	step_2_gate_accuracy: 32.5991
STEP-2	Epoch: 40/200	classification_loss: 0.5484	gate_loss: 0.9803	step2_classification_accuracy: 81.3656	step_2_gate_accuracy: 72.0705
STEP-2	Epoch: 60/200	classification_loss: 0.4259	gate_loss: 0.5284	step2_classification_accuracy: 85.3965	step_2_gate_accuracy: 84.2511
STEP-2	Epoch: 80/200	classification_loss: 0.3515	gate_loss: 0.3473	step2_classification_accuracy: 88.0396	step_2_gate_accuracy: 90.4185
STEP-2	Epoch: 100/200	classification_loss: 0.3074	gate_loss: 0.2683	step2_classification_accuracy: 88.8767	step_2_gate_accuracy: 92.0264
STEP-2	Epoch: 120/200	classification_loss: 0.2759	gate_loss: 0.2164	step2_classification_accuracy: 90.0000	step_2_gate_accuracy: 93.4582
STEP-2	Epoch: 140/200	classification_loss: 0.2492	gate_loss: 0.1802	step2_classification_accuracy: 90.7048	step_2_gate_accuracy: 94.8018
STEP-2	Epoch: 160/200	classification_loss: 0.2266	gate_loss: 0.1560	step2_classification_accuracy: 91.2115	step_2_gate_accuracy: 95.3304
STEP-2	Epoch: 180/200	classification_loss: 0.2308	gate_loss: 0.1511	step2_classification_accuracy: 91.1894	step_2_gate_accuracy: 95.3524
STEP-2	Epoch: 200/200	classification_loss: 0.2088	gate_loss: 0.1281	step2_classification_accuracy: 91.7841	step_2_gate_accuracy: 96.0132
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.0145	gate_accuracy: 77.5362
	Task-1	val_accuracy: 52.7778	gate_accuracy: 70.8333
	Task-2	val_accuracy: 71.7647	gate_accuracy: 77.6471
	Task-3	val_accuracy: 63.9535	gate_accuracy: 66.2791
	Task-4	val_accuracy: 77.5000	gate_accuracy: 76.2500
	Task-5	val_accuracy: 57.8947	gate_accuracy: 67.1053
	Task-6	val_accuracy: 75.6098	gate_accuracy: 78.0488
	Task-7	val_accuracy: 57.6271	gate_accuracy: 57.6271
	Task-8	val_accuracy: 80.7229	gate_accuracy: 80.7229
	Task-9	val_accuracy: 65.9341	gate_accuracy: 67.0330
	Task-10	val_accuracy: 70.4225	gate_accuracy: 74.6479
	Task-11	val_accuracy: 73.7500	gate_accuracy: 78.7500
	Task-12	val_accuracy: 72.9412	gate_accuracy: 69.4118
	Task-13	val_accuracy: 69.2308	gate_accuracy: 78.2051
	Task-14	val_accuracy: 77.5000	gate_accuracy: 72.5000
	Task-15	val_accuracy: 80.5195	gate_accuracy: 76.6234
	Task-16	val_accuracy: 55.1724	gate_accuracy: 52.8736
	Task-17	val_accuracy: 69.8795	gate_accuracy: 66.2651
	Task-18	val_accuracy: 76.5432	gate_accuracy: 76.5432
	Task-19	val_accuracy: 73.5632	gate_accuracy: 73.5632
	Task-20	val_accuracy: 58.1081	gate_accuracy: 58.1081
	Task-21	val_accuracy: 73.3333	gate_accuracy: 66.6667
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.3425


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.5107	step1_train_accuracy: 51.8182
STEP-1	Epoch: 20/50	loss: 1.0652	step1_train_accuracy: 83.9394
STEP-1	Epoch: 30/50	loss: 0.5437	step1_train_accuracy: 91.5152
STEP-1	Epoch: 40/50	loss: 0.3346	step1_train_accuracy: 96.3636
STEP-1	Epoch: 50/50	loss: 0.2255	step1_train_accuracy: 96.9697
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8000	gate_loss: 2.2374	step2_classification_accuracy: 74.4937	step_2_gate_accuracy: 34.3460
STEP-2	Epoch: 40/200	classification_loss: 0.6108	gate_loss: 0.9169	step2_classification_accuracy: 80.8017	step_2_gate_accuracy: 74.4093
STEP-2	Epoch: 60/200	classification_loss: 0.4730	gate_loss: 0.5055	step2_classification_accuracy: 84.5148	step_2_gate_accuracy: 85.6118
STEP-2	Epoch: 80/200	classification_loss: 0.3988	gate_loss: 0.3534	step2_classification_accuracy: 86.6878	step_2_gate_accuracy: 89.4093
STEP-2	Epoch: 100/200	classification_loss: 0.3461	gate_loss: 0.2744	step2_classification_accuracy: 88.4388	step_2_gate_accuracy: 91.6878
STEP-2	Epoch: 120/200	classification_loss: 0.3054	gate_loss: 0.2202	step2_classification_accuracy: 89.3038	step_2_gate_accuracy: 93.2911
STEP-2	Epoch: 140/200	classification_loss: 0.2828	gate_loss: 0.1889	step2_classification_accuracy: 90.0844	step_2_gate_accuracy: 94.3671
STEP-2	Epoch: 160/200	classification_loss: 0.2594	gate_loss: 0.1664	step2_classification_accuracy: 90.6540	step_2_gate_accuracy: 95.0844
STEP-2	Epoch: 180/200	classification_loss: 0.2412	gate_loss: 0.1436	step2_classification_accuracy: 90.9494	step_2_gate_accuracy: 95.5485
STEP-2	Epoch: 200/200	classification_loss: 0.2275	gate_loss: 0.1308	step2_classification_accuracy: 91.6034	step_2_gate_accuracy: 96.1181
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.3623	gate_accuracy: 85.5072
	Task-1	val_accuracy: 56.9444	gate_accuracy: 68.0556
	Task-2	val_accuracy: 60.0000	gate_accuracy: 63.5294
	Task-3	val_accuracy: 65.1163	gate_accuracy: 68.6047
	Task-4	val_accuracy: 68.7500	gate_accuracy: 70.0000
	Task-5	val_accuracy: 63.1579	gate_accuracy: 71.0526
	Task-6	val_accuracy: 75.6098	gate_accuracy: 74.3902
	Task-7	val_accuracy: 59.3220	gate_accuracy: 66.1017
	Task-8	val_accuracy: 72.2892	gate_accuracy: 74.6988
	Task-9	val_accuracy: 60.4396	gate_accuracy: 59.3407
	Task-10	val_accuracy: 66.1972	gate_accuracy: 67.6056
	Task-11	val_accuracy: 80.0000	gate_accuracy: 78.7500
	Task-12	val_accuracy: 74.1176	gate_accuracy: 69.4118
	Task-13	val_accuracy: 61.5385	gate_accuracy: 65.3846
	Task-14	val_accuracy: 80.0000	gate_accuracy: 81.2500
	Task-15	val_accuracy: 74.0260	gate_accuracy: 70.1299
	Task-16	val_accuracy: 59.7701	gate_accuracy: 59.7701
	Task-17	val_accuracy: 67.4699	gate_accuracy: 66.2651
	Task-18	val_accuracy: 76.5432	gate_accuracy: 74.0741
	Task-19	val_accuracy: 72.4138	gate_accuracy: 70.1149
	Task-20	val_accuracy: 63.5135	gate_accuracy: 62.1622
	Task-21	val_accuracy: 75.5556	gate_accuracy: 75.5556
	Task-22	val_accuracy: 79.2683	gate_accuracy: 76.8293
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.8443


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 1.9901	step1_train_accuracy: 59.9455
STEP-1	Epoch: 20/50	loss: 0.6295	step1_train_accuracy: 90.7357
STEP-1	Epoch: 30/50	loss: 0.3585	step1_train_accuracy: 94.5504
STEP-1	Epoch: 40/50	loss: 0.2494	step1_train_accuracy: 95.9128
STEP-1	Epoch: 50/50	loss: 0.1916	step1_train_accuracy: 97.2752
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7778	gate_loss: 2.3281	step2_classification_accuracy: 75.2429	step_2_gate_accuracy: 31.6599
STEP-2	Epoch: 40/200	classification_loss: 0.5879	gate_loss: 0.9906	step2_classification_accuracy: 81.6599	step_2_gate_accuracy: 71.9028
STEP-2	Epoch: 60/200	classification_loss: 0.4655	gate_loss: 0.5351	step2_classification_accuracy: 84.7773	step_2_gate_accuracy: 84.7571
STEP-2	Epoch: 80/200	classification_loss: 0.3914	gate_loss: 0.3662	step2_classification_accuracy: 87.4089	step_2_gate_accuracy: 89.2713
STEP-2	Epoch: 100/200	classification_loss: 0.3364	gate_loss: 0.2801	step2_classification_accuracy: 88.8259	step_2_gate_accuracy: 91.7409
STEP-2	Epoch: 120/200	classification_loss: 0.3108	gate_loss: 0.2356	step2_classification_accuracy: 89.7773	step_2_gate_accuracy: 92.5101
STEP-2	Epoch: 140/200	classification_loss: 0.2782	gate_loss: 0.1983	step2_classification_accuracy: 90.3239	step_2_gate_accuracy: 93.6842
STEP-2	Epoch: 160/200	classification_loss: 0.2589	gate_loss: 0.1722	step2_classification_accuracy: 91.0121	step_2_gate_accuracy: 94.7571
STEP-2	Epoch: 180/200	classification_loss: 0.2368	gate_loss: 0.1547	step2_classification_accuracy: 91.6599	step_2_gate_accuracy: 95.1417
STEP-2	Epoch: 200/200	classification_loss: 0.2175	gate_loss: 0.1346	step2_classification_accuracy: 92.1255	step_2_gate_accuracy: 95.9717
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.4638	gate_accuracy: 78.9855
	Task-1	val_accuracy: 52.7778	gate_accuracy: 63.8889
	Task-2	val_accuracy: 69.4118	gate_accuracy: 74.1176
	Task-3	val_accuracy: 58.1395	gate_accuracy: 59.3023
	Task-4	val_accuracy: 77.5000	gate_accuracy: 77.5000
	Task-5	val_accuracy: 61.8421	gate_accuracy: 67.1053
	Task-6	val_accuracy: 68.2927	gate_accuracy: 69.5122
	Task-7	val_accuracy: 62.7119	gate_accuracy: 76.2712
	Task-8	val_accuracy: 75.9036	gate_accuracy: 79.5181
	Task-9	val_accuracy: 68.1319	gate_accuracy: 68.1319
	Task-10	val_accuracy: 61.9718	gate_accuracy: 66.1972
	Task-11	val_accuracy: 73.7500	gate_accuracy: 73.7500
	Task-12	val_accuracy: 70.5882	gate_accuracy: 62.3529
	Task-13	val_accuracy: 64.1026	gate_accuracy: 62.8205
	Task-14	val_accuracy: 71.2500	gate_accuracy: 67.5000
	Task-15	val_accuracy: 77.9221	gate_accuracy: 77.9221
	Task-16	val_accuracy: 57.4713	gate_accuracy: 55.1724
	Task-17	val_accuracy: 72.2892	gate_accuracy: 72.2892
	Task-18	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-19	val_accuracy: 65.5172	gate_accuracy: 64.3678
	Task-20	val_accuracy: 62.1622	gate_accuracy: 59.4595
	Task-21	val_accuracy: 75.5556	gate_accuracy: 71.1111
	Task-22	val_accuracy: 71.9512	gate_accuracy: 70.7317
	Task-23	val_accuracy: 71.7391	gate_accuracy: 67.3913
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.1846


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.8000	step1_train_accuracy: 46.1794
STEP-1	Epoch: 20/50	loss: 0.9837	step1_train_accuracy: 82.3920
STEP-1	Epoch: 30/50	loss: 0.4928	step1_train_accuracy: 94.0199
STEP-1	Epoch: 40/50	loss: 0.3110	step1_train_accuracy: 98.0066
STEP-1	Epoch: 50/50	loss: 0.2048	step1_train_accuracy: 98.0066
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8070	gate_loss: 2.3593	step2_classification_accuracy: 74.6887	step_2_gate_accuracy: 29.2996
STEP-2	Epoch: 40/200	classification_loss: 0.6251	gate_loss: 0.9938	step2_classification_accuracy: 80.4475	step_2_gate_accuracy: 71.4786
STEP-2	Epoch: 60/200	classification_loss: 0.4868	gate_loss: 0.5513	step2_classification_accuracy: 84.4163	step_2_gate_accuracy: 83.8716
STEP-2	Epoch: 80/200	classification_loss: 0.4051	gate_loss: 0.3800	step2_classification_accuracy: 86.4786	step_2_gate_accuracy: 88.8132
STEP-2	Epoch: 100/200	classification_loss: 0.3483	gate_loss: 0.2848	step2_classification_accuracy: 88.4047	step_2_gate_accuracy: 91.5953
STEP-2	Epoch: 120/200	classification_loss: 0.3045	gate_loss: 0.2296	step2_classification_accuracy: 89.8249	step_2_gate_accuracy: 93.3074
STEP-2	Epoch: 140/200	classification_loss: 0.2704	gate_loss: 0.1931	step2_classification_accuracy: 90.5253	step_2_gate_accuracy: 94.2412
STEP-2	Epoch: 160/200	classification_loss: 0.2579	gate_loss: 0.1720	step2_classification_accuracy: 90.7198	step_2_gate_accuracy: 94.6109
STEP-2	Epoch: 180/200	classification_loss: 0.2366	gate_loss: 0.1494	step2_classification_accuracy: 91.6732	step_2_gate_accuracy: 95.4475
STEP-2	Epoch: 200/200	classification_loss: 0.2234	gate_loss: 0.1378	step2_classification_accuracy: 91.8093	step_2_gate_accuracy: 95.7393
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 68.1159	gate_accuracy: 78.9855
	Task-1	val_accuracy: 43.0556	gate_accuracy: 56.9444
	Task-2	val_accuracy: 60.0000	gate_accuracy: 69.4118
	Task-3	val_accuracy: 67.4419	gate_accuracy: 68.6047
	Task-4	val_accuracy: 81.2500	gate_accuracy: 81.2500
	Task-5	val_accuracy: 48.6842	gate_accuracy: 59.2105
	Task-6	val_accuracy: 70.7317	gate_accuracy: 67.0732
	Task-7	val_accuracy: 64.4068	gate_accuracy: 72.8814
	Task-8	val_accuracy: 75.9036	gate_accuracy: 75.9036
	Task-9	val_accuracy: 65.9341	gate_accuracy: 67.0330
	Task-10	val_accuracy: 63.3803	gate_accuracy: 63.3803
	Task-11	val_accuracy: 78.7500	gate_accuracy: 78.7500
	Task-12	val_accuracy: 68.2353	gate_accuracy: 64.7059
	Task-13	val_accuracy: 65.3846	gate_accuracy: 70.5128
	Task-14	val_accuracy: 70.0000	gate_accuracy: 70.0000
	Task-15	val_accuracy: 77.9221	gate_accuracy: 75.3247
	Task-16	val_accuracy: 60.9195	gate_accuracy: 57.4713
	Task-17	val_accuracy: 66.2651	gate_accuracy: 62.6506
	Task-18	val_accuracy: 76.5432	gate_accuracy: 76.5432
	Task-19	val_accuracy: 65.5172	gate_accuracy: 62.0690
	Task-20	val_accuracy: 60.8108	gate_accuracy: 67.5676
	Task-21	val_accuracy: 78.8889	gate_accuracy: 68.8889
	Task-22	val_accuracy: 76.8293	gate_accuracy: 71.9512
	Task-23	val_accuracy: 76.0870	gate_accuracy: 68.4783
	Task-24	val_accuracy: 78.6667	gate_accuracy: 76.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.4793


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 2.6587	step1_train_accuracy: 43.6137
STEP-1	Epoch: 20/50	loss: 1.0182	step1_train_accuracy: 82.5545
STEP-1	Epoch: 30/50	loss: 0.4611	step1_train_accuracy: 94.0810
STEP-1	Epoch: 40/50	loss: 0.2861	step1_train_accuracy: 95.3271
STEP-1	Epoch: 50/50	loss: 0.1933	step1_train_accuracy: 99.0654
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.7925	gate_loss: 2.3120	step2_classification_accuracy: 74.5880	step_2_gate_accuracy: 33.8015
STEP-2	Epoch: 40/200	classification_loss: 0.6009	gate_loss: 0.9423	step2_classification_accuracy: 81.2734	step_2_gate_accuracy: 73.2022
STEP-2	Epoch: 60/200	classification_loss: 0.4642	gate_loss: 0.5272	step2_classification_accuracy: 85.2996	step_2_gate_accuracy: 85.1124
STEP-2	Epoch: 80/200	classification_loss: 0.3881	gate_loss: 0.3705	step2_classification_accuracy: 86.9288	step_2_gate_accuracy: 88.9888
STEP-2	Epoch: 100/200	classification_loss: 0.3217	gate_loss: 0.2759	step2_classification_accuracy: 89.4007	step_2_gate_accuracy: 91.7041
STEP-2	Epoch: 120/200	classification_loss: 0.2910	gate_loss: 0.2285	step2_classification_accuracy: 90.2996	step_2_gate_accuracy: 93.2210
STEP-2	Epoch: 140/200	classification_loss: 0.2695	gate_loss: 0.1952	step2_classification_accuracy: 90.6742	step_2_gate_accuracy: 94.0637
STEP-2	Epoch: 160/200	classification_loss: 0.2506	gate_loss: 0.1711	step2_classification_accuracy: 91.3109	step_2_gate_accuracy: 95.0749
STEP-2	Epoch: 180/200	classification_loss: 0.2228	gate_loss: 0.1478	step2_classification_accuracy: 92.0787	step_2_gate_accuracy: 95.5618
STEP-2	Epoch: 200/200	classification_loss: 0.2191	gate_loss: 0.1363	step2_classification_accuracy: 92.0787	step_2_gate_accuracy: 95.9176
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.9420	gate_accuracy: 76.0870
	Task-1	val_accuracy: 43.0556	gate_accuracy: 70.8333
	Task-2	val_accuracy: 67.0588	gate_accuracy: 74.1176
	Task-3	val_accuracy: 52.3256	gate_accuracy: 59.3023
	Task-4	val_accuracy: 76.2500	gate_accuracy: 81.2500
	Task-5	val_accuracy: 59.2105	gate_accuracy: 65.7895
	Task-6	val_accuracy: 63.4146	gate_accuracy: 67.0732
	Task-7	val_accuracy: 57.6271	gate_accuracy: 61.0169
	Task-8	val_accuracy: 72.2892	gate_accuracy: 74.6988
	Task-9	val_accuracy: 61.5385	gate_accuracy: 63.7363
	Task-10	val_accuracy: 67.6056	gate_accuracy: 73.2394
	Task-11	val_accuracy: 81.2500	gate_accuracy: 85.0000
	Task-12	val_accuracy: 71.7647	gate_accuracy: 65.8824
	Task-13	val_accuracy: 66.6667	gate_accuracy: 69.2308
	Task-14	val_accuracy: 70.0000	gate_accuracy: 70.0000
	Task-15	val_accuracy: 79.2208	gate_accuracy: 71.4286
	Task-16	val_accuracy: 60.9195	gate_accuracy: 58.6207
	Task-17	val_accuracy: 67.4699	gate_accuracy: 59.0361
	Task-18	val_accuracy: 72.8395	gate_accuracy: 71.6049
	Task-19	val_accuracy: 67.8161	gate_accuracy: 65.5172
	Task-20	val_accuracy: 59.4595	gate_accuracy: 62.1622
	Task-21	val_accuracy: 77.7778	gate_accuracy: 76.6667
	Task-22	val_accuracy: 68.2927	gate_accuracy: 62.1951
	Task-23	val_accuracy: 75.0000	gate_accuracy: 70.6522
	Task-24	val_accuracy: 77.3333	gate_accuracy: 73.3333
	Task-25	val_accuracy: 67.5000	gate_accuracy: 63.7500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.1272


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 2.4892	step1_train_accuracy: 54.9206
STEP-1	Epoch: 20/50	loss: 0.9519	step1_train_accuracy: 79.0476
STEP-1	Epoch: 30/50	loss: 0.4606	step1_train_accuracy: 93.6508
STEP-1	Epoch: 40/50	loss: 0.2833	step1_train_accuracy: 96.8254
STEP-1	Epoch: 50/50	loss: 0.2045	step1_train_accuracy: 97.4603
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8706	gate_loss: 2.4353	step2_classification_accuracy: 71.4982	step_2_gate_accuracy: 28.2671
STEP-2	Epoch: 40/200	classification_loss: 0.6758	gate_loss: 1.0650	step2_classification_accuracy: 78.6101	step_2_gate_accuracy: 69.1516
STEP-2	Epoch: 60/200	classification_loss: 0.5265	gate_loss: 0.5985	step2_classification_accuracy: 83.2130	step_2_gate_accuracy: 82.3105
STEP-2	Epoch: 80/200	classification_loss: 0.4334	gate_loss: 0.4090	step2_classification_accuracy: 86.2996	step_2_gate_accuracy: 87.7617
STEP-2	Epoch: 100/200	classification_loss: 0.3814	gate_loss: 0.3150	step2_classification_accuracy: 87.3466	step_2_gate_accuracy: 90.4332
STEP-2	Epoch: 120/200	classification_loss: 0.3237	gate_loss: 0.2531	step2_classification_accuracy: 89.1155	step_2_gate_accuracy: 92.5451
STEP-2	Epoch: 140/200	classification_loss: 0.3049	gate_loss: 0.2247	step2_classification_accuracy: 89.5307	step_2_gate_accuracy: 92.9603
STEP-2	Epoch: 160/200	classification_loss: 0.2790	gate_loss: 0.1912	step2_classification_accuracy: 90.3430	step_2_gate_accuracy: 94.1516
STEP-2	Epoch: 180/200	classification_loss: 0.2572	gate_loss: 0.1707	step2_classification_accuracy: 90.8664	step_2_gate_accuracy: 94.6931
STEP-2	Epoch: 200/200	classification_loss: 0.2357	gate_loss: 0.1496	step2_classification_accuracy: 91.4079	step_2_gate_accuracy: 95.4513
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 64.4928	gate_accuracy: 79.7101
	Task-1	val_accuracy: 48.6111	gate_accuracy: 68.0556
	Task-2	val_accuracy: 65.8824	gate_accuracy: 76.4706
	Task-3	val_accuracy: 62.7907	gate_accuracy: 67.4419
	Task-4	val_accuracy: 73.7500	gate_accuracy: 81.2500
	Task-5	val_accuracy: 61.8421	gate_accuracy: 76.3158
	Task-6	val_accuracy: 65.8537	gate_accuracy: 69.5122
	Task-7	val_accuracy: 59.3220	gate_accuracy: 66.1017
	Task-8	val_accuracy: 72.2892	gate_accuracy: 74.6988
	Task-9	val_accuracy: 62.6374	gate_accuracy: 61.5385
	Task-10	val_accuracy: 60.5634	gate_accuracy: 66.1972
	Task-11	val_accuracy: 76.2500	gate_accuracy: 72.5000
	Task-12	val_accuracy: 64.7059	gate_accuracy: 57.6471
	Task-13	val_accuracy: 57.6923	gate_accuracy: 64.1026
	Task-14	val_accuracy: 66.2500	gate_accuracy: 63.7500
	Task-15	val_accuracy: 75.3247	gate_accuracy: 71.4286
	Task-16	val_accuracy: 54.0230	gate_accuracy: 48.2759
	Task-17	val_accuracy: 49.3976	gate_accuracy: 45.7831
	Task-18	val_accuracy: 70.3704	gate_accuracy: 59.2593
	Task-19	val_accuracy: 71.2644	gate_accuracy: 68.9655
	Task-20	val_accuracy: 56.7568	gate_accuracy: 58.1081
	Task-21	val_accuracy: 82.2222	gate_accuracy: 82.2222
	Task-22	val_accuracy: 75.6098	gate_accuracy: 69.5122
	Task-23	val_accuracy: 76.0870	gate_accuracy: 71.7391
	Task-24	val_accuracy: 84.0000	gate_accuracy: 81.3333
	Task-25	val_accuracy: 72.5000	gate_accuracy: 68.7500
	Task-26	val_accuracy: 79.7468	gate_accuracy: 74.6835
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.6073


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.4299	step1_train_accuracy: 55.0143
STEP-1	Epoch: 20/50	loss: 0.8976	step1_train_accuracy: 82.5215
STEP-1	Epoch: 30/50	loss: 0.4303	step1_train_accuracy: 95.4155
STEP-1	Epoch: 40/50	loss: 0.2720	step1_train_accuracy: 97.9943
STEP-1	Epoch: 50/50	loss: 0.1985	step1_train_accuracy: 98.2808
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8803	gate_loss: 2.3947	step2_classification_accuracy: 72.3171	step_2_gate_accuracy: 30.9582
STEP-2	Epoch: 40/200	classification_loss: 0.7001	gate_loss: 1.0234	step2_classification_accuracy: 78.4321	step_2_gate_accuracy: 70.1394
STEP-2	Epoch: 60/200	classification_loss: 0.5373	gate_loss: 0.5909	step2_classification_accuracy: 83.0836	step_2_gate_accuracy: 82.2300
STEP-2	Epoch: 80/200	classification_loss: 0.4329	gate_loss: 0.4123	step2_classification_accuracy: 86.4111	step_2_gate_accuracy: 87.1603
STEP-2	Epoch: 100/200	classification_loss: 0.3750	gate_loss: 0.3234	step2_classification_accuracy: 87.5436	step_2_gate_accuracy: 89.9303
STEP-2	Epoch: 120/200	classification_loss: 0.3281	gate_loss: 0.2620	step2_classification_accuracy: 89.2160	step_2_gate_accuracy: 91.7247
STEP-2	Epoch: 140/200	classification_loss: 0.2895	gate_loss: 0.2210	step2_classification_accuracy: 90.2787	step_2_gate_accuracy: 93.1707
STEP-2	Epoch: 160/200	classification_loss: 0.2687	gate_loss: 0.1971	step2_classification_accuracy: 90.5401	step_2_gate_accuracy: 93.7282
STEP-2	Epoch: 180/200	classification_loss: 0.2474	gate_loss: 0.1734	step2_classification_accuracy: 91.2892	step_2_gate_accuracy: 94.5470
STEP-2	Epoch: 200/200	classification_loss: 0.2343	gate_loss: 0.1610	step2_classification_accuracy: 91.7247	step_2_gate_accuracy: 95.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.5652	gate_accuracy: 80.4348
	Task-1	val_accuracy: 48.6111	gate_accuracy: 63.8889
	Task-2	val_accuracy: 60.0000	gate_accuracy: 69.4118
	Task-3	val_accuracy: 55.8140	gate_accuracy: 61.6279
	Task-4	val_accuracy: 70.0000	gate_accuracy: 75.0000
	Task-5	val_accuracy: 64.4737	gate_accuracy: 72.3684
	Task-6	val_accuracy: 75.6098	gate_accuracy: 69.5122
	Task-7	val_accuracy: 52.5424	gate_accuracy: 64.4068
	Task-8	val_accuracy: 77.1084	gate_accuracy: 78.3133
	Task-9	val_accuracy: 53.8462	gate_accuracy: 50.5495
	Task-10	val_accuracy: 59.1549	gate_accuracy: 61.9718
	Task-11	val_accuracy: 73.7500	gate_accuracy: 65.0000
	Task-12	val_accuracy: 69.4118	gate_accuracy: 68.2353
	Task-13	val_accuracy: 62.8205	gate_accuracy: 67.9487
	Task-14	val_accuracy: 76.2500	gate_accuracy: 73.7500
	Task-15	val_accuracy: 83.1169	gate_accuracy: 80.5195
	Task-16	val_accuracy: 63.2184	gate_accuracy: 63.2184
	Task-17	val_accuracy: 62.6506	gate_accuracy: 59.0361
	Task-18	val_accuracy: 74.0741	gate_accuracy: 71.6049
	Task-19	val_accuracy: 70.1149	gate_accuracy: 68.9655
	Task-20	val_accuracy: 66.2162	gate_accuracy: 68.9189
	Task-21	val_accuracy: 70.0000	gate_accuracy: 67.7778
	Task-22	val_accuracy: 68.2927	gate_accuracy: 69.5122
	Task-23	val_accuracy: 70.6522	gate_accuracy: 65.2174
	Task-24	val_accuracy: 73.3333	gate_accuracy: 62.6667
	Task-25	val_accuracy: 67.5000	gate_accuracy: 60.0000
	Task-26	val_accuracy: 70.8861	gate_accuracy: 69.6203
	Task-27	val_accuracy: 75.8621	gate_accuracy: 72.4138
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.1897


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 2.7170	step1_train_accuracy: 54.1033
STEP-1	Epoch: 20/50	loss: 0.9548	step1_train_accuracy: 81.1550
STEP-1	Epoch: 30/50	loss: 0.5308	step1_train_accuracy: 88.4498
STEP-1	Epoch: 40/50	loss: 0.3753	step1_train_accuracy: 93.0091
STEP-1	Epoch: 50/50	loss: 0.2944	step1_train_accuracy: 93.6170
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8858	gate_loss: 2.4666	step2_classification_accuracy: 72.3906	step_2_gate_accuracy: 29.2929
STEP-2	Epoch: 40/200	classification_loss: 0.7061	gate_loss: 1.0825	step2_classification_accuracy: 78.5522	step_2_gate_accuracy: 69.3266
STEP-2	Epoch: 60/200	classification_loss: 0.5393	gate_loss: 0.6127	step2_classification_accuracy: 83.4343	step_2_gate_accuracy: 82.0539
STEP-2	Epoch: 80/200	classification_loss: 0.4459	gate_loss: 0.4317	step2_classification_accuracy: 86.2121	step_2_gate_accuracy: 87.0034
STEP-2	Epoch: 100/200	classification_loss: 0.3787	gate_loss: 0.3267	step2_classification_accuracy: 87.9966	step_2_gate_accuracy: 90.0000
STEP-2	Epoch: 120/200	classification_loss: 0.3297	gate_loss: 0.2670	step2_classification_accuracy: 89.2424	step_2_gate_accuracy: 92.2896
STEP-2	Epoch: 140/200	classification_loss: 0.2944	gate_loss: 0.2228	step2_classification_accuracy: 90.1178	step_2_gate_accuracy: 93.3502
STEP-2	Epoch: 160/200	classification_loss: 0.2755	gate_loss: 0.1989	step2_classification_accuracy: 90.6229	step_2_gate_accuracy: 94.1414
STEP-2	Epoch: 180/200	classification_loss: 0.2531	gate_loss: 0.1750	step2_classification_accuracy: 91.1448	step_2_gate_accuracy: 94.7138
STEP-2	Epoch: 200/200	classification_loss: 0.2360	gate_loss: 0.1557	step2_classification_accuracy: 91.8519	step_2_gate_accuracy: 95.0842
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.8696	gate_accuracy: 73.1884
	Task-1	val_accuracy: 47.2222	gate_accuracy: 63.8889
	Task-2	val_accuracy: 65.8824	gate_accuracy: 67.0588
	Task-3	val_accuracy: 55.8140	gate_accuracy: 54.6512
	Task-4	val_accuracy: 73.7500	gate_accuracy: 76.2500
	Task-5	val_accuracy: 60.5263	gate_accuracy: 65.7895
	Task-6	val_accuracy: 74.3902	gate_accuracy: 80.4878
	Task-7	val_accuracy: 59.3220	gate_accuracy: 62.7119
	Task-8	val_accuracy: 72.2892	gate_accuracy: 71.0843
	Task-9	val_accuracy: 57.1429	gate_accuracy: 56.0440
	Task-10	val_accuracy: 64.7887	gate_accuracy: 71.8310
	Task-11	val_accuracy: 76.2500	gate_accuracy: 70.0000
	Task-12	val_accuracy: 63.5294	gate_accuracy: 58.8235
	Task-13	val_accuracy: 60.2564	gate_accuracy: 58.9744
	Task-14	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-15	val_accuracy: 77.9221	gate_accuracy: 75.3247
	Task-16	val_accuracy: 55.1724	gate_accuracy: 54.0230
	Task-17	val_accuracy: 59.0361	gate_accuracy: 59.0361
	Task-18	val_accuracy: 74.0741	gate_accuracy: 67.9012
	Task-19	val_accuracy: 72.4138	gate_accuracy: 71.2644
	Task-20	val_accuracy: 62.1622	gate_accuracy: 60.8108
	Task-21	val_accuracy: 80.0000	gate_accuracy: 67.7778
	Task-22	val_accuracy: 74.3902	gate_accuracy: 71.9512
	Task-23	val_accuracy: 70.6522	gate_accuracy: 67.3913
	Task-24	val_accuracy: 73.3333	gate_accuracy: 66.6667
	Task-25	val_accuracy: 63.7500	gate_accuracy: 56.2500
	Task-26	val_accuracy: 74.6835	gate_accuracy: 72.1519
	Task-27	val_accuracy: 68.9655	gate_accuracy: 64.3678
	Task-28	val_accuracy: 54.8780	gate_accuracy: 53.6585
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.0699


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.2565	step1_train_accuracy: 60.2273
STEP-1	Epoch: 20/50	loss: 0.7869	step1_train_accuracy: 83.2386
STEP-1	Epoch: 30/50	loss: 0.3898	step1_train_accuracy: 93.1818
STEP-1	Epoch: 40/50	loss: 0.2645	step1_train_accuracy: 96.8750
STEP-1	Epoch: 50/50	loss: 0.2002	step1_train_accuracy: 98.0114
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.8758	gate_loss: 2.4887	step2_classification_accuracy: 72.3941	step_2_gate_accuracy: 28.8925
STEP-2	Epoch: 40/200	classification_loss: 0.6853	gate_loss: 1.0700	step2_classification_accuracy: 78.5016	step_2_gate_accuracy: 68.7948
STEP-2	Epoch: 60/200	classification_loss: 0.5358	gate_loss: 0.6087	step2_classification_accuracy: 83.1759	step_2_gate_accuracy: 82.0521
STEP-2	Epoch: 80/200	classification_loss: 0.4254	gate_loss: 0.4154	step2_classification_accuracy: 86.5147	step_2_gate_accuracy: 87.7362
STEP-2	Epoch: 100/200	classification_loss: 0.3773	gate_loss: 0.3203	step2_classification_accuracy: 87.8176	step_2_gate_accuracy: 90.3909
STEP-2	Epoch: 120/200	classification_loss: 0.3291	gate_loss: 0.2579	step2_classification_accuracy: 89.4625	step_2_gate_accuracy: 92.0521
STEP-2	Epoch: 140/200	classification_loss: 0.3028	gate_loss: 0.2253	step2_classification_accuracy: 89.8208	step_2_gate_accuracy: 93.1107
STEP-2	Epoch: 160/200	classification_loss: 0.2719	gate_loss: 0.1899	step2_classification_accuracy: 90.5700	step_2_gate_accuracy: 93.9251
STEP-2	Epoch: 180/200	classification_loss: 0.2524	gate_loss: 0.1728	step2_classification_accuracy: 91.3355	step_2_gate_accuracy: 94.6580
STEP-2	Epoch: 200/200	classification_loss: 0.2427	gate_loss: 0.1562	step2_classification_accuracy: 91.3518	step_2_gate_accuracy: 95.0651
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.9420	gate_accuracy: 78.2609
	Task-1	val_accuracy: 43.0556	gate_accuracy: 61.1111
	Task-2	val_accuracy: 65.8824	gate_accuracy: 68.2353
	Task-3	val_accuracy: 58.1395	gate_accuracy: 61.6279
	Task-4	val_accuracy: 67.5000	gate_accuracy: 72.5000
	Task-5	val_accuracy: 63.1579	gate_accuracy: 68.4211
	Task-6	val_accuracy: 70.7317	gate_accuracy: 70.7317
	Task-7	val_accuracy: 47.4576	gate_accuracy: 47.4576
	Task-8	val_accuracy: 69.8795	gate_accuracy: 73.4940
	Task-9	val_accuracy: 68.1319	gate_accuracy: 64.8352
	Task-10	val_accuracy: 66.1972	gate_accuracy: 67.6056
	Task-11	val_accuracy: 73.7500	gate_accuracy: 72.5000
	Task-12	val_accuracy: 60.0000	gate_accuracy: 49.4118
	Task-13	val_accuracy: 64.1026	gate_accuracy: 67.9487
	Task-14	val_accuracy: 67.5000	gate_accuracy: 65.0000
	Task-15	val_accuracy: 67.5325	gate_accuracy: 70.1299
	Task-16	val_accuracy: 58.6207	gate_accuracy: 57.4713
	Task-17	val_accuracy: 63.8554	gate_accuracy: 62.6506
	Task-18	val_accuracy: 67.9012	gate_accuracy: 65.4321
	Task-19	val_accuracy: 62.0690	gate_accuracy: 59.7701
	Task-20	val_accuracy: 54.0541	gate_accuracy: 55.4054
	Task-21	val_accuracy: 82.2222	gate_accuracy: 75.5556
	Task-22	val_accuracy: 73.1707	gate_accuracy: 70.7317
	Task-23	val_accuracy: 67.3913	gate_accuracy: 68.4783
	Task-24	val_accuracy: 68.0000	gate_accuracy: 61.3333
	Task-25	val_accuracy: 58.7500	gate_accuracy: 55.0000
	Task-26	val_accuracy: 79.7468	gate_accuracy: 75.9494
	Task-27	val_accuracy: 64.3678	gate_accuracy: 60.9195
	Task-28	val_accuracy: 63.4146	gate_accuracy: 65.8537
	Task-29	val_accuracy: 65.9091	gate_accuracy: 57.9545
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.5020


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 3.1675	step1_train_accuracy: 42.1603
STEP-1	Epoch: 20/50	loss: 1.0354	step1_train_accuracy: 90.2439
STEP-1	Epoch: 30/50	loss: 0.5271	step1_train_accuracy: 94.7735
STEP-1	Epoch: 40/50	loss: 0.3560	step1_train_accuracy: 96.5157
STEP-1	Epoch: 50/50	loss: 0.2503	step1_train_accuracy: 97.9094
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9028	gate_loss: 2.5637	step2_classification_accuracy: 71.0883	step_2_gate_accuracy: 27.9811
STEP-2	Epoch: 40/200	classification_loss: 0.7080	gate_loss: 1.0811	step2_classification_accuracy: 77.9338	step_2_gate_accuracy: 69.0694
STEP-2	Epoch: 60/200	classification_loss: 0.5556	gate_loss: 0.6031	step2_classification_accuracy: 82.5710	step_2_gate_accuracy: 82.0032
STEP-2	Epoch: 80/200	classification_loss: 0.4530	gate_loss: 0.4233	step2_classification_accuracy: 85.8044	step_2_gate_accuracy: 87.3975
STEP-2	Epoch: 100/200	classification_loss: 0.3885	gate_loss: 0.3255	step2_classification_accuracy: 87.7918	step_2_gate_accuracy: 89.9054
STEP-2	Epoch: 120/200	classification_loss: 0.3374	gate_loss: 0.2682	step2_classification_accuracy: 89.2114	step_2_gate_accuracy: 91.9716
STEP-2	Epoch: 140/200	classification_loss: 0.3048	gate_loss: 0.2272	step2_classification_accuracy: 89.7792	step_2_gate_accuracy: 93.0915
STEP-2	Epoch: 160/200	classification_loss: 0.2926	gate_loss: 0.2027	step2_classification_accuracy: 90.3785	step_2_gate_accuracy: 93.8013
STEP-2	Epoch: 180/200	classification_loss: 0.2807	gate_loss: 0.1879	step2_classification_accuracy: 90.6309	step_2_gate_accuracy: 94.2587
STEP-2	Epoch: 200/200	classification_loss: 0.2547	gate_loss: 0.1597	step2_classification_accuracy: 91.1199	step_2_gate_accuracy: 94.8107
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.2899	gate_accuracy: 81.1594
	Task-1	val_accuracy: 45.8333	gate_accuracy: 59.7222
	Task-2	val_accuracy: 65.8824	gate_accuracy: 77.6471
	Task-3	val_accuracy: 60.4651	gate_accuracy: 66.2791
	Task-4	val_accuracy: 63.7500	gate_accuracy: 62.5000
	Task-5	val_accuracy: 61.8421	gate_accuracy: 72.3684
	Task-6	val_accuracy: 75.6098	gate_accuracy: 76.8293
	Task-7	val_accuracy: 54.2373	gate_accuracy: 55.9322
	Task-8	val_accuracy: 72.2892	gate_accuracy: 73.4940
	Task-9	val_accuracy: 58.2418	gate_accuracy: 54.9451
	Task-10	val_accuracy: 71.8310	gate_accuracy: 67.6056
	Task-11	val_accuracy: 78.7500	gate_accuracy: 76.2500
	Task-12	val_accuracy: 64.7059	gate_accuracy: 67.0588
	Task-13	val_accuracy: 66.6667	gate_accuracy: 74.3590
	Task-14	val_accuracy: 68.7500	gate_accuracy: 63.7500
	Task-15	val_accuracy: 61.0390	gate_accuracy: 57.1429
	Task-16	val_accuracy: 51.7241	gate_accuracy: 47.1264
	Task-17	val_accuracy: 57.8313	gate_accuracy: 56.6265
	Task-18	val_accuracy: 75.3086	gate_accuracy: 76.5432
	Task-19	val_accuracy: 60.9195	gate_accuracy: 62.0690
	Task-20	val_accuracy: 50.0000	gate_accuracy: 44.5946
	Task-21	val_accuracy: 68.8889	gate_accuracy: 63.3333
	Task-22	val_accuracy: 70.7317	gate_accuracy: 70.7317
	Task-23	val_accuracy: 67.3913	gate_accuracy: 63.0435
	Task-24	val_accuracy: 76.0000	gate_accuracy: 73.3333
	Task-25	val_accuracy: 62.5000	gate_accuracy: 61.2500
	Task-26	val_accuracy: 77.2152	gate_accuracy: 73.4177
	Task-27	val_accuracy: 77.0115	gate_accuracy: 72.4138
	Task-28	val_accuracy: 68.2927	gate_accuracy: 67.0732
	Task-29	val_accuracy: 63.6364	gate_accuracy: 57.9545
	Task-30	val_accuracy: 59.7222	gate_accuracy: 55.5556
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.9641


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 1.9244	step1_train_accuracy: 69.6884
STEP-1	Epoch: 20/50	loss: 0.6103	step1_train_accuracy: 94.9008
STEP-1	Epoch: 30/50	loss: 0.3024	step1_train_accuracy: 95.1841
STEP-1	Epoch: 40/50	loss: 0.2028	step1_train_accuracy: 96.6006
STEP-1	Epoch: 50/50	loss: 0.1459	step1_train_accuracy: 98.0170
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9046	gate_loss: 2.5291	step2_classification_accuracy: 72.0489	step_2_gate_accuracy: 29.3119
STEP-2	Epoch: 40/200	classification_loss: 0.6897	gate_loss: 1.0634	step2_classification_accuracy: 79.1743	step_2_gate_accuracy: 69.5719
STEP-2	Epoch: 60/200	classification_loss: 0.5383	gate_loss: 0.6128	step2_classification_accuracy: 83.5321	step_2_gate_accuracy: 81.8349
STEP-2	Epoch: 80/200	classification_loss: 0.4521	gate_loss: 0.4365	step2_classification_accuracy: 86.1621	step_2_gate_accuracy: 86.8502
STEP-2	Epoch: 100/200	classification_loss: 0.3862	gate_loss: 0.3378	step2_classification_accuracy: 87.8593	step_2_gate_accuracy: 89.6177
STEP-2	Epoch: 120/200	classification_loss: 0.3390	gate_loss: 0.2795	step2_classification_accuracy: 89.0826	step_2_gate_accuracy: 91.2232
STEP-2	Epoch: 140/200	classification_loss: 0.3090	gate_loss: 0.2387	step2_classification_accuracy: 89.8930	step_2_gate_accuracy: 92.9205
STEP-2	Epoch: 160/200	classification_loss: 0.2809	gate_loss: 0.2074	step2_classification_accuracy: 90.5657	step_2_gate_accuracy: 93.8073
STEP-2	Epoch: 180/200	classification_loss: 0.2575	gate_loss: 0.1843	step2_classification_accuracy: 91.5749	step_2_gate_accuracy: 94.2813
STEP-2	Epoch: 200/200	classification_loss: 0.2398	gate_loss: 0.1654	step2_classification_accuracy: 91.7278	step_2_gate_accuracy: 94.9847
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.1449	gate_accuracy: 73.1884
	Task-1	val_accuracy: 44.4444	gate_accuracy: 62.5000
	Task-2	val_accuracy: 65.8824	gate_accuracy: 71.7647
	Task-3	val_accuracy: 51.1628	gate_accuracy: 59.3023
	Task-4	val_accuracy: 68.7500	gate_accuracy: 73.7500
	Task-5	val_accuracy: 65.7895	gate_accuracy: 68.4211
	Task-6	val_accuracy: 65.8537	gate_accuracy: 64.6341
	Task-7	val_accuracy: 52.5424	gate_accuracy: 52.5424
	Task-8	val_accuracy: 69.8795	gate_accuracy: 67.4699
	Task-9	val_accuracy: 56.0440	gate_accuracy: 52.7473
	Task-10	val_accuracy: 66.1972	gate_accuracy: 66.1972
	Task-11	val_accuracy: 66.2500	gate_accuracy: 66.2500
	Task-12	val_accuracy: 67.0588	gate_accuracy: 62.3529
	Task-13	val_accuracy: 65.3846	gate_accuracy: 65.3846
	Task-14	val_accuracy: 65.0000	gate_accuracy: 65.0000
	Task-15	val_accuracy: 77.9221	gate_accuracy: 76.6234
	Task-16	val_accuracy: 56.3218	gate_accuracy: 50.5747
	Task-17	val_accuracy: 56.6265	gate_accuracy: 48.1928
	Task-18	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-19	val_accuracy: 66.6667	gate_accuracy: 62.0690
	Task-20	val_accuracy: 55.4054	gate_accuracy: 63.5135
	Task-21	val_accuracy: 73.3333	gate_accuracy: 66.6667
	Task-22	val_accuracy: 79.2683	gate_accuracy: 76.8293
	Task-23	val_accuracy: 72.8261	gate_accuracy: 65.2174
	Task-24	val_accuracy: 80.0000	gate_accuracy: 74.6667
	Task-25	val_accuracy: 67.5000	gate_accuracy: 60.0000
	Task-26	val_accuracy: 79.7468	gate_accuracy: 78.4810
	Task-27	val_accuracy: 70.1149	gate_accuracy: 60.9195
	Task-28	val_accuracy: 59.7561	gate_accuracy: 54.8780
	Task-29	val_accuracy: 61.3636	gate_accuracy: 59.0909
	Task-30	val_accuracy: 66.6667	gate_accuracy: 68.0556
	Task-31	val_accuracy: 75.0000	gate_accuracy: 69.3182
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.0189


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 2.6488	step1_train_accuracy: 52.0376
STEP-1	Epoch: 20/50	loss: 1.0623	step1_train_accuracy: 81.8182
STEP-1	Epoch: 30/50	loss: 0.5123	step1_train_accuracy: 93.1034
STEP-1	Epoch: 40/50	loss: 0.3178	step1_train_accuracy: 95.9248
STEP-1	Epoch: 50/50	loss: 0.2217	step1_train_accuracy: 96.5517
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9421	gate_loss: 2.5606	step2_classification_accuracy: 70.9644	step_2_gate_accuracy: 27.7745
STEP-2	Epoch: 40/200	classification_loss: 0.7297	gate_loss: 1.0863	step2_classification_accuracy: 78.0267	step_2_gate_accuracy: 69.3472
STEP-2	Epoch: 60/200	classification_loss: 0.5686	gate_loss: 0.6226	step2_classification_accuracy: 83.1009	step_2_gate_accuracy: 81.7359
STEP-2	Epoch: 80/200	classification_loss: 0.4651	gate_loss: 0.4387	step2_classification_accuracy: 85.5045	step_2_gate_accuracy: 87.0920
STEP-2	Epoch: 100/200	classification_loss: 0.3987	gate_loss: 0.3452	step2_classification_accuracy: 87.3294	step_2_gate_accuracy: 89.5697
STEP-2	Epoch: 120/200	classification_loss: 0.3497	gate_loss: 0.2848	step2_classification_accuracy: 88.6499	step_2_gate_accuracy: 91.1869
STEP-2	Epoch: 140/200	classification_loss: 0.3164	gate_loss: 0.2422	step2_classification_accuracy: 89.6884	step_2_gate_accuracy: 92.6855
STEP-2	Epoch: 160/200	classification_loss: 0.2956	gate_loss: 0.2158	step2_classification_accuracy: 90.1632	step_2_gate_accuracy: 93.5905
STEP-2	Epoch: 180/200	classification_loss: 0.2784	gate_loss: 0.1964	step2_classification_accuracy: 91.0089	step_2_gate_accuracy: 93.8576
STEP-2	Epoch: 200/200	classification_loss: 0.2490	gate_loss: 0.1684	step2_classification_accuracy: 91.5727	step_2_gate_accuracy: 94.5846
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 68.1159	gate_accuracy: 79.7101
	Task-1	val_accuracy: 43.0556	gate_accuracy: 59.7222
	Task-2	val_accuracy: 67.0588	gate_accuracy: 71.7647
	Task-3	val_accuracy: 61.6279	gate_accuracy: 62.7907
	Task-4	val_accuracy: 71.2500	gate_accuracy: 72.5000
	Task-5	val_accuracy: 57.8947	gate_accuracy: 72.3684
	Task-6	val_accuracy: 65.8537	gate_accuracy: 64.6341
	Task-7	val_accuracy: 59.3220	gate_accuracy: 66.1017
	Task-8	val_accuracy: 68.6747	gate_accuracy: 72.2892
	Task-9	val_accuracy: 61.5385	gate_accuracy: 58.2418
	Task-10	val_accuracy: 56.3380	gate_accuracy: 60.5634
	Task-11	val_accuracy: 73.7500	gate_accuracy: 70.0000
	Task-12	val_accuracy: 65.8824	gate_accuracy: 62.3529
	Task-13	val_accuracy: 65.3846	gate_accuracy: 66.6667
	Task-14	val_accuracy: 65.0000	gate_accuracy: 67.5000
	Task-15	val_accuracy: 76.6234	gate_accuracy: 76.6234
	Task-16	val_accuracy: 62.0690	gate_accuracy: 58.6207
	Task-17	val_accuracy: 66.2651	gate_accuracy: 60.2410
	Task-18	val_accuracy: 67.9012	gate_accuracy: 64.1975
	Task-19	val_accuracy: 66.6667	gate_accuracy: 62.0690
	Task-20	val_accuracy: 55.4054	gate_accuracy: 58.1081
	Task-21	val_accuracy: 73.3333	gate_accuracy: 70.0000
	Task-22	val_accuracy: 69.5122	gate_accuracy: 60.9756
	Task-23	val_accuracy: 68.4783	gate_accuracy: 58.6957
	Task-24	val_accuracy: 76.0000	gate_accuracy: 72.0000
	Task-25	val_accuracy: 62.5000	gate_accuracy: 55.0000
	Task-26	val_accuracy: 64.5570	gate_accuracy: 63.2911
	Task-27	val_accuracy: 68.9655	gate_accuracy: 63.2184
	Task-28	val_accuracy: 68.2927	gate_accuracy: 60.9756
	Task-29	val_accuracy: 70.4545	gate_accuracy: 65.9091
	Task-30	val_accuracy: 63.8889	gate_accuracy: 62.5000
	Task-31	val_accuracy: 69.3182	gate_accuracy: 60.2273
	Task-32	val_accuracy: 56.2500	gate_accuracy: 51.2500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.8352


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.3585	step1_train_accuracy: 59.0258
STEP-1	Epoch: 20/50	loss: 0.7572	step1_train_accuracy: 87.6791
STEP-1	Epoch: 30/50	loss: 0.3461	step1_train_accuracy: 97.7077
STEP-1	Epoch: 40/50	loss: 0.2063	step1_train_accuracy: 98.8539
STEP-1	Epoch: 50/50	loss: 0.1437	step1_train_accuracy: 98.8539
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9406	gate_loss: 2.6179	step2_classification_accuracy: 70.5043	step_2_gate_accuracy: 26.2680
STEP-2	Epoch: 40/200	classification_loss: 0.7395	gate_loss: 1.1348	step2_classification_accuracy: 77.5504	step_2_gate_accuracy: 67.4208
STEP-2	Epoch: 60/200	classification_loss: 0.5664	gate_loss: 0.6546	step2_classification_accuracy: 82.6945	step_2_gate_accuracy: 80.3746
STEP-2	Epoch: 80/200	classification_loss: 0.4743	gate_loss: 0.4575	step2_classification_accuracy: 84.9568	step_2_gate_accuracy: 86.4986
STEP-2	Epoch: 100/200	classification_loss: 0.4113	gate_loss: 0.3645	step2_classification_accuracy: 86.8300	step_2_gate_accuracy: 88.8329
STEP-2	Epoch: 120/200	classification_loss: 0.3568	gate_loss: 0.2895	step2_classification_accuracy: 88.6023	step_2_gate_accuracy: 91.4409
STEP-2	Epoch: 140/200	classification_loss: 0.3288	gate_loss: 0.2511	step2_classification_accuracy: 89.4380	step_2_gate_accuracy: 92.4063
STEP-2	Epoch: 160/200	classification_loss: 0.2953	gate_loss: 0.2201	step2_classification_accuracy: 90.2594	step_2_gate_accuracy: 93.4006
STEP-2	Epoch: 180/200	classification_loss: 0.2751	gate_loss: 0.1969	step2_classification_accuracy: 90.7781	step_2_gate_accuracy: 94.1210
STEP-2	Epoch: 200/200	classification_loss: 0.2798	gate_loss: 0.1935	step2_classification_accuracy: 90.8501	step_2_gate_accuracy: 94.1499
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 62.3188	gate_accuracy: 73.9130
	Task-1	val_accuracy: 34.7222	gate_accuracy: 50.0000
	Task-2	val_accuracy: 55.2941	gate_accuracy: 67.0588
	Task-3	val_accuracy: 58.1395	gate_accuracy: 56.9767
	Task-4	val_accuracy: 73.7500	gate_accuracy: 81.2500
	Task-5	val_accuracy: 67.1053	gate_accuracy: 77.6316
	Task-6	val_accuracy: 58.5366	gate_accuracy: 64.6341
	Task-7	val_accuracy: 57.6271	gate_accuracy: 62.7119
	Task-8	val_accuracy: 72.2892	gate_accuracy: 74.6988
	Task-9	val_accuracy: 47.2527	gate_accuracy: 43.9560
	Task-10	val_accuracy: 66.1972	gate_accuracy: 59.1549
	Task-11	val_accuracy: 80.0000	gate_accuracy: 68.7500
	Task-12	val_accuracy: 58.8235	gate_accuracy: 55.2941
	Task-13	val_accuracy: 58.9744	gate_accuracy: 62.8205
	Task-14	val_accuracy: 66.2500	gate_accuracy: 60.0000
	Task-15	val_accuracy: 67.5325	gate_accuracy: 67.5325
	Task-16	val_accuracy: 50.5747	gate_accuracy: 48.2759
	Task-17	val_accuracy: 60.2410	gate_accuracy: 54.2169
	Task-18	val_accuracy: 72.8395	gate_accuracy: 70.3704
	Task-19	val_accuracy: 68.9655	gate_accuracy: 60.9195
	Task-20	val_accuracy: 55.4054	gate_accuracy: 50.0000
	Task-21	val_accuracy: 71.1111	gate_accuracy: 70.0000
	Task-22	val_accuracy: 78.0488	gate_accuracy: 70.7317
	Task-23	val_accuracy: 68.4783	gate_accuracy: 57.6087
	Task-24	val_accuracy: 80.0000	gate_accuracy: 72.0000
	Task-25	val_accuracy: 61.2500	gate_accuracy: 58.7500
	Task-26	val_accuracy: 68.3544	gate_accuracy: 67.0886
	Task-27	val_accuracy: 64.3678	gate_accuracy: 50.5747
	Task-28	val_accuracy: 63.4146	gate_accuracy: 59.7561
	Task-29	val_accuracy: 65.9091	gate_accuracy: 54.5455
	Task-30	val_accuracy: 62.5000	gate_accuracy: 54.1667
	Task-31	val_accuracy: 80.6818	gate_accuracy: 69.3182
	Task-32	val_accuracy: 56.2500	gate_accuracy: 53.7500
	Task-33	val_accuracy: 64.3678	gate_accuracy: 58.6207
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.1228


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 2.9185	step1_train_accuracy: 35.5634
STEP-1	Epoch: 20/50	loss: 0.9427	step1_train_accuracy: 82.7465
STEP-1	Epoch: 30/50	loss: 0.5279	step1_train_accuracy: 95.0704
STEP-1	Epoch: 40/50	loss: 0.3354	step1_train_accuracy: 97.8873
STEP-1	Epoch: 50/50	loss: 0.2253	step1_train_accuracy: 98.9437
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9718	gate_loss: 2.5838	step2_classification_accuracy: 70.7423	step_2_gate_accuracy: 27.8431
STEP-2	Epoch: 40/200	classification_loss: 0.7833	gate_loss: 1.1205	step2_classification_accuracy: 76.4286	step_2_gate_accuracy: 67.4790
STEP-2	Epoch: 60/200	classification_loss: 0.6159	gate_loss: 0.6671	step2_classification_accuracy: 81.2745	step_2_gate_accuracy: 79.7339
STEP-2	Epoch: 80/200	classification_loss: 0.5088	gate_loss: 0.4796	step2_classification_accuracy: 84.3557	step_2_gate_accuracy: 85.1681
STEP-2	Epoch: 100/200	classification_loss: 0.4329	gate_loss: 0.3758	step2_classification_accuracy: 86.5686	step_2_gate_accuracy: 88.7255
STEP-2	Epoch: 120/200	classification_loss: 0.3874	gate_loss: 0.3141	step2_classification_accuracy: 87.6611	step_2_gate_accuracy: 90.0700
STEP-2	Epoch: 140/200	classification_loss: 0.3488	gate_loss: 0.2656	step2_classification_accuracy: 89.2437	step_2_gate_accuracy: 92.0168
STEP-2	Epoch: 160/200	classification_loss: 0.3195	gate_loss: 0.2334	step2_classification_accuracy: 89.3838	step_2_gate_accuracy: 92.7591
STEP-2	Epoch: 180/200	classification_loss: 0.2999	gate_loss: 0.2094	step2_classification_accuracy: 90.2521	step_2_gate_accuracy: 93.6134
STEP-2	Epoch: 200/200	classification_loss: 0.2837	gate_loss: 0.1911	step2_classification_accuracy: 90.7843	step_2_gate_accuracy: 93.7815
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.7681	gate_accuracy: 75.3623
	Task-1	val_accuracy: 45.8333	gate_accuracy: 66.6667
	Task-2	val_accuracy: 57.6471	gate_accuracy: 64.7059
	Task-3	val_accuracy: 55.8140	gate_accuracy: 55.8140
	Task-4	val_accuracy: 70.0000	gate_accuracy: 67.5000
	Task-5	val_accuracy: 64.4737	gate_accuracy: 69.7368
	Task-6	val_accuracy: 68.2927	gate_accuracy: 70.7317
	Task-7	val_accuracy: 61.0169	gate_accuracy: 71.1864
	Task-8	val_accuracy: 74.6988	gate_accuracy: 79.5181
	Task-9	val_accuracy: 51.6484	gate_accuracy: 49.4505
	Task-10	val_accuracy: 69.0141	gate_accuracy: 71.8310
	Task-11	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-12	val_accuracy: 64.7059	gate_accuracy: 55.2941
	Task-13	val_accuracy: 64.1026	gate_accuracy: 67.9487
	Task-14	val_accuracy: 58.7500	gate_accuracy: 56.2500
	Task-15	val_accuracy: 74.0260	gate_accuracy: 74.0260
	Task-16	val_accuracy: 54.0230	gate_accuracy: 48.2759
	Task-17	val_accuracy: 51.8072	gate_accuracy: 55.4217
	Task-18	val_accuracy: 62.9630	gate_accuracy: 61.7284
	Task-19	val_accuracy: 72.4138	gate_accuracy: 70.1149
	Task-20	val_accuracy: 54.0541	gate_accuracy: 59.4595
	Task-21	val_accuracy: 71.1111	gate_accuracy: 57.7778
	Task-22	val_accuracy: 64.6341	gate_accuracy: 60.9756
	Task-23	val_accuracy: 73.9130	gate_accuracy: 67.3913
	Task-24	val_accuracy: 73.3333	gate_accuracy: 62.6667
	Task-25	val_accuracy: 65.0000	gate_accuracy: 62.5000
	Task-26	val_accuracy: 69.6203	gate_accuracy: 65.8228
	Task-27	val_accuracy: 70.1149	gate_accuracy: 58.6207
	Task-28	val_accuracy: 67.0732	gate_accuracy: 62.1951
	Task-29	val_accuracy: 62.5000	gate_accuracy: 56.8182
	Task-30	val_accuracy: 65.2778	gate_accuracy: 61.1111
	Task-31	val_accuracy: 63.6364	gate_accuracy: 56.8182
	Task-32	val_accuracy: 58.7500	gate_accuracy: 55.0000
	Task-33	val_accuracy: 72.4138	gate_accuracy: 65.5172
	Task-34	val_accuracy: 66.1972	gate_accuracy: 61.9718
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 63.4349


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.5741	step1_train_accuracy: 45.5738
STEP-1	Epoch: 20/50	loss: 0.9342	step1_train_accuracy: 88.1967
STEP-1	Epoch: 30/50	loss: 0.4687	step1_train_accuracy: 93.7705
STEP-1	Epoch: 40/50	loss: 0.3130	step1_train_accuracy: 93.7705
STEP-1	Epoch: 50/50	loss: 0.2453	step1_train_accuracy: 97.3771
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9967	gate_loss: 2.6727	step2_classification_accuracy: 68.9101	step_2_gate_accuracy: 24.4278
STEP-2	Epoch: 40/200	classification_loss: 0.7955	gate_loss: 1.1447	step2_classification_accuracy: 75.9809	step_2_gate_accuracy: 67.4387
STEP-2	Epoch: 60/200	classification_loss: 0.6199	gate_loss: 0.6768	step2_classification_accuracy: 81.3760	step_2_gate_accuracy: 79.7275
STEP-2	Epoch: 80/200	classification_loss: 0.5107	gate_loss: 0.4882	step2_classification_accuracy: 83.9101	step_2_gate_accuracy: 85.2452
STEP-2	Epoch: 100/200	classification_loss: 0.4319	gate_loss: 0.3804	step2_classification_accuracy: 86.2670	step_2_gate_accuracy: 88.7602
STEP-2	Epoch: 120/200	classification_loss: 0.3751	gate_loss: 0.3101	step2_classification_accuracy: 87.8747	step_2_gate_accuracy: 90.8447
STEP-2	Epoch: 140/200	classification_loss: 0.3486	gate_loss: 0.2704	step2_classification_accuracy: 88.5559	step_2_gate_accuracy: 91.4578
STEP-2	Epoch: 160/200	classification_loss: 0.3187	gate_loss: 0.2384	step2_classification_accuracy: 89.3597	step_2_gate_accuracy: 92.4387
STEP-2	Epoch: 180/200	classification_loss: 0.2995	gate_loss: 0.2175	step2_classification_accuracy: 89.7820	step_2_gate_accuracy: 92.9292
STEP-2	Epoch: 200/200	classification_loss: 0.2787	gate_loss: 0.1934	step2_classification_accuracy: 90.5177	step_2_gate_accuracy: 94.1144
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.6957	gate_accuracy: 71.7391
	Task-1	val_accuracy: 34.7222	gate_accuracy: 56.9444
	Task-2	val_accuracy: 64.7059	gate_accuracy: 70.5882
	Task-3	val_accuracy: 66.2791	gate_accuracy: 70.9302
	Task-4	val_accuracy: 70.0000	gate_accuracy: 71.2500
	Task-5	val_accuracy: 57.8947	gate_accuracy: 64.4737
	Task-6	val_accuracy: 65.8537	gate_accuracy: 68.2927
	Task-7	val_accuracy: 55.9322	gate_accuracy: 61.0169
	Task-8	val_accuracy: 78.3133	gate_accuracy: 74.6988
	Task-9	val_accuracy: 49.4505	gate_accuracy: 48.3516
	Task-10	val_accuracy: 66.1972	gate_accuracy: 67.6056
	Task-11	val_accuracy: 75.0000	gate_accuracy: 71.2500
	Task-12	val_accuracy: 68.2353	gate_accuracy: 65.8824
	Task-13	val_accuracy: 62.8205	gate_accuracy: 66.6667
	Task-14	val_accuracy: 65.0000	gate_accuracy: 58.7500
	Task-15	val_accuracy: 72.7273	gate_accuracy: 67.5325
	Task-16	val_accuracy: 49.4253	gate_accuracy: 50.5747
	Task-17	val_accuracy: 61.4458	gate_accuracy: 60.2410
	Task-18	val_accuracy: 61.7284	gate_accuracy: 62.9630
	Task-19	val_accuracy: 68.9655	gate_accuracy: 67.8161
	Task-20	val_accuracy: 54.0541	gate_accuracy: 50.0000
	Task-21	val_accuracy: 72.2222	gate_accuracy: 71.1111
	Task-22	val_accuracy: 69.5122	gate_accuracy: 69.5122
	Task-23	val_accuracy: 72.8261	gate_accuracy: 71.7391
	Task-24	val_accuracy: 78.6667	gate_accuracy: 73.3333
	Task-25	val_accuracy: 60.0000	gate_accuracy: 56.2500
	Task-26	val_accuracy: 67.0886	gate_accuracy: 65.8228
	Task-27	val_accuracy: 72.4138	gate_accuracy: 67.8161
	Task-28	val_accuracy: 59.7561	gate_accuracy: 59.7561
	Task-29	val_accuracy: 64.7727	gate_accuracy: 56.8182
	Task-30	val_accuracy: 63.8889	gate_accuracy: 59.7222
	Task-31	val_accuracy: 69.3182	gate_accuracy: 65.9091
	Task-32	val_accuracy: 60.0000	gate_accuracy: 57.5000
	Task-33	val_accuracy: 72.4138	gate_accuracy: 71.2644
	Task-34	val_accuracy: 69.0141	gate_accuracy: 64.7887
	Task-35	val_accuracy: 63.1579	gate_accuracy: 57.8947
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 64.5749


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 2.2463	step1_train_accuracy: 61.2245
STEP-1	Epoch: 20/50	loss: 0.6860	step1_train_accuracy: 88.6297
STEP-1	Epoch: 30/50	loss: 0.3180	step1_train_accuracy: 97.6676
STEP-1	Epoch: 40/50	loss: 0.2113	step1_train_accuracy: 97.9592
STEP-1	Epoch: 50/50	loss: 0.1510	step1_train_accuracy: 97.9592
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10})
STEP-2	Epoch: 20/200	classification_loss: 0.9755	gate_loss: 2.5936	step2_classification_accuracy: 70.4907	step_2_gate_accuracy: 28.9125
STEP-2	Epoch: 40/200	classification_loss: 0.7726	gate_loss: 1.0973	step2_classification_accuracy: 77.2281	step_2_gate_accuracy: 68.3554
STEP-2	Epoch: 60/200	classification_loss: 0.6201	gate_loss: 0.6646	step2_classification_accuracy: 82.0690	step_2_gate_accuracy: 79.7082
STEP-2	Epoch: 80/200	classification_loss: 0.5313	gate_loss: 0.4911	step2_classification_accuracy: 84.3236	step_2_gate_accuracy: 85.0796
STEP-2	Epoch: 100/200	classification_loss: 0.4559	gate_loss: 0.3908	step2_classification_accuracy: 85.9416	step_2_gate_accuracy: 87.5199
STEP-2	Epoch: 120/200	classification_loss: 0.4023	gate_loss: 0.3240	step2_classification_accuracy: 87.7851	step_2_gate_accuracy: 89.6552
STEP-2	Epoch: 140/200	classification_loss: 0.3687	gate_loss: 0.2824	step2_classification_accuracy: 88.4748	step_2_gate_accuracy: 90.7560
STEP-2	Epoch: 160/200	classification_loss: 0.3360	gate_loss: 0.2492	step2_classification_accuracy: 89.4164	step_2_gate_accuracy: 91.9496
STEP-2	Epoch: 180/200	classification_loss: 0.3167	gate_loss: 0.2279	step2_classification_accuracy: 89.9735	step_2_gate_accuracy: 92.6658
STEP-2	Epoch: 200/200	classification_loss: 0.3013	gate_loss: 0.2080	step2_classification_accuracy: 90.3581	step_2_gate_accuracy: 93.3289
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 63.0435	gate_accuracy: 76.0870
	Task-1	val_accuracy: 50.0000	gate_accuracy: 62.5000
	Task-2	val_accuracy: 60.0000	gate_accuracy: 71.7647
	Task-3	val_accuracy: 55.8140	gate_accuracy: 54.6512
	Task-4	val_accuracy: 70.0000	gate_accuracy: 73.7500
	Task-5	val_accuracy: 55.2632	gate_accuracy: 59.2105
	Task-6	val_accuracy: 69.5122	gate_accuracy: 65.8537
	Task-7	val_accuracy: 54.2373	gate_accuracy: 54.2373
	Task-8	val_accuracy: 73.4940	gate_accuracy: 69.8795
	Task-9	val_accuracy: 59.3407	gate_accuracy: 52.7473
	Task-10	val_accuracy: 60.5634	gate_accuracy: 63.3803
	Task-11	val_accuracy: 75.0000	gate_accuracy: 70.0000
	Task-12	val_accuracy: 70.5882	gate_accuracy: 63.5294
	Task-13	val_accuracy: 65.3846	gate_accuracy: 66.6667
	Task-14	val_accuracy: 68.7500	gate_accuracy: 65.0000
	Task-15	val_accuracy: 72.7273	gate_accuracy: 70.1299
	Task-16	val_accuracy: 52.8736	gate_accuracy: 50.5747
	Task-17	val_accuracy: 49.3976	gate_accuracy: 44.5783
	Task-18	val_accuracy: 66.6667	gate_accuracy: 62.9630
	Task-19	val_accuracy: 66.6667	gate_accuracy: 68.9655
	Task-20	val_accuracy: 60.8108	gate_accuracy: 60.8108
	Task-21	val_accuracy: 74.4444	gate_accuracy: 71.1111
	Task-22	val_accuracy: 68.2927	gate_accuracy: 67.0732
	Task-23	val_accuracy: 66.3043	gate_accuracy: 56.5217
	Task-24	val_accuracy: 76.0000	gate_accuracy: 69.3333
	Task-25	val_accuracy: 63.7500	gate_accuracy: 61.2500
	Task-26	val_accuracy: 73.4177	gate_accuracy: 74.6835
	Task-27	val_accuracy: 67.8161	gate_accuracy: 62.0690
	Task-28	val_accuracy: 52.4390	gate_accuracy: 48.7805
	Task-29	val_accuracy: 56.8182	gate_accuracy: 55.6818
	Task-30	val_accuracy: 58.3333	gate_accuracy: 59.7222
	Task-31	val_accuracy: 67.0455	gate_accuracy: 60.2273
	Task-32	val_accuracy: 57.5000	gate_accuracy: 51.2500
	Task-33	val_accuracy: 74.7126	gate_accuracy: 73.5632
	Task-34	val_accuracy: 67.6056	gate_accuracy: 61.9718
	Task-35	val_accuracy: 56.5789	gate_accuracy: 52.6316
	Task-36	val_accuracy: 72.0930	gate_accuracy: 66.2791
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 62.9508


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 1.9062	step1_train_accuracy: 59.4005
STEP-1	Epoch: 20/50	loss: 0.7312	step1_train_accuracy: 91.0082
STEP-1	Epoch: 30/50	loss: 0.4086	step1_train_accuracy: 95.9128
STEP-1	Epoch: 40/50	loss: 0.2784	step1_train_accuracy: 96.4578
STEP-1	Epoch: 50/50	loss: 0.2089	step1_train_accuracy: 97.0027
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0272	gate_loss: 2.7247	step2_classification_accuracy: 69.6124	step_2_gate_accuracy: 24.8579
STEP-2	Epoch: 40/200	classification_loss: 0.8264	gate_loss: 1.2016	step2_classification_accuracy: 75.2455	step_2_gate_accuracy: 65.7364
STEP-2	Epoch: 60/200	classification_loss: 0.6391	gate_loss: 0.7070	step2_classification_accuracy: 80.4522	step_2_gate_accuracy: 78.6434
STEP-2	Epoch: 80/200	classification_loss: 0.5305	gate_loss: 0.5119	step2_classification_accuracy: 83.8630	step_2_gate_accuracy: 83.8501
STEP-2	Epoch: 100/200	classification_loss: 0.4562	gate_loss: 0.4024	step2_classification_accuracy: 86.3307	step_2_gate_accuracy: 87.7390
STEP-2	Epoch: 120/200	classification_loss: 0.4076	gate_loss: 0.3355	step2_classification_accuracy: 87.0413	step_2_gate_accuracy: 89.6382
STEP-2	Epoch: 140/200	classification_loss: 0.3755	gate_loss: 0.2951	step2_classification_accuracy: 88.1008	step_2_gate_accuracy: 90.6589
STEP-2	Epoch: 160/200	classification_loss: 0.3376	gate_loss: 0.2535	step2_classification_accuracy: 89.3411	step_2_gate_accuracy: 92.0930
STEP-2	Epoch: 180/200	classification_loss: 0.3156	gate_loss: 0.2308	step2_classification_accuracy: 89.7804	step_2_gate_accuracy: 92.7649
STEP-2	Epoch: 200/200	classification_loss: 0.3052	gate_loss: 0.2176	step2_classification_accuracy: 90.0775	step_2_gate_accuracy: 93.1525
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 50.7246	gate_accuracy: 67.3913
	Task-1	val_accuracy: 45.8333	gate_accuracy: 62.5000
	Task-2	val_accuracy: 61.1765	gate_accuracy: 67.0588
	Task-3	val_accuracy: 54.6512	gate_accuracy: 60.4651
	Task-4	val_accuracy: 66.2500	gate_accuracy: 68.7500
	Task-5	val_accuracy: 60.5263	gate_accuracy: 71.0526
	Task-6	val_accuracy: 56.0976	gate_accuracy: 53.6585
	Task-7	val_accuracy: 47.4576	gate_accuracy: 59.3220
	Task-8	val_accuracy: 71.0843	gate_accuracy: 75.9036
	Task-9	val_accuracy: 52.7473	gate_accuracy: 51.6484
	Task-10	val_accuracy: 59.1549	gate_accuracy: 59.1549
	Task-11	val_accuracy: 75.0000	gate_accuracy: 66.2500
	Task-12	val_accuracy: 71.7647	gate_accuracy: 57.6471
	Task-13	val_accuracy: 61.5385	gate_accuracy: 66.6667
	Task-14	val_accuracy: 62.5000	gate_accuracy: 57.5000
	Task-15	val_accuracy: 70.1299	gate_accuracy: 64.9351
	Task-16	val_accuracy: 35.6322	gate_accuracy: 32.1839
	Task-17	val_accuracy: 53.0120	gate_accuracy: 50.6024
	Task-18	val_accuracy: 69.1358	gate_accuracy: 67.9012
	Task-19	val_accuracy: 58.6207	gate_accuracy: 56.3218
	Task-20	val_accuracy: 58.1081	gate_accuracy: 55.4054
	Task-21	val_accuracy: 70.0000	gate_accuracy: 62.2222
	Task-22	val_accuracy: 59.7561	gate_accuracy: 59.7561
	Task-23	val_accuracy: 68.4783	gate_accuracy: 60.8696
	Task-24	val_accuracy: 80.0000	gate_accuracy: 70.6667
	Task-25	val_accuracy: 63.7500	gate_accuracy: 61.2500
	Task-26	val_accuracy: 75.9494	gate_accuracy: 70.8861
	Task-27	val_accuracy: 62.0690	gate_accuracy: 52.8736
	Task-28	val_accuracy: 59.7561	gate_accuracy: 57.3171
	Task-29	val_accuracy: 61.3636	gate_accuracy: 54.5455
	Task-30	val_accuracy: 59.7222	gate_accuracy: 62.5000
	Task-31	val_accuracy: 67.0455	gate_accuracy: 57.9545
	Task-32	val_accuracy: 62.5000	gate_accuracy: 57.5000
	Task-33	val_accuracy: 75.8621	gate_accuracy: 77.0115
	Task-34	val_accuracy: 57.7465	gate_accuracy: 50.7042
	Task-35	val_accuracy: 52.6316	gate_accuracy: 52.6316
	Task-36	val_accuracy: 65.1163	gate_accuracy: 54.6512
	Task-37	val_accuracy: 66.3043	gate_accuracy: 53.2609
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.2482


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 2.3060	step1_train_accuracy: 53.0973
STEP-1	Epoch: 20/50	loss: 0.9445	step1_train_accuracy: 80.8260
STEP-1	Epoch: 30/50	loss: 0.5266	step1_train_accuracy: 86.1357
STEP-1	Epoch: 40/50	loss: 0.3773	step1_train_accuracy: 90.2655
STEP-1	Epoch: 50/50	loss: 0.3019	step1_train_accuracy: 91.4454
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0673	gate_loss: 2.7458	step2_classification_accuracy: 68.5013	step_2_gate_accuracy: 24.3073
STEP-2	Epoch: 40/200	classification_loss: 0.8644	gate_loss: 1.2267	step2_classification_accuracy: 74.8615	step_2_gate_accuracy: 64.6725
STEP-2	Epoch: 60/200	classification_loss: 0.6788	gate_loss: 0.7282	step2_classification_accuracy: 80.7935	step_2_gate_accuracy: 78.4635
STEP-2	Epoch: 80/200	classification_loss: 0.5651	gate_loss: 0.5297	step2_classification_accuracy: 83.3879	step_2_gate_accuracy: 83.7028
STEP-2	Epoch: 100/200	classification_loss: 0.4913	gate_loss: 0.4206	step2_classification_accuracy: 85.2771	step_2_gate_accuracy: 87.0403
STEP-2	Epoch: 120/200	classification_loss: 0.4440	gate_loss: 0.3593	step2_classification_accuracy: 86.6877	step_2_gate_accuracy: 88.7909
STEP-2	Epoch: 140/200	classification_loss: 0.4173	gate_loss: 0.3239	step2_classification_accuracy: 87.3300	step_2_gate_accuracy: 90.0504
STEP-2	Epoch: 160/200	classification_loss: 0.3589	gate_loss: 0.2642	step2_classification_accuracy: 88.9421	step_2_gate_accuracy: 91.6625
STEP-2	Epoch: 180/200	classification_loss: 0.3560	gate_loss: 0.2569	step2_classification_accuracy: 88.9043	step_2_gate_accuracy: 91.9521
STEP-2	Epoch: 200/200	classification_loss: 0.3230	gate_loss: 0.2226	step2_classification_accuracy: 89.7859	step_2_gate_accuracy: 92.9093
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 53.6232	gate_accuracy: 63.0435
	Task-1	val_accuracy: 33.3333	gate_accuracy: 47.2222
	Task-2	val_accuracy: 63.5294	gate_accuracy: 69.4118
	Task-3	val_accuracy: 54.6512	gate_accuracy: 59.3023
	Task-4	val_accuracy: 68.7500	gate_accuracy: 71.2500
	Task-5	val_accuracy: 55.2632	gate_accuracy: 63.1579
	Task-6	val_accuracy: 68.2927	gate_accuracy: 73.1707
	Task-7	val_accuracy: 55.9322	gate_accuracy: 66.1017
	Task-8	val_accuracy: 72.2892	gate_accuracy: 66.2651
	Task-9	val_accuracy: 48.3516	gate_accuracy: 43.9560
	Task-10	val_accuracy: 66.1972	gate_accuracy: 67.6056
	Task-11	val_accuracy: 76.2500	gate_accuracy: 68.7500
	Task-12	val_accuracy: 65.8824	gate_accuracy: 67.0588
	Task-13	val_accuracy: 60.2564	gate_accuracy: 61.5385
	Task-14	val_accuracy: 72.5000	gate_accuracy: 68.7500
	Task-15	val_accuracy: 70.1299	gate_accuracy: 68.8312
	Task-16	val_accuracy: 48.2759	gate_accuracy: 44.8276
	Task-17	val_accuracy: 57.8313	gate_accuracy: 59.0361
	Task-18	val_accuracy: 66.6667	gate_accuracy: 64.1975
	Task-19	val_accuracy: 64.3678	gate_accuracy: 65.5172
	Task-20	val_accuracy: 54.0541	gate_accuracy: 55.4054
	Task-21	val_accuracy: 74.4444	gate_accuracy: 74.4444
	Task-22	val_accuracy: 75.6098	gate_accuracy: 71.9512
	Task-23	val_accuracy: 61.9565	gate_accuracy: 54.3478
	Task-24	val_accuracy: 81.3333	gate_accuracy: 78.6667
	Task-25	val_accuracy: 62.5000	gate_accuracy: 51.2500
	Task-26	val_accuracy: 65.8228	gate_accuracy: 60.7595
	Task-27	val_accuracy: 65.5172	gate_accuracy: 59.7701
	Task-28	val_accuracy: 54.8780	gate_accuracy: 52.4390
	Task-29	val_accuracy: 53.4091	gate_accuracy: 51.1364
	Task-30	val_accuracy: 62.5000	gate_accuracy: 58.3333
	Task-31	val_accuracy: 62.5000	gate_accuracy: 59.0909
	Task-32	val_accuracy: 61.2500	gate_accuracy: 60.0000
	Task-33	val_accuracy: 64.3678	gate_accuracy: 55.1724
	Task-34	val_accuracy: 66.1972	gate_accuracy: 63.3803
	Task-35	val_accuracy: 55.2632	gate_accuracy: 51.3158
	Task-36	val_accuracy: 74.4186	gate_accuracy: 61.6279
	Task-37	val_accuracy: 71.7391	gate_accuracy: 63.0435
	Task-38	val_accuracy: 54.1176	gate_accuracy: 51.7647
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 61.2643


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 2.3785	step1_train_accuracy: 59.4675
STEP-1	Epoch: 20/50	loss: 0.9344	step1_train_accuracy: 84.6154
STEP-1	Epoch: 30/50	loss: 0.4907	step1_train_accuracy: 89.9408
STEP-1	Epoch: 40/50	loss: 0.3320	step1_train_accuracy: 93.4911
STEP-1	Epoch: 50/50	loss: 0.2497	step1_train_accuracy: 95.5621
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0707	gate_loss: 2.7605	step2_classification_accuracy: 68.1572	step_2_gate_accuracy: 25.0246
STEP-2	Epoch: 40/200	classification_loss: 0.8734	gate_loss: 1.2452	step2_classification_accuracy: 74.3366	step_2_gate_accuracy: 64.0786
STEP-2	Epoch: 60/200	classification_loss: 0.6968	gate_loss: 0.7570	step2_classification_accuracy: 79.0295	step_2_gate_accuracy: 77.3587
STEP-2	Epoch: 80/200	classification_loss: 0.5769	gate_loss: 0.5564	step2_classification_accuracy: 82.4324	step_2_gate_accuracy: 82.9607
STEP-2	Epoch: 100/200	classification_loss: 0.5046	gate_loss: 0.4455	step2_classification_accuracy: 84.4840	step_2_gate_accuracy: 86.1179
STEP-2	Epoch: 120/200	classification_loss: 0.4582	gate_loss: 0.3777	step2_classification_accuracy: 85.6388	step_2_gate_accuracy: 88.1327
STEP-2	Epoch: 140/200	classification_loss: 0.4130	gate_loss: 0.3227	step2_classification_accuracy: 86.9165	step_2_gate_accuracy: 89.8157
STEP-2	Epoch: 160/200	classification_loss: 0.3873	gate_loss: 0.2914	step2_classification_accuracy: 87.6167	step_2_gate_accuracy: 90.6143
STEP-2	Epoch: 180/200	classification_loss: 0.3478	gate_loss: 0.2516	step2_classification_accuracy: 88.5012	step_2_gate_accuracy: 91.9410
STEP-2	Epoch: 200/200	classification_loss: 0.3228	gate_loss: 0.2302	step2_classification_accuracy: 89.3243	step_2_gate_accuracy: 92.7764
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.0145	gate_accuracy: 77.5362
	Task-1	val_accuracy: 40.2778	gate_accuracy: 54.1667
	Task-2	val_accuracy: 50.5882	gate_accuracy: 61.1765
	Task-3	val_accuracy: 50.0000	gate_accuracy: 47.6744
	Task-4	val_accuracy: 70.0000	gate_accuracy: 68.7500
	Task-5	val_accuracy: 44.7368	gate_accuracy: 59.2105
	Task-6	val_accuracy: 65.8537	gate_accuracy: 65.8537
	Task-7	val_accuracy: 50.8475	gate_accuracy: 59.3220
	Task-8	val_accuracy: 66.2651	gate_accuracy: 66.2651
	Task-9	val_accuracy: 49.4505	gate_accuracy: 50.5495
	Task-10	val_accuracy: 59.1549	gate_accuracy: 67.6056
	Task-11	val_accuracy: 70.0000	gate_accuracy: 61.2500
	Task-12	val_accuracy: 65.8824	gate_accuracy: 55.2941
	Task-13	val_accuracy: 65.3846	gate_accuracy: 64.1026
	Task-14	val_accuracy: 71.2500	gate_accuracy: 65.0000
	Task-15	val_accuracy: 72.7273	gate_accuracy: 72.7273
	Task-16	val_accuracy: 49.4253	gate_accuracy: 49.4253
	Task-17	val_accuracy: 51.8072	gate_accuracy: 50.6024
	Task-18	val_accuracy: 58.0247	gate_accuracy: 58.0247
	Task-19	val_accuracy: 57.4713	gate_accuracy: 52.8736
	Task-20	val_accuracy: 56.7568	gate_accuracy: 54.0541
	Task-21	val_accuracy: 74.4444	gate_accuracy: 68.8889
	Task-22	val_accuracy: 63.4146	gate_accuracy: 58.5366
	Task-23	val_accuracy: 68.4783	gate_accuracy: 65.2174
	Task-24	val_accuracy: 73.3333	gate_accuracy: 69.3333
	Task-25	val_accuracy: 60.0000	gate_accuracy: 58.7500
	Task-26	val_accuracy: 64.5570	gate_accuracy: 63.2911
	Task-27	val_accuracy: 78.1609	gate_accuracy: 68.9655
	Task-28	val_accuracy: 56.0976	gate_accuracy: 56.0976
	Task-29	val_accuracy: 65.9091	gate_accuracy: 54.5455
	Task-30	val_accuracy: 56.9444	gate_accuracy: 55.5556
	Task-31	val_accuracy: 69.3182	gate_accuracy: 63.6364
	Task-32	val_accuracy: 66.2500	gate_accuracy: 58.7500
	Task-33	val_accuracy: 71.2644	gate_accuracy: 67.8161
	Task-34	val_accuracy: 67.6056	gate_accuracy: 61.9718
	Task-35	val_accuracy: 53.9474	gate_accuracy: 47.3684
	Task-36	val_accuracy: 69.7674	gate_accuracy: 63.9535
	Task-37	val_accuracy: 64.1304	gate_accuracy: 59.7826
	Task-38	val_accuracy: 56.4706	gate_accuracy: 56.4706
	Task-39	val_accuracy: 64.2857	gate_accuracy: 55.9524
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.6765


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 2.7934	step1_train_accuracy: 49.7024
STEP-1	Epoch: 20/50	loss: 0.9773	step1_train_accuracy: 77.3810
STEP-1	Epoch: 30/50	loss: 0.5053	step1_train_accuracy: 90.1786
STEP-1	Epoch: 40/50	loss: 0.3495	step1_train_accuracy: 92.5595
STEP-1	Epoch: 50/50	loss: 0.2671	step1_train_accuracy: 94.0476
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1104	gate_loss: 2.7675	step2_classification_accuracy: 67.1703	step_2_gate_accuracy: 23.7770
STEP-2	Epoch: 40/200	classification_loss: 0.8987	gate_loss: 1.2382	step2_classification_accuracy: 73.5132	step_2_gate_accuracy: 63.9209
STEP-2	Epoch: 60/200	classification_loss: 0.7210	gate_loss: 0.7693	step2_classification_accuracy: 78.8249	step_2_gate_accuracy: 76.4389
STEP-2	Epoch: 80/200	classification_loss: 0.5977	gate_loss: 0.5687	step2_classification_accuracy: 82.3022	step_2_gate_accuracy: 82.0504
STEP-2	Epoch: 100/200	classification_loss: 0.5369	gate_loss: 0.4605	step2_classification_accuracy: 84.0288	step_2_gate_accuracy: 85.3477
STEP-2	Epoch: 120/200	classification_loss: 0.4763	gate_loss: 0.3874	step2_classification_accuracy: 85.4676	step_2_gate_accuracy: 87.8058
STEP-2	Epoch: 140/200	classification_loss: 0.4346	gate_loss: 0.3415	step2_classification_accuracy: 86.5707	step_2_gate_accuracy: 89.2086
STEP-2	Epoch: 160/200	classification_loss: 0.4072	gate_loss: 0.3068	step2_classification_accuracy: 87.2182	step_2_gate_accuracy: 90.2998
STEP-2	Epoch: 180/200	classification_loss: 0.3811	gate_loss: 0.2767	step2_classification_accuracy: 87.8897	step_2_gate_accuracy: 91.3549
STEP-2	Epoch: 200/200	classification_loss: 0.3503	gate_loss: 0.2455	step2_classification_accuracy: 88.9808	step_2_gate_accuracy: 92.1463
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.6957	gate_accuracy: 66.6667
	Task-1	val_accuracy: 45.8333	gate_accuracy: 62.5000
	Task-2	val_accuracy: 55.2941	gate_accuracy: 64.7059
	Task-3	val_accuracy: 45.3488	gate_accuracy: 51.1628
	Task-4	val_accuracy: 66.2500	gate_accuracy: 67.5000
	Task-5	val_accuracy: 56.5789	gate_accuracy: 68.4211
	Task-6	val_accuracy: 68.2927	gate_accuracy: 74.3902
	Task-7	val_accuracy: 55.9322	gate_accuracy: 57.6271
	Task-8	val_accuracy: 68.6747	gate_accuracy: 63.8554
	Task-9	val_accuracy: 59.3407	gate_accuracy: 58.2418
	Task-10	val_accuracy: 59.1549	gate_accuracy: 61.9718
	Task-11	val_accuracy: 77.5000	gate_accuracy: 72.5000
	Task-12	val_accuracy: 63.5294	gate_accuracy: 61.1765
	Task-13	val_accuracy: 60.2564	gate_accuracy: 67.9487
	Task-14	val_accuracy: 56.2500	gate_accuracy: 56.2500
	Task-15	val_accuracy: 70.1299	gate_accuracy: 66.2338
	Task-16	val_accuracy: 56.3218	gate_accuracy: 50.5747
	Task-17	val_accuracy: 63.8554	gate_accuracy: 60.2410
	Task-18	val_accuracy: 66.6667	gate_accuracy: 61.7284
	Task-19	val_accuracy: 59.7701	gate_accuracy: 60.9195
	Task-20	val_accuracy: 48.6486	gate_accuracy: 52.7027
	Task-21	val_accuracy: 61.1111	gate_accuracy: 58.8889
	Task-22	val_accuracy: 63.4146	gate_accuracy: 59.7561
	Task-23	val_accuracy: 66.3043	gate_accuracy: 63.0435
	Task-24	val_accuracy: 66.6667	gate_accuracy: 58.6667
	Task-25	val_accuracy: 65.0000	gate_accuracy: 53.7500
	Task-26	val_accuracy: 63.2911	gate_accuracy: 64.5570
	Task-27	val_accuracy: 67.8161	gate_accuracy: 64.3678
	Task-28	val_accuracy: 50.0000	gate_accuracy: 46.3415
	Task-29	val_accuracy: 64.7727	gate_accuracy: 55.6818
	Task-30	val_accuracy: 56.9444	gate_accuracy: 58.3333
	Task-31	val_accuracy: 71.5909	gate_accuracy: 67.0455
	Task-32	val_accuracy: 58.7500	gate_accuracy: 51.2500
	Task-33	val_accuracy: 63.2184	gate_accuracy: 59.7701
	Task-34	val_accuracy: 63.3803	gate_accuracy: 60.5634
	Task-35	val_accuracy: 60.5263	gate_accuracy: 55.2632
	Task-36	val_accuracy: 73.2558	gate_accuracy: 65.1163
	Task-37	val_accuracy: 61.9565	gate_accuracy: 57.6087
	Task-38	val_accuracy: 50.5882	gate_accuracy: 50.5882
	Task-39	val_accuracy: 55.9524	gate_accuracy: 53.5714
	Task-40	val_accuracy: 64.2857	gate_accuracy: 60.7143
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.3829


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 2.2554	step1_train_accuracy: 64.4022
STEP-1	Epoch: 20/50	loss: 0.7427	step1_train_accuracy: 85.5978
STEP-1	Epoch: 30/50	loss: 0.3067	step1_train_accuracy: 95.9239
STEP-1	Epoch: 40/50	loss: 0.1867	step1_train_accuracy: 98.3696
STEP-1	Epoch: 50/50	loss: 0.1261	step1_train_accuracy: 99.1848
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.0705	gate_loss: 2.7031	step2_classification_accuracy: 68.0562	step_2_gate_accuracy: 25.9485
STEP-2	Epoch: 40/200	classification_loss: 0.8684	gate_loss: 1.1946	step2_classification_accuracy: 74.1218	step_2_gate_accuracy: 66.3700
STEP-2	Epoch: 60/200	classification_loss: 0.6934	gate_loss: 0.7423	step2_classification_accuracy: 79.4614	step_2_gate_accuracy: 78.0796
STEP-2	Epoch: 80/200	classification_loss: 0.5691	gate_loss: 0.5491	step2_classification_accuracy: 82.4707	step_2_gate_accuracy: 82.7986
STEP-2	Epoch: 100/200	classification_loss: 0.4945	gate_loss: 0.4356	step2_classification_accuracy: 84.8946	step_2_gate_accuracy: 86.4520
STEP-2	Epoch: 120/200	classification_loss: 0.4458	gate_loss: 0.3709	step2_classification_accuracy: 86.2061	step_2_gate_accuracy: 88.1499
STEP-2	Epoch: 140/200	classification_loss: 0.4114	gate_loss: 0.3241	step2_classification_accuracy: 87.5059	step_2_gate_accuracy: 89.9649
STEP-2	Epoch: 160/200	classification_loss: 0.3669	gate_loss: 0.2824	step2_classification_accuracy: 88.1616	step_2_gate_accuracy: 91.1944
STEP-2	Epoch: 180/200	classification_loss: 0.3356	gate_loss: 0.2463	step2_classification_accuracy: 89.1335	step_2_gate_accuracy: 92.3653
STEP-2	Epoch: 200/200	classification_loss: 0.3251	gate_loss: 0.2304	step2_classification_accuracy: 89.8126	step_2_gate_accuracy: 92.8689
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 58.6957	gate_accuracy: 62.3188
	Task-1	val_accuracy: 41.6667	gate_accuracy: 54.1667
	Task-2	val_accuracy: 64.7059	gate_accuracy: 67.0588
	Task-3	val_accuracy: 53.4884	gate_accuracy: 52.3256
	Task-4	val_accuracy: 75.0000	gate_accuracy: 75.0000
	Task-5	val_accuracy: 57.8947	gate_accuracy: 63.1579
	Task-6	val_accuracy: 62.1951	gate_accuracy: 60.9756
	Task-7	val_accuracy: 50.8475	gate_accuracy: 61.0169
	Task-8	val_accuracy: 74.6988	gate_accuracy: 73.4940
	Task-9	val_accuracy: 49.4505	gate_accuracy: 42.8571
	Task-10	val_accuracy: 60.5634	gate_accuracy: 57.7465
	Task-11	val_accuracy: 78.7500	gate_accuracy: 78.7500
	Task-12	val_accuracy: 60.0000	gate_accuracy: 50.5882
	Task-13	val_accuracy: 60.2564	gate_accuracy: 64.1026
	Task-14	val_accuracy: 65.0000	gate_accuracy: 58.7500
	Task-15	val_accuracy: 68.8312	gate_accuracy: 70.1299
	Task-16	val_accuracy: 41.3793	gate_accuracy: 34.4828
	Task-17	val_accuracy: 49.3976	gate_accuracy: 50.6024
	Task-18	val_accuracy: 58.0247	gate_accuracy: 54.3210
	Task-19	val_accuracy: 63.2184	gate_accuracy: 58.6207
	Task-20	val_accuracy: 58.1081	gate_accuracy: 59.4595
	Task-21	val_accuracy: 66.6667	gate_accuracy: 65.5556
	Task-22	val_accuracy: 74.3902	gate_accuracy: 68.2927
	Task-23	val_accuracy: 59.7826	gate_accuracy: 55.4348
	Task-24	val_accuracy: 72.0000	gate_accuracy: 70.6667
	Task-25	val_accuracy: 65.0000	gate_accuracy: 52.5000
	Task-26	val_accuracy: 70.8861	gate_accuracy: 68.3544
	Task-27	val_accuracy: 66.6667	gate_accuracy: 63.2184
	Task-28	val_accuracy: 58.5366	gate_accuracy: 54.8780
	Task-29	val_accuracy: 56.8182	gate_accuracy: 48.8636
	Task-30	val_accuracy: 65.2778	gate_accuracy: 70.8333
	Task-31	val_accuracy: 72.7273	gate_accuracy: 63.6364
	Task-32	val_accuracy: 58.7500	gate_accuracy: 52.5000
	Task-33	val_accuracy: 55.1724	gate_accuracy: 50.5747
	Task-34	val_accuracy: 66.1972	gate_accuracy: 61.9718
	Task-35	val_accuracy: 57.8947	gate_accuracy: 56.5789
	Task-36	val_accuracy: 67.4419	gate_accuracy: 61.6279
	Task-37	val_accuracy: 59.7826	gate_accuracy: 55.4348
	Task-38	val_accuracy: 61.1765	gate_accuracy: 56.4706
	Task-39	val_accuracy: 64.2857	gate_accuracy: 63.0952
	Task-40	val_accuracy: 69.0476	gate_accuracy: 59.5238
	Task-41	val_accuracy: 69.5652	gate_accuracy: 69.5652
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.8509


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 3.1341	step1_train_accuracy: 50.0000
STEP-1	Epoch: 20/50	loss: 1.0166	step1_train_accuracy: 78.8194
STEP-1	Epoch: 30/50	loss: 0.6105	step1_train_accuracy: 89.5833
STEP-1	Epoch: 40/50	loss: 0.4414	step1_train_accuracy: 92.3611
STEP-1	Epoch: 50/50	loss: 0.3426	step1_train_accuracy: 94.4444
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1213	gate_loss: 2.7278	step2_classification_accuracy: 67.0023	step_2_gate_accuracy: 25.3547
STEP-2	Epoch: 40/200	classification_loss: 0.9026	gate_loss: 1.2179	step2_classification_accuracy: 73.5584	step_2_gate_accuracy: 64.8398
STEP-2	Epoch: 60/200	classification_loss: 0.7191	gate_loss: 0.7631	step2_classification_accuracy: 79.0046	step_2_gate_accuracy: 76.5561
STEP-2	Epoch: 80/200	classification_loss: 0.6098	gate_loss: 0.5768	step2_classification_accuracy: 81.6476	step_2_gate_accuracy: 81.7506
STEP-2	Epoch: 100/200	classification_loss: 0.5174	gate_loss: 0.4577	step2_classification_accuracy: 84.4394	step_2_gate_accuracy: 85.2860
STEP-2	Epoch: 120/200	classification_loss: 0.4702	gate_loss: 0.3908	step2_classification_accuracy: 85.5721	step_2_gate_accuracy: 87.2197
STEP-2	Epoch: 140/200	classification_loss: 0.4264	gate_loss: 0.3426	step2_classification_accuracy: 86.9451	step_2_gate_accuracy: 88.9130
STEP-2	Epoch: 160/200	classification_loss: 0.3949	gate_loss: 0.3037	step2_classification_accuracy: 87.5973	step_2_gate_accuracy: 90.1373
STEP-2	Epoch: 180/200	classification_loss: 0.3689	gate_loss: 0.2788	step2_classification_accuracy: 88.1922	step_2_gate_accuracy: 90.8467
STEP-2	Epoch: 200/200	classification_loss: 0.3445	gate_loss: 0.2479	step2_classification_accuracy: 89.1648	step_2_gate_accuracy: 91.8307
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.1449	gate_accuracy: 74.6377
	Task-1	val_accuracy: 41.6667	gate_accuracy: 54.1667
	Task-2	val_accuracy: 55.2941	gate_accuracy: 58.8235
	Task-3	val_accuracy: 46.5116	gate_accuracy: 54.6512
	Task-4	val_accuracy: 75.0000	gate_accuracy: 76.2500
	Task-5	val_accuracy: 60.5263	gate_accuracy: 63.1579
	Task-6	val_accuracy: 67.0732	gate_accuracy: 64.6341
	Task-7	val_accuracy: 47.4576	gate_accuracy: 55.9322
	Task-8	val_accuracy: 67.4699	gate_accuracy: 62.6506
	Task-9	val_accuracy: 50.5495	gate_accuracy: 47.2527
	Task-10	val_accuracy: 64.7887	gate_accuracy: 70.4225
	Task-11	val_accuracy: 73.7500	gate_accuracy: 71.2500
	Task-12	val_accuracy: 61.1765	gate_accuracy: 56.4706
	Task-13	val_accuracy: 61.5385	gate_accuracy: 56.4103
	Task-14	val_accuracy: 63.7500	gate_accuracy: 61.2500
	Task-15	val_accuracy: 61.0390	gate_accuracy: 67.5325
	Task-16	val_accuracy: 52.8736	gate_accuracy: 49.4253
	Task-17	val_accuracy: 50.6024	gate_accuracy: 50.6024
	Task-18	val_accuracy: 65.4321	gate_accuracy: 58.0247
	Task-19	val_accuracy: 62.0690	gate_accuracy: 58.6207
	Task-20	val_accuracy: 52.7027	gate_accuracy: 55.4054
	Task-21	val_accuracy: 68.8889	gate_accuracy: 62.2222
	Task-22	val_accuracy: 68.2927	gate_accuracy: 59.7561
	Task-23	val_accuracy: 63.0435	gate_accuracy: 59.7826
	Task-24	val_accuracy: 76.0000	gate_accuracy: 68.0000
	Task-25	val_accuracy: 57.5000	gate_accuracy: 55.0000
	Task-26	val_accuracy: 64.5570	gate_accuracy: 62.0253
	Task-27	val_accuracy: 64.3678	gate_accuracy: 55.1724
	Task-28	val_accuracy: 50.0000	gate_accuracy: 36.5854
	Task-29	val_accuracy: 48.8636	gate_accuracy: 51.1364
	Task-30	val_accuracy: 54.1667	gate_accuracy: 55.5556
	Task-31	val_accuracy: 63.6364	gate_accuracy: 62.5000
	Task-32	val_accuracy: 62.5000	gate_accuracy: 58.7500
	Task-33	val_accuracy: 65.5172	gate_accuracy: 59.7701
	Task-34	val_accuracy: 69.0141	gate_accuracy: 60.5634
	Task-35	val_accuracy: 60.5263	gate_accuracy: 52.6316
	Task-36	val_accuracy: 72.0930	gate_accuracy: 65.1163
	Task-37	val_accuracy: 58.6957	gate_accuracy: 55.4348
	Task-38	val_accuracy: 57.6471	gate_accuracy: 58.8235
	Task-39	val_accuracy: 60.7143	gate_accuracy: 59.5238
	Task-40	val_accuracy: 61.9048	gate_accuracy: 60.7143
	Task-41	val_accuracy: 66.3043	gate_accuracy: 66.3043
	Task-42	val_accuracy: 56.9444	gate_accuracy: 52.7778
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 59.3987


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 3.6552	step1_train_accuracy: 36.1702
STEP-1	Epoch: 20/50	loss: 1.3804	step1_train_accuracy: 70.9220
STEP-1	Epoch: 30/50	loss: 0.5896	step1_train_accuracy: 90.7801
STEP-1	Epoch: 40/50	loss: 0.3626	step1_train_accuracy: 94.3262
STEP-1	Epoch: 50/50	loss: 0.2551	step1_train_accuracy: 97.1631
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10, 80: 10, 81: 10, 82: 10, 83: 10, 84: 10, 85: 10, 86: 10, 87: 10, 88: 10, 89: 10, 90: 10, 91: 10, 92: 10, 93: 10, 94: 10, 95: 10, 96: 10, 97: 10, 98: 10, 99: 10, 100: 10, 101: 10, 102: 10, 103: 10, 104: 10, 105: 10, 106: 10, 107: 10, 108: 10, 109: 10, 110: 10, 111: 10, 112: 10, 113: 10, 114: 10, 115: 10, 116: 10, 117: 10, 118: 10, 119: 10, 120: 10, 121: 10, 122: 10, 123: 10, 124: 10, 125: 10, 126: 10, 127: 10, 128: 10, 129: 10, 130: 10, 131: 10, 132: 10, 133: 10, 134: 10, 135: 10, 136: 10, 137: 10, 138: 10, 139: 10, 140: 10, 141: 10, 142: 10, 143: 10, 144: 10, 145: 10, 146: 10, 147: 10, 148: 10, 149: 10, 150: 10, 151: 10, 152: 10, 153: 10, 154: 10, 155: 10, 156: 10, 157: 10, 158: 10, 159: 10, 160: 10, 161: 10, 162: 10, 163: 10, 164: 10, 165: 10, 166: 10, 167: 10, 168: 10, 169: 10, 170: 10, 171: 10, 172: 10, 173: 10, 174: 10, 175: 10, 176: 10, 177: 10, 178: 10, 179: 10, 180: 10, 181: 10, 182: 10, 183: 10, 184: 10, 185: 10, 186: 10, 187: 10, 188: 10, 189: 10, 190: 10, 191: 10, 192: 10, 193: 10, 194: 10, 195: 10, 196: 10, 197: 10, 198: 10, 199: 10, 200: 10, 201: 10, 202: 10, 203: 10, 204: 10, 205: 10, 206: 10, 207: 10, 208: 10, 209: 10, 210: 10, 211: 10, 212: 10, 213: 10, 214: 10, 215: 10, 216: 10, 217: 10, 218: 10, 219: 10, 220: 10, 221: 10, 222: 10, 223: 10, 224: 10, 225: 10, 226: 10, 227: 10, 228: 10, 229: 10, 230: 10, 231: 10, 232: 10, 233: 10, 234: 10, 235: 10, 236: 10, 237: 10, 238: 10, 239: 10, 240: 10, 241: 10, 242: 10, 243: 10, 244: 10, 245: 10, 246: 10, 247: 10, 248: 10, 249: 10, 250: 10, 251: 10, 252: 10, 253: 10, 254: 10, 255: 10, 256: 10, 257: 10, 258: 10, 259: 10, 260: 10, 261: 10, 262: 10, 263: 10, 264: 10, 265: 10, 266: 10, 267: 10, 268: 10, 269: 10, 270: 10, 271: 10, 272: 10, 273: 10, 274: 10, 275: 10, 276: 10, 277: 10, 278: 10, 279: 10, 280: 10, 281: 10, 282: 10, 283: 10, 284: 10, 285: 10, 286: 10, 287: 10, 288: 10, 289: 10, 290: 10, 291: 10, 292: 10, 293: 10, 294: 10, 295: 10, 296: 10, 297: 10, 298: 10, 299: 10, 300: 10, 301: 10, 302: 10, 303: 10, 304: 10, 305: 10, 306: 10, 307: 10, 308: 10, 309: 10, 310: 10, 311: 10, 312: 10, 313: 10, 314: 10, 315: 10, 316: 10, 317: 10, 318: 10, 319: 10, 320: 10, 321: 10, 322: 10, 323: 10, 324: 10, 325: 10, 326: 10, 327: 10, 328: 10, 329: 10, 330: 10, 331: 10, 332: 10, 333: 10, 334: 10, 335: 10, 336: 10, 337: 10, 338: 10, 339: 10, 340: 10, 341: 10, 342: 10, 343: 10, 344: 10, 345: 10, 346: 10, 347: 10, 348: 10, 349: 10, 350: 10, 351: 10, 352: 10, 353: 10, 354: 10, 355: 10, 356: 10, 357: 10, 358: 10, 359: 10, 360: 10, 361: 10, 362: 10, 363: 10, 364: 10, 365: 10, 366: 10, 367: 10, 368: 10, 369: 10, 370: 10, 371: 10, 372: 10, 373: 10, 374: 10, 375: 10, 376: 10, 377: 10, 378: 10, 379: 10, 380: 10, 381: 10, 382: 10, 383: 10, 384: 10, 385: 10, 386: 10, 387: 10, 388: 10, 389: 10, 390: 10, 391: 10, 392: 10, 393: 10, 394: 10, 395: 10, 396: 10, 397: 10, 398: 10, 399: 10, 400: 10, 401: 10, 402: 10, 403: 10, 404: 10, 405: 10, 406: 10, 407: 10, 408: 10, 409: 10, 410: 10, 411: 10, 412: 10, 413: 10, 414: 10, 415: 10, 416: 10, 417: 10, 418: 10, 419: 10, 420: 10, 421: 10, 422: 10, 423: 10, 424: 10, 425: 10, 426: 10, 427: 10, 428: 10, 429: 10, 430: 10, 431: 10, 432: 10, 433: 10, 434: 10, 435: 10, 436: 10, 437: 10, 438: 10, 439: 10, 440: 10, 441: 10, 442: 10, 443: 10, 444: 10, 445: 10, 446: 10, 447: 10, 448: 10, 449: 10, 450: 10, 451: 10, 452: 10, 453: 10, 454: 10, 455: 10, 456: 10, 457: 10, 458: 10, 459: 10, 460: 10, 461: 10, 462: 10, 463: 10, 464: 10, 465: 10, 466: 10, 467: 10, 468: 10, 469: 10, 470: 10, 471: 10, 472: 10, 473: 10, 474: 10, 475: 10, 476: 10, 477: 10, 478: 10, 479: 10, 480: 10, 481: 10, 482: 10, 483: 10, 484: 10, 485: 10, 486: 10, 487: 10, 488: 10, 489: 10, 490: 10, 491: 10, 492: 10, 493: 10, 494: 10, 495: 10, 496: 10, 497: 10, 498: 10, 499: 10, 500: 10, 501: 10, 502: 10, 503: 10, 504: 10, 505: 10, 506: 10, 507: 10, 508: 10, 509: 10, 510: 10, 511: 10, 512: 10, 513: 10, 514: 10, 515: 10, 516: 10, 517: 10, 518: 10, 519: 10, 520: 10, 521: 10, 522: 10, 523: 10, 524: 10, 525: 10, 526: 10, 527: 10, 528: 10, 529: 10, 530: 10, 531: 10, 532: 10, 533: 10, 534: 10, 535: 10, 536: 10, 537: 10, 538: 10, 539: 10, 540: 10, 541: 10, 542: 10, 543: 10, 544: 10, 545: 10, 546: 10, 547: 10, 548: 10, 549: 10, 550: 10, 551: 10, 552: 10, 553: 10, 554: 10, 555: 10, 556: 10, 557: 10, 558: 10, 559: 10, 560: 10, 561: 10, 562: 10, 563: 10, 564: 10, 565: 10, 566: 10, 567: 10, 568: 10, 569: 10, 570: 10, 571: 10, 572: 10, 573: 10, 574: 10, 575: 10, 576: 10, 577: 10, 578: 10, 579: 10, 580: 10, 581: 10, 582: 10, 583: 10, 584: 10, 585: 10, 586: 10, 587: 10, 588: 10, 589: 10, 590: 10, 591: 10, 592: 10, 593: 10, 594: 10, 595: 10, 596: 10, 597: 10, 598: 10, 599: 10, 600: 10, 601: 10, 602: 10, 603: 10, 604: 10, 605: 10, 606: 10, 607: 10, 608: 10, 609: 10, 610: 10, 611: 10, 612: 10, 613: 10, 614: 10, 615: 10, 616: 10, 617: 10, 618: 10, 619: 10, 620: 10, 621: 10, 622: 10, 623: 10, 624: 10, 625: 10, 626: 10, 627: 10, 628: 10, 629: 10, 630: 10, 631: 10, 632: 10, 633: 10, 634: 10, 635: 10, 636: 10, 637: 10, 638: 10, 639: 10, 640: 10, 641: 10, 642: 10, 643: 10, 644: 10, 645: 10, 646: 10, 647: 10, 648: 10, 649: 10, 650: 10, 651: 10, 652: 10, 653: 10, 654: 10, 655: 10, 656: 10, 657: 10, 658: 10, 659: 10, 660: 10, 661: 10, 662: 10, 663: 10, 664: 10, 665: 10, 666: 10, 667: 10, 668: 10, 669: 10, 670: 10, 671: 10, 672: 10, 673: 10, 674: 10, 675: 10, 676: 10, 677: 10, 678: 10, 679: 10, 680: 10, 681: 10, 682: 10, 683: 10, 684: 10, 685: 10, 686: 10, 687: 10, 688: 10, 689: 10, 690: 10, 691: 10, 692: 10, 693: 10, 694: 10, 695: 10, 696: 10, 697: 10, 698: 10, 699: 10, 700: 10, 701: 10, 702: 10, 703: 10, 704: 10, 705: 10, 706: 10, 707: 10, 708: 10, 709: 10, 710: 10, 711: 10, 712: 10, 713: 10, 714: 10, 715: 10, 716: 10, 717: 10, 718: 10, 719: 10, 720: 10, 721: 10, 722: 10, 723: 10, 724: 10, 725: 10, 726: 10, 727: 10, 728: 10, 729: 10, 730: 10, 731: 10, 732: 10, 733: 10, 734: 10, 735: 10, 736: 10, 737: 10, 738: 10, 739: 10, 740: 10, 741: 10, 742: 10, 743: 10, 744: 10, 745: 10, 746: 10, 747: 10, 748: 10, 749: 10, 750: 10, 751: 10, 752: 10, 753: 10, 754: 10, 755: 10, 756: 10, 757: 10, 758: 10, 759: 10, 760: 10, 761: 10, 762: 10, 763: 10, 764: 10, 765: 10, 766: 10, 767: 10, 768: 10, 769: 10, 770: 10, 771: 10, 772: 10, 773: 10, 774: 10, 775: 10, 776: 10, 777: 10, 778: 10, 779: 10, 780: 10, 781: 10, 782: 10, 783: 10, 784: 10, 785: 10, 786: 10, 787: 10, 788: 10, 789: 10, 790: 10, 791: 10, 792: 10, 793: 10, 794: 10, 795: 10, 796: 10, 797: 10, 798: 10, 799: 10, 800: 10, 801: 10, 802: 10, 803: 10, 804: 10, 805: 10, 806: 10, 807: 10, 808: 10, 809: 10, 810: 10, 811: 10, 812: 10, 813: 10, 814: 10, 815: 10, 816: 10, 817: 10, 818: 10, 819: 10, 820: 10, 821: 10, 822: 10, 823: 10, 824: 10, 825: 10, 826: 10, 827: 10, 828: 10, 829: 10, 830: 10, 831: 10, 832: 10, 833: 10, 834: 10, 835: 10, 836: 10, 837: 10, 838: 10, 839: 10, 840: 10, 841: 10, 842: 10, 843: 10, 844: 10, 845: 10, 846: 10, 847: 10, 848: 10, 849: 10, 850: 10, 851: 10, 852: 10, 853: 10, 854: 10, 855: 10, 856: 10, 857: 10, 858: 10, 859: 10, 860: 10, 861: 10, 862: 10, 863: 10, 864: 10, 865: 10, 866: 10, 867: 10, 868: 10, 869: 10, 870: 10, 871: 10, 872: 10, 873: 10, 874: 10, 875: 10, 876: 10, 877: 10, 878: 10, 879: 10, 880: 10, 881: 10, 882: 10, 883: 10, 884: 10, 885: 10, 886: 10, 887: 10, 888: 10, 889: 10, 890: 10, 891: 10, 892: 10, 893: 10})
STEP-2	Epoch: 20/200	classification_loss: 1.1784	gate_loss: 2.7616	step2_classification_accuracy: 65.1566	step_2_gate_accuracy: 24.1275
STEP-2	Epoch: 40/200	classification_loss: 0.9518	gate_loss: 1.2445	step2_classification_accuracy: 72.8523	step_2_gate_accuracy: 64.1051
STEP-2	Epoch: 60/200	classification_loss: 0.7530	gate_loss: 0.7916	step2_classification_accuracy: 78.1991	step_2_gate_accuracy: 76.1633
STEP-2	Epoch: 80/200	classification_loss: 0.6281	gate_loss: 0.5982	step2_classification_accuracy: 81.2752	step_2_gate_accuracy: 80.9955
STEP-2	Epoch: 100/200	classification_loss: 0.5582	gate_loss: 0.4861	step2_classification_accuracy: 83.7696	step_2_gate_accuracy: 85.0783
STEP-2	Epoch: 120/200	classification_loss: 0.4981	gate_loss: 0.4150	step2_classification_accuracy: 84.8098	step_2_gate_accuracy: 86.8568
STEP-2	Epoch: 140/200	classification_loss: 0.4423	gate_loss: 0.3550	step2_classification_accuracy: 86.3870	step_2_gate_accuracy: 88.7248
STEP-2	Epoch: 160/200	classification_loss: 0.4140	gate_loss: 0.3219	step2_classification_accuracy: 87.2371	step_2_gate_accuracy: 89.8546
STEP-2	Epoch: 180/200	classification_loss: 0.3856	gate_loss: 0.2970	step2_classification_accuracy: 87.9418	step_2_gate_accuracy: 90.1678
STEP-2	Epoch: 200/200	classification_loss: 0.3578	gate_loss: 0.2635	step2_classification_accuracy: 88.9374	step_2_gate_accuracy: 91.6331
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 57.9710	gate_accuracy: 68.1159
	Task-1	val_accuracy: 45.8333	gate_accuracy: 70.8333
	Task-2	val_accuracy: 49.4118	gate_accuracy: 56.4706
	Task-3	val_accuracy: 63.9535	gate_accuracy: 65.1163
	Task-4	val_accuracy: 58.7500	gate_accuracy: 63.7500
	Task-5	val_accuracy: 55.2632	gate_accuracy: 67.1053
	Task-6	val_accuracy: 63.4146	gate_accuracy: 63.4146
	Task-7	val_accuracy: 55.9322	gate_accuracy: 61.0169
	Task-8	val_accuracy: 68.6747	gate_accuracy: 67.4699
	Task-9	val_accuracy: 54.9451	gate_accuracy: 54.9451
	Task-10	val_accuracy: 64.7887	gate_accuracy: 69.0141
	Task-11	val_accuracy: 68.7500	gate_accuracy: 62.5000
	Task-12	val_accuracy: 65.8824	gate_accuracy: 55.2941
	Task-13	val_accuracy: 60.2564	gate_accuracy: 60.2564
	Task-14	val_accuracy: 66.2500	gate_accuracy: 65.0000
	Task-15	val_accuracy: 72.7273	gate_accuracy: 71.4286
	Task-16	val_accuracy: 47.1264	gate_accuracy: 42.5287
	Task-17	val_accuracy: 56.6265	gate_accuracy: 51.8072
	Task-18	val_accuracy: 60.4938	gate_accuracy: 61.7284
	Task-19	val_accuracy: 58.6207	gate_accuracy: 58.6207
	Task-20	val_accuracy: 59.4595	gate_accuracy: 55.4054
	Task-21	val_accuracy: 63.3333	gate_accuracy: 58.8889
	Task-22	val_accuracy: 69.5122	gate_accuracy: 62.1951
	Task-23	val_accuracy: 65.2174	gate_accuracy: 57.6087
	Task-24	val_accuracy: 77.3333	gate_accuracy: 69.3333
	Task-25	val_accuracy: 58.7500	gate_accuracy: 51.2500
	Task-26	val_accuracy: 64.5570	gate_accuracy: 65.8228
	Task-27	val_accuracy: 70.1149	gate_accuracy: 60.9195
	Task-28	val_accuracy: 57.3171	gate_accuracy: 54.8780
	Task-29	val_accuracy: 57.9545	gate_accuracy: 48.8636
	Task-30	val_accuracy: 58.3333	gate_accuracy: 58.3333
	Task-31	val_accuracy: 61.3636	gate_accuracy: 61.3636
	Task-32	val_accuracy: 60.0000	gate_accuracy: 57.5000
	Task-33	val_accuracy: 72.4138	gate_accuracy: 71.2644
	Task-34	val_accuracy: 64.7887	gate_accuracy: 57.7465
	Task-35	val_accuracy: 55.2632	gate_accuracy: 50.0000
	Task-36	val_accuracy: 72.0930	gate_accuracy: 67.4419
	Task-37	val_accuracy: 64.1304	gate_accuracy: 58.6957
	Task-38	val_accuracy: 58.8235	gate_accuracy: 56.4706
	Task-39	val_accuracy: 59.5238	gate_accuracy: 54.7619
	Task-40	val_accuracy: 69.0476	gate_accuracy: 64.2857
	Task-41	val_accuracy: 72.8261	gate_accuracy: 70.6522
	Task-42	val_accuracy: 38.8889	gate_accuracy: 34.7222
	Task-43	val_accuracy: 62.8571	gate_accuracy: 61.4286
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 60.2370


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 06:34:41	Wall time: 05:45:51
CPU time: 23681.777028808	Wall time: 20751.5970621109
FAA: 72.71119457487909
FF: 28.31699322644861

TRAINER.METRIC.ACCURACY
0: [95.65217391304348]
1: [93.47826086956522, 87.5]
2: [92.02898550724638, 87.5, 87.05882352941177]
3: [92.02898550724638, 79.16666666666666, 83.52941176470588, 86.04651162790698]
4: [89.13043478260869, 79.16666666666666, 85.88235294117646, 83.72093023255815, 88.75]
5: [88.40579710144928, 73.61111111111111, 80.0, 82.55813953488372, 91.25, 75.0]
6: [82.6086956521739, 70.83333333333334, 78.82352941176471, 81.3953488372093, 85.0, 76.31578947368422, 92.6829268292683]
7: [86.23188405797102, 77.77777777777779, 80.0, 81.3953488372093, 88.75, 73.68421052631578, 89.02439024390245, 71.1864406779661]
8: [82.6086956521739, 68.05555555555556, 82.35294117647058, 75.5813953488372, 85.0, 77.63157894736842, 86.58536585365853, 69.49152542372882, 73.49397590361446]
9: [80.43478260869566, 73.61111111111111, 72.94117647058823, 76.74418604651163, 87.5, 75.0, 79.26829268292683, 72.88135593220339, 85.54216867469879, 82.41758241758241]
10: [80.43478260869566, 75.0, 77.64705882352942, 63.95348837209303, 87.5, 73.68421052631578, 80.48780487804879, 71.1864406779661, 73.49397590361446, 81.31868131868131, 71.83098591549296]
11: [77.53623188405797, 69.44444444444444, 74.11764705882354, 68.6046511627907, 87.5, 71.05263157894737, 82.92682926829268, 66.10169491525424, 85.54216867469879, 79.12087912087912, 80.28169014084507, 83.75]
12: [79.71014492753623, 63.888888888888886, 68.23529411764706, 61.627906976744185, 85.0, 75.0, 84.14634146341463, 61.016949152542374, 79.51807228915662, 78.02197802197803, 73.23943661971832, 83.75, 82.35294117647058]
13: [78.98550724637681, 58.333333333333336, 69.41176470588235, 68.6046511627907, 77.5, 72.36842105263158, 82.92682926829268, 67.79661016949152, 78.3132530120482, 78.02197802197803, 71.83098591549296, 82.5, 82.35294117647058, 71.7948717948718]
14: [77.53623188405797, 50.0, 76.47058823529412, 72.09302325581395, 76.25, 71.05263157894737, 79.26829268292683, 59.32203389830508, 75.90361445783132, 78.02197802197803, 74.64788732394366, 80.0, 80.0, 70.51282051282051, 81.25]
15: [72.46376811594203, 61.111111111111114, 71.76470588235294, 74.4186046511628, 80.0, 60.526315789473685, 75.60975609756098, 74.57627118644068, 78.3132530120482, 74.72527472527473, 70.4225352112676, 80.0, 75.29411764705883, 70.51282051282051, 73.75, 83.11688311688312]
16: [73.91304347826086, 45.83333333333333, 68.23529411764706, 67.44186046511628, 73.75, 69.73684210526315, 82.92682926829268, 64.40677966101694, 73.49397590361446, 64.83516483516483, 66.19718309859155, 78.75, 77.64705882352942, 67.94871794871796, 67.5, 80.51948051948052, 64.36781609195403]
17: [73.18840579710145, 52.77777777777778, 64.70588235294117, 60.46511627906976, 78.75, 64.47368421052632, 84.14634146341463, 67.79661016949152, 72.28915662650603, 70.32967032967034, 61.97183098591549, 85.0, 75.29411764705883, 70.51282051282051, 78.75, 87.01298701298701, 62.06896551724138, 71.08433734939759]
18: [65.21739130434783, 59.72222222222222, 61.1764705882353, 60.46511627906976, 77.5, 59.210526315789465, 80.48780487804879, 59.32203389830508, 74.69879518072288, 72.52747252747253, 66.19718309859155, 77.5, 69.41176470588235, 69.23076923076923, 71.25, 83.11688311688312, 58.620689655172406, 60.24096385542169, 74.07407407407408]
19: [74.63768115942028, 56.94444444444444, 62.35294117647059, 65.11627906976744, 72.5, 68.42105263157895, 75.60975609756098, 50.847457627118644, 78.3132530120482, 70.32967032967034, 70.4225352112676, 81.25, 68.23529411764706, 64.1025641025641, 71.25, 79.22077922077922, 58.620689655172406, 67.46987951807229, 76.5432098765432, 68.96551724137932]
20: [64.4927536231884, 52.77777777777778, 70.58823529411765, 63.95348837209303, 86.25, 64.47368421052632, 71.95121951219512, 57.6271186440678, 73.49397590361446, 68.13186813186813, 67.6056338028169, 81.25, 72.94117647058823, 62.82051282051282, 77.5, 80.51948051948052, 55.172413793103445, 69.87951807228916, 72.8395061728395, 67.81609195402298, 64.86486486486487]
21: [71.01449275362319, 52.77777777777778, 71.76470588235294, 63.95348837209303, 77.5, 57.89473684210527, 75.60975609756098, 57.6271186440678, 80.72289156626506, 65.93406593406593, 70.4225352112676, 73.75, 72.94117647058823, 69.23076923076923, 77.5, 80.51948051948052, 55.172413793103445, 69.87951807228916, 76.5432098765432, 73.5632183908046, 58.108108108108105, 73.33333333333333]
22: [75.36231884057972, 56.94444444444444, 60.0, 65.11627906976744, 68.75, 63.1578947368421, 75.60975609756098, 59.32203389830508, 72.28915662650603, 60.43956043956044, 66.19718309859155, 80.0, 74.11764705882354, 61.53846153846154, 80.0, 74.02597402597402, 59.77011494252874, 67.46987951807229, 76.5432098765432, 72.41379310344827, 63.51351351351351, 75.55555555555556, 79.26829268292683]
23: [72.46376811594203, 52.77777777777778, 69.41176470588235, 58.139534883720934, 77.5, 61.8421052631579, 68.29268292682927, 62.71186440677966, 75.90361445783132, 68.13186813186813, 61.97183098591549, 73.75, 70.58823529411765, 64.1025641025641, 71.25, 77.92207792207793, 57.47126436781609, 72.28915662650603, 72.8395061728395, 65.51724137931035, 62.16216216216216, 75.55555555555556, 71.95121951219512, 71.73913043478261]
24: [68.11594202898551, 43.05555555555556, 60.0, 67.44186046511628, 81.25, 48.68421052631579, 70.73170731707317, 64.40677966101694, 75.90361445783132, 65.93406593406593, 63.38028169014085, 78.75, 68.23529411764706, 65.38461538461539, 70.0, 77.92207792207793, 60.91954022988506, 66.26506024096386, 76.5432098765432, 65.51724137931035, 60.810810810810814, 78.88888888888889, 76.82926829268293, 76.08695652173914, 78.66666666666666]
25: [65.94202898550725, 43.05555555555556, 67.05882352941175, 52.32558139534884, 76.25, 59.210526315789465, 63.41463414634146, 57.6271186440678, 72.28915662650603, 61.53846153846154, 67.6056338028169, 81.25, 71.76470588235294, 66.66666666666666, 70.0, 79.22077922077922, 60.91954022988506, 67.46987951807229, 72.8395061728395, 67.81609195402298, 59.45945945945946, 77.77777777777779, 68.29268292682927, 75.0, 77.33333333333333, 67.5]
26: [64.4927536231884, 48.61111111111111, 65.88235294117646, 62.7906976744186, 73.75, 61.8421052631579, 65.85365853658537, 59.32203389830508, 72.28915662650603, 62.637362637362635, 60.56338028169014, 76.25, 64.70588235294117, 57.692307692307686, 66.25, 75.32467532467533, 54.02298850574713, 49.39759036144578, 70.37037037037037, 71.26436781609196, 56.75675675675676, 82.22222222222221, 75.60975609756098, 76.08695652173914, 84.0, 72.5, 79.74683544303798]
27: [69.56521739130434, 48.61111111111111, 60.0, 55.81395348837209, 70.0, 64.47368421052632, 75.60975609756098, 52.54237288135594, 77.10843373493977, 53.84615384615385, 59.154929577464785, 73.75, 69.41176470588235, 62.82051282051282, 76.25, 83.11688311688312, 63.2183908045977, 62.65060240963856, 74.07407407407408, 70.11494252873564, 66.21621621621621, 70.0, 68.29268292682927, 70.65217391304348, 73.33333333333333, 67.5, 70.88607594936708, 75.86206896551724]
28: [60.86956521739131, 47.22222222222222, 65.88235294117646, 55.81395348837209, 73.75, 60.526315789473685, 74.39024390243902, 59.32203389830508, 72.28915662650603, 57.14285714285714, 64.7887323943662, 76.25, 63.52941176470588, 60.256410256410255, 75.0, 77.92207792207793, 55.172413793103445, 59.036144578313255, 74.07407407407408, 72.41379310344827, 62.16216216216216, 80.0, 74.39024390243902, 70.65217391304348, 73.33333333333333, 63.74999999999999, 74.68354430379746, 68.96551724137932, 54.87804878048781]
29: [65.94202898550725, 43.05555555555556, 65.88235294117646, 58.139534883720934, 67.5, 63.1578947368421, 70.73170731707317, 47.45762711864407, 69.87951807228916, 68.13186813186813, 66.19718309859155, 73.75, 60.0, 64.1025641025641, 67.5, 67.53246753246754, 58.620689655172406, 63.85542168674698, 67.90123456790124, 62.06896551724138, 54.054054054054056, 82.22222222222221, 73.17073170731707, 67.3913043478261, 68.0, 58.75, 79.74683544303798, 64.36781609195403, 63.41463414634146, 65.9090909090909]
30: [70.28985507246377, 45.83333333333333, 65.88235294117646, 60.46511627906976, 63.74999999999999, 61.8421052631579, 75.60975609756098, 54.23728813559322, 72.28915662650603, 58.24175824175825, 71.83098591549296, 78.75, 64.70588235294117, 66.66666666666666, 68.75, 61.038961038961034, 51.724137931034484, 57.831325301204814, 75.30864197530865, 60.91954022988506, 50.0, 68.88888888888889, 70.73170731707317, 67.3913043478261, 76.0, 62.5, 77.21518987341773, 77.01149425287356, 68.29268292682927, 63.63636363636363, 59.72222222222222]
31: [60.14492753623188, 44.44444444444444, 65.88235294117646, 51.162790697674424, 68.75, 65.78947368421053, 65.85365853658537, 52.54237288135594, 69.87951807228916, 56.043956043956044, 66.19718309859155, 66.25, 67.05882352941175, 65.38461538461539, 65.0, 77.92207792207793, 56.32183908045977, 56.62650602409639, 72.8395061728395, 66.66666666666666, 55.4054054054054, 73.33333333333333, 79.26829268292683, 72.82608695652173, 80.0, 67.5, 79.74683544303798, 70.11494252873564, 59.756097560975604, 61.36363636363637, 66.66666666666666, 75.0]
32: [68.11594202898551, 43.05555555555556, 67.05882352941175, 61.627906976744185, 71.25, 57.89473684210527, 65.85365853658537, 59.32203389830508, 68.67469879518072, 61.53846153846154, 56.33802816901409, 73.75, 65.88235294117646, 65.38461538461539, 65.0, 76.62337662337663, 62.06896551724138, 66.26506024096386, 67.90123456790124, 66.66666666666666, 55.4054054054054, 73.33333333333333, 69.51219512195121, 68.47826086956522, 76.0, 62.5, 64.55696202531645, 68.96551724137932, 68.29268292682927, 70.45454545454545, 63.888888888888886, 69.31818181818183, 56.25]
33: [62.31884057971014, 34.72222222222222, 55.294117647058826, 58.139534883720934, 73.75, 67.10526315789474, 58.536585365853654, 57.6271186440678, 72.28915662650603, 47.25274725274725, 66.19718309859155, 80.0, 58.82352941176471, 58.97435897435898, 66.25, 67.53246753246754, 50.57471264367817, 60.24096385542169, 72.8395061728395, 68.96551724137932, 55.4054054054054, 71.11111111111111, 78.04878048780488, 68.47826086956522, 80.0, 61.25000000000001, 68.35443037974683, 64.36781609195403, 63.41463414634146, 65.9090909090909, 62.5, 80.68181818181817, 56.25, 64.36781609195403]
34: [63.76811594202898, 45.83333333333333, 57.647058823529406, 55.81395348837209, 70.0, 64.47368421052632, 68.29268292682927, 61.016949152542374, 74.69879518072288, 51.64835164835166, 69.01408450704226, 75.0, 64.70588235294117, 64.1025641025641, 58.75, 74.02597402597402, 54.02298850574713, 51.80722891566265, 62.96296296296296, 72.41379310344827, 54.054054054054056, 71.11111111111111, 64.63414634146342, 73.91304347826086, 73.33333333333333, 65.0, 69.62025316455697, 70.11494252873564, 67.07317073170732, 62.5, 65.27777777777779, 63.63636363636363, 58.75, 72.41379310344827, 66.19718309859155]
35: [58.69565217391305, 34.72222222222222, 64.70588235294117, 66.27906976744185, 70.0, 57.89473684210527, 65.85365853658537, 55.932203389830505, 78.3132530120482, 49.45054945054945, 66.19718309859155, 75.0, 68.23529411764706, 62.82051282051282, 65.0, 72.72727272727273, 49.42528735632184, 61.44578313253012, 61.72839506172839, 68.96551724137932, 54.054054054054056, 72.22222222222221, 69.51219512195121, 72.82608695652173, 78.66666666666666, 60.0, 67.08860759493672, 72.41379310344827, 59.756097560975604, 64.77272727272727, 63.888888888888886, 69.31818181818183, 60.0, 72.41379310344827, 69.01408450704226, 63.1578947368421]
36: [63.04347826086957, 50.0, 60.0, 55.81395348837209, 70.0, 55.26315789473685, 69.51219512195121, 54.23728813559322, 73.49397590361446, 59.34065934065934, 60.56338028169014, 75.0, 70.58823529411765, 65.38461538461539, 68.75, 72.72727272727273, 52.87356321839081, 49.39759036144578, 66.66666666666666, 66.66666666666666, 60.810810810810814, 74.44444444444444, 68.29268292682927, 66.30434782608695, 76.0, 63.74999999999999, 73.41772151898735, 67.81609195402298, 52.4390243902439, 56.81818181818182, 58.333333333333336, 67.04545454545455, 57.49999999999999, 74.71264367816092, 67.6056338028169, 56.57894736842105, 72.09302325581395]
37: [50.72463768115942, 45.83333333333333, 61.1764705882353, 54.65116279069767, 66.25, 60.526315789473685, 56.09756097560976, 47.45762711864407, 71.08433734939759, 52.74725274725275, 59.154929577464785, 75.0, 71.76470588235294, 61.53846153846154, 62.5, 70.12987012987013, 35.63218390804598, 53.01204819277109, 69.1358024691358, 58.620689655172406, 58.108108108108105, 70.0, 59.756097560975604, 68.47826086956522, 80.0, 63.74999999999999, 75.9493670886076, 62.06896551724138, 59.756097560975604, 61.36363636363637, 59.72222222222222, 67.04545454545455, 62.5, 75.86206896551724, 57.74647887323944, 52.63157894736842, 65.11627906976744, 66.30434782608695]
38: [53.62318840579711, 33.33333333333333, 63.52941176470588, 54.65116279069767, 68.75, 55.26315789473685, 68.29268292682927, 55.932203389830505, 72.28915662650603, 48.35164835164835, 66.19718309859155, 76.25, 65.88235294117646, 60.256410256410255, 72.5, 70.12987012987013, 48.275862068965516, 57.831325301204814, 66.66666666666666, 64.36781609195403, 54.054054054054056, 74.44444444444444, 75.60975609756098, 61.95652173913043, 81.33333333333333, 62.5, 65.82278481012658, 65.51724137931035, 54.87804878048781, 53.40909090909091, 62.5, 62.5, 61.25000000000001, 64.36781609195403, 66.19718309859155, 55.26315789473685, 74.4186046511628, 71.73913043478261, 54.11764705882353]
39: [71.01449275362319, 40.27777777777778, 50.588235294117645, 50.0, 70.0, 44.73684210526316, 65.85365853658537, 50.847457627118644, 66.26506024096386, 49.45054945054945, 59.154929577464785, 70.0, 65.88235294117646, 65.38461538461539, 71.25, 72.72727272727273, 49.42528735632184, 51.80722891566265, 58.0246913580247, 57.47126436781609, 56.75675675675676, 74.44444444444444, 63.41463414634146, 68.47826086956522, 73.33333333333333, 60.0, 64.55696202531645, 78.16091954022988, 56.09756097560976, 65.9090909090909, 56.94444444444444, 69.31818181818183, 66.25, 71.26436781609196, 67.6056338028169, 53.94736842105263, 69.76744186046511, 64.13043478260869, 56.470588235294116, 64.28571428571429]
40: [58.69565217391305, 45.83333333333333, 55.294117647058826, 45.348837209302324, 66.25, 56.57894736842105, 68.29268292682927, 55.932203389830505, 68.67469879518072, 59.34065934065934, 59.154929577464785, 77.5, 63.52941176470588, 60.256410256410255, 56.25, 70.12987012987013, 56.32183908045977, 63.85542168674698, 66.66666666666666, 59.77011494252874, 48.64864864864865, 61.111111111111114, 63.41463414634146, 66.30434782608695, 66.66666666666666, 65.0, 63.29113924050633, 67.81609195402298, 50.0, 64.77272727272727, 56.94444444444444, 71.5909090909091, 58.75, 63.2183908045977, 63.38028169014085, 60.526315789473685, 73.25581395348837, 61.95652173913043, 50.588235294117645, 55.952380952380956, 64.28571428571429]
41: [58.69565217391305, 41.66666666666667, 64.70588235294117, 53.48837209302325, 75.0, 57.89473684210527, 62.19512195121951, 50.847457627118644, 74.69879518072288, 49.45054945054945, 60.56338028169014, 78.75, 60.0, 60.256410256410255, 65.0, 68.83116883116884, 41.37931034482759, 49.39759036144578, 58.0246913580247, 63.2183908045977, 58.108108108108105, 66.66666666666666, 74.39024390243902, 59.78260869565217, 72.0, 65.0, 70.88607594936708, 66.66666666666666, 58.536585365853654, 56.81818181818182, 65.27777777777779, 72.72727272727273, 58.75, 55.172413793103445, 66.19718309859155, 57.89473684210527, 67.44186046511628, 59.78260869565217, 61.1764705882353, 64.28571428571429, 69.04761904761905, 69.56521739130434]
42: [60.14492753623188, 41.66666666666667, 55.294117647058826, 46.51162790697674, 75.0, 60.526315789473685, 67.07317073170732, 47.45762711864407, 67.46987951807229, 50.54945054945055, 64.7887323943662, 73.75, 61.1764705882353, 61.53846153846154, 63.74999999999999, 61.038961038961034, 52.87356321839081, 50.602409638554214, 65.4320987654321, 62.06896551724138, 52.702702702702695, 68.88888888888889, 68.29268292682927, 63.04347826086957, 76.0, 57.49999999999999, 64.55696202531645, 64.36781609195403, 50.0, 48.86363636363637, 54.166666666666664, 63.63636363636363, 62.5, 65.51724137931035, 69.01408450704226, 60.526315789473685, 72.09302325581395, 58.69565217391305, 57.647058823529406, 60.71428571428571, 61.904761904761905, 66.30434782608695, 56.94444444444444]
43: [57.971014492753625, 45.83333333333333, 49.411764705882355, 63.95348837209303, 58.75, 55.26315789473685, 63.41463414634146, 55.932203389830505, 68.67469879518072, 54.94505494505495, 64.7887323943662, 68.75, 65.88235294117646, 60.256410256410255, 66.25, 72.72727272727273, 47.12643678160919, 56.62650602409639, 60.49382716049383, 58.620689655172406, 59.45945945945946, 63.33333333333333, 69.51219512195121, 65.21739130434783, 77.33333333333333, 58.75, 64.55696202531645, 70.11494252873564, 57.3170731707317, 57.95454545454546, 58.333333333333336, 61.36363636363637, 60.0, 72.41379310344827, 64.7887323943662, 55.26315789473685, 72.09302325581395, 64.13043478260869, 58.82352941176471, 59.523809523809526, 69.04761904761905, 72.82608695652173, 38.88888888888889, 62.857142857142854]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 56.1497
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 39.0000
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 44.7368
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 51.7241
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 54.5455
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 50.0000
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 54.5455
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 61.1765
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 70.5357
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 52.0661
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 56.7010
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 62.7273
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 59.6491
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 58.4906
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 70.0000
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 66.9811
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 55.5556
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 60.7143
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 59.0909
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 64.9573
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 57.8431
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 64.1667
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 62.1622
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 60.6557
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 81.5534
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 54.1284
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 67.2897
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 69.2308
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 62.7273
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 62.3932
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 58.5859
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 61.8644
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 70.0935
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 64.9573
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 68.6869
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 60.5769
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 72.1739
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 64.7541
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 59.6491
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 55.2632
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 56.6372
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 68.8525
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 58.5859
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 73.4694

====================

f1_score(micro): 60.972137482204595
f1_score(macro): 57.91063011279947
Classification report:
              precision    recall  f1-score   support

           0       1.00      0.67      0.80         9
           1       1.00      0.25      0.40         4
           2       1.00      0.50      0.67         4
           3       1.00      0.75      0.86         4
           4       1.00      0.00      0.00         4
           5       0.00      0.00      0.00         4
           6       0.75      0.33      0.46         9
           7       1.00      1.00      1.00         5
           8       0.25      0.25      0.25         4
           9       0.67      0.44      0.53         9
          10       0.50      0.25      0.33         4
          11       0.71      1.00      0.83         5
          12       1.00      0.50      0.67         4
          13       1.00      0.75      0.86         4
          14       0.50      0.25      0.33         4
          15       0.40      0.80      0.53         5
          16       1.00      0.50      0.67         4
          17       0.00      0.00      0.00         4
          18       0.50      0.50      0.50         4
          19       0.86      0.67      0.75         9
          20       0.80      0.44      0.57         9
          21       1.00      0.25      0.40         4
          22       1.00      1.00      1.00         9
          23       0.75      0.75      0.75         4
          24       0.08      0.25      0.12         4
          25       0.00      0.00      0.00         4
          26       1.00      0.89      0.94         9
          27       1.00      1.00      1.00         5
          28       0.82      1.00      0.90         9
          29       1.00      0.75      0.86         4
          30       1.00      1.00      1.00         5
          31       0.50      0.33      0.40         9
          32       0.67      0.50      0.57         4
          33       1.00      0.20      0.33         5
          34       0.14      0.25      0.18         4
          35       1.00      0.40      0.57         5
          36       1.00      0.00      0.00         4
          37       0.00      0.00      0.00         9
          38       0.88      0.78      0.82         9
          39       1.00      0.40      0.57         5
          40       1.00      0.00      0.00         4
          41       1.00      0.00      0.00         4
          42       1.00      0.40      0.57         5
          43       0.50      0.25      0.33         4
          44       0.33      0.50      0.40         4
          45       1.00      0.75      0.86         4
          46       0.33      0.11      0.17         9
          47       0.67      1.00      0.80         4
          48       0.00      0.00      0.00         4
          49       0.67      0.50      0.57         4
          50       0.25      0.20      0.22         5
          51       1.00      0.75      0.86         4
          52       1.00      1.00      1.00         4
          53       1.00      0.80      0.89         5
          54       1.00      0.40      0.57         5
          55       1.00      0.56      0.71         9
          56       0.50      0.40      0.44         5
          57       0.33      0.20      0.25         5
          58       0.50      0.60      0.55         5
          59       1.00      0.00      0.00         4
          60       0.80      0.44      0.57         9
          61       1.00      0.00      0.00         4
          62       1.00      0.75      0.86         4
          63       0.33      0.25      0.29         4
          64       0.71      0.56      0.63         9
          65       0.89      0.89      0.89         9
          66       1.00      0.75      0.86         4
          67       0.50      0.75      0.60         4
          68       1.00      0.75      0.86         4
          69       1.00      0.00      0.00         4
          70       0.50      0.22      0.31         9
          71       1.00      0.00      0.00         4
          72       1.00      0.67      0.80         9
          73       0.00      0.00      0.00         4
          74       1.00      0.89      0.94         9
          75       1.00      0.75      0.86         8
          76       0.00      0.00      0.00         9
          77       0.50      0.80      0.62         5
          78       1.00      1.00      1.00         5
          79       0.80      0.44      0.57         9
          80       0.75      0.50      0.60         6
          81       1.00      0.40      0.57         5
          82       1.00      0.75      0.86         4
          83       1.00      0.50      0.67         4
          84       1.00      0.00      0.00         4
          85       0.70      0.78      0.74         9
          86       1.00      0.75      0.86         4
          87       0.00      0.00      0.00         4
          88       1.00      0.80      0.89         5
          89       0.00      0.00      0.00         4
          90       0.14      0.25      0.18         4
          91       0.75      0.75      0.75         4
          92       1.00      0.56      0.71         9
          93       0.00      0.00      0.00         5
          94       1.00      0.75      0.86         4
          95       1.00      1.00      1.00         9
          96       1.00      0.33      0.50         9
          97       1.00      1.00      1.00         4
          98       1.00      0.25      0.40         4
          99       0.80      1.00      0.89         4
         100       0.75      0.75      0.75         4
         101       1.00      0.40      0.57         5
         102       0.67      0.22      0.33         9
         103       1.00      0.20      0.33         5
         104       0.00      0.00      0.00         5
         105       0.67      0.50      0.57         4
         106       1.00      0.80      0.89         5
         107       1.00      1.00      1.00         5
         108       1.00      0.50      0.67         4
         109       0.45      0.56      0.50         9
         110       1.00      0.50      0.67         4
         111       0.33      0.25      0.29         4
         112       1.00      0.67      0.80         9
         113       0.33      0.25      0.29         4
         114       1.00      0.67      0.80         9
         115       1.00      0.75      0.86         4
         116       1.00      0.75      0.86         4
         117       1.00      0.25      0.40         4
         118       0.75      0.75      0.75         4
         119       0.00      0.00      0.00         4
         120       1.00      0.00      0.00         4
         121       0.75      0.60      0.67         5
         122       0.11      0.20      0.14         5
         123       0.55      0.67      0.60         9
         124       0.33      0.75      0.46         4
         125       0.43      0.33      0.38         9
         126       0.67      0.80      0.73         5
         127       1.00      0.00      0.00         4
         128       0.44      1.00      0.62         4
         129       0.12      0.25      0.17         4
         130       1.00      0.20      0.33         5
         131       1.00      0.00      0.00         4
         132       0.88      0.78      0.82         9
         133       1.00      0.75      0.86         4
         134       1.00      0.75      0.86         4
         135       1.00      1.00      1.00         4
         136       0.73      0.89      0.80         9
         137       0.00      0.00      0.00         4
         138       0.80      1.00      0.89         4
         139       0.50      0.50      0.50         4
         140       1.00      0.75      0.86         4
         141       0.00      0.00      0.00         4
         142       1.00      0.89      0.94         9
         143       1.00      0.25      0.40         4
         144       0.00      0.00      0.00         7
         145       1.00      1.00      1.00         4
         146       1.00      0.56      0.71         9
         147       1.00      1.00      1.00         5
         148       0.43      0.33      0.38         9
         149       0.00      0.00      0.00         4
         150       0.86      0.67      0.75         9
         151       0.67      0.50      0.57         4
         152       0.67      0.40      0.50         5
         153       1.00      0.00      0.00         4
         154       0.57      1.00      0.73         4
         155       1.00      0.25      0.40         4
         156       1.00      1.00      1.00         4
         157       0.83      1.00      0.91         5
         158       1.00      0.75      0.86         4
         159       1.00      0.00      0.00         4
         160       0.40      0.40      0.40         5
         161       0.50      0.40      0.44         5
         162       0.50      0.75      0.60         4
         163       0.50      0.75      0.60         4
         164       0.00      0.00      0.00         4
         165       0.67      0.50      0.57         4
         166       0.60      0.60      0.60         5
         167       0.80      1.00      0.89         4
         168       1.00      0.50      0.67         4
         169       0.75      0.75      0.75         4
         170       1.00      1.00      1.00         5
         171       1.00      0.50      0.67         4
         172       0.33      0.75      0.46         4
         173       0.33      0.25      0.29         4
         174       1.00      1.00      1.00         5
         175       1.00      0.25      0.40         4
         176       1.00      0.50      0.67         4
         177       1.00      1.00      1.00         4
         178       0.64      0.78      0.70         9
         179       0.00      0.00      0.00         4
         180       1.00      0.89      0.94         9
         181       1.00      1.00      1.00         4
         182       1.00      0.50      0.67         4
         183       0.00      0.00      0.00         4
         184       0.80      0.80      0.80         5
         185       1.00      0.75      0.86         4
         186       1.00      0.00      0.00         4
         187       0.75      0.67      0.71         9
         188       0.89      0.89      0.89         9
         189       1.00      0.50      0.67         4
         190       0.78      0.78      0.78         9
         191       0.80      0.89      0.84         9
         192       0.80      1.00      0.89         4
         193       1.00      1.00      1.00         4
         194       0.83      1.00      0.91         5
         195       0.60      0.75      0.67         4
         196       0.00      0.00      0.00         4
         197       0.56      0.56      0.56         9
         198       0.00      0.00      0.00         4
         199       0.14      0.25      0.18         4
         200       0.67      0.22      0.33         9
         201       1.00      0.75      0.86         4
         202       1.00      0.75      0.86         4
         203       1.00      0.75      0.86         4
         204       0.89      0.89      0.89         9
         205       1.00      0.50      0.67         4
         206       0.90      1.00      0.95         9
         207       0.21      0.33      0.26         9
         208       1.00      0.50      0.67         4
         209       0.50      1.00      0.67         4
         210       0.24      0.56      0.33         9
         211       1.00      0.33      0.50         9
         212       0.67      0.50      0.57         4
         213       0.00      0.00      0.00         9
         214       1.00      1.00      1.00         4
         215       0.86      0.67      0.75         9
         216       1.00      0.50      0.67         4
         217       1.00      0.00      0.00         4
         218       0.78      0.78      0.78         9
         219       0.00      0.00      0.00         4
         220       0.40      0.50      0.44         4
         221       0.75      0.75      0.75         4
         222       1.00      0.80      0.89         5
         223       0.00      0.00      0.00         4
         224       1.00      1.00      1.00         4
         225       0.00      0.00      0.00         4
         226       1.00      0.00      0.00         4
         227       0.60      0.75      0.67         4
         228       1.00      1.00      1.00         9
         229       0.67      0.40      0.50         5
         230       0.57      1.00      0.73         4
         231       1.00      0.75      0.86         4
         232       0.29      0.50      0.36         4
         233       0.00      0.00      0.00         4
         234       0.86      0.67      0.75         9
         235       0.33      0.60      0.43         5
         236       0.50      0.25      0.33         4
         237       1.00      0.00      0.00         4
         238       1.00      1.00      1.00         4
         239       1.00      1.00      1.00         4
         240       1.00      0.75      0.86         4
         241       1.00      0.89      0.94         9
         242       0.67      0.40      0.50         5
         243       0.41      0.78      0.54         9
         244       0.75      0.60      0.67         5
         245       0.62      0.89      0.73         9
         246       1.00      0.75      0.86         4
         247       0.00      0.00      0.00         4
         248       1.00      0.75      0.86         4
         249       1.00      0.75      0.86         4
         250       0.11      0.25      0.15         4
         251       0.75      0.60      0.67         5
         252       0.80      0.80      0.80         5
         253       1.00      0.33      0.50         9
         254       0.75      0.75      0.75         4
         255       0.75      0.60      0.67         5
         256       1.00      0.00      0.00         4
         257       0.33      0.25      0.29         4
         258       0.33      0.25      0.29         4
         259       0.83      1.00      0.91         5
         260       1.00      1.00      1.00         5
         261       1.00      0.00      0.00         4
         262       1.00      1.00      1.00         4
         263       0.86      0.67      0.75         9
         264       1.00      1.00      1.00         5
         265       0.40      0.50      0.44         4
         266       0.22      0.50      0.31         4
         267       0.80      1.00      0.89         4
         268       1.00      0.75      0.86         4
         269       0.60      0.33      0.43         9
         270       0.20      0.22      0.21         9
         271       0.90      1.00      0.95         9
         272       1.00      0.89      0.94         9
         273       0.40      0.22      0.29         9
         274       1.00      0.00      0.00         4
         275       1.00      0.00      0.00         4
         276       1.00      0.75      0.86         4
         277       0.86      0.75      0.80         8
         278       1.00      1.00      1.00         4
         279       0.57      0.80      0.67         5
         280       0.44      0.44      0.44         9
         281       1.00      1.00      1.00         5
         282       0.00      0.00      0.00         4
         283       1.00      0.67      0.80         9
         284       1.00      0.50      0.67         4
         285       0.54      0.78      0.64         9
         286       1.00      0.00      0.00         4
         287       0.50      1.00      0.67         4
         288       0.50      1.00      0.67         4
         289       0.00      0.00      0.00         9
         290       1.00      0.50      0.67         4
         291       0.67      1.00      0.80         4
         292       0.75      0.75      0.75         4
         293       1.00      1.00      1.00         4
         294       0.88      0.78      0.82         9
         295       1.00      0.75      0.86         4
         296       0.71      1.00      0.83         5
         297       0.00      0.00      0.00         4
         298       0.80      1.00      0.89         4
         299       0.33      0.20      0.25         5
         300       0.62      1.00      0.77         5
         301       0.50      0.75      0.60         4
         302       0.33      0.25      0.29         4
         303       0.50      0.25      0.33         4
         304       0.75      1.00      0.86         9
         305       0.62      1.00      0.77         5
         306       0.67      0.80      0.73         5
         307       0.20      0.11      0.14         9
         308       1.00      1.00      1.00         4
         309       1.00      1.00      1.00         4
         310       0.57      0.89      0.70         9
         311       0.71      0.56      0.63         9
         312       0.60      0.75      0.67         4
         313       0.57      1.00      0.73         4
         314       0.71      0.56      0.63         9
         315       0.62      0.56      0.59         9
         316       1.00      0.50      0.67         4
         317       1.00      0.00      0.00         4
         318       0.67      1.00      0.80         4
         319       0.80      1.00      0.89         4
         320       0.83      1.00      0.91         5
         321       0.80      0.80      0.80         5
         322       1.00      0.80      0.89         5
         323       0.75      0.60      0.67         5
         324       0.00      0.00      0.00         4
         325       0.86      0.67      0.75         9
         326       1.00      0.50      0.67         4
         327       0.50      0.50      0.50         4
         328       0.67      0.40      0.50         5
         329       0.67      0.89      0.76         9
         330       1.00      0.75      0.86         4
         331       1.00      1.00      1.00         4
         332       0.75      0.75      0.75         4
         333       0.83      1.00      0.91         5
         334       1.00      1.00      1.00         4
         335       1.00      0.80      0.89         5
         336       0.89      0.89      0.89         9
         337       1.00      1.00      1.00         5
         338       0.88      0.78      0.82         9
         339       0.00      0.00      0.00         4
         340       0.00      0.00      0.00         4
         341       1.00      0.00      0.00         4
         342       0.08      0.11      0.10         9
         343       1.00      0.75      0.86         4
         344       0.30      0.33      0.32         9
         345       0.12      0.25      0.17         4
         346       0.67      1.00      0.80         4
         347       1.00      0.67      0.80         9
         348       0.40      0.50      0.44         4
         349       0.67      0.50      0.57         4
         350       0.69      1.00      0.82         9
         351       0.50      0.25      0.33         4
         352       0.80      1.00      0.89         4
         353       0.17      0.11      0.13         9
         354       1.00      1.00      1.00         9
         355       0.33      0.25      0.29         4
         356       0.57      1.00      0.73         4
         357       0.50      0.40      0.44         5
         358       1.00      1.00      1.00         4
         359       0.50      0.50      0.50         4
         360       0.33      0.25      0.29         4
         361       0.88      0.78      0.82         9
         362       0.78      0.78      0.78         9
         363       0.67      1.00      0.80         4
         364       0.80      1.00      0.89         4
         365       0.67      0.67      0.67         9
         366       0.67      0.40      0.50         5
         367       0.67      0.89      0.76         9
         368       1.00      0.00      0.00         4
         369       1.00      0.75      0.86         4
         370       0.60      0.75      0.67         4
         371       0.25      0.11      0.15         9
         372       0.00      0.00      0.00         4
         373       0.00      0.00      0.00         4
         374       0.70      0.78      0.74         9
         375       0.50      0.75      0.60         4
         376       1.00      0.75      0.86         4
         377       0.50      0.89      0.64         9
         378       1.00      0.80      0.89         5
         379       0.50      0.50      0.50         4
         380       0.67      1.00      0.80         4
         381       1.00      0.75      0.86         4
         382       0.00      0.00      0.00         4
         383       0.36      0.44      0.40         9
         384       0.33      0.20      0.25         5
         385       0.38      0.60      0.46         5
         386       0.50      0.67      0.57         9
         387       1.00      1.00      1.00         5
         388       0.00      0.00      0.00         4
         389       0.80      1.00      0.89         4
         390       0.00      0.00      0.00         5
         391       0.00      0.00      0.00         4
         392       0.62      0.89      0.73         9
         393       0.00      0.00      0.00         4
         394       1.00      0.50      0.67         4
         395       0.00      0.00      0.00         4
         396       1.00      0.75      0.86         8
         397       1.00      0.56      0.71         9
         398       0.75      0.75      0.75         4
         399       1.00      0.50      0.67         4
         400       0.86      0.67      0.75         9
         401       0.80      0.44      0.57         9
         402       0.67      0.50      0.57         4
         403       0.86      0.67      0.75         9
         404       0.67      1.00      0.80         4
         405       0.50      0.40      0.44         5
         406       0.71      1.00      0.83         5
         407       0.00      0.00      0.00         4
         408       0.67      1.00      0.80         4
         409       0.00      0.00      0.00         4
         410       0.90      1.00      0.95         9
         411       1.00      1.00      1.00         5
         412       0.73      0.89      0.80         9
         413       0.75      0.75      0.75         4
         414       0.07      0.11      0.09         9
         415       0.00      0.00      0.00         4
         416       0.67      0.40      0.50         5
         417       1.00      0.75      0.86         4
         418       0.75      0.75      0.75         4
         419       1.00      0.44      0.62         9
         420       0.50      0.50      0.50         4
         421       1.00      1.00      1.00         4
         422       0.50      1.00      0.67         4
         423       0.00      0.00      0.00         4
         424       1.00      0.89      0.94         9
         425       1.00      1.00      1.00         4
         426       0.67      0.50      0.57         4
         427       1.00      0.78      0.88         9
         428       0.67      1.00      0.80         4
         429       1.00      0.50      0.67         4
         430       0.43      0.75      0.55         4
         431       0.75      0.75      0.75         4
         432       0.33      0.50      0.40         4
         433       0.12      0.20      0.15         5
         434       0.75      0.75      0.75         4
         435       0.00      0.00      0.00         9
         436       1.00      0.80      0.89         5
         437       0.33      0.25      0.29         4
         438       0.57      0.80      0.67         5
         439       1.00      1.00      1.00         4
         440       0.30      0.33      0.32         9
         441       1.00      0.60      0.75         5
         442       0.00      0.00      0.00         4
         443       0.60      1.00      0.75         9
         444       0.64      1.00      0.78         9
         445       0.80      0.80      0.80         5
         446       1.00      0.50      0.67         4
         447       0.80      1.00      0.89         4
         448       1.00      0.25      0.40         4
         449       0.88      0.78      0.82         9
         450       0.62      0.56      0.59         9
         451       0.83      1.00      0.91         5
         452       0.55      0.67      0.60         9
         453       1.00      0.75      0.86         4
         454       0.60      0.75      0.67         4
         455       1.00      0.75      0.86         4
         456       0.71      0.56      0.63         9
         457       0.50      0.44      0.47         9
         458       0.90      1.00      0.95         9
         459       0.75      0.75      0.75         4
         460       0.36      0.44      0.40         9
         461       0.50      0.75      0.60         4
         462       0.80      1.00      0.89         4
         463       1.00      1.00      1.00         4
         464       1.00      0.75      0.86         4
         465       0.67      0.89      0.76         9
         466       1.00      0.75      0.86         4
         467       0.67      0.50      0.57         4
         468       1.00      0.25      0.40         4
         469       1.00      0.00      0.00         4
         470       0.00      0.00      0.00         4
         471       0.50      0.20      0.29         5
         472       0.90      1.00      0.95         9
         473       1.00      0.00      0.00         4
         474       0.90      1.00      0.95         9
         475       0.25      0.50      0.33         4
         476       0.71      0.56      0.63         9
         477       0.56      1.00      0.71         5
         478       0.89      0.89      0.89         9
         479       1.00      0.00      0.00         4
         480       0.00      0.00      0.00         9
         481       0.50      0.89      0.64         9
         482       0.70      0.78      0.74         9
         483       0.00      0.00      0.00         4
         484       0.50      0.75      0.60         4
         485       0.90      1.00      0.95         9
         486       1.00      0.75      0.86         4
         487       0.80      0.80      0.80         5
         488       0.00      0.00      0.00         9
         489       1.00      0.75      0.86         4
         490       0.00      0.00      0.00         4
         491       1.00      0.00      0.00         4
         492       0.67      1.00      0.80         4
         493       1.00      1.00      1.00         4
         494       0.67      0.50      0.57         4
         495       0.40      0.50      0.44         4
         496       0.75      0.75      0.75         4
         497       1.00      0.89      0.94         9
         498       1.00      1.00      1.00         4
         499       1.00      0.89      0.94         9
         500       0.57      1.00      0.73         4
         501       0.67      1.00      0.80         4
         502       0.50      0.75      0.60         4
         503       0.78      0.78      0.78         9
         504       0.67      0.80      0.73         5
         505       1.00      0.75      0.86         4
         506       0.80      1.00      0.89         4
         507       0.80      1.00      0.89         4
         508       0.80      1.00      0.89         4
         509       0.67      1.00      0.80         4
         510       0.71      1.00      0.83         5
         511       0.83      1.00      0.91         5
         512       0.57      0.44      0.50         9
         513       0.11      0.50      0.18         4
         514       0.33      0.50      0.40         4
         515       1.00      1.00      1.00         4
         516       0.90      1.00      0.95         9
         517       0.50      0.22      0.31         9
         518       1.00      1.00      1.00         4
         519       0.00      0.00      0.00         4
         520       1.00      0.50      0.67         4
         521       0.50      0.50      0.50         4
         522       0.00      0.00      0.00         4
         523       0.25      0.20      0.22         5
         524       1.00      0.50      0.67         4
         525       0.75      0.75      0.75         4
         526       1.00      0.60      0.75         5
         527       0.00      0.00      0.00         4
         528       0.27      0.33      0.30         9
         529       0.40      0.50      0.44         4
         530       1.00      0.56      0.71         9
         531       0.50      0.60      0.55         5
         532       0.75      0.60      0.67         5
         533       0.82      1.00      0.90         9
         534       0.00      0.00      0.00         4
         535       0.67      0.40      0.50         5
         536       0.75      0.75      0.75         4
         537       0.60      0.75      0.67         4
         538       0.00      0.00      0.00         4
         539       0.60      0.75      0.67         4
         540       0.75      0.75      0.75         4
         541       1.00      1.00      1.00         4
         542       0.83      1.00      0.91         5
         543       1.00      1.00      1.00         4
         544       0.80      1.00      0.89         4
         545       0.12      0.22      0.16         9
         546       0.80      1.00      0.89         4
         547       0.80      1.00      0.89         4
         548       1.00      0.33      0.50         9
         549       0.67      0.89      0.76         9
         550       0.64      1.00      0.78         9
         551       0.89      0.89      0.89         9
         552       1.00      0.00      0.00         4
         553       1.00      0.75      0.86         4
         554       1.00      0.75      0.86         4
         555       0.64      0.78      0.70         9
         556       0.50      1.00      0.67         4
         557       0.67      1.00      0.80         4
         558       0.50      0.25      0.33         4
         559       1.00      0.89      0.94         9
         560       0.90      1.00      0.95         9
         561       0.40      0.50      0.44         4
         562       0.67      1.00      0.80         4
         563       1.00      0.75      0.86         4
         564       0.83      1.00      0.91         5
         565       0.60      0.75      0.67         4
         566       0.00      0.00      0.00         4
         567       0.73      0.89      0.80         9
         568       0.00      0.00      0.00         4
         569       0.80      0.89      0.84         9
         570       1.00      0.75      0.86         4
         571       1.00      0.40      0.57         5
         572       0.46      0.67      0.55         9
         573       0.25      0.11      0.15         9
         574       0.33      1.00      0.50         4
         575       0.67      0.44      0.53         9
         576       0.67      1.00      0.80         4
         577       0.50      0.50      0.50         4
         578       1.00      1.00      1.00         4
         579       0.75      0.67      0.71         9
         580       1.00      0.00      0.00         4
         581       0.33      0.33      0.33         9
         582       0.50      0.75      0.60         4
         583       0.67      1.00      0.80         4
         584       0.40      0.67      0.50         9
         585       1.00      0.75      0.86         4
         586       1.00      0.75      0.86         4
         587       0.16      0.75      0.26         4
         588       0.80      0.89      0.84         9
         589       0.00      0.00      0.00         4
         590       0.60      0.75      0.67         4
         591       0.30      0.75      0.43         4
         592       0.00      0.00      0.00         4
         593       0.75      0.67      0.71         9
         594       0.46      0.67      0.55         9
         595       0.89      0.89      0.89         9
         596       0.40      0.50      0.44         4
         597       0.00      0.00      0.00         4
         598       0.50      0.75      0.60         4
         599       0.40      0.22      0.29         9
         600       1.00      0.75      0.86         4
         601       0.78      0.78      0.78         9
         602       0.00      0.00      0.00         4
         603       0.18      0.22      0.20         9
         604       0.80      1.00      0.89         4
         605       0.45      0.56      0.50         9
         606       0.33      0.50      0.40         4
         607       0.60      0.75      0.67         4
         608       0.70      0.78      0.74         9
         609       0.80      1.00      0.89         4
         610       0.80      1.00      0.89         4
         611       1.00      0.80      0.89         5
         612       0.67      0.50      0.57         4
         613       0.71      1.00      0.83         5
         614       1.00      0.78      0.88         9
         615       0.75      0.60      0.67         5
         616       1.00      0.75      0.86         4
         617       0.67      0.50      0.57         4
         618       0.50      0.75      0.60         4
         619       0.80      1.00      0.89         4
         620       1.00      0.67      0.80         9
         621       1.00      0.50      0.67         4
         622       0.83      1.00      0.91         5
         623       0.10      0.11      0.11         9
         624       0.00      0.00      0.00         5
         625       1.00      1.00      1.00         4
         626       0.33      0.50      0.40         4
         627       0.11      0.50      0.18         4
         628       0.60      0.75      0.67         4
         629       0.75      0.60      0.67         5
         630       0.00      0.00      0.00         4
         631       0.75      0.75      0.75         4
         632       0.50      0.25      0.33         4
         633       0.57      1.00      0.73         4
         634       0.60      0.67      0.63         9
         635       1.00      0.75      0.86         4
         636       0.09      0.25      0.13         4
         637       0.67      1.00      0.80         4
         638       1.00      0.50      0.67         4
         639       0.33      0.22      0.27         9
         640       0.67      0.89      0.76         9
         641       0.50      0.78      0.61         9
         642       0.67      0.50      0.57         4
         643       0.12      0.20      0.15         5
         644       0.70      0.78      0.74         9
         645       1.00      1.00      1.00         4
         646       0.00      0.00      0.00         4
         647       0.80      1.00      0.89         4
         648       1.00      1.00      1.00         4
         649       0.83      1.00      0.91         5
         650       1.00      0.00      0.00         4
         651       0.80      0.89      0.84         9
         652       0.33      0.33      0.33         9
         653       0.67      0.40      0.50         5
         654       0.80      0.89      0.84         9
         655       0.67      1.00      0.80         4
         656       0.71      1.00      0.83         5
         657       1.00      1.00      1.00         4
         658       0.67      1.00      0.80         4
         659       0.50      0.50      0.50         4
         660       0.00      0.00      0.00         4
         661       0.67      0.50      0.57         4
         662       0.50      0.67      0.57         9
         663       0.75      0.75      0.75         4
         664       1.00      0.50      0.67         4
         665       0.69      1.00      0.82         9
         666       0.67      0.67      0.67         9
         667       1.00      0.50      0.67         4
         668       0.80      0.89      0.84         9
         669       0.71      1.00      0.83         5
         670       0.50      0.25      0.33         4
         671       0.67      1.00      0.80         4
         672       0.00      0.00      0.00         4
         673       0.00      0.00      0.00         4
         674       0.88      0.78      0.82         9
         675       0.14      0.25      0.18         4
         676       0.17      0.25      0.20         4
         677       0.60      0.67      0.63         9
         678       0.80      0.89      0.84         9
         679       0.73      0.89      0.80         9
         680       0.67      0.89      0.76         9
         681       1.00      0.50      0.67         4
         682       0.75      0.60      0.67         5
         683       0.67      1.00      0.80         4
         684       1.00      0.25      0.40         4
         685       0.08      0.22      0.12         9
         686       0.33      0.50      0.40         4
         687       0.83      1.00      0.91         5
         688       0.75      0.75      0.75         4
         689       1.00      1.00      1.00         4
         690       0.50      0.25      0.33         4
         691       0.50      0.75      0.60         4
         692       0.58      0.78      0.67         9
         693       0.00      0.00      0.00         4
         694       0.33      0.75      0.46         4
         695       0.80      1.00      0.89         4
         696       0.00      0.00      0.00         4
         697       0.00      0.00      0.00         4
         698       0.71      1.00      0.83         5
         699       0.57      1.00      0.73         4
         700       0.17      0.75      0.27         4
         701       0.09      0.25      0.13         4
         702       0.67      0.22      0.33         9
         703       1.00      1.00      1.00         4
         704       0.71      1.00      0.83         5
         705       0.50      0.75      0.60         4
         706       0.67      0.80      0.73         5
         707       1.00      0.75      0.86         4
         708       0.30      0.75      0.43         4
         709       0.05      0.25      0.09         4
         710       0.67      1.00      0.80         4
         711       0.44      0.78      0.56         9
         712       1.00      0.78      0.88         9
         713       0.62      1.00      0.77         5
         714       0.67      0.50      0.57         4
         715       0.75      0.67      0.71         9
         716       0.89      0.89      0.89         9
         717       1.00      0.75      0.86         4
         718       0.80      0.89      0.84         9
         719       0.25      0.25      0.25         4
         720       0.57      0.44      0.50         9
         721       0.43      0.75      0.55         4
         722       0.50      0.20      0.29         5
         723       0.80      1.00      0.89         4
         724       1.00      0.00      0.00         4
         725       1.00      0.75      0.86         4
         726       1.00      1.00      1.00         4
         727       1.00      0.00      0.00         4
         728       0.00      0.00      0.00         4
         729       0.80      1.00      0.89         4
         730       1.00      1.00      1.00         5
         731       0.57      0.80      0.67         5
         732       0.50      0.20      0.29         5
         733       1.00      0.50      0.67         4
         734       1.00      0.75      0.86         4
         735       0.60      0.75      0.67         4
         736       0.43      0.60      0.50         5
         737       0.57      0.80      0.67         5
         738       0.62      1.00      0.77         5
         739       0.54      0.78      0.64         9
         740       0.67      0.44      0.53         9
         741       0.50      0.80      0.62         5
         742       0.75      0.75      0.75         4
         743       1.00      0.67      0.80         9
         744       0.67      0.50      0.57         4
         745       0.50      0.50      0.50         4
         746       0.30      0.75      0.43         4
         747       0.69      1.00      0.82         9
         748       0.67      0.80      0.73         5
         749       0.00      0.00      0.00         4
         750       0.78      0.78      0.78         9
         751       1.00      1.00      1.00         4
         752       0.50      0.50      0.50         4
         753       0.62      0.89      0.73         9
         754       0.88      0.78      0.82         9
         755       1.00      1.00      1.00         4
         756       0.89      0.89      0.89         9
         757       0.82      1.00      0.90         9
         758       0.75      0.33      0.46         9
         759       0.75      0.60      0.67         5
         760       0.50      0.67      0.57         9
         761       0.80      0.89      0.84         9
         762       0.25      0.33      0.29         9
         763       1.00      1.00      1.00         5
         764       1.00      0.00      0.00         4
         765       1.00      0.75      0.86         4
         766       1.00      1.00      1.00         4
         767       1.00      0.50      0.67         4
         768       0.33      0.50      0.40         4
         769       0.67      1.00      0.80         4
         770       0.67      0.44      0.53         9
         771       1.00      0.00      0.00         4
         772       1.00      0.25      0.40         4
         773       0.60      0.75      0.67         4
         774       0.20      0.25      0.22         4
         775       0.54      0.78      0.64         9
         776       0.75      0.75      0.75         4
         777       0.30      0.33      0.32         9
         778       0.43      0.75      0.55         4
         779       0.00      0.00      0.00         4
         780       0.82      1.00      0.90         9
         781       0.50      1.00      0.67         4
         782       1.00      1.00      1.00         5
         783       0.90      1.00      0.95         9
         784       1.00      1.00      1.00         5
         785       1.00      0.00      0.00         4
         786       1.00      0.25      0.40         4
         787       1.00      0.75      0.86         4
         788       0.83      0.56      0.67         9
         789       1.00      0.60      0.75         5
         790       1.00      0.80      0.89         5
         791       0.00      0.00      0.00         4
         792       0.40      0.22      0.29         9
         793       1.00      0.25      0.40         4
         794       1.00      0.75      0.86         4
         795       0.67      0.86      0.75         7
         796       1.00      0.00      0.00         5
         797       0.50      0.25      0.33         4
         798       0.73      0.89      0.80         9
         799       0.67      0.29      0.40         7
         800       1.00      0.25      0.40         4
         801       0.50      0.60      0.55         5
         802       1.00      1.00      1.00         4
         803       1.00      0.00      0.00         4
         804       1.00      0.00      0.00         4
         805       1.00      0.00      0.00         4
         806       1.00      0.75      0.86         4
         807       0.67      0.80      0.73         5
         808       0.36      0.56      0.43         9
         809       0.57      0.44      0.50         9
         810       0.78      0.78      0.78         9
         811       0.57      1.00      0.73         4
         812       0.00      0.00      0.00         4
         813       0.89      0.89      0.89         9
         814       0.54      0.78      0.64         9
         815       0.33      0.50      0.40         4
         816       0.75      0.75      0.75         4
         817       1.00      1.00      1.00         4
         818       1.00      0.75      0.86         4
         819       0.67      0.67      0.67         9
         820       0.00      0.00      0.00         4
         821       0.57      0.80      0.67         5
         822       0.67      0.44      0.53         9
         823       1.00      1.00      1.00         4
         824       0.64      0.78      0.70         9
         825       0.00      0.00      0.00         4
         826       0.88      0.78      0.82         9
         827       1.00      0.00      0.00         4
         828       1.00      0.80      0.89         5
         829       0.25      0.33      0.29         9
         830       0.00      0.00      0.00         4
         831       1.00      0.00      0.00         4
         832       1.00      0.60      0.75         5
         833       0.60      0.75      0.67         4
         834       0.25      0.50      0.33         4
         835       0.67      0.50      0.57         4
         836       0.45      0.56      0.50         9
         837       0.75      1.00      0.86         9
         838       1.00      1.00      1.00         4
         839       1.00      0.89      0.94         9
         840       0.44      0.80      0.57         5
         841       1.00      0.75      0.86         4
         842       1.00      0.75      0.86         4
         843       0.00      0.00      0.00         9
         844       0.13      0.75      0.22         4
         845       0.57      0.89      0.70         9
         846       0.60      0.75      0.67         4
         847       0.20      0.22      0.21         9
         848       0.82      1.00      0.90         9
         849       0.75      1.00      0.86         9
         850       1.00      0.00      0.00         4
         851       1.00      1.00      1.00         4
         852       0.08      0.50      0.13         4
         853       1.00      0.80      0.89         5
         854       0.33      0.50      0.40         4
         855       0.50      0.25      0.33         4
         856       0.75      0.75      0.75         4
         857       0.67      0.40      0.50         5
         858       0.40      0.40      0.40         5
         859       0.50      0.50      0.50         4
         860       0.40      0.50      0.44         4
         861       0.50      1.00      0.67         5
         862       0.20      0.25      0.22         4
         863       0.00      0.00      0.00         4
         864       0.53      0.89      0.67         9
         865       0.50      0.20      0.29         5
         866       1.00      0.75      0.86         4
         867       0.67      0.50      0.57         4
         868       0.75      0.67      0.71         9
         869       0.50      0.50      0.50         4
         870       0.75      1.00      0.86         9
         871       1.00      0.00      0.00         4
         872       0.44      1.00      0.62         4
         873       1.00      0.75      0.86         4
         874       0.50      1.00      0.67         5
         875       0.54      0.78      0.64         9
         876       0.43      0.75      0.55         4
         877       0.53      0.89      0.67         9
         878       0.67      1.00      0.80         4
         879       0.50      0.25      0.33         4
         880       0.00      0.00      0.00         4
         881       0.44      1.00      0.62         4
         882       0.83      1.00      0.91         5
         883       0.29      0.80      0.42         5
         884       0.50      0.50      0.50         4
         885       0.00      0.00      0.00         4
         886       0.80      0.80      0.80         5
         887       1.00      0.75      0.86         4
         888       1.00      1.00      1.00         5
         889       0.11      0.25      0.15         4
         890       1.00      1.00      1.00         5
         891       0.50      0.50      0.50         4
         892       0.83      1.00      0.91         5
         893       0.80      0.80      0.80         5

    accuracy                           0.61      4917
   macro avg       0.67      0.59      0.58      4917
weighted avg       0.67      0.61      0.60      4917

