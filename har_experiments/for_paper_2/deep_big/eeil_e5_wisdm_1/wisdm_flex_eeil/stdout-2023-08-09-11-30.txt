	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [589, 185, 811, 478, 761, 673, 170, 328, 600, 552, 244, 83, 482, 603, 112, 410, 68, 6, 557, 775, 467, 232, 782, 763, 774, 736, 494, 527, 624, 807, 270, 172, 25, 378, 115, 98, 777, 46, 474, 348, 91, 447, 610, 400, 250, 689, 53, 43, 32, 121, 860, 781, 258, 165, 657, 532, 772, 313, 719, 685, 615, 765, 107, 206, 636, 329, 820, 854, 175, 744, 338, 192, 738, 227, 74, 321, 235, 11, 50, 824, 776, 857, 305, 90, 472, 188, 376, 888, 509, 841, 879, 740, 443, 55, 812, 444, 87, 373, 334, 523, 162, 588, 86, 451, 533, 92, 499, 221, 755, 481, 152, 886, 352, 773, 276, 30, 108, 666, 139, 9, 344, 285, 614, 726, 662, 298, 862, 704, 861, 360, 845, 218, 658, 364, 758, 850, 796, 752, 0, 741, 700, 76, 621, 674, 247, 395, 737, 296, 815, 489, 37, 842, 174, 357, 827, 797, 491, 130, 255, 265, 256, 116, 160, 703, 751, 138, 167, 146, 476, 339, 723, 157, 381, 712, 475, 274, 448, 100, 754, 613, 303, 26, 605, 40, 619, 631, 171, 865, 159, 186, 242, 679, 426, 263, 391, 21, 502, 455, 722, 325, 249, 651, 484, 318, 386, 406, 209, 606, 271, 526, 158, 317, 779, 189, 609, 798, 792, 823, 883, 473, 844, 511, 628, 122, 392, 659, 385, 745, 377, 220, 423, 110, 681, 394, 541, 583, 505, 294, 106, 44, 95, 273, 668, 522, 594, 616, 421, 506, 548, 411, 563, 669, 519, 653, 13, 534, 457, 529, 757, 770, 168, 710, 266, 124, 579, 210, 734, 314, 401, 300, 164, 780, 343, 287, 869, 462, 809, 859, 762, 507, 460, 12, 465, 129, 559, 684, 692, 555, 149, 262, 415, 264, 114, 402, 500, 874, 251, 281, 253, 544, 446, 884, 843, 487, 291, 350, 73, 223, 768, 316, 711, 109, 577, 38, 783, 161, 834, 198, 458, 694, 239, 561, 531, 347, 872, 382, 816, 632, 96, 495, 302, 63, 434, 241, 767, 599, 591, 521, 48, 539, 89, 23, 537, 5, 756, 881, 202, 75, 22, 483, 362, 732, 193, 477, 840, 530, 675, 602, 671, 422, 408, 10, 389, 696, 27, 875, 144, 863, 661, 430, 54, 562, 309, 333, 47, 804, 297, 340, 715, 259, 230, 65, 721, 492, 437, 788, 20, 219, 542, 892, 187, 369, 699, 379, 58, 335, 113, 51, 716, 181, 718, 211, 452, 290, 766, 717, 829, 838, 733, 626, 496, 194, 428, 764, 310, 601, 638, 490, 686, 890, 639, 403, 795, 503, 463, 630, 224, 806, 356, 445, 35, 746, 57, 147, 549, 784, 257, 66, 515, 397, 593, 307, 358, 228, 802, 622, 440, 743, 750, 324, 587, 634, 330, 647, 760, 248, 546, 190, 103, 540, 1, 427, 876, 573, 143, 260, 826, 195, 655, 7, 52, 810, 893, 849, 101, 225, 848, 641, 508, 80, 543, 867, 268, 678, 93, 597, 498, 29, 163, 637, 571, 604, 514, 284, 19, 319, 81, 346, 536, 633, 77, 787, 370, 15, 207, 295, 173, 880, 454, 618, 42, 650, 695, 384, 14, 31, 590, 572, 488, 598, 166, 306, 665, 286, 45, 320, 805, 550, 683, 140, 361, 518, 134, 513, 510, 416, 808, 617, 69, 471, 714, 18, 864, 213, 877, 620, 420, 524, 580, 644, 520, 642, 688, 200, 528, 71, 136, 581, 24, 566, 366, 269, 345, 418, 835, 545, 177, 33, 800, 847, 697, 425, 813, 120, 803, 393, 280, 331, 204, 132, 660, 794, 282, 238, 436, 308, 608, 771, 574, 199, 388, 398, 891, 627, 304, 873, 887, 485, 145, 312, 41, 117, 390, 2, 125, 486, 833, 461, 575, 565, 611, 852, 435, 747, 182, 374, 293, 885, 640, 822, 560, 464, 419, 584, 72, 698, 283, 131, 786, 512, 821, 49, 439, 817, 569, 246, 709, 493, 470, 450, 882, 830, 538, 468, 556, 625, 99, 61, 371, 278, 649, 730, 56, 878, 323, 676, 105, 596, 693, 727, 667, 342, 237, 424, 856, 405, 236, 828, 680, 178, 341, 753, 670, 720, 432, 702, 353, 404, 128, 568, 153, 831, 635, 858, 133, 497, 687, 222, 359, 97, 196, 868, 254, 819, 479, 169, 554, 456, 142, 853, 272, 148, 870, 279, 407, 582, 299, 292, 839, 123, 17, 234, 585, 504, 646, 212, 301, 725, 595, 387, 607, 365, 429, 36, 438, 78, 501, 567, 179, 119, 656, 85, 4, 94, 233, 866, 154, 176, 846, 799, 375, 327, 691, 180, 245, 275, 412, 570, 742, 648, 453, 706, 243, 216, 643, 34, 517, 67, 150, 372, 801, 480, 442, 836, 267, 769, 623, 739, 414, 383, 677, 252, 629, 8, 785, 576, 690, 102, 790, 337, 789, 708, 64, 289, 814, 226, 326, 871, 466, 535, 261, 469, 793, 214, 28, 197, 127, 156, 104, 759, 368, 332, 240, 731, 664, 778, 431, 62, 735, 336, 351, 201, 349, 229, 791, 578, 654, 39, 208, 728, 183, 409, 558, 889, 3, 231, 137, 88, 748, 417, 837, 749, 84, 354, 363, 191, 551, 70, 82, 79, 682, 203, 118, 16, 217, 713, 459, 215, 516, 729, 433, 705, 547, 355, 612, 586, 277, 126, 184, 141, 315, 205, 288, 525, 592, 60, 553, 724, 311, 151, 396, 441, 399, 380, 322, 645, 672, 652, 825, 564, 832, 367, 111, 663, 707, 851, 135, 59, 449, 701, 413, 818, 855, 155]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.944, TAw acc= 30.3% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.554, TAw acc= 43.2% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.242, TAw acc= 48.5% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.992, TAw acc= 56.8% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.794, TAw acc= 62.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 264 train exemplars, time=  0.0s
264
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.580 | TAw acc= 71.7%, forg=  0.0%| TAg acc= 71.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.981, TAw acc= 38.4% | *
| Epoch   2, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.425, TAw acc= 69.8% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.045, TAw acc= 75.6% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.754, TAw acc= 80.2% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.532, TAw acc= 80.2% | *
| Epoch   1, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.530, TAw acc= 80.2% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.529, TAw acc= 80.2% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.528, TAw acc= 80.2% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.527, TAw acc= 80.2% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.526, TAw acc= 80.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 404 train exemplars, time=  0.0s
404
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.654 | TAw acc= 87.8%, forg=-16.1%| TAg acc= 77.8%, forg= -6.1% <<<
>>> Test on task  1 : loss=1.432 | TAw acc= 80.9%, forg=  0.0%| TAg acc= 76.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.779, TAw acc= 39.1% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.033, TAw acc= 54.0% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.472, TAw acc= 65.5% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.072, TAw acc= 75.9% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.774, TAw acc= 80.5% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.771, TAw acc= 80.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.769, TAw acc= 80.5% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.766, TAw acc= 80.5% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.763, TAw acc= 80.5% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.761, TAw acc= 80.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 544 train exemplars, time=  0.0s
544
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.542 | TAw acc= 86.7%, forg=  1.1%| TAg acc= 72.2%, forg=  5.6% <<<
>>> Test on task  1 : loss=1.577 | TAw acc= 93.0%, forg=-12.2%| TAg acc= 77.4%, forg= -0.9% <<<
>>> Test on task  2 : loss=1.682 | TAw acc= 81.0%, forg=  0.0%| TAg acc= 69.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.038, TAw acc= 40.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.311, TAw acc= 54.8% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.891, TAw acc= 63.1% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.574, TAw acc= 77.4% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.360, TAw acc= 94.0% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.358, TAw acc= 94.0% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.356, TAw acc= 94.0% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.355, TAw acc= 94.0% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.353, TAw acc= 94.0% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.351, TAw acc= 94.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 684 train exemplars, time=  0.0s
684
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.414 | TAw acc= 88.9%, forg= -1.1%| TAg acc= 82.2%, forg= -4.4% <<<
>>> Test on task  1 : loss=1.513 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 80.9%, forg= -3.5% <<<
>>> Test on task  2 : loss=1.757 | TAw acc= 93.1%, forg=-12.1%| TAg acc= 61.2%, forg=  7.8% <<<
>>> Test on task  3 : loss=1.276 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 72.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.567, TAw acc= 40.7% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.648, TAw acc= 51.9% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.122, TAw acc= 67.9% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.819, TAw acc= 81.5% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.593, TAw acc= 85.2% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.590, TAw acc= 85.2% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.587, TAw acc= 85.2% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.584, TAw acc= 85.2% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.581, TAw acc= 85.2% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.578, TAw acc= 85.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 824 train exemplars, time=  0.0s
824
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.297 | TAw acc= 90.0%, forg= -1.1%| TAg acc= 81.7%, forg=  0.6% <<<
>>> Test on task  1 : loss=1.366 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 86.1%, forg= -5.2% <<<
>>> Test on task  2 : loss=1.657 | TAw acc= 96.6%, forg= -3.4%| TAg acc= 59.5%, forg=  9.5% <<<
>>> Test on task  3 : loss=1.385 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 77.0%, forg= -4.4% <<<
>>> Test on task  4 : loss=1.570 | TAw acc= 88.1%, forg=  0.0%| TAg acc= 69.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.251, TAw acc= 53.2% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.115, TAw acc= 61.0% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.477, TAw acc= 79.2% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.101, TAw acc= 89.6% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.847, TAw acc= 89.6% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.845, TAw acc= 89.6% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.842, TAw acc= 89.6% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.840, TAw acc= 89.6% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.838, TAw acc= 89.6% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.836, TAw acc= 89.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 964 train exemplars, time=  0.0s
964
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.222 | TAw acc= 92.2%, forg= -2.2%| TAg acc= 85.6%, forg= -3.3% <<<
>>> Test on task  1 : loss=1.275 | TAw acc= 95.7%, forg= -2.6%| TAg acc= 84.3%, forg=  1.7% <<<
>>> Test on task  2 : loss=1.579 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 62.1%, forg=  6.9% <<<
>>> Test on task  3 : loss=1.170 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 83.2%, forg= -6.2% <<<
>>> Test on task  4 : loss=1.645 | TAw acc= 96.3%, forg= -8.3%| TAg acc= 62.4%, forg=  7.3% <<<
>>> Test on task  5 : loss=1.887 | TAw acc= 88.6%, forg=  0.0%| TAg acc= 58.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.203, TAw acc= 39.6% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.001, TAw acc= 60.4% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=2.269, TAw acc= 79.1% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.777, TAw acc= 87.9% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.491, TAw acc= 93.4% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.488, TAw acc= 93.4% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.485, TAw acc= 93.4% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.482, TAw acc= 93.4% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.479, TAw acc= 93.4% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.477, TAw acc= 93.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1104 train exemplars, time=  0.0s
1104
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.172 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 84.4%, forg=  1.1% <<<
>>> Test on task  1 : loss=1.261 | TAw acc= 93.9%, forg=  1.7%| TAg acc= 83.5%, forg=  2.6% <<<
>>> Test on task  2 : loss=1.480 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 66.4%, forg=  2.6% <<<
>>> Test on task  3 : loss=1.162 | TAw acc= 93.8%, forg=  2.7%| TAg acc= 74.3%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.557 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 66.1%, forg=  3.7% <<<
>>> Test on task  5 : loss=1.952 | TAw acc= 94.3%, forg= -5.7%| TAg acc= 54.3%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.495 | TAw acc= 92.6%, forg=  0.0%| TAg acc= 72.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.863, TAw acc= 47.9% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.496, TAw acc= 50.7% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.718, TAw acc= 64.4% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.209, TAw acc= 76.7% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.906, TAw acc= 80.8% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.904, TAw acc= 80.8% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.901, TAw acc= 80.8% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.899, TAw acc= 80.8% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.896, TAw acc= 80.8% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.894, TAw acc= 80.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1244 train exemplars, time=  0.0s
1244
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.208 | TAw acc= 91.1%, forg=  1.1%| TAg acc= 81.7%, forg=  3.9% <<<
>>> Test on task  1 : loss=1.199 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 81.7%, forg=  4.3% <<<
>>> Test on task  2 : loss=1.334 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 76.7%, forg= -7.8% <<<
>>> Test on task  3 : loss=0.990 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 83.2%, forg=  0.0% <<<
>>> Test on task  4 : loss=1.387 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 73.4%, forg= -3.7% <<<
>>> Test on task  5 : loss=1.678 | TAw acc= 92.4%, forg=  1.9%| TAg acc= 67.6%, forg= -9.5% <<<
>>> Test on task  6 : loss=1.683 | TAw acc= 95.9%, forg= -3.3%| TAg acc= 61.5%, forg= 10.7% <<<
>>> Test on task  7 : loss=1.914 | TAw acc= 82.2%, forg=  0.0%| TAg acc= 53.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.668, TAw acc= 55.8% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.547, TAw acc= 60.5% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.022, TAw acc= 68.6% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.759, TAw acc= 88.4% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.537, TAw acc= 90.7% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.536, TAw acc= 90.7% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.534, TAw acc= 90.7% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.533, TAw acc= 90.7% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.532, TAw acc= 91.9% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.530, TAw acc= 91.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1384 train exemplars, time=  0.0s
1384
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.153 | TAw acc= 95.6%, forg= -3.3%| TAg acc= 85.6%, forg=  0.0% <<<
>>> Test on task  1 : loss=1.263 | TAw acc= 93.0%, forg=  3.5%| TAg acc= 74.8%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.295 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task  3 : loss=0.927 | TAw acc= 93.8%, forg=  2.7%| TAg acc= 84.1%, forg= -0.9% <<<
>>> Test on task  4 : loss=1.450 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 67.0%, forg=  6.4% <<<
>>> Test on task  5 : loss=1.551 | TAw acc= 95.2%, forg= -1.0%| TAg acc= 63.8%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.562 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 66.4%, forg=  5.7% <<<
>>> Test on task  7 : loss=2.040 | TAw acc= 92.1%, forg= -9.9%| TAg acc= 22.8%, forg= 30.7% <<<
>>> Test on task  8 : loss=1.477 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 67.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=5.109, TAw acc= 38.0% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.757, TAw acc= 43.7% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.107, TAw acc= 60.6% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.645, TAw acc= 71.8% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.318, TAw acc= 76.1% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.316, TAw acc= 76.1% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.313, TAw acc= 76.1% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.310, TAw acc= 76.1% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.308, TAw acc= 76.1% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.305, TAw acc= 76.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1524 train exemplars, time=  0.0s
1524
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.141 | TAw acc= 94.4%, forg=  1.1%| TAg acc= 84.4%, forg=  1.1% <<<
>>> Test on task  1 : loss=1.123 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 82.6%, forg=  3.5% <<<
>>> Test on task  2 : loss=1.311 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.0%, forg=  7.8% <<<
>>> Test on task  3 : loss=0.878 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 84.1%, forg=  0.0% <<<
>>> Test on task  4 : loss=1.307 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 77.1%, forg= -3.7% <<<
>>> Test on task  5 : loss=1.381 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 79.0%, forg=-11.4% <<<
>>> Test on task  6 : loss=1.401 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 69.7%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.829 | TAw acc= 90.1%, forg=  2.0%| TAg acc= 47.5%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.715 | TAw acc= 98.3%, forg= -5.2%| TAg acc= 60.0%, forg=  7.0% <<<
>>> Test on task  9 : loss=2.070 | TAw acc= 84.7%, forg=  0.0%| TAg acc= 48.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=4.312, TAw acc= 46.4% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.784, TAw acc= 58.3% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.976, TAw acc= 82.1% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.538, TAw acc= 92.9% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.393, TAw acc= 94.0% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.390, TAw acc= 94.0% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.388, TAw acc= 94.0% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.386, TAw acc= 94.0% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.384, TAw acc= 94.0% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.381, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1664 train exemplars, time=  0.0s
1664
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.142 | TAw acc= 95.0%, forg=  0.6%| TAg acc= 87.2%, forg= -1.7% <<<
>>> Test on task  1 : loss=1.189 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 78.3%, forg=  7.8% <<<
>>> Test on task  2 : loss=1.250 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 74.1%, forg=  2.6% <<<
>>> Test on task  3 : loss=0.901 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 82.3%, forg=  1.8% <<<
>>> Test on task  4 : loss=1.229 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 81.7%, forg= -4.6% <<<
>>> Test on task  5 : loss=1.416 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 66.7%, forg= 12.4% <<<
>>> Test on task  6 : loss=1.267 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.0%, forg= -0.8% <<<
>>> Test on task  7 : loss=1.749 | TAw acc= 92.1%, forg=  0.0%| TAg acc= 53.5%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.585 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.4%, forg= -3.5% <<<
>>> Test on task  9 : loss=2.050 | TAw acc= 91.8%, forg= -7.1%| TAg acc= 36.7%, forg= 11.2% <<<
>>> Test on task 10 : loss=1.331 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 73.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=4.539, TAw acc= 60.0% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.770, TAw acc= 65.9% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.122, TAw acc= 72.9% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.707, TAw acc= 80.0% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 80.0% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.490, TAw acc= 80.0% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.488, TAw acc= 80.0% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.485, TAw acc= 80.0% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.482, TAw acc= 80.0% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.480, TAw acc= 80.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1804 train exemplars, time=  0.0s
1804
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.163 | TAw acc= 96.1%, forg= -0.6%| TAg acc= 83.3%, forg=  3.9% <<<
>>> Test on task  1 : loss=1.130 | TAw acc= 93.9%, forg=  2.6%| TAg acc= 82.6%, forg=  3.5% <<<
>>> Test on task  2 : loss=1.265 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task  3 : loss=0.944 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 76.1%, forg=  8.0% <<<
>>> Test on task  4 : loss=1.213 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.252 | TAw acc= 91.4%, forg=  3.8%| TAg acc= 81.9%, forg= -2.9% <<<
>>> Test on task  6 : loss=1.279 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 66.4%, forg=  6.6% <<<
>>> Test on task  7 : loss=1.671 | TAw acc= 95.0%, forg= -3.0%| TAg acc= 63.4%, forg= -9.9% <<<
>>> Test on task  8 : loss=1.447 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.804 | TAw acc= 96.9%, forg= -5.1%| TAg acc= 44.9%, forg=  3.1% <<<
>>> Test on task 10 : loss=1.529 | TAw acc= 98.2%, forg= -2.7%| TAg acc= 48.2%, forg= 25.0% <<<
>>> Test on task 11 : loss=1.464 | TAw acc= 89.6%, forg=  0.0%| TAg acc= 67.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.000, TAw acc= 42.5% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=3.122, TAw acc= 58.8% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.288, TAw acc= 85.0% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.820, TAw acc=100.0% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.510, TAw acc= 96.2% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.507, TAw acc= 96.2% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.505, TAw acc= 96.2% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.502, TAw acc= 96.2% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.499, TAw acc= 96.2% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.496, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 1944 train exemplars, time=  0.0s
1944
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.109 | TAw acc= 95.6%, forg=  0.6%| TAg acc= 83.9%, forg=  3.3% <<<
>>> Test on task  1 : loss=1.073 | TAw acc= 93.0%, forg=  3.5%| TAg acc= 83.5%, forg=  2.6% <<<
>>> Test on task  2 : loss=1.213 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.0%, forg=  7.8% <<<
>>> Test on task  3 : loss=0.860 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 83.2%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.206 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 78.9%, forg=  2.8% <<<
>>> Test on task  5 : loss=1.294 | TAw acc= 90.5%, forg=  4.8%| TAg acc= 74.3%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.164 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 74.6%, forg= -1.6% <<<
>>> Test on task  7 : loss=1.608 | TAw acc= 93.1%, forg=  2.0%| TAg acc= 63.4%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.455 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 68.7%, forg=  1.7% <<<
>>> Test on task  9 : loss=1.737 | TAw acc= 96.9%, forg=  0.0%| TAg acc= 49.0%, forg= -1.0% <<<
>>> Test on task 10 : loss=1.461 | TAw acc= 99.1%, forg= -0.9%| TAg acc= 56.2%, forg= 17.0% <<<
>>> Test on task 11 : loss=1.770 | TAw acc= 88.7%, forg=  0.9%| TAg acc= 45.2%, forg= 22.6% <<<
>>> Test on task 12 : loss=1.588 | TAw acc= 92.7%, forg=  0.0%| TAg acc= 56.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=6.208, TAw acc= 20.0% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=4.394, TAw acc= 53.8% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=3.268, TAw acc= 66.2% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.579, TAw acc= 61.5% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.189, TAw acc= 70.8% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.185, TAw acc= 70.8% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.181, TAw acc= 70.8% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.178, TAw acc= 70.8% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.175, TAw acc= 70.8% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.171, TAw acc= 70.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2084 train exemplars, time=  0.0s
2084
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.119 | TAw acc= 95.6%, forg=  0.6%| TAg acc= 81.1%, forg=  6.1% <<<
>>> Test on task  1 : loss=1.077 | TAw acc= 94.8%, forg=  1.7%| TAg acc= 79.1%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.144 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 74.1%, forg=  2.6% <<<
>>> Test on task  3 : loss=0.875 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 78.8%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.155 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 80.7%, forg=  0.9% <<<
>>> Test on task  5 : loss=1.184 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 81.9%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.093 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 78.7%, forg= -4.1% <<<
>>> Test on task  7 : loss=1.552 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 64.4%, forg= -1.0% <<<
>>> Test on task  8 : loss=1.380 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.637 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 57.1%, forg= -8.2% <<<
>>> Test on task 10 : loss=1.320 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 67.0%, forg=  6.2% <<<
>>> Test on task 11 : loss=1.563 | TAw acc= 91.3%, forg= -1.7%| TAg acc= 53.0%, forg= 14.8% <<<
>>> Test on task 12 : loss=1.882 | TAw acc= 95.4%, forg= -2.8%| TAg acc= 33.9%, forg= 22.9% <<<
>>> Test on task 13 : loss=1.972 | TAw acc= 79.1%, forg=  0.0%| TAg acc= 44.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=4.874, TAw acc= 78.4% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.486, TAw acc= 86.4% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.641, TAw acc= 88.6% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.330, TAw acc= 89.8% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 90.9% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.159, TAw acc= 90.9% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.157, TAw acc= 90.9% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.156, TAw acc= 90.9% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.155, TAw acc= 90.9% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.154, TAw acc= 90.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2224 train exemplars, time=  0.0s
2224
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.090 | TAw acc= 95.0%, forg=  1.1%| TAg acc= 85.6%, forg=  1.7% <<<
>>> Test on task  1 : loss=1.053 | TAw acc= 93.0%, forg=  3.5%| TAg acc= 79.1%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.217 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 65.5%, forg= 11.2% <<<
>>> Test on task  3 : loss=0.889 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 79.6%, forg=  4.4% <<<
>>> Test on task  4 : loss=1.155 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.9%, forg=  2.8% <<<
>>> Test on task  5 : loss=1.186 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 74.3%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.158 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.0%, forg=  5.7% <<<
>>> Test on task  7 : loss=1.691 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 50.5%, forg= 13.9% <<<
>>> Test on task  8 : loss=1.350 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 72.2%, forg= -1.7% <<<
>>> Test on task  9 : loss=1.582 | TAw acc= 94.9%, forg=  3.1%| TAg acc= 58.2%, forg= -1.0% <<<
>>> Test on task 10 : loss=1.337 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.2%, forg=  8.0% <<<
>>> Test on task 11 : loss=1.409 | TAw acc= 89.6%, forg=  1.7%| TAg acc= 57.4%, forg= 10.4% <<<
>>> Test on task 12 : loss=1.740 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 52.3%, forg=  4.6% <<<
>>> Test on task 13 : loss=2.074 | TAw acc= 90.1%, forg=-11.0%| TAg acc= 29.7%, forg= 14.3% <<<
>>> Test on task 14 : loss=1.139 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 79.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=4.575, TAw acc= 43.6% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.805, TAw acc= 66.7% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.041, TAw acc= 85.9% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.661, TAw acc= 92.3% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.465, TAw acc= 92.3% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.462, TAw acc= 92.3% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.458, TAw acc= 92.3% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 92.3% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.452, TAw acc= 92.3% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.449, TAw acc= 92.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2364 train exemplars, time=  0.0s
2364
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.123 | TAw acc= 95.0%, forg=  1.1%| TAg acc= 82.2%, forg=  5.0% <<<
>>> Test on task  1 : loss=1.031 | TAw acc= 93.9%, forg=  2.6%| TAg acc= 81.7%, forg=  4.3% <<<
>>> Test on task  2 : loss=1.140 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 75.0%, forg=  1.7% <<<
>>> Test on task  3 : loss=0.849 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 77.9%, forg=  6.2% <<<
>>> Test on task  4 : loss=1.176 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.9%, forg=  2.8% <<<
>>> Test on task  5 : loss=1.090 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 79.0%, forg=  2.9% <<<
>>> Test on task  6 : loss=1.049 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 77.9%, forg=  0.8% <<<
>>> Test on task  7 : loss=1.509 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 69.3%, forg= -5.0% <<<
>>> Test on task  8 : loss=1.299 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.4%, forg=  1.7% <<<
>>> Test on task  9 : loss=1.517 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 61.2%, forg= -3.1% <<<
>>> Test on task 10 : loss=1.294 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 65.2%, forg=  8.0% <<<
>>> Test on task 11 : loss=1.251 | TAw acc= 89.6%, forg=  1.7%| TAg acc= 67.0%, forg=  0.9% <<<
>>> Test on task 12 : loss=1.690 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 55.0%, forg=  1.8% <<<
>>> Test on task 13 : loss=1.943 | TAw acc= 85.7%, forg=  4.4%| TAg acc= 36.3%, forg=  7.7% <<<
>>> Test on task 14 : loss=1.479 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 49.6%, forg= 30.3% <<<
>>> Test on task 15 : loss=1.413 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 63.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=4.788, TAw acc= 46.4% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.829, TAw acc= 66.7% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.920, TAw acc= 78.6% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.560, TAw acc= 83.3% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.392, TAw acc= 89.3% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.390, TAw acc= 89.3% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.389, TAw acc= 89.3% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.387, TAw acc= 89.3% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.385, TAw acc= 89.3% | *
| Epoch   5, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.384, TAw acc= 89.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2504 train exemplars, time=  0.0s
2504
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.070 | TAw acc= 95.0%, forg=  1.1%| TAg acc= 84.4%, forg=  2.8% <<<
>>> Test on task  1 : loss=1.042 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 79.1%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.161 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 72.4%, forg=  4.3% <<<
>>> Test on task  3 : loss=0.867 | TAw acc= 94.7%, forg=  2.7%| TAg acc= 79.6%, forg=  4.4% <<<
>>> Test on task  4 : loss=1.144 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 76.1%, forg=  5.5% <<<
>>> Test on task  5 : loss=1.116 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 74.3%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.169 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 71.3%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.581 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 61.4%, forg=  7.9% <<<
>>> Test on task  8 : loss=1.260 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.3%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.547 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 60.2%, forg=  1.0% <<<
>>> Test on task 10 : loss=1.258 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 73.2%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.242 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 66.1%, forg=  1.7% <<<
>>> Test on task 12 : loss=1.567 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 66.1%, forg= -9.2% <<<
>>> Test on task 13 : loss=1.885 | TAw acc= 87.9%, forg=  2.2%| TAg acc= 37.4%, forg=  6.6% <<<
>>> Test on task 14 : loss=1.415 | TAw acc= 95.0%, forg= -2.5%| TAg acc= 61.3%, forg= 18.5% <<<
>>> Test on task 15 : loss=1.507 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 56.6%, forg=  6.6% <<<
>>> Test on task 16 : loss=1.610 | TAw acc= 87.6%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=5.998, TAw acc= 37.3% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.3s loss=3.937, TAw acc= 44.0% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=3.070, TAw acc= 58.7% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.463, TAw acc= 70.7% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.159, TAw acc= 76.0% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.155, TAw acc= 76.0% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.152, TAw acc= 76.0% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.149, TAw acc= 76.0% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.147, TAw acc= 76.0% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.144, TAw acc= 76.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2644 train exemplars, time=  0.0s
2644
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.100 | TAw acc= 95.6%, forg=  0.6%| TAg acc= 80.6%, forg=  6.7% <<<
>>> Test on task  1 : loss=1.068 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 79.1%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.190 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 73.3%, forg=  3.4% <<<
>>> Test on task  3 : loss=0.836 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 82.3%, forg=  1.8% <<<
>>> Test on task  4 : loss=1.117 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.196 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 74.3%, forg=  7.6% <<<
>>> Test on task  6 : loss=1.059 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 74.6%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.528 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 65.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.239 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.9%, forg= -1.7% <<<
>>> Test on task  9 : loss=1.545 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 57.1%, forg=  4.1% <<<
>>> Test on task 10 : loss=1.309 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 66.1%, forg=  7.1% <<<
>>> Test on task 11 : loss=1.215 | TAw acc= 91.3%, forg=  0.0%| TAg acc= 67.0%, forg=  0.9% <<<
>>> Test on task 12 : loss=1.610 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 59.6%, forg=  6.4% <<<
>>> Test on task 13 : loss=1.828 | TAw acc= 87.9%, forg=  2.2%| TAg acc= 40.7%, forg=  3.3% <<<
>>> Test on task 14 : loss=1.377 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 61.3%, forg= 18.5% <<<
>>> Test on task 15 : loss=1.299 | TAw acc= 97.2%, forg= -1.9%| TAg acc= 61.3%, forg=  1.9% <<<
>>> Test on task 16 : loss=1.940 | TAw acc= 93.8%, forg= -6.2%| TAg acc= 45.1%, forg= 29.2% <<<
>>> Test on task 17 : loss=1.854 | TAw acc= 88.3%, forg=  0.0%| TAg acc= 51.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=4.018, TAw acc= 54.8% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=2.620, TAw acc= 77.4% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.895, TAw acc= 85.7% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.463, TAw acc= 95.2% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.323, TAw acc= 96.4% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 96.4% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.268, TAw acc= 96.4% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.250, TAw acc= 96.4% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.236, TAw acc= 96.4% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.225, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2784 train exemplars, time=  0.0s
2784
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.144 | TAw acc= 95.0%, forg=  1.1%| TAg acc= 81.1%, forg=  6.1% <<<
>>> Test on task  1 : loss=1.084 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 79.1%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.192 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task  3 : loss=1.065 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.190 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.9%, forg=  2.8% <<<
>>> Test on task  5 : loss=1.087 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 75.2%, forg=  6.7% <<<
>>> Test on task  6 : loss=1.112 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 75.4%, forg=  3.3% <<<
>>> Test on task  7 : loss=1.484 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 69.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.273 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.6%, forg=  4.3% <<<
>>> Test on task  9 : loss=1.512 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 61.2%, forg=  0.0% <<<
>>> Test on task 10 : loss=1.337 | TAw acc= 95.5%, forg=  3.6%| TAg acc= 67.0%, forg=  6.2% <<<
>>> Test on task 11 : loss=1.159 | TAw acc= 92.2%, forg= -0.9%| TAg acc= 72.2%, forg= -4.3% <<<
>>> Test on task 12 : loss=1.575 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 63.3%, forg=  2.8% <<<
>>> Test on task 13 : loss=1.907 | TAw acc= 87.9%, forg=  2.2%| TAg acc= 40.7%, forg=  3.3% <<<
>>> Test on task 14 : loss=1.316 | TAw acc= 91.6%, forg=  3.4%| TAg acc= 66.4%, forg= 13.4% <<<
>>> Test on task 15 : loss=1.330 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 64.2%, forg= -0.9% <<<
>>> Test on task 16 : loss=1.789 | TAw acc= 94.7%, forg= -0.9%| TAg acc= 60.2%, forg= 14.2% <<<
>>> Test on task 17 : loss=2.128 | TAw acc= 98.1%, forg= -9.7%| TAg acc= 30.1%, forg= 21.4% <<<
>>> Test on task 18 : loss=1.247 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 70.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=5.110, TAw acc= 51.3% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.946, TAw acc= 72.4% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.994, TAw acc= 86.8% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.566, TAw acc= 88.2% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.438, TAw acc= 92.1% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.435, TAw acc= 92.1% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.431, TAw acc= 92.1% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.428, TAw acc= 92.1% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.424, TAw acc= 92.1% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.421, TAw acc= 92.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 2924 train exemplars, time=  0.0s
2924
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.085 | TAw acc= 95.6%, forg=  0.6%| TAg acc= 83.9%, forg=  3.3% <<<
>>> Test on task  1 : loss=1.060 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 79.1%, forg=  7.0% <<<
>>> Test on task  2 : loss=1.186 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task  3 : loss=0.962 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 75.2%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.155 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 81.7%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.039 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 81.0%, forg=  1.0% <<<
>>> Test on task  6 : loss=1.099 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.8%, forg=  4.9% <<<
>>> Test on task  7 : loss=1.429 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 70.3%, forg= -1.0% <<<
>>> Test on task  8 : loss=1.224 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg=  4.3% <<<
>>> Test on task  9 : loss=1.540 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 60.2%, forg=  1.0% <<<
>>> Test on task 10 : loss=1.266 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.9%, forg=  5.4% <<<
>>> Test on task 11 : loss=1.141 | TAw acc= 91.3%, forg=  0.9%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.513 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  0.9% <<<
>>> Test on task 13 : loss=1.725 | TAw acc= 86.8%, forg=  3.3%| TAg acc= 41.8%, forg=  2.2% <<<
>>> Test on task 14 : loss=1.254 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 69.7%, forg= 10.1% <<<
>>> Test on task 15 : loss=1.196 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 71.7%, forg= -7.5% <<<
>>> Test on task 16 : loss=1.744 | TAw acc= 94.7%, forg=  0.0%| TAg acc= 61.1%, forg= 13.3% <<<
>>> Test on task 17 : loss=1.780 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 52.4%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.460 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 57.5%, forg= 13.3% <<<
>>> Test on task 19 : loss=1.357 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 71.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=4.900, TAw acc= 44.9% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.679, TAw acc= 76.4% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.830, TAw acc= 88.8% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.462, TAw acc= 88.8% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.239, TAw acc= 92.1% | *
| Epoch   1, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.235, TAw acc= 92.1% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.232, TAw acc= 92.1% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.229, TAw acc= 93.3% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.226, TAw acc= 95.5% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.223, TAw acc= 95.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3064 train exemplars, time=  0.1s
3064
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.133 | TAw acc= 95.0%, forg=  1.1%| TAg acc= 76.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=1.099 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 78.3%, forg=  7.8% <<<
>>> Test on task  2 : loss=1.219 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task  3 : loss=0.971 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 77.9%, forg=  6.2% <<<
>>> Test on task  4 : loss=1.227 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task  5 : loss=1.048 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 77.1%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.158 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 75.4%, forg=  3.3% <<<
>>> Test on task  7 : loss=1.425 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 68.3%, forg=  2.0% <<<
>>> Test on task  8 : loss=1.224 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 73.9%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.527 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 63.3%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.318 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 70.5%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.155 | TAw acc= 91.3%, forg=  0.9%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.473 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.9%, forg= -1.8% <<<
>>> Test on task 13 : loss=1.729 | TAw acc= 87.9%, forg=  2.2%| TAg acc= 45.1%, forg= -1.1% <<<
>>> Test on task 14 : loss=1.230 | TAw acc= 94.1%, forg=  0.8%| TAg acc= 68.1%, forg= 11.8% <<<
>>> Test on task 15 : loss=1.218 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.6%, forg= -1.9% <<<
>>> Test on task 16 : loss=1.565 | TAw acc= 95.6%, forg= -0.9%| TAg acc= 71.7%, forg=  2.7% <<<
>>> Test on task 17 : loss=1.695 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 62.1%, forg= -9.7% <<<
>>> Test on task 18 : loss=1.370 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 61.9%, forg=  8.8% <<<
>>> Test on task 19 : loss=1.522 | TAw acc= 99.0%, forg= -2.9%| TAg acc= 62.5%, forg=  8.7% <<<
>>> Test on task 20 : loss=1.385 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 72.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=6.190, TAw acc= 46.8% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=3.054, TAw acc= 77.2% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.940, TAw acc= 93.7% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.426, TAw acc=100.0% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc=100.0% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.283, TAw acc=100.0% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.281, TAw acc=100.0% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc=100.0% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.276, TAw acc=100.0% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3204 train exemplars, time=  0.0s
3204
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.113 | TAw acc= 96.7%, forg= -0.6%| TAg acc= 81.7%, forg=  5.6% <<<
>>> Test on task  1 : loss=0.999 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 76.5%, forg=  9.6% <<<
>>> Test on task  2 : loss=1.176 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 71.6%, forg=  5.2% <<<
>>> Test on task  3 : loss=0.946 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 77.0%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.172 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  5 : loss=1.024 | TAw acc= 94.3%, forg=  1.0%| TAg acc= 77.1%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.201 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.8%, forg=  4.9% <<<
>>> Test on task  7 : loss=1.379 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 74.3%, forg= -4.0% <<<
>>> Test on task  8 : loss=1.229 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.0%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.579 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 60.2%, forg=  3.1% <<<
>>> Test on task 10 : loss=1.275 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 70.5%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.118 | TAw acc= 90.4%, forg=  1.7%| TAg acc= 67.0%, forg=  5.2% <<<
>>> Test on task 12 : loss=1.521 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 61.5%, forg=  6.4% <<<
>>> Test on task 13 : loss=1.673 | TAw acc= 89.0%, forg=  1.1%| TAg acc= 52.7%, forg= -7.7% <<<
>>> Test on task 14 : loss=1.312 | TAw acc= 95.8%, forg= -0.8%| TAg acc= 66.4%, forg= 13.4% <<<
>>> Test on task 15 : loss=1.205 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.8%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.628 | TAw acc= 94.7%, forg=  0.9%| TAg acc= 66.4%, forg=  8.0% <<<
>>> Test on task 17 : loss=1.619 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 56.3%, forg=  5.8% <<<
>>> Test on task 18 : loss=1.256 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 66.4%, forg=  4.4% <<<
>>> Test on task 19 : loss=1.389 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 62.5%, forg=  8.7% <<<
>>> Test on task 20 : loss=1.722 | TAw acc= 94.1%, forg= -0.8%| TAg acc= 52.9%, forg= 19.3% <<<
>>> Test on task 21 : loss=1.383 | TAw acc= 90.7%, forg=  0.0%| TAg acc= 69.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=5.552, TAw acc= 51.2% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.717, TAw acc= 78.6% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.976, TAw acc= 88.1% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.530, TAw acc= 89.3% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.312, TAw acc= 96.4% | *
| Epoch   1, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.312, TAw acc= 96.4% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.312, TAw acc= 96.4% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.311, TAw acc= 96.4% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.311, TAw acc= 96.4% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.311, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3344 train exemplars, time=  0.0s
3344
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.098 | TAw acc= 97.2%, forg= -0.6%| TAg acc= 82.2%, forg=  5.0% <<<
>>> Test on task  1 : loss=1.044 | TAw acc= 98.3%, forg= -1.7%| TAg acc= 77.4%, forg=  8.7% <<<
>>> Test on task  2 : loss=1.219 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 67.2%, forg=  9.5% <<<
>>> Test on task  3 : loss=1.069 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 67.3%, forg= 16.8% <<<
>>> Test on task  4 : loss=1.159 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 78.0%, forg=  3.7% <<<
>>> Test on task  5 : loss=1.106 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 72.4%, forg=  9.5% <<<
>>> Test on task  6 : loss=1.172 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.0%, forg=  5.7% <<<
>>> Test on task  7 : loss=1.420 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 68.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.234 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 68.7%, forg=  5.2% <<<
>>> Test on task  9 : loss=1.575 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 65.3%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.262 | TAw acc= 96.4%, forg=  2.7%| TAg acc= 69.6%, forg=  3.6% <<<
>>> Test on task 11 : loss=1.184 | TAw acc= 90.4%, forg=  1.7%| TAg acc= 67.0%, forg=  5.2% <<<
>>> Test on task 12 : loss=1.510 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 63.3%, forg=  4.6% <<<
>>> Test on task 13 : loss=1.707 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 51.6%, forg=  1.1% <<<
>>> Test on task 14 : loss=1.204 | TAw acc= 93.3%, forg=  2.5%| TAg acc= 73.9%, forg=  5.9% <<<
>>> Test on task 15 : loss=1.152 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 70.8%, forg=  2.8% <<<
>>> Test on task 16 : loss=1.601 | TAw acc= 94.7%, forg=  0.9%| TAg acc= 66.4%, forg=  8.0% <<<
>>> Test on task 17 : loss=1.491 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 66.0%, forg= -3.9% <<<
>>> Test on task 18 : loss=1.194 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 65.5%, forg=  5.3% <<<
>>> Test on task 19 : loss=1.326 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 20 : loss=1.667 | TAw acc= 95.8%, forg= -1.7%| TAg acc= 53.8%, forg= 18.5% <<<
>>> Test on task 21 : loss=1.700 | TAw acc= 92.5%, forg= -1.9%| TAg acc= 55.1%, forg= 14.0% <<<
>>> Test on task 22 : loss=1.269 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 71.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=4.899, TAw acc= 53.1% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=2.535, TAw acc= 61.7% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.770, TAw acc= 92.6% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.526, TAw acc= 92.6% | *
| Epoch   5, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.298, TAw acc= 90.1% | *
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 90.1% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 90.1% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 90.1% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.288, TAw acc= 90.1% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc= 90.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3484 train exemplars, time=  0.0s
3484
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.144 | TAw acc= 96.1%, forg=  1.1%| TAg acc= 77.2%, forg= 10.0% <<<
>>> Test on task  1 : loss=1.049 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 74.8%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.187 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 69.8%, forg=  6.9% <<<
>>> Test on task  3 : loss=0.978 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 74.3%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.250 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task  5 : loss=1.029 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 75.2%, forg=  6.7% <<<
>>> Test on task  6 : loss=1.166 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 72.1%, forg=  6.6% <<<
>>> Test on task  7 : loss=1.454 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 73.3%, forg=  1.0% <<<
>>> Test on task  8 : loss=1.221 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.0%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.589 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 60.2%, forg=  5.1% <<<
>>> Test on task 10 : loss=1.301 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 70.5%, forg=  2.7% <<<
>>> Test on task 11 : loss=1.119 | TAw acc= 93.0%, forg= -0.9%| TAg acc= 69.6%, forg=  2.6% <<<
>>> Test on task 12 : loss=1.522 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.0%, forg=  0.9% <<<
>>> Test on task 13 : loss=1.660 | TAw acc= 91.2%, forg= -1.1%| TAg acc= 52.7%, forg=  0.0% <<<
>>> Test on task 14 : loss=1.152 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 75.6%, forg=  4.2% <<<
>>> Test on task 15 : loss=1.187 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 71.7%, forg=  1.9% <<<
>>> Test on task 16 : loss=1.497 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 70.8%, forg=  3.5% <<<
>>> Test on task 17 : loss=1.434 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 65.0%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.212 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 69.0%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.202 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.2%, forg=  1.0% <<<
>>> Test on task 20 : loss=1.616 | TAw acc= 96.6%, forg= -0.8%| TAg acc= 59.7%, forg= 12.6% <<<
>>> Test on task 21 : loss=1.572 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 64.5%, forg=  4.7% <<<
>>> Test on task 22 : loss=1.565 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 56.1%, forg= 14.9% <<<
>>> Test on task 23 : loss=1.289 | TAw acc= 90.0%, forg=  0.0%| TAg acc= 68.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=4.841, TAw acc= 36.9% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=2.909, TAw acc= 70.2% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.122, TAw acc= 82.1% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.770, TAw acc= 90.5% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.490, TAw acc= 94.0% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.488, TAw acc= 94.0% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.487, TAw acc= 94.0% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.485, TAw acc= 95.2% | *
| Epoch   4, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.484, TAw acc= 95.2% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.482, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3624 train exemplars, time=  0.0s
3624
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.109 | TAw acc= 96.7%, forg=  0.6%| TAg acc= 81.1%, forg=  6.1% <<<
>>> Test on task  1 : loss=1.059 | TAw acc= 95.7%, forg=  2.6%| TAg acc= 75.7%, forg= 10.4% <<<
>>> Test on task  2 : loss=1.171 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 68.1%, forg=  8.6% <<<
>>> Test on task  3 : loss=0.981 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 74.3%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.297 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.013 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 76.2%, forg=  5.7% <<<
>>> Test on task  6 : loss=1.198 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.8%, forg=  4.9% <<<
>>> Test on task  7 : loss=1.396 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 71.3%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.196 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 76.5%, forg= -2.6% <<<
>>> Test on task  9 : loss=1.526 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.3%, forg=  1.0% <<<
>>> Test on task 10 : loss=1.370 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 65.2%, forg=  8.0% <<<
>>> Test on task 11 : loss=1.106 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 67.8%, forg=  4.3% <<<
>>> Test on task 12 : loss=1.562 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 62.4%, forg=  5.5% <<<
>>> Test on task 13 : loss=1.667 | TAw acc= 90.1%, forg=  1.1%| TAg acc= 47.3%, forg=  5.5% <<<
>>> Test on task 14 : loss=1.163 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 75.6%, forg=  4.2% <<<
>>> Test on task 15 : loss=1.181 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.9%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.523 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 73.5%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.355 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 68.9%, forg= -2.9% <<<
>>> Test on task 18 : loss=1.134 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 70.8%, forg=  0.0% <<<
>>> Test on task 19 : loss=1.188 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 20 : loss=1.581 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 58.8%, forg= 13.4% <<<
>>> Test on task 21 : loss=1.501 | TAw acc= 91.6%, forg=  0.9%| TAg acc= 67.3%, forg=  1.9% <<<
>>> Test on task 22 : loss=1.506 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 57.9%, forg= 13.2% <<<
>>> Test on task 23 : loss=1.712 | TAw acc= 98.2%, forg= -8.2%| TAg acc= 47.3%, forg= 20.9% <<<
>>> Test on task 24 : loss=1.499 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 67.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=5.677, TAw acc= 39.7% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=3.405, TAw acc= 66.7% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.220, TAw acc= 83.3% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.659, TAw acc= 85.9% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.514, TAw acc= 94.9% | *
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.510, TAw acc= 94.9% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.506, TAw acc= 94.9% | *
| Epoch   3, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.502, TAw acc= 94.9% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.498, TAw acc= 94.9% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.495, TAw acc= 94.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3764 train exemplars, time=  0.0s
3764
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.098 | TAw acc= 96.7%, forg=  0.6%| TAg acc= 81.1%, forg=  6.1% <<<
>>> Test on task  1 : loss=1.023 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 77.4%, forg=  8.7% <<<
>>> Test on task  2 : loss=1.246 | TAw acc= 95.7%, forg=  1.7%| TAg acc= 63.8%, forg= 12.9% <<<
>>> Test on task  3 : loss=1.086 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 71.7%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.257 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.5%, forg=  9.2% <<<
>>> Test on task  5 : loss=1.040 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 70.5%, forg= 11.4% <<<
>>> Test on task  6 : loss=1.180 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 72.1%, forg=  6.6% <<<
>>> Test on task  7 : loss=1.419 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 71.3%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.273 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 75.7%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.580 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 58.2%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.240 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 76.8%, forg= -3.6% <<<
>>> Test on task 11 : loss=1.168 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 67.0%, forg=  5.2% <<<
>>> Test on task 12 : loss=1.511 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 66.1%, forg=  1.8% <<<
>>> Test on task 13 : loss=1.694 | TAw acc= 90.1%, forg=  1.1%| TAg acc= 44.0%, forg=  8.8% <<<
>>> Test on task 14 : loss=1.196 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 74.8%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.146 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task 16 : loss=1.468 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 72.6%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.329 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.9%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.099 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 74.3%, forg= -3.5% <<<
>>> Test on task 19 : loss=1.123 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg= -3.8% <<<
>>> Test on task 20 : loss=1.518 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.9%, forg=  8.4% <<<
>>> Test on task 21 : loss=1.442 | TAw acc= 94.4%, forg= -1.9%| TAg acc= 73.8%, forg= -4.7% <<<
>>> Test on task 22 : loss=1.381 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 62.3%, forg=  8.8% <<<
>>> Test on task 23 : loss=1.597 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 53.6%, forg= 14.5% <<<
>>> Test on task 24 : loss=1.781 | TAw acc= 94.7%, forg= -2.7%| TAg acc= 61.9%, forg=  5.3% <<<
>>> Test on task 25 : loss=1.482 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 59.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=5.943, TAw acc= 48.1% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=3.149, TAw acc= 61.7% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.237, TAw acc= 85.2% | *
| Epoch   4, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.854, TAw acc= 92.6% | *
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.558, TAw acc= 96.3% | *
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.557, TAw acc= 96.3% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.556, TAw acc= 96.3% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.555, TAw acc= 96.3% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.554, TAw acc= 96.3% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.553, TAw acc= 96.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 3904 train exemplars, time=  0.0s
3904
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.130 | TAw acc= 96.1%, forg=  1.1%| TAg acc= 77.2%, forg= 10.0% <<<
>>> Test on task  1 : loss=1.117 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 74.8%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.275 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 65.5%, forg= 11.2% <<<
>>> Test on task  3 : loss=1.056 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 74.3%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.254 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.026 | TAw acc= 95.2%, forg=  0.0%| TAg acc= 78.1%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.135 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 77.0%, forg=  1.6% <<<
>>> Test on task  7 : loss=1.442 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 70.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.329 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 67.8%, forg=  8.7% <<<
>>> Test on task  9 : loss=1.617 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 60.2%, forg=  5.1% <<<
>>> Test on task 10 : loss=1.363 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 66.1%, forg= 10.7% <<<
>>> Test on task 11 : loss=1.141 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.542 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 68.8%, forg= -0.9% <<<
>>> Test on task 13 : loss=1.680 | TAw acc= 89.0%, forg=  2.2%| TAg acc= 51.6%, forg=  1.1% <<<
>>> Test on task 14 : loss=1.231 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 66.4%, forg= 13.4% <<<
>>> Test on task 15 : loss=1.233 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.9%, forg=  4.7% <<<
>>> Test on task 16 : loss=1.432 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 72.6%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.332 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 71.8%, forg= -1.9% <<<
>>> Test on task 18 : loss=1.070 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 73.5%, forg=  0.9% <<<
>>> Test on task 19 : loss=1.143 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.2%, forg=  3.8% <<<
>>> Test on task 20 : loss=1.546 | TAw acc= 95.0%, forg=  1.7%| TAg acc= 63.9%, forg=  8.4% <<<
>>> Test on task 21 : loss=1.431 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 73.8%, forg=  0.0% <<<
>>> Test on task 22 : loss=1.346 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 64.0%, forg=  7.0% <<<
>>> Test on task 23 : loss=1.510 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 55.5%, forg= 12.7% <<<
>>> Test on task 24 : loss=1.678 | TAw acc= 92.0%, forg=  2.7%| TAg acc= 67.3%, forg=  0.0% <<<
>>> Test on task 25 : loss=1.740 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 48.6%, forg= 11.2% <<<
>>> Test on task 26 : loss=1.658 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 53.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=6.079, TAw acc= 44.0% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=3.541, TAw acc= 52.0% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.300, TAw acc= 81.3% | *
| Epoch   4, time=  3.2s | Train: skip eval | Valid: time=  0.3s loss=1.805, TAw acc= 90.7% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.511, TAw acc= 90.7% | *
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.510, TAw acc= 90.7% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.510, TAw acc= 90.7% | *
| Epoch   3, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.509, TAw acc= 90.7% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.508, TAw acc= 90.7% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.507, TAw acc= 90.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4044 train exemplars, time=  0.0s
4044
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.108 | TAw acc= 96.1%, forg=  1.1%| TAg acc= 78.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=1.020 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 74.8%, forg= 11.3% <<<
>>> Test on task  2 : loss=1.248 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 65.5%, forg= 11.2% <<<
>>> Test on task  3 : loss=1.067 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 77.0%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.244 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.003 | TAw acc= 96.2%, forg= -1.0%| TAg acc= 78.1%, forg=  3.8% <<<
>>> Test on task  6 : loss=1.099 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 74.6%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.465 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 70.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.330 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 65.2%, forg= 11.3% <<<
>>> Test on task  9 : loss=1.666 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 61.2%, forg=  4.1% <<<
>>> Test on task 10 : loss=1.384 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 67.9%, forg=  8.9% <<<
>>> Test on task 11 : loss=1.118 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.552 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  3.7% <<<
>>> Test on task 13 : loss=1.596 | TAw acc= 87.9%, forg=  3.3%| TAg acc= 59.3%, forg= -6.6% <<<
>>> Test on task 14 : loss=1.204 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 72.3%, forg=  7.6% <<<
>>> Test on task 15 : loss=1.190 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.8%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.440 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 71.7%, forg=  2.7% <<<
>>> Test on task 17 : loss=1.331 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.9%, forg=  2.9% <<<
>>> Test on task 18 : loss=1.010 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 76.1%, forg= -1.8% <<<
>>> Test on task 19 : loss=1.124 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.1%, forg=  2.9% <<<
>>> Test on task 20 : loss=1.494 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 65.5%, forg=  6.7% <<<
>>> Test on task 21 : loss=1.408 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 73.8%, forg=  0.0% <<<
>>> Test on task 22 : loss=1.269 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg=  4.4% <<<
>>> Test on task 23 : loss=1.432 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.2%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.547 | TAw acc= 92.9%, forg=  1.8%| TAg acc= 74.3%, forg= -7.1% <<<
>>> Test on task 25 : loss=1.596 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 57.0%, forg=  2.8% <<<
>>> Test on task 26 : loss=2.186 | TAw acc= 95.4%, forg= -1.8%| TAg acc= 35.8%, forg= 17.4% <<<
>>> Test on task 27 : loss=1.319 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 73.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=5.779, TAw acc= 51.2% | *
| Epoch   2, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=3.111, TAw acc= 75.0% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.906, TAw acc= 95.2% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.406, TAw acc= 96.4% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.304, TAw acc= 96.4% | *
| Epoch   1, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.301, TAw acc= 96.4% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.298, TAw acc= 96.4% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 96.4% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 97.6% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.290, TAw acc= 97.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4184 train exemplars, time=  0.0s
4184
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.176 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.8%, forg=  9.4% <<<
>>> Test on task  1 : loss=1.069 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.3%, forg= 14.8% <<<
>>> Test on task  2 : loss=1.337 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 62.1%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.047 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 74.3%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.216 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.5%, forg=  9.2% <<<
>>> Test on task  5 : loss=1.133 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 69.5%, forg= 12.4% <<<
>>> Test on task  6 : loss=1.148 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 76.2%, forg=  2.5% <<<
>>> Test on task  7 : loss=1.396 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 74.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.369 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 68.7%, forg=  7.8% <<<
>>> Test on task  9 : loss=1.710 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 59.2%, forg=  6.1% <<<
>>> Test on task 10 : loss=1.406 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 65.2%, forg= 11.6% <<<
>>> Test on task 11 : loss=1.170 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 67.0%, forg=  5.2% <<<
>>> Test on task 12 : loss=1.571 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  3.7% <<<
>>> Test on task 13 : loss=1.593 | TAw acc= 86.8%, forg=  4.4%| TAg acc= 50.5%, forg=  8.8% <<<
>>> Test on task 14 : loss=1.155 | TAw acc= 93.3%, forg=  2.5%| TAg acc= 74.8%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.206 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.8%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.385 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 73.5%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.319 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.9%, forg=  1.9% <<<
>>> Test on task 18 : loss=1.103 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.2%, forg=  0.9% <<<
>>> Test on task 19 : loss=1.109 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.0%, forg= -1.0% <<<
>>> Test on task 20 : loss=1.540 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 64.7%, forg=  7.6% <<<
>>> Test on task 21 : loss=1.416 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.0%, forg=  1.9% <<<
>>> Test on task 22 : loss=1.473 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 64.9%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.356 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.2%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.626 | TAw acc= 95.6%, forg= -0.9%| TAg acc= 64.6%, forg=  9.7% <<<
>>> Test on task 25 : loss=1.628 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 56.1%, forg=  3.7% <<<
>>> Test on task 26 : loss=2.160 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 38.5%, forg= 14.7% <<<
>>> Test on task 27 : loss=1.376 | TAw acc= 94.2%, forg=  0.0%| TAg acc= 68.0%, forg=  5.8% <<<
>>> Test on task 28 : loss=1.371 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 63.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=5.091, TAw acc= 48.3% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=3.172, TAw acc= 60.7% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=2.252, TAw acc= 82.0% | *
| Epoch   4, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.838, TAw acc= 89.9% | *
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.603, TAw acc= 96.6% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.604, TAw acc= 96.6% | *
| Epoch   2, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.604, TAw acc= 96.6% |
| Epoch   3, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.604, TAw acc= 96.6% |
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.604, TAw acc= 96.6% |
| Epoch   5, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.603, TAw acc= 96.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4324 train exemplars, time=  0.0s
4324
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.133 | TAw acc= 96.7%, forg=  0.6%| TAg acc= 78.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=1.110 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 73.0%, forg= 13.0% <<<
>>> Test on task  2 : loss=1.268 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 67.2%, forg=  9.5% <<<
>>> Test on task  3 : loss=1.054 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 75.2%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.213 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.064 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 77.1%, forg=  4.8% <<<
>>> Test on task  6 : loss=1.163 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 72.1%, forg=  6.6% <<<
>>> Test on task  7 : loss=1.444 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 71.3%, forg=  3.0% <<<
>>> Test on task  8 : loss=1.402 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.3%, forg= 12.2% <<<
>>> Test on task  9 : loss=1.835 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 58.2%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.483 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 66.1%, forg= 10.7% <<<
>>> Test on task 11 : loss=1.146 | TAw acc= 92.2%, forg=  0.9%| TAg acc= 71.3%, forg=  0.9% <<<
>>> Test on task 12 : loss=1.540 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.0%, forg=  1.8% <<<
>>> Test on task 13 : loss=1.697 | TAw acc= 87.9%, forg=  3.3%| TAg acc= 46.2%, forg= 13.2% <<<
>>> Test on task 14 : loss=1.175 | TAw acc= 91.6%, forg=  4.2%| TAg acc= 72.3%, forg=  7.6% <<<
>>> Test on task 15 : loss=1.223 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.8%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.402 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 75.2%, forg= -0.9% <<<
>>> Test on task 17 : loss=1.270 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 71.8%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.003 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 72.6%, forg=  3.5% <<<
>>> Test on task 19 : loss=1.017 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 79.8%, forg= -3.8% <<<
>>> Test on task 20 : loss=1.496 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.0%, forg=  9.2% <<<
>>> Test on task 21 : loss=1.385 | TAw acc= 93.5%, forg=  0.9%| TAg acc= 72.9%, forg=  0.9% <<<
>>> Test on task 22 : loss=1.290 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 65.8%, forg=  5.3% <<<
>>> Test on task 23 : loss=1.341 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 67.3%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.572 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 66.4%, forg=  8.0% <<<
>>> Test on task 25 : loss=1.520 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 59.8%, forg=  0.0% <<<
>>> Test on task 26 : loss=1.981 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 51.4%, forg=  1.8% <<<
>>> Test on task 27 : loss=1.289 | TAw acc= 95.1%, forg= -1.0%| TAg acc= 72.8%, forg=  1.0% <<<
>>> Test on task 28 : loss=1.756 | TAw acc= 97.3%, forg= -1.8%| TAg acc= 49.6%, forg= 14.2% <<<
>>> Test on task 29 : loss=1.419 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 68.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=5.097, TAw acc= 50.6% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=2.751, TAw acc= 72.4% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.838, TAw acc= 81.6% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.468, TAw acc= 95.4% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.276, TAw acc= 94.3% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.272, TAw acc= 94.3% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.269, TAw acc= 94.3% | *
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.267, TAw acc= 94.3% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.265, TAw acc= 94.3% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.263, TAw acc= 94.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4464 train exemplars, time=  0.1s
4464
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.170 | TAw acc= 96.7%, forg=  0.6%| TAg acc= 78.3%, forg=  8.9% <<<
>>> Test on task  1 : loss=1.121 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.7%, forg= 10.4% <<<
>>> Test on task  2 : loss=1.350 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 61.2%, forg= 15.5% <<<
>>> Test on task  3 : loss=1.064 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.189 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 75.2%, forg=  6.4% <<<
>>> Test on task  5 : loss=1.055 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 75.2%, forg=  6.7% <<<
>>> Test on task  6 : loss=1.119 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 75.4%, forg=  3.3% <<<
>>> Test on task  7 : loss=1.411 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 70.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.360 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 67.8%, forg=  8.7% <<<
>>> Test on task  9 : loss=1.769 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 57.1%, forg=  8.2% <<<
>>> Test on task 10 : loss=1.413 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.0%, forg=  9.8% <<<
>>> Test on task 11 : loss=1.166 | TAw acc= 92.2%, forg=  0.9%| TAg acc= 67.8%, forg=  4.3% <<<
>>> Test on task 12 : loss=1.569 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.0%, forg=  1.8% <<<
>>> Test on task 13 : loss=1.669 | TAw acc= 87.9%, forg=  3.3%| TAg acc= 51.6%, forg=  7.7% <<<
>>> Test on task 14 : loss=1.146 | TAw acc= 93.3%, forg=  2.5%| TAg acc= 74.8%, forg=  5.0% <<<
>>> Test on task 15 : loss=1.182 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.6%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.423 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 73.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.227 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.9%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.130 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.7%, forg=  4.4% <<<
>>> Test on task 19 : loss=1.043 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.8%, forg=  1.0% <<<
>>> Test on task 20 : loss=1.612 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 52.9%, forg= 19.3% <<<
>>> Test on task 21 : loss=1.426 | TAw acc= 92.5%, forg=  1.9%| TAg acc= 67.3%, forg=  6.5% <<<
>>> Test on task 22 : loss=1.302 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.5%, forg=  3.5% <<<
>>> Test on task 23 : loss=1.305 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 67.3%, forg=  0.9% <<<
>>> Test on task 24 : loss=1.477 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 67.3%, forg=  7.1% <<<
>>> Test on task 25 : loss=1.528 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 52.3%, forg=  7.5% <<<
>>> Test on task 26 : loss=1.955 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 52.3%, forg=  0.9% <<<
>>> Test on task 27 : loss=1.170 | TAw acc= 94.2%, forg=  1.0%| TAg acc= 72.8%, forg=  1.0% <<<
>>> Test on task 28 : loss=1.695 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 48.7%, forg= 15.0% <<<
>>> Test on task 29 : loss=1.630 | TAw acc= 97.5%, forg= -0.8%| TAg acc= 60.5%, forg=  8.4% <<<
>>> Test on task 30 : loss=1.161 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 71.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=6.757, TAw acc= 46.3% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=3.297, TAw acc= 82.1% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.971, TAw acc= 85.1% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.840, TAw acc= 88.1% | *
| Epoch   5, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.383, TAw acc= 89.6% | *
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.385, TAw acc= 89.6% | *
| Epoch   2, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.387, TAw acc= 89.6% |
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.388, TAw acc= 89.6% |
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.390, TAw acc= 89.6% |
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.392, TAw acc= 89.6% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4604 train exemplars, time=  0.0s
4604
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.128 | TAw acc= 97.8%, forg= -0.6%| TAg acc= 81.1%, forg=  6.1% <<<
>>> Test on task  1 : loss=1.054 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 77.4%, forg=  8.7% <<<
>>> Test on task  2 : loss=1.359 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 62.1%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.051 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.210 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 74.3%, forg=  7.3% <<<
>>> Test on task  5 : loss=1.160 | TAw acc= 94.3%, forg=  1.9%| TAg acc= 69.5%, forg= 12.4% <<<
>>> Test on task  6 : loss=1.140 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 74.6%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.426 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 72.3%, forg=  2.0% <<<
>>> Test on task  8 : loss=1.398 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 63.5%, forg= 13.0% <<<
>>> Test on task  9 : loss=1.767 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 57.1%, forg=  8.2% <<<
>>> Test on task 10 : loss=1.448 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 60.7%, forg= 16.1% <<<
>>> Test on task 11 : loss=1.215 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 69.6%, forg=  2.6% <<<
>>> Test on task 12 : loss=1.666 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.2%, forg=  4.6% <<<
>>> Test on task 13 : loss=1.708 | TAw acc= 87.9%, forg=  3.3%| TAg acc= 51.6%, forg=  7.7% <<<
>>> Test on task 14 : loss=1.205 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 69.7%, forg= 10.1% <<<
>>> Test on task 15 : loss=1.229 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.9%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.437 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 73.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.199 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.8%, forg= -1.0% <<<
>>> Test on task 18 : loss=1.064 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 72.6%, forg=  3.5% <<<
>>> Test on task 19 : loss=1.017 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.9%, forg=  1.9% <<<
>>> Test on task 20 : loss=1.547 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 61.3%, forg= 10.9% <<<
>>> Test on task 21 : loss=1.344 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 77.6%, forg= -3.7% <<<
>>> Test on task 22 : loss=1.276 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg=  4.4% <<<
>>> Test on task 23 : loss=1.274 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.2%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.491 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 68.1%, forg=  6.2% <<<
>>> Test on task 25 : loss=1.498 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 55.1%, forg=  4.7% <<<
>>> Test on task 26 : loss=1.870 | TAw acc= 93.6%, forg=  2.8%| TAg acc= 56.9%, forg= -3.7% <<<
>>> Test on task 27 : loss=1.109 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 78.6%, forg= -4.9% <<<
>>> Test on task 28 : loss=1.569 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 56.6%, forg=  7.1% <<<
>>> Test on task 29 : loss=1.471 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.2%, forg=  6.7% <<<
>>> Test on task 30 : loss=1.437 | TAw acc= 98.3%, forg= -2.6%| TAg acc= 56.9%, forg= 14.7% <<<
>>> Test on task 31 : loss=1.323 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 67.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=5.838, TAw acc= 60.7% | *
| Epoch   2, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=2.490, TAw acc= 86.5% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.555, TAw acc= 92.1% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.204, TAw acc= 94.4% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.129, TAw acc= 96.6% | *
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.126, TAw acc= 96.6% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.123, TAw acc= 96.6% | *
| Epoch   3, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=1.120, TAw acc= 96.6% | *
| Epoch   4, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.118, TAw acc= 96.6% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.115, TAw acc= 96.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4744 train exemplars, time=  0.0s
4744
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.124 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 83.3%, forg=  3.9% <<<
>>> Test on task  1 : loss=1.154 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.367 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 63.8%, forg= 12.9% <<<
>>> Test on task  3 : loss=1.042 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.221 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task  5 : loss=1.063 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 75.2%, forg=  6.7% <<<
>>> Test on task  6 : loss=1.240 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.0%, forg=  5.7% <<<
>>> Test on task  7 : loss=1.441 | TAw acc= 94.1%, forg=  1.0%| TAg acc= 69.3%, forg=  5.0% <<<
>>> Test on task  8 : loss=1.303 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 72.2%, forg=  4.3% <<<
>>> Test on task  9 : loss=1.793 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 61.2%, forg=  4.1% <<<
>>> Test on task 10 : loss=1.487 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 61.6%, forg= 15.2% <<<
>>> Test on task 11 : loss=1.190 | TAw acc= 91.3%, forg=  1.7%| TAg acc= 67.8%, forg=  4.3% <<<
>>> Test on task 12 : loss=1.615 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 62.4%, forg=  6.4% <<<
>>> Test on task 13 : loss=1.670 | TAw acc= 86.8%, forg=  4.4%| TAg acc= 51.6%, forg=  7.7% <<<
>>> Test on task 14 : loss=1.188 | TAw acc= 95.0%, forg=  0.8%| TAg acc= 75.6%, forg=  4.2% <<<
>>> Test on task 15 : loss=1.261 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.9%, forg=  4.7% <<<
>>> Test on task 16 : loss=1.441 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 74.3%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.197 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.9%, forg=  2.9% <<<
>>> Test on task 18 : loss=1.083 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 73.5%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.060 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.0%, forg=  3.8% <<<
>>> Test on task 20 : loss=1.613 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 60.5%, forg= 11.8% <<<
>>> Test on task 21 : loss=1.433 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 74.8%, forg=  2.8% <<<
>>> Test on task 22 : loss=1.314 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 65.8%, forg=  5.3% <<<
>>> Test on task 23 : loss=1.273 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 71.8%, forg= -3.6% <<<
>>> Test on task 24 : loss=1.459 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 69.0%, forg=  5.3% <<<
>>> Test on task 25 : loss=1.484 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.6%, forg= -3.7% <<<
>>> Test on task 26 : loss=1.866 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 56.0%, forg=  0.9% <<<
>>> Test on task 27 : loss=1.043 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 77.7%, forg=  1.0% <<<
>>> Test on task 28 : loss=1.611 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 53.1%, forg= 10.6% <<<
>>> Test on task 29 : loss=1.426 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.9%, forg=  5.0% <<<
>>> Test on task 30 : loss=1.260 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 71.6%, forg=  0.0% <<<
>>> Test on task 31 : loss=1.589 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 50.5%, forg= 17.2% <<<
>>> Test on task 32 : loss=1.198 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 66.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=4.831, TAw acc= 71.9% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.729, TAw acc= 92.1% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.114, TAw acc= 94.4% | *
| Epoch   4, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.057, TAw acc= 98.9% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=0.948, TAw acc= 98.9% | *
| Epoch   1, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=0.945, TAw acc= 98.9% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=0.942, TAw acc= 98.9% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 98.9% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 98.9% | *
| Epoch   5, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=0.935, TAw acc= 98.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 4884 train exemplars, time=  0.0s
4884
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.177 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 76.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=1.136 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 70.4%, forg= 15.7% <<<
>>> Test on task  2 : loss=1.340 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 62.1%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.110 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 73.5%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.243 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 70.6%, forg= 11.0% <<<
>>> Test on task  5 : loss=1.107 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.5%, forg= 11.4% <<<
>>> Test on task  6 : loss=1.198 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 75.4%, forg=  3.3% <<<
>>> Test on task  7 : loss=1.407 | TAw acc= 96.0%, forg= -1.0%| TAg acc= 74.3%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.463 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 65.2%, forg= 11.3% <<<
>>> Test on task  9 : loss=1.787 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.2%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.449 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 63.4%, forg= 13.4% <<<
>>> Test on task 11 : loss=1.158 | TAw acc= 92.2%, forg=  0.9%| TAg acc= 69.6%, forg=  2.6% <<<
>>> Test on task 12 : loss=1.622 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  3.7% <<<
>>> Test on task 13 : loss=1.752 | TAw acc= 85.7%, forg=  5.5%| TAg acc= 47.3%, forg= 12.1% <<<
>>> Test on task 14 : loss=1.217 | TAw acc= 92.4%, forg=  3.4%| TAg acc= 73.9%, forg=  5.9% <<<
>>> Test on task 15 : loss=1.269 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.0%, forg=  7.5% <<<
>>> Test on task 16 : loss=1.428 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 70.8%, forg=  4.4% <<<
>>> Test on task 17 : loss=1.183 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 71.8%, forg=  1.0% <<<
>>> Test on task 18 : loss=1.040 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 73.5%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.008 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.0%, forg=  3.8% <<<
>>> Test on task 20 : loss=1.601 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 63.0%, forg=  9.2% <<<
>>> Test on task 21 : loss=1.388 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 73.8%, forg=  3.7% <<<
>>> Test on task 22 : loss=1.329 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 64.9%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.277 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.2%, forg=  3.6% <<<
>>> Test on task 24 : loss=1.475 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 62.8%, forg= 11.5% <<<
>>> Test on task 25 : loss=1.483 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 59.8%, forg=  3.7% <<<
>>> Test on task 26 : loss=1.841 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 55.0%, forg=  1.8% <<<
>>> Test on task 27 : loss=1.049 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 77.7%, forg=  1.0% <<<
>>> Test on task 28 : loss=1.556 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 53.1%, forg= 10.6% <<<
>>> Test on task 29 : loss=1.381 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.2%, forg=  6.7% <<<
>>> Test on task 30 : loss=1.215 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.9%, forg= -4.3% <<<
>>> Test on task 31 : loss=1.420 | TAw acc= 93.5%, forg=  1.1%| TAg acc= 52.7%, forg= 15.1% <<<
>>> Test on task 32 : loss=1.499 | TAw acc= 94.9%, forg=  1.7%| TAg acc= 56.8%, forg= 10.2% <<<
>>> Test on task 33 : loss=0.912 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 83.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=5.130, TAw acc= 58.1% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=2.760, TAw acc= 75.6% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=2.053, TAw acc= 81.4% | *
| Epoch   4, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.683, TAw acc= 88.4% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.566, TAw acc= 87.2% | *
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.562, TAw acc= 87.2% | *
| Epoch   2, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.558, TAw acc= 87.2% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.555, TAw acc= 87.2% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.551, TAw acc= 87.2% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.548, TAw acc= 87.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5024 train exemplars, time=  0.0s
5024
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.168 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 78.9%, forg=  8.3% <<<
>>> Test on task  1 : loss=1.155 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.342 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 62.1%, forg= 14.7% <<<
>>> Test on task  3 : loss=1.162 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.231 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.100 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.5%, forg= 11.4% <<<
>>> Test on task  6 : loss=1.169 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 74.6%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.458 | TAw acc= 95.0%, forg=  1.0%| TAg acc= 69.3%, forg=  5.0% <<<
>>> Test on task  8 : loss=1.453 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 62.6%, forg= 13.9% <<<
>>> Test on task  9 : loss=1.804 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 57.1%, forg=  8.2% <<<
>>> Test on task 10 : loss=1.468 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 58.9%, forg= 17.9% <<<
>>> Test on task 11 : loss=1.196 | TAw acc= 92.2%, forg=  0.9%| TAg acc= 69.6%, forg=  2.6% <<<
>>> Test on task 12 : loss=1.637 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.2%, forg=  4.6% <<<
>>> Test on task 13 : loss=1.701 | TAw acc= 89.0%, forg=  2.2%| TAg acc= 49.5%, forg=  9.9% <<<
>>> Test on task 14 : loss=1.223 | TAw acc= 92.4%, forg=  3.4%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 15 : loss=1.303 | TAw acc= 96.2%, forg=  0.9%| TAg acc= 67.0%, forg=  6.6% <<<
>>> Test on task 16 : loss=1.420 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 74.3%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.168 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 76.7%, forg= -3.9% <<<
>>> Test on task 18 : loss=1.111 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.7%, forg=  4.4% <<<
>>> Test on task 19 : loss=0.968 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.9%, forg=  2.9% <<<
>>> Test on task 20 : loss=1.661 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 61.3%, forg= 10.9% <<<
>>> Test on task 21 : loss=1.401 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 74.8%, forg=  2.8% <<<
>>> Test on task 22 : loss=1.256 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg=  4.4% <<<
>>> Test on task 23 : loss=1.209 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 71.8%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.413 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 68.1%, forg=  6.2% <<<
>>> Test on task 25 : loss=1.540 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 57.9%, forg=  5.6% <<<
>>> Test on task 26 : loss=1.801 | TAw acc= 94.5%, forg=  1.8%| TAg acc= 56.0%, forg=  0.9% <<<
>>> Test on task 27 : loss=1.035 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 70.9%, forg=  7.8% <<<
>>> Test on task 28 : loss=1.510 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 55.8%, forg=  8.0% <<<
>>> Test on task 29 : loss=1.434 | TAw acc= 96.6%, forg=  0.8%| TAg acc= 55.5%, forg= 13.4% <<<
>>> Test on task 30 : loss=1.151 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.3%, forg= -3.4% <<<
>>> Test on task 31 : loss=1.417 | TAw acc= 96.8%, forg= -2.2%| TAg acc= 60.2%, forg=  7.5% <<<
>>> Test on task 32 : loss=1.537 | TAw acc= 94.9%, forg=  1.7%| TAg acc= 55.1%, forg= 11.9% <<<
>>> Test on task 33 : loss=1.232 | TAw acc= 97.5%, forg=  1.7%| TAg acc= 65.3%, forg= 18.6% <<<
>>> Test on task 34 : loss=1.227 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 70.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=5.777, TAw acc= 43.8% | *
| Epoch   2, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=2.995, TAw acc= 82.5% | *
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.877, TAw acc= 96.2% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.459, TAw acc= 97.5% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.251, TAw acc= 98.8% | *
| Epoch   1, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.253, TAw acc= 98.8% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.255, TAw acc= 98.8% |
| Epoch   3, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.257, TAw acc= 98.8% |
| Epoch   4, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.258, TAw acc= 98.8% |
| Epoch   5, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.259, TAw acc= 98.8% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5164 train exemplars, time=  0.0s
5164
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.174 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 80.6%, forg=  6.7% <<<
>>> Test on task  1 : loss=1.210 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 67.8%, forg= 18.3% <<<
>>> Test on task  2 : loss=1.473 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task  3 : loss=1.189 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 70.8%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.295 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task  5 : loss=1.231 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.6%, forg= 14.3% <<<
>>> Test on task  6 : loss=1.158 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 74.6%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.375 | TAw acc= 95.0%, forg=  1.0%| TAg acc= 76.2%, forg= -2.0% <<<
>>> Test on task  8 : loss=1.467 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.3%, forg= 12.2% <<<
>>> Test on task  9 : loss=2.051 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 52.0%, forg= 13.3% <<<
>>> Test on task 10 : loss=1.494 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 61.6%, forg= 15.2% <<<
>>> Test on task 11 : loss=1.172 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.710 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 61.5%, forg=  7.3% <<<
>>> Test on task 13 : loss=1.750 | TAw acc= 85.7%, forg=  5.5%| TAg acc= 45.1%, forg= 14.3% <<<
>>> Test on task 14 : loss=1.255 | TAw acc= 92.4%, forg=  3.4%| TAg acc= 73.9%, forg=  5.9% <<<
>>> Test on task 15 : loss=1.288 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.0%, forg=  7.5% <<<
>>> Test on task 16 : loss=1.480 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 73.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.223 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.0%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.107 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 69.9%, forg=  6.2% <<<
>>> Test on task 19 : loss=0.959 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.0%, forg=  3.8% <<<
>>> Test on task 20 : loss=1.551 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 63.9%, forg=  8.4% <<<
>>> Test on task 21 : loss=1.371 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 74.8%, forg=  2.8% <<<
>>> Test on task 22 : loss=1.312 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 66.7%, forg=  4.4% <<<
>>> Test on task 23 : loss=1.253 | TAw acc= 94.5%, forg=  3.6%| TAg acc= 70.0%, forg=  1.8% <<<
>>> Test on task 24 : loss=1.370 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 69.9%, forg=  4.4% <<<
>>> Test on task 25 : loss=1.448 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 62.6%, forg=  0.9% <<<
>>> Test on task 26 : loss=1.909 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 56.9%, forg=  0.0% <<<
>>> Test on task 27 : loss=0.983 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 80.6%, forg= -1.9% <<<
>>> Test on task 28 : loss=1.657 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 53.1%, forg= 10.6% <<<
>>> Test on task 29 : loss=1.364 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.0%, forg=  5.9% <<<
>>> Test on task 30 : loss=1.124 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.3%, forg=  0.0% <<<
>>> Test on task 31 : loss=1.413 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 61.3%, forg=  6.5% <<<
>>> Test on task 32 : loss=1.436 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 66.9%, forg=  0.0% <<<
>>> Test on task 33 : loss=1.137 | TAw acc= 97.5%, forg=  1.7%| TAg acc= 70.3%, forg= 13.6% <<<
>>> Test on task 34 : loss=1.572 | TAw acc= 95.7%, forg= -0.9%| TAg acc= 53.9%, forg= 16.5% <<<
>>> Test on task 35 : loss=1.126 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 77.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=6.682, TAw acc= 35.5% | *
| Epoch   2, time=  4.5s | Train: skip eval | Valid: time=  0.3s loss=3.509, TAw acc= 75.0% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.3s loss=2.124, TAw acc= 81.6% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.593, TAw acc= 89.5% | *
| Epoch   5, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.397, TAw acc= 89.5% | *
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.394, TAw acc= 89.5% | *
| Epoch   2, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.390, TAw acc= 89.5% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.387, TAw acc= 89.5% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.385, TAw acc= 89.5% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.382, TAw acc= 89.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5304 train exemplars, time=  0.0s
5304
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.199 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 79.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=1.192 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 69.6%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.460 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task  3 : loss=1.094 | TAw acc= 94.7%, forg=  2.7%| TAg acc= 74.3%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.331 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 69.7%, forg= 11.9% <<<
>>> Test on task  5 : loss=1.311 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 65.7%, forg= 16.2% <<<
>>> Test on task  6 : loss=1.219 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.0%, forg=  5.7% <<<
>>> Test on task  7 : loss=1.407 | TAw acc= 94.1%, forg=  2.0%| TAg acc= 70.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.453 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 63.5%, forg= 13.0% <<<
>>> Test on task  9 : loss=1.884 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.2%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.464 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.3%, forg= 12.5% <<<
>>> Test on task 11 : loss=1.171 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 69.6%, forg=  2.6% <<<
>>> Test on task 12 : loss=1.702 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 61.5%, forg=  7.3% <<<
>>> Test on task 13 : loss=1.769 | TAw acc= 85.7%, forg=  5.5%| TAg acc= 45.1%, forg= 14.3% <<<
>>> Test on task 14 : loss=1.196 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 78.2%, forg=  1.7% <<<
>>> Test on task 15 : loss=1.265 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 70.8%, forg=  2.8% <<<
>>> Test on task 16 : loss=1.462 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 74.3%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.180 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 18 : loss=1.081 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.8%, forg=  5.3% <<<
>>> Test on task 19 : loss=0.969 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.9%, forg=  1.9% <<<
>>> Test on task 20 : loss=1.653 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 62.2%, forg= 10.1% <<<
>>> Test on task 21 : loss=1.429 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.9%, forg=  4.7% <<<
>>> Test on task 22 : loss=1.400 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 63.2%, forg=  7.9% <<<
>>> Test on task 23 : loss=1.215 | TAw acc= 95.5%, forg=  2.7%| TAg acc= 69.1%, forg=  2.7% <<<
>>> Test on task 24 : loss=1.365 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 71.7%, forg=  2.7% <<<
>>> Test on task 25 : loss=1.502 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 59.8%, forg=  3.7% <<<
>>> Test on task 26 : loss=1.835 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 55.0%, forg=  1.8% <<<
>>> Test on task 27 : loss=0.991 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 79.6%, forg=  1.0% <<<
>>> Test on task 28 : loss=1.578 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 54.9%, forg=  8.8% <<<
>>> Test on task 29 : loss=1.357 | TAw acc= 98.3%, forg= -0.8%| TAg acc= 63.9%, forg=  5.0% <<<
>>> Test on task 30 : loss=1.088 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 81.0%, forg= -1.7% <<<
>>> Test on task 31 : loss=1.365 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 65.6%, forg=  2.2% <<<
>>> Test on task 32 : loss=1.397 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 63.6%, forg=  3.4% <<<
>>> Test on task 33 : loss=1.083 | TAw acc= 96.6%, forg=  2.5%| TAg acc= 69.5%, forg= 14.4% <<<
>>> Test on task 34 : loss=1.395 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 66.1%, forg=  4.3% <<<
>>> Test on task 35 : loss=1.588 | TAw acc=100.0%, forg= -0.9%| TAg acc= 46.3%, forg= 31.5% <<<
>>> Test on task 36 : loss=1.424 | TAw acc= 92.4%, forg=  0.0%| TAg acc= 58.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=6.579, TAw acc= 57.5% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=3.095, TAw acc= 76.2% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.851, TAw acc= 95.0% | *
| Epoch   4, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.405, TAw acc= 93.8% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.424, TAw acc= 92.5% |
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.406, TAw acc= 95.0% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.406, TAw acc= 95.0% |
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.407, TAw acc= 95.0% |
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.407, TAw acc= 95.0% |
| Epoch   5, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.407, TAw acc= 95.0% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5444 train exemplars, time=  0.0s
5444
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.232 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 76.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=1.320 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.3%, forg= 21.7% <<<
>>> Test on task  2 : loss=1.417 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 61.2%, forg= 15.5% <<<
>>> Test on task  3 : loss=1.114 | TAw acc= 94.7%, forg=  2.7%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.268 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task  5 : loss=1.166 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 69.5%, forg= 12.4% <<<
>>> Test on task  6 : loss=1.189 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 73.8%, forg=  4.9% <<<
>>> Test on task  7 : loss=1.468 | TAw acc= 95.0%, forg=  1.0%| TAg acc= 69.3%, forg=  6.9% <<<
>>> Test on task  8 : loss=1.420 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 67.0%, forg=  9.6% <<<
>>> Test on task  9 : loss=1.955 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.2%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.471 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 63.4%, forg= 13.4% <<<
>>> Test on task 11 : loss=1.202 | TAw acc= 92.2%, forg=  0.9%| TAg acc= 67.8%, forg=  4.3% <<<
>>> Test on task 12 : loss=1.712 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 64.2%, forg=  4.6% <<<
>>> Test on task 13 : loss=1.765 | TAw acc= 90.1%, forg=  1.1%| TAg acc= 42.9%, forg= 16.5% <<<
>>> Test on task 14 : loss=1.203 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 78.2%, forg=  1.7% <<<
>>> Test on task 15 : loss=1.296 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.8%, forg=  3.8% <<<
>>> Test on task 16 : loss=1.521 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 71.7%, forg=  3.5% <<<
>>> Test on task 17 : loss=1.266 | TAw acc= 99.0%, forg= -1.0%| TAg acc= 64.1%, forg= 12.6% <<<
>>> Test on task 18 : loss=1.306 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 62.8%, forg= 13.3% <<<
>>> Test on task 19 : loss=1.073 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.2%, forg=  8.7% <<<
>>> Test on task 20 : loss=1.637 | TAw acc= 95.0%, forg=  1.7%| TAg acc= 62.2%, forg= 10.1% <<<
>>> Test on task 21 : loss=1.490 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 69.2%, forg=  8.4% <<<
>>> Test on task 22 : loss=1.317 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 67.5%, forg=  3.5% <<<
>>> Test on task 23 : loss=1.383 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  5.5% <<<
>>> Test on task 24 : loss=1.461 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 65.5%, forg=  8.8% <<<
>>> Test on task 25 : loss=1.485 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 61.7%, forg=  1.9% <<<
>>> Test on task 26 : loss=1.891 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 56.0%, forg=  0.9% <<<
>>> Test on task 27 : loss=0.999 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 79.6%, forg=  1.0% <<<
>>> Test on task 28 : loss=1.539 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 57.5%, forg=  6.2% <<<
>>> Test on task 29 : loss=1.347 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 66.4%, forg=  2.5% <<<
>>> Test on task 30 : loss=1.169 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task 31 : loss=1.299 | TAw acc= 95.7%, forg=  1.1%| TAg acc= 64.5%, forg=  3.2% <<<
>>> Test on task 32 : loss=1.399 | TAw acc= 94.9%, forg=  1.7%| TAg acc= 66.1%, forg=  0.8% <<<
>>> Test on task 33 : loss=1.044 | TAw acc= 96.6%, forg=  2.5%| TAg acc= 72.0%, forg= 11.9% <<<
>>> Test on task 34 : loss=1.332 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 66.1%, forg=  4.3% <<<
>>> Test on task 35 : loss=1.506 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 51.9%, forg= 25.9% <<<
>>> Test on task 36 : loss=1.757 | TAw acc= 89.5%, forg=  2.9%| TAg acc= 39.0%, forg= 19.0% <<<
>>> Test on task 37 : loss=1.300 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=4.723, TAw acc= 56.4% | *
| Epoch   2, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.850, TAw acc= 80.9% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.294, TAw acc= 95.7% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.066, TAw acc= 98.9% | *
| Epoch   5, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.011, TAw acc= 94.7% | *
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.011, TAw acc= 94.7% | *
| Epoch   2, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.011, TAw acc= 95.7% | *
| Epoch   3, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.010, TAw acc= 94.7% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.010, TAw acc= 94.7% | *
| Epoch   5, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.010, TAw acc= 94.7% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5584 train exemplars, time=  0.0s
5584
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.265 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 76.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=1.127 | TAw acc= 96.5%, forg=  1.7%| TAg acc= 69.6%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.417 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task  3 : loss=1.120 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 71.7%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.298 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 68.8%, forg= 12.8% <<<
>>> Test on task  5 : loss=1.258 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.6%, forg= 14.3% <<<
>>> Test on task  6 : loss=1.243 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 72.1%, forg=  6.6% <<<
>>> Test on task  7 : loss=1.403 | TAw acc= 94.1%, forg=  2.0%| TAg acc= 74.3%, forg=  2.0% <<<
>>> Test on task  8 : loss=1.438 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 66.1%, forg= 10.4% <<<
>>> Test on task  9 : loss=2.008 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 59.2%, forg=  6.1% <<<
>>> Test on task 10 : loss=1.555 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.3%, forg= 12.5% <<<
>>> Test on task 11 : loss=1.144 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.778 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 62.4%, forg=  6.4% <<<
>>> Test on task 13 : loss=1.823 | TAw acc= 89.0%, forg=  2.2%| TAg acc= 46.2%, forg= 13.2% <<<
>>> Test on task 14 : loss=1.308 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 71.4%, forg=  8.4% <<<
>>> Test on task 15 : loss=1.392 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 62.3%, forg= 11.3% <<<
>>> Test on task 16 : loss=1.471 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 73.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.188 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 18 : loss=1.151 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.9%, forg=  6.2% <<<
>>> Test on task 19 : loss=1.055 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  4.8% <<<
>>> Test on task 20 : loss=1.631 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 59.7%, forg= 12.6% <<<
>>> Test on task 21 : loss=1.494 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 67.3%, forg= 10.3% <<<
>>> Test on task 22 : loss=1.346 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 64.9%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.287 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 67.3%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.323 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 68.1%, forg=  6.2% <<<
>>> Test on task 25 : loss=1.474 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 57.0%, forg=  6.5% <<<
>>> Test on task 26 : loss=1.851 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 58.7%, forg= -1.8% <<<
>>> Test on task 27 : loss=0.985 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 80.6%, forg=  0.0% <<<
>>> Test on task 28 : loss=1.571 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 54.0%, forg=  9.7% <<<
>>> Test on task 29 : loss=1.374 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 63.9%, forg=  5.0% <<<
>>> Test on task 30 : loss=1.194 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 75.9%, forg=  5.2% <<<
>>> Test on task 31 : loss=1.332 | TAw acc= 96.8%, forg=  0.0%| TAg acc= 67.7%, forg=  0.0% <<<
>>> Test on task 32 : loss=1.364 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 65.3%, forg=  1.7% <<<
>>> Test on task 33 : loss=1.020 | TAw acc= 97.5%, forg=  1.7%| TAg acc= 72.9%, forg= 11.0% <<<
>>> Test on task 34 : loss=1.361 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 59.1%, forg= 11.3% <<<
>>> Test on task 35 : loss=1.456 | TAw acc=100.0%, forg=  0.0%| TAg acc= 50.9%, forg= 26.9% <<<
>>> Test on task 36 : loss=1.721 | TAw acc= 89.5%, forg=  2.9%| TAg acc= 42.9%, forg= 15.2% <<<
>>> Test on task 37 : loss=1.554 | TAw acc= 97.2%, forg= -1.8%| TAg acc= 52.3%, forg= 12.8% <<<
>>> Test on task 38 : loss=1.124 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 71.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=5.868, TAw acc= 48.1% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=2.664, TAw acc= 79.7% | *
| Epoch   3, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.830, TAw acc= 88.6% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.538, TAw acc= 96.2% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.265, TAw acc= 96.2% | *
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.261, TAw acc= 96.2% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.259, TAw acc= 96.2% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.257, TAw acc= 96.2% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.255, TAw acc= 96.2% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.253, TAw acc= 96.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5724 train exemplars, time=  0.0s
5724
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.234 | TAw acc= 97.2%, forg=  0.6%| TAg acc= 77.8%, forg=  9.4% <<<
>>> Test on task  1 : loss=1.213 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 65.2%, forg= 20.9% <<<
>>> Test on task  2 : loss=1.566 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 54.3%, forg= 22.4% <<<
>>> Test on task  3 : loss=1.174 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 72.6%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.297 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 70.6%, forg= 11.0% <<<
>>> Test on task  5 : loss=1.169 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.5%, forg= 11.4% <<<
>>> Test on task  6 : loss=1.243 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 71.3%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.431 | TAw acc= 95.0%, forg=  1.0%| TAg acc= 71.3%, forg=  5.0% <<<
>>> Test on task  8 : loss=1.543 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 57.4%, forg= 19.1% <<<
>>> Test on task  9 : loss=1.999 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 58.2%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.413 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 67.0%, forg=  9.8% <<<
>>> Test on task 11 : loss=1.212 | TAw acc= 94.8%, forg= -1.7%| TAg acc= 66.1%, forg=  6.1% <<<
>>> Test on task 12 : loss=1.781 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 58.7%, forg= 10.1% <<<
>>> Test on task 13 : loss=1.814 | TAw acc= 85.7%, forg=  5.5%| TAg acc= 45.1%, forg= 14.3% <<<
>>> Test on task 14 : loss=1.295 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 73.9%, forg=  5.9% <<<
>>> Test on task 15 : loss=1.377 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.0%, forg=  7.5% <<<
>>> Test on task 16 : loss=1.521 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 75.2%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.126 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 74.8%, forg=  1.9% <<<
>>> Test on task 18 : loss=1.220 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.9%, forg=  6.2% <<<
>>> Test on task 19 : loss=1.006 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.0%, forg=  3.8% <<<
>>> Test on task 20 : loss=1.703 | TAw acc= 96.6%, forg=  0.0%| TAg acc= 59.7%, forg= 12.6% <<<
>>> Test on task 21 : loss=1.469 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.9%, forg=  4.7% <<<
>>> Test on task 22 : loss=1.296 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 69.3%, forg=  1.8% <<<
>>> Test on task 23 : loss=1.231 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 71.8%, forg=  0.0% <<<
>>> Test on task 24 : loss=1.357 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 68.1%, forg=  6.2% <<<
>>> Test on task 25 : loss=1.463 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 64.5%, forg= -0.9% <<<
>>> Test on task 26 : loss=1.850 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 54.1%, forg=  4.6% <<<
>>> Test on task 27 : loss=0.970 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 80.6%, forg=  0.0% <<<
>>> Test on task 28 : loss=1.521 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 57.5%, forg=  6.2% <<<
>>> Test on task 29 : loss=1.311 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 64.7%, forg=  4.2% <<<
>>> Test on task 30 : loss=1.181 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 79.3%, forg=  1.7% <<<
>>> Test on task 31 : loss=1.379 | TAw acc= 95.7%, forg=  1.1%| TAg acc= 61.3%, forg=  6.5% <<<
>>> Test on task 32 : loss=1.362 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 66.1%, forg=  0.8% <<<
>>> Test on task 33 : loss=0.919 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 78.0%, forg=  5.9% <<<
>>> Test on task 34 : loss=1.347 | TAw acc= 97.4%, forg= -0.9%| TAg acc= 62.6%, forg=  7.8% <<<
>>> Test on task 35 : loss=1.404 | TAw acc=100.0%, forg=  0.0%| TAg acc= 55.6%, forg= 22.2% <<<
>>> Test on task 36 : loss=1.625 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 51.4%, forg=  6.7% <<<
>>> Test on task 37 : loss=1.306 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg= -1.8% <<<
>>> Test on task 38 : loss=1.487 | TAw acc= 97.6%, forg= -3.2%| TAg acc= 58.1%, forg= 13.7% <<<
>>> Test on task 39 : loss=1.221 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=5.902, TAw acc= 43.4% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=2.926, TAw acc= 68.4% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.940, TAw acc= 82.9% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.579, TAw acc= 92.1% | *
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.475, TAw acc= 93.4% | *
| Epoch   1, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.467, TAw acc= 96.1% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.461, TAw acc= 96.1% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.455, TAw acc= 96.1% | *
| Epoch   4, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.451, TAw acc= 96.1% | *
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.447, TAw acc= 96.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 5864 train exemplars, time=  0.0s
5864
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.207 | TAw acc= 96.7%, forg=  1.1%| TAg acc= 79.4%, forg=  7.8% <<<
>>> Test on task  1 : loss=1.207 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 67.0%, forg= 19.1% <<<
>>> Test on task  2 : loss=1.470 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task  3 : loss=1.176 | TAw acc= 95.6%, forg=  1.8%| TAg acc= 77.0%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.348 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.169 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.5%, forg= 11.4% <<<
>>> Test on task  6 : loss=1.233 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 70.5%, forg=  8.2% <<<
>>> Test on task  7 : loss=1.490 | TAw acc= 94.1%, forg=  2.0%| TAg acc= 71.3%, forg=  5.0% <<<
>>> Test on task  8 : loss=1.460 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 67.0%, forg=  9.6% <<<
>>> Test on task  9 : loss=1.982 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 59.2%, forg=  6.1% <<<
>>> Test on task 10 : loss=1.542 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 60.7%, forg= 16.1% <<<
>>> Test on task 11 : loss=1.208 | TAw acc= 95.7%, forg= -0.9%| TAg acc= 65.2%, forg=  7.0% <<<
>>> Test on task 12 : loss=1.787 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 62.4%, forg=  6.4% <<<
>>> Test on task 13 : loss=1.898 | TAw acc= 85.7%, forg=  5.5%| TAg acc= 44.0%, forg= 15.4% <<<
>>> Test on task 14 : loss=1.283 | TAw acc= 94.1%, forg=  1.7%| TAg acc= 73.1%, forg=  6.7% <<<
>>> Test on task 15 : loss=1.347 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.0%, forg=  7.5% <<<
>>> Test on task 16 : loss=1.487 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 73.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.188 | TAw acc= 97.1%, forg=  1.9%| TAg acc= 67.0%, forg=  9.7% <<<
>>> Test on task 18 : loss=1.083 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 76.1%, forg=  0.0% <<<
>>> Test on task 19 : loss=0.955 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.9%, forg=  1.9% <<<
>>> Test on task 20 : loss=1.738 | TAw acc= 95.0%, forg=  1.7%| TAg acc= 59.7%, forg= 12.6% <<<
>>> Test on task 21 : loss=1.499 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 72.0%, forg=  5.6% <<<
>>> Test on task 22 : loss=1.362 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.9%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.273 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 69.1%, forg=  2.7% <<<
>>> Test on task 24 : loss=1.394 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 66.4%, forg=  8.0% <<<
>>> Test on task 25 : loss=1.449 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.2%, forg= -3.7% <<<
>>> Test on task 26 : loss=1.883 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 58.7%, forg=  0.0% <<<
>>> Test on task 27 : loss=0.928 | TAw acc= 96.1%, forg= -1.0%| TAg acc= 81.6%, forg= -1.0% <<<
>>> Test on task 28 : loss=1.546 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 57.5%, forg=  6.2% <<<
>>> Test on task 29 : loss=1.282 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 64.7%, forg=  4.2% <<<
>>> Test on task 30 : loss=1.105 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task 31 : loss=1.300 | TAw acc= 97.8%, forg= -1.1%| TAg acc= 66.7%, forg=  1.1% <<<
>>> Test on task 32 : loss=1.327 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 66.9%, forg=  0.0% <<<
>>> Test on task 33 : loss=0.986 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 70.3%, forg= 13.6% <<<
>>> Test on task 34 : loss=1.330 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 65.2%, forg=  5.2% <<<
>>> Test on task 35 : loss=1.371 | TAw acc=100.0%, forg=  0.0%| TAg acc= 58.3%, forg= 19.4% <<<
>>> Test on task 36 : loss=1.638 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 49.5%, forg=  8.6% <<<
>>> Test on task 37 : loss=1.299 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.7%, forg= -2.8% <<<
>>> Test on task 38 : loss=1.438 | TAw acc= 98.4%, forg= -0.8%| TAg acc= 58.1%, forg= 13.7% <<<
>>> Test on task 39 : loss=1.473 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 61.1%, forg= 13.9% <<<
>>> Test on task 40 : loss=1.463 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=6.594, TAw acc= 36.6% | *
| Epoch   2, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=3.142, TAw acc= 83.1% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=2.192, TAw acc= 84.5% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.830, TAw acc= 93.0% | *
| Epoch   5, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.788, TAw acc= 94.4% | *
| Epoch   1, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.778, TAw acc= 94.4% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.769, TAw acc= 94.4% | *
| Epoch   3, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=1.760, TAw acc= 94.4% | *
| Epoch   4, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.752, TAw acc= 94.4% | *
| Epoch   5, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=1.744, TAw acc= 94.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6004 train exemplars, time=  0.0s
6004
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.251 | TAw acc= 96.7%, forg=  1.1%| TAg acc= 77.2%, forg= 10.0% <<<
>>> Test on task  1 : loss=1.148 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.506 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 60.3%, forg= 16.4% <<<
>>> Test on task  3 : loss=1.230 | TAw acc= 94.7%, forg=  2.7%| TAg acc= 71.7%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.318 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 73.4%, forg=  8.3% <<<
>>> Test on task  5 : loss=1.328 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 67.6%, forg= 14.3% <<<
>>> Test on task  6 : loss=1.241 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 72.1%, forg=  6.6% <<<
>>> Test on task  7 : loss=1.427 | TAw acc= 95.0%, forg=  1.0%| TAg acc= 72.3%, forg=  4.0% <<<
>>> Test on task  8 : loss=1.477 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 66.1%, forg= 10.4% <<<
>>> Test on task  9 : loss=1.976 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 56.1%, forg=  9.2% <<<
>>> Test on task 10 : loss=1.482 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 65.2%, forg= 11.6% <<<
>>> Test on task 11 : loss=1.133 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 70.4%, forg=  1.7% <<<
>>> Test on task 12 : loss=1.874 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 59.6%, forg=  9.2% <<<
>>> Test on task 13 : loss=1.835 | TAw acc= 86.8%, forg=  4.4%| TAg acc= 46.2%, forg= 13.2% <<<
>>> Test on task 14 : loss=1.279 | TAw acc= 93.3%, forg=  2.5%| TAg acc= 70.6%, forg=  9.2% <<<
>>> Test on task 15 : loss=1.302 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.0%, forg=  6.6% <<<
>>> Test on task 16 : loss=1.516 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 74.3%, forg=  0.9% <<<
>>> Test on task 17 : loss=1.193 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.9%, forg=  6.8% <<<
>>> Test on task 18 : loss=1.204 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 72.6%, forg=  3.5% <<<
>>> Test on task 19 : loss=0.966 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.9%, forg=  2.9% <<<
>>> Test on task 20 : loss=1.676 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 62.2%, forg= 10.1% <<<
>>> Test on task 21 : loss=1.467 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 75.7%, forg=  1.9% <<<
>>> Test on task 22 : loss=1.366 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.9%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.327 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 67.3%, forg=  4.5% <<<
>>> Test on task 24 : loss=1.410 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 63.7%, forg= 10.6% <<<
>>> Test on task 25 : loss=1.485 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 60.7%, forg=  7.5% <<<
>>> Test on task 26 : loss=2.003 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 50.5%, forg=  8.3% <<<
>>> Test on task 27 : loss=1.011 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 81.6%, forg=  0.0% <<<
>>> Test on task 28 : loss=1.521 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 57.5%, forg=  6.2% <<<
>>> Test on task 29 : loss=1.306 | TAw acc= 99.2%, forg= -0.8%| TAg acc= 64.7%, forg=  4.2% <<<
>>> Test on task 30 : loss=1.050 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task 31 : loss=1.325 | TAw acc= 95.7%, forg=  2.2%| TAg acc= 64.5%, forg=  3.2% <<<
>>> Test on task 32 : loss=1.345 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 68.6%, forg= -1.7% <<<
>>> Test on task 33 : loss=0.967 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 75.4%, forg=  8.5% <<<
>>> Test on task 34 : loss=1.281 | TAw acc= 97.4%, forg=  0.0%| TAg acc= 68.7%, forg=  1.7% <<<
>>> Test on task 35 : loss=1.280 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 56.5%, forg= 21.3% <<<
>>> Test on task 36 : loss=1.552 | TAw acc= 90.5%, forg=  1.9%| TAg acc= 59.0%, forg= -1.0% <<<
>>> Test on task 37 : loss=1.190 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 73.4%, forg= -3.7% <<<
>>> Test on task 38 : loss=1.407 | TAw acc= 97.6%, forg=  0.8%| TAg acc= 62.1%, forg=  9.7% <<<
>>> Test on task 39 : loss=1.431 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.5%, forg=  6.5% <<<
>>> Test on task 40 : loss=1.745 | TAw acc= 96.2%, forg=  0.0%| TAg acc= 52.9%, forg=  6.7% <<<
>>> Test on task 41 : loss=1.658 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 51.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=4.284, TAw acc= 80.2% | *
| Epoch   2, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.739, TAw acc= 88.5% | *
| Epoch   3, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=1.154, TAw acc= 99.0% | *
| Epoch   4, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.963, TAw acc=100.0% | *
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.908, TAw acc=100.0% | *
| Epoch   1, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.902, TAw acc=100.0% | *
| Epoch   2, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=0.896, TAw acc=100.0% | *
| Epoch   3, time=  6.3s | Train: skip eval | Valid: time=  0.2s loss=0.892, TAw acc=100.0% | *
| Epoch   4, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=0.889, TAw acc=100.0% | *
| Epoch   5, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=0.885, TAw acc=100.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6144 train exemplars, time=  0.1s
6144
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.254 | TAw acc= 96.1%, forg=  1.7%| TAg acc= 77.8%, forg=  9.4% <<<
>>> Test on task  1 : loss=1.179 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 72.2%, forg= 13.9% <<<
>>> Test on task  2 : loss=1.498 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 57.8%, forg= 19.0% <<<
>>> Test on task  3 : loss=1.355 | TAw acc= 92.9%, forg=  4.4%| TAg acc= 68.1%, forg= 15.9% <<<
>>> Test on task  4 : loss=1.363 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 71.6%, forg= 10.1% <<<
>>> Test on task  5 : loss=1.259 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 70.5%, forg= 11.4% <<<
>>> Test on task  6 : loss=1.301 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 68.9%, forg=  9.8% <<<
>>> Test on task  7 : loss=1.500 | TAw acc= 95.0%, forg=  1.0%| TAg acc= 71.3%, forg=  5.0% <<<
>>> Test on task  8 : loss=1.628 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 58.3%, forg= 18.3% <<<
>>> Test on task  9 : loss=2.049 | TAw acc= 96.9%, forg=  1.0%| TAg acc= 57.1%, forg=  8.2% <<<
>>> Test on task 10 : loss=1.526 | TAw acc= 98.2%, forg=  0.9%| TAg acc= 64.3%, forg= 12.5% <<<
>>> Test on task 11 : loss=1.132 | TAw acc= 93.9%, forg=  1.7%| TAg acc= 69.6%, forg=  2.6% <<<
>>> Test on task 12 : loss=1.728 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 63.3%, forg=  5.5% <<<
>>> Test on task 13 : loss=1.871 | TAw acc= 86.8%, forg=  4.4%| TAg acc= 41.8%, forg= 17.6% <<<
>>> Test on task 14 : loss=1.328 | TAw acc= 91.6%, forg=  4.2%| TAg acc= 68.9%, forg= 10.9% <<<
>>> Test on task 15 : loss=1.389 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 66.0%, forg=  7.5% <<<
>>> Test on task 16 : loss=1.555 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 73.5%, forg=  1.8% <<<
>>> Test on task 17 : loss=1.190 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.9%, forg=  7.8% <<<
>>> Test on task 18 : loss=1.262 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 68.1%, forg=  8.0% <<<
>>> Test on task 19 : loss=0.996 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.9%, forg=  2.9% <<<
>>> Test on task 20 : loss=1.722 | TAw acc= 95.0%, forg=  1.7%| TAg acc= 61.3%, forg= 10.9% <<<
>>> Test on task 21 : loss=1.470 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 75.7%, forg=  1.9% <<<
>>> Test on task 22 : loss=1.390 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.9%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.296 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 66.4%, forg=  5.5% <<<
>>> Test on task 24 : loss=1.431 | TAw acc= 95.6%, forg=  0.9%| TAg acc= 62.8%, forg= 11.5% <<<
>>> Test on task 25 : loss=1.541 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 60.7%, forg=  7.5% <<<
>>> Test on task 26 : loss=1.797 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 61.5%, forg= -2.8% <<<
>>> Test on task 27 : loss=0.953 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 79.6%, forg=  1.9% <<<
>>> Test on task 28 : loss=1.579 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 55.8%, forg=  8.0% <<<
>>> Test on task 29 : loss=1.362 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 61.3%, forg=  7.6% <<<
>>> Test on task 30 : loss=1.069 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task 31 : loss=1.303 | TAw acc= 96.8%, forg=  1.1%| TAg acc= 60.2%, forg=  7.5% <<<
>>> Test on task 32 : loss=1.354 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 65.3%, forg=  3.4% <<<
>>> Test on task 33 : loss=0.977 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 73.7%, forg= 10.2% <<<
>>> Test on task 34 : loss=1.307 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 65.2%, forg=  5.2% <<<
>>> Test on task 35 : loss=1.344 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 61.1%, forg= 16.7% <<<
>>> Test on task 36 : loss=1.566 | TAw acc= 91.4%, forg=  1.0%| TAg acc= 57.1%, forg=  1.9% <<<
>>> Test on task 37 : loss=1.174 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.3%, forg= -0.9% <<<
>>> Test on task 38 : loss=1.419 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 58.1%, forg= 13.7% <<<
>>> Test on task 39 : loss=1.393 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 66.7%, forg=  8.3% <<<
>>> Test on task 40 : loss=1.808 | TAw acc= 90.4%, forg=  5.8%| TAg acc= 51.0%, forg=  8.7% <<<
>>> Test on task 41 : loss=1.743 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 49.5%, forg=  2.0% <<<
>>> Test on task 42 : loss=1.242 | TAw acc= 97.6%, forg=  0.0%| TAg acc= 74.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=6.906, TAw acc= 43.1% | *
| Epoch   2, time=  6.2s | Train: skip eval | Valid: time=  0.2s loss=2.984, TAw acc= 86.1% | *
| Epoch   3, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=1.535, TAw acc= 93.1% | *
| Epoch   4, time=  6.1s | Train: skip eval | Valid: time=  0.2s loss=1.380, TAw acc= 98.6% | *
| Epoch   5, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.165, TAw acc= 98.6% | *
| Epoch   1, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 98.6% | *
| Epoch   2, time=  6.6s | Train: skip eval | Valid: time=  0.2s loss=1.156, TAw acc= 98.6% | *
| Epoch   3, time=  6.5s | Train: skip eval | Valid: time=  0.2s loss=1.152, TAw acc= 98.6% | *
| Epoch   4, time=  6.4s | Train: skip eval | Valid: time=  0.2s loss=1.149, TAw acc= 98.6% | *
| Epoch   5, time=  6.6s | Train: skip eval | Valid: time=  0.2s loss=1.146, TAw acc= 98.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 8
| Selected 6284 train exemplars, time=  0.0s
6284
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.261 | TAw acc= 96.7%, forg=  1.1%| TAg acc= 76.7%, forg= 10.6% <<<
>>> Test on task  1 : loss=1.176 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 69.6%, forg= 16.5% <<<
>>> Test on task  2 : loss=1.535 | TAw acc= 96.6%, forg=  0.9%| TAg acc= 57.8%, forg= 19.0% <<<
>>> Test on task  3 : loss=1.222 | TAw acc= 93.8%, forg=  3.5%| TAg acc= 71.7%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.394 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.7%, forg= 11.9% <<<
>>> Test on task  5 : loss=1.316 | TAw acc= 95.2%, forg=  1.0%| TAg acc= 66.7%, forg= 15.2% <<<
>>> Test on task  6 : loss=1.286 | TAw acc= 95.9%, forg=  0.0%| TAg acc= 71.3%, forg=  7.4% <<<
>>> Test on task  7 : loss=1.489 | TAw acc= 94.1%, forg=  2.0%| TAg acc= 70.3%, forg=  5.9% <<<
>>> Test on task  8 : loss=1.581 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 62.6%, forg= 13.9% <<<
>>> Test on task  9 : loss=2.047 | TAw acc= 95.9%, forg=  2.0%| TAg acc= 56.1%, forg=  9.2% <<<
>>> Test on task 10 : loss=1.473 | TAw acc= 97.3%, forg=  1.8%| TAg acc= 64.3%, forg= 12.5% <<<
>>> Test on task 11 : loss=1.127 | TAw acc= 93.9%, forg=  1.7%| TAg acc= 68.7%, forg=  3.5% <<<
>>> Test on task 12 : loss=1.865 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 62.4%, forg=  6.4% <<<
>>> Test on task 13 : loss=2.016 | TAw acc= 89.0%, forg=  2.2%| TAg acc= 42.9%, forg= 16.5% <<<
>>> Test on task 14 : loss=1.298 | TAw acc= 93.3%, forg=  2.5%| TAg acc= 72.3%, forg=  7.6% <<<
>>> Test on task 15 : loss=1.357 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.9%, forg=  5.7% <<<
>>> Test on task 16 : loss=1.523 | TAw acc= 92.9%, forg=  3.5%| TAg acc= 75.2%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.223 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 63.1%, forg= 13.6% <<<
>>> Test on task 18 : loss=1.295 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 69.0%, forg=  7.1% <<<
>>> Test on task 19 : loss=0.960 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.8%, forg=  1.0% <<<
>>> Test on task 20 : loss=1.661 | TAw acc= 95.0%, forg=  1.7%| TAg acc= 63.0%, forg=  9.2% <<<
>>> Test on task 21 : loss=1.409 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 75.7%, forg=  1.9% <<<
>>> Test on task 22 : loss=1.371 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 64.0%, forg=  7.0% <<<
>>> Test on task 23 : loss=1.278 | TAw acc= 96.4%, forg=  1.8%| TAg acc= 66.4%, forg=  5.5% <<<
>>> Test on task 24 : loss=1.392 | TAw acc= 94.7%, forg=  1.8%| TAg acc= 67.3%, forg=  7.1% <<<
>>> Test on task 25 : loss=1.520 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 59.8%, forg=  8.4% <<<
>>> Test on task 26 : loss=1.912 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 56.9%, forg=  4.6% <<<
>>> Test on task 27 : loss=0.947 | TAw acc= 96.1%, forg=  0.0%| TAg acc= 79.6%, forg=  1.9% <<<
>>> Test on task 28 : loss=1.553 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 55.8%, forg=  8.0% <<<
>>> Test on task 29 : loss=1.270 | TAw acc= 99.2%, forg=  0.0%| TAg acc= 66.4%, forg=  2.5% <<<
>>> Test on task 30 : loss=1.045 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task 31 : loss=1.292 | TAw acc= 97.8%, forg=  0.0%| TAg acc= 66.7%, forg=  1.1% <<<
>>> Test on task 32 : loss=1.340 | TAw acc= 95.8%, forg=  0.8%| TAg acc= 67.8%, forg=  0.8% <<<
>>> Test on task 33 : loss=0.992 | TAw acc= 98.3%, forg=  0.8%| TAg acc= 73.7%, forg= 10.2% <<<
>>> Test on task 34 : loss=1.304 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 65.2%, forg=  5.2% <<<
>>> Test on task 35 : loss=1.393 | TAw acc= 99.1%, forg=  0.9%| TAg acc= 56.5%, forg= 21.3% <<<
>>> Test on task 36 : loss=1.590 | TAw acc= 90.5%, forg=  1.9%| TAg acc= 59.0%, forg=  0.0% <<<
>>> Test on task 37 : loss=1.156 | TAw acc= 98.2%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
>>> Test on task 38 : loss=1.389 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 60.5%, forg= 11.3% <<<
>>> Test on task 39 : loss=1.376 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 63.9%, forg= 11.1% <<<
>>> Test on task 40 : loss=1.794 | TAw acc= 91.3%, forg=  4.8%| TAg acc= 51.0%, forg=  8.7% <<<
>>> Test on task 41 : loss=1.670 | TAw acc= 97.0%, forg= -1.0%| TAg acc= 49.5%, forg=  2.0% <<<
>>> Test on task 42 : loss=1.710 | TAw acc= 96.9%, forg=  0.8%| TAg acc= 47.2%, forg= 26.8% <<<
>>> Test on task 43 : loss=1.401 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 64.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_1/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 71.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.7% 
	 87.8%  80.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 84.3% 
	 86.7%  93.0%  81.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 86.9% 
	 88.9%  93.0%  93.1%  95.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.7% 
	 90.0%  93.0%  96.6%  96.5%  88.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 92.8% 
	 92.2%  95.7%  96.6%  96.5%  96.3%  88.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.3% 
	 92.2%  93.9%  96.6%  93.8%  96.3%  94.3%  92.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.2% 
	 91.1%  96.5%  97.4%  95.6%  96.3%  92.4%  95.9%  82.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.4% 
	 95.6%  93.0%  96.6%  93.8%  95.4%  95.2%  95.9%  92.1%  93.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.5% 
	 94.4%  96.5%  96.6%  95.6%  95.4%  94.3%  95.9%  90.1%  98.3%  84.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.2% 
	 95.0%  95.7%  96.6%  95.6%  96.3%  95.2%  95.9%  92.1%  98.3%  91.8%  95.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 96.1%  93.9%  96.6%  96.5%  96.3%  91.4%  95.9%  95.0%  98.3%  96.9%  98.2%  89.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.4% 
	 95.6%  93.0%  96.6%  97.3%  95.4%  90.5%  95.9%  93.1%  97.4%  96.9%  99.1%  88.7%  92.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 95.6%  94.8%  96.6%  96.5%  95.4%  94.3%  95.9%  95.0%  98.3%  98.0%  99.1%  91.3%  95.4%  79.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 95.0%  93.0%  96.6%  95.6%  96.3%  94.3%  95.9%  95.0%  98.3%  94.9%  99.1%  89.6%  94.5%  90.1%  92.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 95.0%  93.9%  96.6%  96.5%  96.3%  95.2%  95.9%  94.1%  98.3%  98.0%  99.1%  89.6%  95.4%  85.7%  92.4%  95.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 95.0%  95.7%  96.6%  94.7%  96.3%  94.3%  95.9%  94.1%  98.3%  98.0%  99.1%  91.3%  95.4%  87.9%  95.0%  95.3%  87.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 95.6%  96.5%  96.6%  96.5%  96.3%  95.2%  95.9%  94.1%  97.4%  95.9%  98.2%  91.3%  94.5%  87.9%  94.1%  97.2%  93.8%  88.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 95.0%  95.7%  96.6%  92.9%  96.3%  95.2%  95.9%  95.0%  97.4%  95.9%  95.5%  92.2%  95.4%  87.9%  91.6%  97.2%  94.7%  98.1%  95.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 95.6%  96.5%  95.7%  92.9%  96.3%  95.2%  95.9%  94.1%  98.3%  98.0%  98.2%  91.3%  95.4%  86.8%  94.1%  97.2%  94.7%  98.1%  96.5%  96.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 95.0%  96.5%  95.7%  92.9%  96.3%  94.3%  95.9%  94.1%  96.5%  95.9%  98.2%  91.3%  95.4%  87.9%  94.1%  97.2%  95.6%  97.1%  96.5%  99.0%  93.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 96.7%  96.5%  95.7%  93.8%  96.3%  94.3%  95.9%  94.1%  97.4%  98.0%  98.2%  90.4%  95.4%  89.0%  95.8%  97.2%  94.7%  97.1%  96.5%  99.0%  94.1%  90.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.3% 
	 97.2%  98.3%  95.7%  92.9%  96.3%  95.2%  95.9%  94.1%  97.4%  98.0%  96.4%  90.4%  95.4%  90.1%  93.3%  97.2%  94.7%  98.1%  96.5%  99.0%  95.8%  92.5%  97.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.1%  96.5%  95.7%  92.9%  96.3%  95.2%  95.9%  95.0%  97.4%  98.0%  98.2%  93.0%  95.4%  91.2%  95.0%  97.2%  96.5%  97.1%  96.5%  99.0%  96.6%  92.5%  98.2%  90.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 96.7%  95.7%  95.7%  93.8%  96.3%  95.2%  95.9%  95.0%  97.4%  98.0%  98.2%  91.3%  95.4%  90.1%  95.0%  97.2%  96.5%  97.1%  96.5%  99.0%  96.6%  91.6%  98.2%  98.2%  92.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 96.7%  96.5%  95.7%  92.9%  96.3%  95.2%  95.9%  94.1%  97.4%  98.0%  97.3%  93.0%  95.4%  90.1%  95.0%  97.2%  94.7%  98.1%  96.5%  99.0%  96.6%  94.4%  98.2%  96.4%  94.7%  96.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 96.1%  98.3%  96.6%  92.9%  96.3%  95.2%  95.9%  94.1%  97.4%  98.0%  97.3%  93.0%  95.4%  89.0%  94.1%  97.2%  94.7%  98.1%  96.5%  99.0%  95.0%  92.5%  98.2%  97.3%  92.0%  95.3%  93.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.5% 
	 96.1%  96.5%  96.6%  93.8%  96.3%  96.2%  95.9%  94.1%  96.5%  98.0%  97.3%  91.3%  95.4%  87.9%  94.1%  97.2%  94.7%  98.1%  97.3%  99.0%  95.8%  94.4%  98.2%  97.3%  92.9%  95.3%  95.4%  94.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.6% 
	 97.2%  98.3%  96.6%  93.8%  96.3%  96.2%  95.9%  94.1%  97.4%  98.0%  97.3%  91.3%  95.4%  86.8%  93.3%  97.2%  94.7%  98.1%  97.3%  99.0%  96.6%  94.4%  98.2%  97.3%  95.6%  97.2%  96.3%  94.2%  95.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.8% 
	 96.7%  97.4%  96.6%  93.8%  95.4%  95.2%  95.9%  95.0%  97.4%  98.0%  98.2%  92.2%  95.4%  87.9%  91.6%  97.2%  94.7%  98.1%  97.3%  99.0%  96.6%  93.5%  98.2%  97.3%  95.6%  96.3%  96.3%  95.1%  97.3%  96.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 96.7%  98.3%  96.6%  93.8%  95.4%  95.2%  95.9%  95.0%  97.4%  98.0%  98.2%  92.2%  95.4%  87.9%  93.3%  97.2%  92.9%  97.1%  97.3%  99.0%  96.6%  92.5%  98.2%  96.4%  95.6%  96.3%  94.5%  94.2%  97.3%  97.5%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.7% 
	 97.8%  98.3%  96.6%  93.8%  95.4%  94.3%  95.9%  94.1%  96.5%  96.9%  98.2%  91.3%  95.4%  87.9%  95.0%  97.2%  92.9%  98.1%  97.3%  99.0%  96.6%  94.4%  98.2%  97.3%  96.5%  96.3%  93.6%  95.1%  97.3%  97.5%  98.3%  94.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.2%  98.3%  96.6%  93.8%  95.4%  95.2%  95.9%  94.1%  97.4%  96.9%  98.2%  91.3%  95.4%  86.8%  95.0%  97.2%  92.9%  97.1%  97.3%  99.0%  96.6%  94.4%  98.2%  97.3%  95.6%  97.2%  94.5%  95.1%  97.3%  97.5%  98.3%  94.6%  96.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.8%  98.3%  96.6%  92.9%  95.4%  95.2%  95.9%  96.0%  97.4%  96.9%  98.2%  92.2%  95.4%  85.7%  92.4%  97.2%  92.9%  98.1%  97.3%  99.0%  96.6%  94.4%  98.2%  97.3%  95.6%  97.2%  94.5%  95.1%  97.3%  97.5%  98.3%  93.5%  94.9%  99.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.8%  98.3%  96.6%  92.9%  95.4%  95.2%  95.9%  95.0%  97.4%  96.9%  98.2%  92.2%  95.4%  89.0%  92.4%  96.2%  92.9%  97.1%  97.3%  99.0%  96.6%  94.4%  98.2%  97.3%  95.6%  97.2%  94.5%  95.1%  97.3%  96.6%  98.3%  96.8%  94.9%  97.5%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.8%  98.3%  96.6%  92.9%  95.4%  95.2%  95.9%  95.0%  97.4%  95.9%  98.2%  93.0%  95.4%  85.7%  92.4%  97.2%  92.9%  98.1%  98.2%  99.0%  95.8%  94.4%  98.2%  94.5%  96.5%  95.3%  95.4%  95.1%  97.3%  97.5%  98.3%  96.8%  95.8%  97.5%  95.7%  99.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.8%  96.5%  96.6%  94.7%  95.4%  95.2%  95.9%  94.1%  97.4%  96.9%  98.2%  93.0%  95.4%  85.7%  94.1%  97.2%  92.9%  98.1%  97.3%  99.0%  95.8%  94.4%  97.4%  95.5%  95.6%  96.3%  95.4%  95.1%  97.3%  98.3%  98.3%  96.8%  95.8%  96.6%  95.7% 100.0%  92.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.2%  97.4%  96.6%  94.7%  95.4%  95.2%  95.9%  95.0%  97.4%  96.9%  98.2%  92.2%  95.4%  90.1%  94.1%  97.2%  92.9%  99.0%  97.3%  99.0%  95.0%  94.4%  98.2%  97.3%  95.6%  96.3%  95.4%  95.1%  97.3%  98.3%  98.3%  95.7%  94.9%  96.6%  95.7%  99.1%  89.5%  95.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.9% 
	 97.8%  96.5%  96.6%  93.8%  95.4%  95.2%  95.9%  94.1%  97.4%  96.9%  98.2%  93.0%  95.4%  89.0%  94.1%  97.2%  92.9%  98.1%  97.3%  99.0%  95.8%  94.4%  98.2%  96.4%  96.5%  97.2%  95.4%  95.1%  97.3%  98.3%  98.3%  96.8%  95.8%  97.5%  96.5% 100.0%  89.5%  97.2%  94.4%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.0% 
	 97.2%  98.3%  96.6%  93.8%  95.4%  95.2%  95.9%  95.0%  97.4%  96.9%  98.2%  94.8%  95.4%  85.7%  94.1%  97.2%  92.9%  98.1%  97.3%  99.0%  96.6%  94.4%  97.4%  98.2%  94.7%  97.2%  95.4%  95.1%  97.3%  98.3%  98.3%  95.7%  95.8%  98.3%  97.4% 100.0%  91.4%  97.2%  97.6%  97.2%   0.0%   0.0%   0.0%   0.0% 	Avg.: 96.2% 
	 96.7%  98.3%  96.6%  95.6%  95.4%  95.2%  95.9%  94.1%  97.4%  96.9%  98.2%  95.7%  95.4%  85.7%  94.1%  97.2%  92.9%  97.1%  97.3%  99.0%  95.0%  94.4%  97.4%  98.2%  94.7%  97.2%  95.4%  96.1%  97.3%  98.3%  98.3%  97.8%  95.8%  98.3%  97.4% 100.0%  91.4%  97.2%  98.4%  96.3%  96.2%   0.0%   0.0%   0.0% 	Avg.: 96.2% 
	 96.7%  98.3%  96.6%  94.7%  95.4%  95.2%  95.9%  95.0%  97.4%  96.9%  98.2%  94.8%  95.4%  86.8%  93.3%  97.2%  92.9%  99.0%  97.3%  99.0%  95.8%  94.4%  97.4%  97.3%  95.6%  97.2%  95.4%  96.1%  97.3%  99.2%  98.3%  95.7%  95.8%  98.3%  97.4%  99.1%  90.5%  98.2%  97.6%  96.3%  96.2%  96.0%   0.0%   0.0% 	Avg.: 96.2% 
	 96.1%  97.4%  96.6%  92.9%  95.4%  95.2%  95.9%  95.0%  97.4%  96.9%  98.2%  93.9%  95.4%  86.8%  91.6%  97.2%  92.9%  99.0%  97.3%  99.0%  95.0%  94.4%  97.4%  96.4%  95.6%  97.2%  95.4%  96.1%  97.3%  99.2%  98.3%  96.8%  95.8%  98.3%  96.5%  99.1%  91.4%  98.2%  98.4%  96.3%  90.4%  96.0%  97.6%   0.0% 	Avg.: 96.0% 
	 96.7%  98.3%  96.6%  93.8%  96.3%  95.2%  95.9%  94.1%  97.4%  95.9%  97.3%  93.9%  95.4%  89.0%  93.3%  97.2%  92.9%  99.0%  97.3%  99.0%  95.0%  94.4%  97.4%  96.4%  94.7%  97.2%  95.4%  96.1%  97.3%  99.2%  98.3%  97.8%  95.8%  98.3%  96.5%  99.1%  90.5%  98.2%  98.4%  97.2%  91.3%  97.0%  96.9%  98.0% 	Avg.: 96.2% 
************************************************************************************************************
TAg Acc
	 71.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.7% 
	 77.8%  76.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 77.1% 
	 72.2%  77.4%  69.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.9% 
	 82.2%  80.9%  61.2%  72.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.2% 
	 81.7%  86.1%  59.5%  77.0%  69.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 74.8% 
	 85.6%  84.3%  62.1%  83.2%  62.4%  58.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.6% 
	 84.4%  83.5%  66.4%  74.3%  66.1%  54.3%  72.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 71.6% 
	 81.7%  81.7%  76.7%  83.2%  73.4%  67.6%  61.5%  53.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 72.4% 
	 85.6%  74.8%  69.8%  84.1%  67.0%  63.8%  66.4%  22.8%  67.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.8% 
	 84.4%  82.6%  69.0%  84.1%  77.1%  79.0%  69.7%  47.5%  60.0%  48.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.1% 
	 87.2%  78.3%  74.1%  82.3%  81.7%  66.7%  73.0%  53.5%  70.4%  36.7%  73.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 70.6% 
	 83.3%  82.6%  69.8%  76.1%  81.7%  81.9%  66.4%  63.4%  69.6%  44.9%  48.2%  67.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.6% 
	 83.9%  83.5%  69.0%  83.2%  78.9%  74.3%  74.6%  63.4%  68.7%  49.0%  56.2%  45.2%  56.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.2% 
	 81.1%  79.1%  74.1%  78.8%  80.7%  81.9%  78.7%  64.4%  70.4%  57.1%  67.0%  53.0%  33.9%  44.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.5% 
	 85.6%  79.1%  65.5%  79.6%  78.9%  74.3%  73.0%  50.5%  72.2%  58.2%  65.2%  57.4%  52.3%  29.7%  79.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.7% 
	 82.2%  81.7%  75.0%  77.9%  78.9%  79.0%  77.9%  69.3%  70.4%  61.2%  65.2%  67.0%  55.0%  36.3%  49.6%  63.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.1% 
	 84.4%  79.1%  72.4%  79.6%  76.1%  74.3%  71.3%  61.4%  71.3%  60.2%  73.2%  66.1%  66.1%  37.4%  61.3%  56.6%  74.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.5% 
	 80.6%  79.1%  73.3%  82.3%  81.7%  74.3%  74.6%  65.3%  73.9%  57.1%  66.1%  67.0%  59.6%  40.7%  61.3%  61.3%  45.1%  51.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.4% 
	 81.1%  79.1%  69.8%  72.6%  78.9%  75.2%  75.4%  69.3%  69.6%  61.2%  67.0%  72.2%  63.3%  40.7%  66.4%  64.2%  60.2%  30.1%  70.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.7% 
	 83.9%  79.1%  69.8%  75.2%  81.7%  81.0%  73.8%  70.3%  69.6%  60.2%  67.9%  68.7%  65.1%  41.8%  69.7%  71.7%  61.1%  52.4%  57.5%  71.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 76.7%  78.3%  69.8%  77.9%  75.2%  77.1%  75.4%  68.3%  73.9%  63.3%  70.5%  68.7%  67.9%  45.1%  68.1%  73.6%  71.7%  62.1%  61.9%  62.5%  72.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.5% 
	 81.7%  76.5%  71.6%  77.0%  78.0%  77.1%  73.8%  74.3%  73.0%  60.2%  70.5%  67.0%  61.5%  52.7%  66.4%  69.8%  66.4%  56.3%  66.4%  62.5%  52.9%  69.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.4% 
	 82.2%  77.4%  67.2%  67.3%  78.0%  72.4%  73.0%  68.3%  68.7%  65.3%  69.6%  67.0%  63.3%  51.6%  73.9%  70.8%  66.4%  66.0%  65.5%  69.2%  53.8%  55.1%  71.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 77.2%  74.8%  69.8%  74.3%  75.2%  75.2%  72.1%  73.3%  73.0%  60.2%  70.5%  69.6%  67.0%  52.7%  75.6%  71.7%  70.8%  65.0%  69.0%  70.2%  59.7%  64.5%  56.1%  68.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 81.1%  75.7%  68.1%  74.3%  73.4%  76.2%  73.8%  71.3%  76.5%  64.3%  65.2%  67.8%  62.4%  47.3%  75.6%  67.9%  73.5%  68.9%  70.8%  69.2%  58.8%  67.3%  57.9%  47.3%  67.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.1% 
	 81.1%  77.4%  63.8%  71.7%  72.5%  70.5%  72.1%  71.3%  75.7%  58.2%  76.8%  67.0%  66.1%  44.0%  74.8%  72.6%  72.6%  69.9%  74.3%  75.0%  63.9%  73.8%  62.3%  53.6%  61.9%  59.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 77.2%  74.8%  65.5%  74.3%  73.4%  78.1%  77.0%  70.3%  67.8%  60.2%  66.1%  68.7%  68.8%  51.6%  66.4%  68.9%  72.6%  71.8%  73.5%  71.2%  63.9%  73.8%  64.0%  55.5%  67.3%  48.6%  53.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 78.3%  74.8%  65.5%  77.0%  73.4%  78.1%  74.6%  70.3%  65.2%  61.2%  67.9%  68.7%  65.1%  59.3%  72.3%  69.8%  71.7%  68.9%  76.1%  72.1%  65.5%  73.8%  66.7%  68.2%  74.3%  57.0%  35.8%  73.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.8% 
	 77.8%  71.3%  62.1%  74.3%  72.5%  69.5%  76.2%  74.3%  68.7%  59.2%  65.2%  67.0%  65.1%  50.5%  74.8%  69.8%  73.5%  69.9%  75.2%  76.0%  64.7%  72.0%  64.9%  68.2%  64.6%  56.1%  38.5%  68.0%  63.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.4% 
	 78.3%  73.0%  67.2%  75.2%  73.4%  77.1%  72.1%  71.3%  64.3%  58.2%  66.1%  71.3%  67.0%  46.2%  72.3%  69.8%  75.2%  71.8%  72.6%  79.8%  63.0%  72.9%  65.8%  67.3%  66.4%  59.8%  51.4%  72.8%  49.6%  68.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 78.3%  75.7%  61.2%  72.6%  75.2%  75.2%  75.4%  70.3%  67.8%  57.1%  67.0%  67.8%  67.0%  51.6%  74.8%  73.6%  73.5%  70.9%  71.7%  78.8%  52.9%  67.3%  67.5%  67.3%  67.3%  52.3%  52.3%  72.8%  48.7%  60.5%  71.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.3% 
	 81.1%  77.4%  62.1%  72.6%  74.3%  69.5%  74.6%  72.3%  63.5%  57.1%  60.7%  69.6%  64.2%  51.6%  69.7%  67.9%  73.5%  72.8%  72.6%  77.9%  61.3%  77.6%  66.7%  68.2%  68.1%  55.1%  56.9%  78.6%  56.6%  62.2%  56.9%  67.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.5% 
	 83.3%  69.6%  63.8%  72.6%  71.6%  75.2%  73.0%  69.3%  72.2%  61.2%  61.6%  67.8%  62.4%  51.6%  75.6%  68.9%  74.3%  69.9%  73.5%  76.0%  60.5%  74.8%  65.8%  71.8%  69.0%  63.6%  56.0%  77.7%  53.1%  63.9%  71.6%  50.5%  66.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 76.7%  70.4%  62.1%  73.5%  70.6%  70.5%  75.4%  74.3%  65.2%  58.2%  63.4%  69.6%  65.1%  47.3%  73.9%  66.0%  70.8%  71.8%  73.5%  76.0%  63.0%  73.8%  64.9%  68.2%  62.8%  59.8%  55.0%  77.7%  53.1%  62.2%  75.9%  52.7%  56.8%  83.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.2% 
	 78.9%  69.6%  62.1%  72.6%  73.4%  70.5%  74.6%  69.3%  62.6%  57.1%  58.9%  69.6%  64.2%  49.5%  73.1%  67.0%  74.3%  76.7%  71.7%  76.9%  61.3%  74.8%  66.7%  71.8%  68.1%  57.9%  56.0%  70.9%  55.8%  55.5%  79.3%  60.2%  55.1%  65.3%  70.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.9% 
	 80.6%  67.8%  60.3%  70.8%  71.6%  67.6%  74.6%  76.2%  64.3%  52.0%  61.6%  68.7%  61.5%  45.1%  73.9%  66.0%  73.5%  67.0%  69.9%  76.0%  63.9%  74.8%  66.7%  70.0%  69.9%  62.6%  56.9%  80.6%  53.1%  63.0%  79.3%  61.3%  66.9%  70.3%  53.9%  77.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.2% 
	 79.4%  69.6%  60.3%  74.3%  69.7%  65.7%  73.0%  70.3%  63.5%  58.2%  64.3%  69.6%  61.5%  45.1%  78.2%  70.8%  74.3%  68.9%  70.8%  77.9%  62.2%  72.9%  63.2%  69.1%  71.7%  59.8%  55.0%  79.6%  54.9%  63.9%  81.0%  65.6%  63.6%  69.5%  66.1%  46.3%  58.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.7% 
	 76.7%  64.3%  61.2%  72.6%  71.6%  69.5%  73.8%  69.3%  67.0%  58.2%  63.4%  67.8%  64.2%  42.9%  78.2%  69.8%  71.7%  64.1%  62.8%  71.2%  62.2%  69.2%  67.5%  66.4%  65.5%  61.7%  56.0%  79.6%  57.5%  66.4%  78.4%  64.5%  66.1%  72.0%  66.1%  51.9%  39.0%  65.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.7% 
	 76.7%  69.6%  60.3%  71.7%  68.8%  67.6%  72.1%  74.3%  66.1%  59.2%  64.3%  68.7%  62.4%  46.2%  71.4%  62.3%  73.5%  68.9%  69.9%  75.0%  59.7%  67.3%  64.9%  67.3%  68.1%  57.0%  58.7%  80.6%  54.0%  63.9%  75.9%  67.7%  65.3%  72.9%  59.1%  50.9%  42.9%  52.3%  71.8%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.4% 
	 77.8%  65.2%  54.3%  72.6%  70.6%  70.5%  71.3%  71.3%  57.4%  58.2%  67.0%  66.1%  58.7%  45.1%  73.9%  66.0%  75.2%  74.8%  69.9%  76.0%  59.7%  72.9%  69.3%  71.8%  68.1%  64.5%  54.1%  80.6%  57.5%  64.7%  79.3%  61.3%  66.1%  78.0%  62.6%  55.6%  51.4%  67.0%  58.1%  75.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.5% 
	 79.4%  67.0%  60.3%  77.0%  73.4%  70.5%  70.5%  71.3%  67.0%  59.2%  60.7%  65.2%  62.4%  44.0%  73.1%  66.0%  73.5%  67.0%  76.1%  77.9%  59.7%  72.0%  64.9%  69.1%  66.4%  68.2%  58.7%  81.6%  57.5%  64.7%  81.0%  66.7%  66.9%  70.3%  65.2%  58.3%  49.5%  69.7%  58.1%  61.1%  59.6%   0.0%   0.0%   0.0% 	Avg.: 66.6% 
	 77.2%  69.6%  60.3%  71.7%  73.4%  67.6%  72.1%  72.3%  66.1%  56.1%  65.2%  70.4%  59.6%  46.2%  70.6%  67.0%  74.3%  69.9%  72.6%  76.9%  62.2%  75.7%  64.9%  67.3%  63.7%  60.7%  50.5%  81.6%  57.5%  64.7%  81.0%  64.5%  68.6%  75.4%  68.7%  56.5%  59.0%  73.4%  62.1%  68.5%  52.9%  51.5%   0.0%   0.0% 	Avg.: 66.4% 
	 77.8%  72.2%  57.8%  68.1%  71.6%  70.5%  68.9%  71.3%  58.3%  57.1%  64.3%  69.6%  63.3%  41.8%  68.9%  66.0%  73.5%  68.9%  68.1%  76.9%  61.3%  75.7%  64.9%  66.4%  62.8%  60.7%  61.5%  79.6%  55.8%  61.3%  80.2%  60.2%  65.3%  73.7%  65.2%  61.1%  57.1%  74.3%  58.1%  66.7%  51.0%  49.5%  74.0%   0.0% 	Avg.: 65.6% 
	 76.7%  69.6%  57.8%  71.7%  69.7%  66.7%  71.3%  70.3%  62.6%  56.1%  64.3%  68.7%  62.4%  42.9%  72.3%  67.9%  75.2%  63.1%  69.0%  78.8%  63.0%  75.7%  64.0%  66.4%  67.3%  59.8%  56.9%  79.6%  55.8%  66.4%  78.4%  66.7%  67.8%  73.7%  65.2%  56.5%  59.0%  74.3%  60.5%  63.9%  51.0%  49.5%  47.2%  64.0% 	Avg.: 65.2% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	-16.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:-16.1% 
	  1.1% -12.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -5.5% 
	 -1.1%   0.0% -12.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.4% 
	 -1.1%   0.0%  -3.4%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.4% 
	 -2.2%  -2.6%   0.0%   0.0%  -8.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.6% 
	  0.0%   1.7%   0.0%   2.7%   0.0%  -5.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	  1.1%  -0.9%  -0.9%   0.9%   0.0%   1.9%  -3.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	 -3.3%   3.5%   0.9%   2.7%   0.9%  -1.0%   0.0%  -9.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.8% 
	  1.1%   0.0%   0.9%   0.9%   0.9%   1.0%   0.0%   2.0%  -5.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  0.6%   0.9%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%  -7.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.4% 
	 -0.6%   2.6%   0.9%   0.0%   0.0%   3.8%   0.0%  -3.0%   0.0%  -5.1%  -2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.4% 
	  0.6%   3.5%   0.9%  -0.9%   0.9%   4.8%   0.0%   2.0%   0.9%   0.0%  -0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.0% 
	  0.6%   1.7%   0.9%   0.9%   0.9%   1.0%   0.0%   0.0%   0.0%  -1.0%   0.0%  -1.7%  -2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  1.1%   3.5%   0.9%   1.8%   0.0%   1.0%   0.0%   0.0%   0.0%   3.1%   0.0%   1.7%   0.9% -11.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.1%   2.6%   0.9%   0.9%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   1.7%   0.0%   4.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.1%   0.9%   0.9%   2.7%   0.0%   1.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   2.2%  -2.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.0%   0.9%   0.9%   0.0%   0.0%   0.0%   1.0%   0.9%   2.0%   0.9%   0.0%   0.9%   2.2%   0.8%  -1.9%  -6.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.1%   0.9%   0.9%   4.4%   0.0%   0.0%   0.0%   0.0%   0.9%   2.0%   3.6%  -0.9%   0.0%   2.2%   3.4%   0.0%  -0.9%  -9.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  0.6%   0.0%   1.7%   4.4%   0.0%   0.0%   0.0%   1.0%   0.0%   0.0%   0.9%   0.9%   0.0%   3.3%   0.8%   0.0%   0.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.1%   0.0%   1.7%   4.4%   0.0%   1.0%   0.0%   1.0%   1.7%   2.0%   0.9%   0.9%   0.0%   2.2%   0.8%   0.0%  -0.9%   1.0%   0.0%  -2.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	 -0.6%   0.0%   1.7%   3.5%   0.0%   1.0%   0.0%   1.0%   0.9%   0.0%   0.9%   1.7%   0.0%   1.1%  -0.8%   0.0%   0.9%   1.0%   0.0%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	 -0.6%  -1.7%   1.7%   4.4%   0.0%   0.0%   0.0%   1.0%   0.9%   0.0%   2.7%   1.7%   0.0%   0.0%   2.5%   0.0%   0.9%   0.0%   0.0%   0.0%  -1.7%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   1.7%   1.7%   4.4%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%  -0.9%   0.0%  -1.1%   0.8%   0.0%  -0.9%   1.0%   0.0%   0.0%  -0.8%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   2.6%   1.7%   3.5%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   1.7%   0.0%   1.1%   0.8%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.9%   0.0%  -8.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  0.6%   1.7%   1.7%   4.4%   0.0%   0.0%   0.0%   1.0%   0.9%   0.0%   1.8%   0.0%   0.0%   1.1%   0.8%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%  -1.9%   0.0%   1.8%  -2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.1%   0.0%   0.9%   4.4%   0.0%   0.0%   0.0%   1.0%   0.9%   0.0%   1.8%   0.0%   0.0%   2.2%   1.7%   0.0%   1.8%   0.0%   0.0%   0.0%   1.7%   1.9%   0.0%   0.9%   2.7%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  1.1%   1.7%   0.9%   3.5%   0.0%  -1.0%   0.0%   1.0%   1.7%   0.0%   1.8%   1.7%   0.0%   3.3%   1.7%   0.0%   1.8%   0.0%  -0.9%   0.0%   0.8%   0.0%   0.0%   0.9%   1.8%   0.9%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.0%   0.0%   0.9%   3.5%   0.0%   0.0%   0.0%   1.0%   0.9%   0.0%   1.8%   1.7%   0.0%   4.4%   2.5%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%  -0.9%  -0.9%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  0.6%   0.9%   0.9%   3.5%   0.9%   1.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.9%   0.0%   3.3%   4.2%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.0%   0.9%   0.0%  -1.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.0%   0.9%   3.5%   0.9%   1.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.9%   0.0%   3.3%   2.5%   0.0%   3.5%   1.0%   0.0%   0.0%   0.0%   1.9%   0.0%   1.8%   0.0%   0.9%   1.8%   1.0%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	 -0.6%   0.0%   0.9%   3.5%   0.9%   1.9%   0.0%   1.0%   1.7%   1.0%   0.9%   1.7%   0.0%   3.3%   0.8%   0.0%   3.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%  -0.9%   0.9%   2.8%   0.0%   0.0%   0.0%  -2.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.0%   0.9%   3.5%   0.9%   1.0%   0.0%   1.0%   0.9%   1.0%   0.9%   1.7%   0.0%   4.4%   0.8%   0.0%   3.5%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   0.9%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.0%   0.0%   0.9%   4.4%   0.9%   1.0%   0.0%  -1.0%   0.9%   1.0%   0.9%   0.9%   0.0%   5.5%   3.4%   0.0%   3.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   0.9%   0.0%   1.8%   0.0%   0.0%   0.0%   0.0%   1.1%   1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.0%   0.0%   0.9%   4.4%   0.9%   1.0%   0.0%   1.0%   0.9%   1.0%   0.9%   0.9%   0.0%   2.2%   3.4%   0.9%   3.5%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%   0.9%   0.0%   1.8%   0.0%   0.0%   0.8%   0.0%  -2.2%   1.7%   1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.0%   0.0%   0.9%   4.4%   0.9%   1.0%   0.0%   1.0%   0.9%   2.0%   0.9%   0.0%   0.0%   5.5%   3.4%   0.0%   3.5%   0.0%  -0.9%   0.0%   0.8%   0.0%   0.0%   3.6%   0.0%   1.9%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   1.7%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.0%   1.7%   0.9%   2.7%   0.9%   1.0%   0.0%   2.0%   0.9%   1.0%   0.9%   0.0%   0.0%   5.5%   1.7%   0.0%   3.5%   0.0%   0.9%   0.0%   0.8%   0.0%   0.9%   2.7%   0.9%   0.9%   0.9%   0.0%   0.0%  -0.8%   0.0%   0.0%   0.8%   2.5%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.6%   0.9%   0.9%   2.7%   0.9%   1.0%   0.0%   1.0%   0.9%   1.0%   0.9%   0.9%   0.0%   1.1%   1.7%   0.0%   3.5%  -1.0%   0.9%   0.0%   1.7%   0.0%   0.0%   0.9%   0.9%   0.9%   0.9%   0.0%   0.0%   0.0%   0.0%   1.1%   1.7%   2.5%   0.0%   0.9%   2.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  0.0%   1.7%   0.9%   3.5%   0.9%   1.0%   0.0%   2.0%   0.9%   1.0%   0.9%   0.0%   0.0%   2.2%   1.7%   0.0%   3.5%   1.0%   0.9%   0.0%   0.8%   0.0%   0.0%   1.8%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   1.7%  -0.9%   0.0%   2.9%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  0.6%   0.0%   0.9%   3.5%   0.9%   1.0%   0.0%   1.0%   0.9%   1.0%   0.9%  -1.7%   0.0%   5.5%   1.7%   0.0%   3.5%   1.0%   0.9%   0.0%   0.0%   0.0%   0.9%   0.0%   1.8%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.1%   0.8%   0.8%  -0.9%   0.0%   1.0%   0.0%  -3.2%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  1.1%   0.0%   0.9%   1.8%   0.9%   1.0%   0.0%   2.0%   0.9%   1.0%   0.9%  -0.9%   0.0%   5.5%   1.7%   0.0%   3.5%   1.9%   0.9%   0.0%   1.7%   0.0%   0.9%   0.0%   1.8%   0.0%   0.9%  -1.0%   0.0%   0.0%   0.0%  -1.1%   0.8%   0.8%   0.0%   0.0%   1.0%   0.0%  -0.8%   0.9%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.1%   0.0%   0.9%   2.7%   0.9%   1.0%   0.0%   1.0%   0.9%   1.0%   0.9%   0.9%   0.0%   4.4%   2.5%   0.0%   3.5%   0.0%   0.9%   0.0%   0.8%   0.0%   0.9%   0.9%   0.9%   0.0%   0.9%   0.0%   0.0%  -0.8%   0.0%   2.2%   0.8%   0.8%   0.0%   0.9%   1.9%  -0.9%   0.8%   0.9%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.7%   0.9%   0.9%   4.4%   0.9%   1.0%   0.0%   1.0%   0.9%   1.0%   0.9%   1.7%   0.0%   4.4%   4.2%   0.0%   3.5%   0.0%   0.9%   0.0%   1.7%   0.0%   0.9%   1.8%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   1.1%   0.8%   0.8%   0.9%   0.9%   1.0%   0.0%   0.0%   0.9%   5.8%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  1.1%   0.0%   0.9%   3.5%   0.0%   1.0%   0.0%   2.0%   0.9%   2.0%   1.8%   1.7%   0.0%   2.2%   2.5%   0.0%   3.5%   0.0%   0.9%   0.0%   1.7%   0.0%   0.9%   1.8%   1.8%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.8%   0.8%   0.9%   0.9%   1.9%   0.0%   0.0%   0.0%   4.8%  -1.0%   0.8%   0.0% 	Avg.:  1.0% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	 -6.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -6.1% 
	  5.6%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.3% 
	 -4.4%  -3.5%   7.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.1% 
	  0.6%  -5.2%   9.5%  -4.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	 -3.3%   1.7%   6.9%  -6.2%   7.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.3% 
	  1.1%   2.6%   2.6%   8.8%   3.7%   3.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.8% 
	  3.9%   4.3%  -7.8%   0.0%  -3.7%  -9.5%  10.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.3% 
	  0.0%  11.3%   6.9%  -0.9%   6.4%   3.8%   5.7%  30.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.0% 
	  1.1%   3.5%   7.8%   0.0%  -3.7% -11.4%   2.5%   5.9%   7.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.4% 
	 -1.7%   7.8%   2.6%   1.8%  -4.6%  12.4%  -0.8%   0.0%  -3.5%  11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.5% 
	  3.9%   3.5%   6.9%   8.0%   0.0%  -2.9%   6.6%  -9.9%   0.9%   3.1%  25.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.1% 
	  3.3%   2.6%   7.8%   0.9%   2.8%   7.6%  -1.6%   0.0%   1.7%  -1.0%  17.0%  22.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.3% 
	  6.1%   7.0%   2.6%   5.3%   0.9%   0.0%  -4.1%  -1.0%   0.0%  -8.2%   6.2%  14.8%  22.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.0% 
	  1.7%   7.0%  11.2%   4.4%   2.8%   7.6%   5.7%  13.9%  -1.7%  -1.0%   8.0%  10.4%   4.6%  14.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	  5.0%   4.3%   1.7%   6.2%   2.8%   2.9%   0.8%  -5.0%   1.7%  -3.1%   8.0%   0.9%   1.8%   7.7%  30.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.4% 
	  2.8%   7.0%   4.3%   4.4%   5.5%   7.6%   7.4%   7.9%   0.9%   1.0%   0.0%   1.7%  -9.2%   6.6%  18.5%   6.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.6% 
	  6.7%   7.0%   3.4%   1.8%   0.0%   7.6%   4.1%   4.0%  -1.7%   4.1%   7.1%   0.9%   6.4%   3.3%  18.5%   1.9%  29.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	  6.1%   7.0%   6.9%  11.5%   2.8%   6.7%   3.3%   0.0%   4.3%   0.0%   6.2%  -4.3%   2.8%   3.3%  13.4%  -0.9%  14.2%  21.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  3.3%   7.0%   6.9%   8.8%   0.0%   1.0%   4.9%  -1.0%   4.3%   1.0%   5.4%   3.5%   0.9%   2.2%  10.1%  -7.5%  13.3%  -1.0%  13.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.0% 
	 10.6%   7.8%   6.9%   6.2%   6.4%   4.8%   3.3%   2.0%   0.0%  -2.0%   2.7%   3.5%  -1.8%  -1.1%  11.8%  -1.9%   2.7%  -9.7%   8.8%   8.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.5% 
	  5.6%   9.6%   5.2%   7.1%   3.7%   4.8%   4.9%  -4.0%   0.9%   3.1%   2.7%   5.2%   6.4%  -7.7%  13.4%   3.8%   8.0%   5.8%   4.4%   8.7%  19.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.3% 
	  5.0%   8.7%   9.5%  16.8%   3.7%   9.5%   5.7%   5.9%   5.2%  -2.0%   3.6%   5.2%   4.6%   1.1%   5.9%   2.8%   8.0%  -3.9%   5.3%   1.9%  18.5%  14.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.1% 
	 10.0%  11.3%   6.9%   9.7%   6.4%   6.7%   6.6%   1.0%   0.9%   5.1%   2.7%   2.6%   0.9%   0.0%   4.2%   1.9%   3.5%   1.0%   1.8%   1.0%  12.6%   4.7%  14.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.1% 
	  6.1%  10.4%   8.6%   9.7%   8.3%   5.7%   4.9%   3.0%  -2.6%   1.0%   8.0%   4.3%   5.5%   5.5%   4.2%   5.7%   0.9%  -2.9%   0.0%   1.9%  13.4%   1.9%  13.2%  20.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.7% 
	  6.1%   8.7%  12.9%  12.4%   9.2%  11.4%   6.6%   3.0%   0.9%   7.1%  -3.6%   5.2%   1.8%   8.8%   5.0%   0.9%   1.8%  -1.0%  -3.5%  -3.8%   8.4%  -4.7%   8.8%  14.5%   5.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.9% 
	 10.0%  11.3%  11.2%   9.7%   8.3%   3.8%   1.6%   4.0%   8.7%   5.1%  10.7%   3.5%  -0.9%   1.1%  13.4%   4.7%   1.8%  -1.9%   0.9%   3.8%   8.4%   0.0%   7.0%  12.7%   0.0%  11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	  8.9%  11.3%  11.2%   7.1%   8.3%   3.8%   4.1%   4.0%  11.3%   4.1%   8.9%   3.5%   3.7%  -6.6%   7.6%   3.8%   2.7%   2.9%  -1.8%   2.9%   6.7%   0.0%   4.4%   0.0%  -7.1%   2.8%  17.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.7% 
	  9.4%  14.8%  14.7%   9.7%   9.2%  12.4%   2.5%   0.0%   7.8%   6.1%  11.6%   5.2%   3.7%   8.8%   5.0%   3.8%   0.9%   1.9%   0.9%  -1.0%   7.6%   1.9%   6.1%   0.0%   9.7%   3.7%  14.7%   5.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.3% 
	  8.9%  13.0%   9.5%   8.8%   8.3%   4.8%   6.6%   3.0%  12.2%   7.1%  10.7%   0.9%   1.8%  13.2%   7.6%   3.8%  -0.9%   0.0%   3.5%  -3.8%   9.2%   0.9%   5.3%   0.9%   8.0%   0.0%   1.8%   1.0%  14.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.5% 
	  8.9%  10.4%  15.5%  11.5%   6.4%   6.7%   3.3%   4.0%   8.7%   8.2%   9.8%   4.3%   1.8%   7.7%   5.0%   0.0%   1.8%   1.0%   4.4%   1.0%  19.3%   6.5%   3.5%   0.9%   7.1%   7.5%   0.9%   1.0%  15.0%   8.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.4% 
	  6.1%   8.7%  14.7%  11.5%   7.3%  12.4%   4.1%   2.0%  13.0%   8.2%  16.1%   2.6%   4.6%   7.7%  10.1%   5.7%   1.8%  -1.0%   3.5%   1.9%  10.9%  -3.7%   4.4%   0.0%   6.2%   4.7%  -3.7%  -4.9%   7.1%   6.7%  14.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.9% 
	  3.9%  16.5%  12.9%  11.5%  10.1%   6.7%   5.7%   5.0%   4.3%   4.1%  15.2%   4.3%   6.4%   7.7%   4.2%   4.7%   0.9%   2.9%   2.7%   3.8%  11.8%   2.8%   5.3%  -3.6%   5.3%  -3.7%   0.9%   1.0%  10.6%   5.0%   0.0%  17.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	 10.6%  15.7%  14.7%  10.6%  11.0%  11.4%   3.3%   0.0%  11.3%   7.1%  13.4%   2.6%   3.7%  12.1%   5.9%   7.5%   4.4%   1.0%   2.7%   3.8%   9.2%   3.7%   6.1%   3.6%  11.5%   3.7%   1.8%   1.0%  10.6%   6.7%  -4.3%  15.1%  10.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.0% 
	  8.3%  16.5%  14.7%  11.5%   8.3%  11.4%   4.1%   5.0%  13.9%   8.2%  17.9%   2.6%   4.6%   9.9%   6.7%   6.6%   0.9%  -3.9%   4.4%   2.9%  10.9%   2.8%   4.4%   0.0%   6.2%   5.6%   0.9%   7.8%   8.0%  13.4%  -3.4%   7.5%  11.9%  18.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.3% 
	  6.7%  18.3%  16.4%  13.3%  10.1%  14.3%   4.1%  -2.0%  12.2%  13.3%  15.2%   3.5%   7.3%  14.3%   5.9%   7.5%   1.8%   9.7%   6.2%   3.8%   8.4%   2.8%   4.4%   1.8%   4.4%   0.9%   0.0%  -1.9%  10.6%   5.9%   0.0%   6.5%   0.0%  13.6%  16.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.3% 
	  7.8%  16.5%  16.4%   9.7%  11.9%  16.2%   5.7%   5.9%  13.0%   7.1%  12.5%   2.6%   7.3%  14.3%   1.7%   2.8%   0.9%   7.8%   5.3%   1.9%  10.1%   4.7%   7.9%   2.7%   2.7%   3.7%   1.8%   1.0%   8.8%   5.0%  -1.7%   2.2%   3.4%  14.4%   4.3%  31.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	 10.6%  21.7%  15.5%  11.5%  10.1%  12.4%   4.9%   6.9%   9.6%   7.1%  13.4%   4.3%   4.6%  16.5%   1.7%   3.8%   3.5%  12.6%  13.3%   8.7%  10.1%   8.4%   3.5%   5.5%   8.8%   1.9%   0.9%   1.0%   6.2%   2.5%   2.6%   3.2%   0.8%  11.9%   4.3%  25.9%  19.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 10.6%  16.5%  16.4%  12.4%  12.8%  14.3%   6.6%   2.0%  10.4%   6.1%  12.5%   3.5%   6.4%  13.2%   8.4%  11.3%   1.8%   7.8%   6.2%   4.8%  12.6%  10.3%   6.1%   4.5%   6.2%   6.5%  -1.8%   0.0%   9.7%   5.0%   5.2%   0.0%   1.7%  11.0%  11.3%  26.9%  15.2%  12.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	  9.4%  20.9%  22.4%  11.5%  11.0%  11.4%   7.4%   5.0%  19.1%   7.1%   9.8%   6.1%  10.1%  14.3%   5.9%   7.5%   0.0%   1.9%   6.2%   3.8%  12.6%   4.7%   1.8%   0.0%   6.2%  -0.9%   4.6%   0.0%   6.2%   4.2%   1.7%   6.5%   0.8%   5.9%   7.8%  22.2%   6.7%  -1.8%  13.7%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	  7.8%  19.1%  16.4%   7.1%   8.3%  11.4%   8.2%   5.0%   9.6%   6.1%  16.1%   7.0%   6.4%  15.4%   6.7%   7.5%   1.8%   9.7%   0.0%   1.9%  12.6%   5.6%   6.1%   2.7%   8.0%  -3.7%   0.0%  -1.0%   6.2%   4.2%   0.0%   1.1%   0.0%  13.6%   5.2%  19.4%   8.6%  -2.8%  13.7%  13.9%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.1% 
	 10.0%  16.5%  16.4%  12.4%   8.3%  14.3%   6.6%   4.0%  10.4%   9.2%  11.6%   1.7%   9.2%  13.2%   9.2%   6.6%   0.9%   6.8%   3.5%   2.9%  10.1%   1.9%   6.1%   4.5%  10.6%   7.5%   8.3%   0.0%   6.2%   4.2%   0.0%   3.2%  -1.7%   8.5%   1.7%  21.3%  -1.0%  -3.7%   9.7%   6.5%   6.7%   0.0%   0.0%   0.0% 	Avg.:  6.9% 
	  9.4%  13.9%  19.0%  15.9%  10.1%  11.4%   9.8%   5.0%  18.3%   8.2%  12.5%   2.6%   5.5%  17.6%  10.9%   7.5%   1.8%   7.8%   8.0%   2.9%  10.9%   1.9%   6.1%   5.5%  11.5%   7.5%  -2.8%   1.9%   8.0%   7.6%   0.9%   7.5%   3.4%  10.2%   5.2%  16.7%   1.9%  -0.9%  13.7%   8.3%   8.7%   2.0%   0.0%   0.0% 	Avg.:  7.9% 
	 10.6%  16.5%  19.0%  12.4%  11.9%  15.2%   7.4%   5.9%  13.9%   9.2%  12.5%   3.5%   6.4%  16.5%   7.6%   5.7%   0.0%  13.6%   7.1%   1.0%   9.2%   1.9%   7.0%   5.5%   7.1%   8.4%   4.6%   1.9%   8.0%   2.5%   2.6%   1.1%   0.8%  10.2%   5.2%  21.3%   0.0%   0.0%  11.3%  11.1%   8.7%   2.0%  26.8%   0.0% 	Avg.:  8.2% 
************************************************************************************************************
[Elapsed time = 0.4 h]
Done!

f1_score_micro: 0.6548708562131381
f1_score_macro: 0.6243467543603497
              precision    recall  f1-score   support

           0       0.90      1.00      0.95         9
           1       0.75      0.75      0.75         4
           2       1.00      1.00      1.00         4
           3       1.00      0.75      0.86         4
           4       0.75      0.75      0.75         4
           5       0.50      0.50      0.50         4
           6       0.67      1.00      0.80         4
           7       0.38      0.75      0.50         4
           8       0.45      1.00      0.62         5
           9       0.67      0.80      0.73         5
          10       0.44      0.80      0.57         5
          11       1.00      1.00      1.00         9
          12       0.40      0.50      0.44         4
          13       0.80      0.89      0.84         9
          14       0.00      0.00      0.00         4
          15       0.06      0.25      0.10         4
          16       1.00      0.75      0.86         4
          17       1.00      1.00      1.00         4
          18       0.14      0.25      0.18         4
          19       1.00      0.50      0.67         4
          20       0.80      1.00      0.89         4
          21       0.64      0.78      0.70         9
          22       1.00      1.00      1.00         4
          23       0.43      0.60      0.50         5
          24       1.00      0.75      0.86         4
          25       0.75      0.67      0.71         9
          26       1.00      1.00      1.00         9
          27       0.50      0.50      0.50         4
          28       1.00      1.00      1.00         4
          29       0.20      0.25      0.22         4
          30       0.57      0.89      0.70         9
          31       0.86      0.67      0.75         9
          32       1.00      1.00      1.00         4
          33       0.43      0.75      0.55         4
          34       0.00      0.00      0.00         4
          35       0.90      1.00      0.95         9
          36       0.44      1.00      0.62         4
          37       1.00      0.80      0.89         5
          38       0.60      0.75      0.67         4
          39       0.80      0.80      0.80         5
          40       1.00      1.00      1.00         9
          41       0.80      0.80      0.80         5
          42       0.12      0.22      0.15         9
          43       0.83      1.00      0.91         5
          44       0.50      0.50      0.50         4
          45       0.80      0.44      0.57         9
          46       1.00      1.00      1.00         5
          47       1.00      0.67      0.80         9
          48       0.90      1.00      0.95         9
          49       1.00      1.00      1.00         4
          50       0.00      0.00      0.00         4
          51       0.75      0.75      0.75         4
          52       0.00      0.00      0.00         4
          53       0.38      0.75      0.50         4
          54       0.00      0.00      0.00         9
          55       1.00      0.25      0.40         4
          56       1.00      0.75      0.86         4
          57       0.00      0.00      0.00         4
          58       1.00      1.00      1.00         4
          59       0.50      0.44      0.47         9
          60       0.43      0.60      0.50         5
          61       1.00      0.75      0.86         4
          62       1.00      1.00      1.00         4
          63       0.80      1.00      0.89         4
          64       1.00      0.89      0.94         9
          65       0.43      0.75      0.55         4
          66       0.50      0.56      0.53         9
          67       0.00      0.00      0.00         4
          68       0.00      0.00      0.00         4
          69       1.00      0.25      0.40         4
          70       0.88      0.78      0.82         9
          71       0.80      0.89      0.84         9
          72       0.45      0.56      0.50         9
          73       0.80      1.00      0.89         4
          74       1.00      1.00      1.00         9
          75       1.00      1.00      1.00         4
          76       0.29      0.50      0.36         4
          77       0.12      0.25      0.17         4
          78       0.80      1.00      0.89         4
          79       0.75      0.75      0.75         4
          80       1.00      1.00      1.00         4
          81       0.00      0.00      0.00         4
          82       0.60      0.75      0.67         4
          83       1.00      1.00      1.00         9
          84       0.43      0.60      0.50         5
          85       0.25      0.44      0.32         9
          86       0.50      0.50      0.50         4
          87       0.90      1.00      0.95         9
          88       0.00      0.00      0.00         4
          89       0.64      0.78      0.70         9
          90       1.00      0.75      0.86         4
          91       0.67      0.40      0.50         5
          92       1.00      0.89      0.94         9
          93       1.00      0.80      0.89         5
          94       0.62      1.00      0.77         5
          95       0.80      0.80      0.80         5
          96       1.00      0.50      0.67         4
          97       0.29      0.22      0.25         9
          98       0.75      0.75      0.75         4
          99       0.69      1.00      0.82         9
         100       0.00      0.00      0.00         4
         101       0.67      0.89      0.76         9
         102       0.25      0.25      0.25         4
         103       1.00      1.00      1.00         5
         104       0.43      0.75      0.55         4
         105       1.00      0.67      0.80         9
         106       0.75      0.75      0.75         4
         107       0.50      0.25      0.33         4
         108       0.00      0.00      0.00         4
         109       0.67      1.00      0.80         4
         110       0.67      1.00      0.80         4
         111       0.89      0.89      0.89         9
         112       1.00      0.80      0.89         5
         113       0.80      1.00      0.89         4
         114       0.43      0.75      0.55         4
         115       1.00      0.75      0.86         4
         116       1.00      0.75      0.86         4
         117       0.50      0.50      0.50         4
         118       0.90      1.00      0.95         9
         119       0.80      1.00      0.89         4
         120       1.00      0.89      0.94         9
         121       1.00      1.00      1.00         4
         122       1.00      0.75      0.86         4
         123       0.80      1.00      0.89         4
         124       1.00      1.00      1.00         5
         125       0.62      1.00      0.77         5
         126       0.57      0.80      0.67         5
         127       0.00      0.00      0.00         9
         128       0.60      0.75      0.67         4
         129       0.00      0.00      0.00         4
         130       0.75      0.60      0.67         5
         131       0.56      1.00      0.71         5
         132       0.17      0.11      0.13         9
         133       0.25      0.25      0.25         4
         134       0.00      0.00      0.00         4
         135       0.80      1.00      0.89         4
         136       0.50      0.75      0.60         4
         137       1.00      0.75      0.86         4
         138       1.00      1.00      1.00         4
         139       0.56      0.56      0.56         9
         140       0.75      0.75      0.75         4
         141       0.90      1.00      0.95         9
         142       0.69      1.00      0.82         9
         143       0.80      1.00      0.89         4
         144       0.90      1.00      0.95         9
         145       0.62      1.00      0.77         5
         146       0.75      0.67      0.71         9
         147       1.00      0.78      0.88         9
         148       1.00      0.75      0.86         4
         149       0.33      0.22      0.27         9
         150       0.50      0.50      0.50         4
         151       1.00      0.60      0.75         5
         152       0.62      0.56      0.59         9
         153       0.20      0.25      0.22         4
         154       1.00      0.50      0.67         4
         155       1.00      0.80      0.89         5
         156       0.33      0.20      0.25         5
         157       1.00      1.00      1.00         4
         158       1.00      0.80      0.89         5
         159       0.33      0.75      0.46         4
         160       1.00      0.75      0.86         4
         161       0.60      0.75      0.67         4
         162       0.00      0.00      0.00         4
         163       1.00      1.00      1.00         4
         164       1.00      0.80      0.89         5
         165       0.67      0.50      0.57         4
         166       0.67      0.22      0.33         9
         167       0.83      1.00      0.91         5
         168       0.67      0.50      0.57         4
         169       1.00      0.89      0.94         9
         170       1.00      0.75      0.86         4
         171       1.00      0.75      0.86         4
         172       0.90      1.00      0.95         9
         173       1.00      1.00      1.00         5
         174       1.00      0.50      0.67         4
         175       0.75      0.67      0.71         9
         176       1.00      0.56      0.71         9
         177       1.00      0.80      0.89         5
         178       0.00      0.00      0.00         4
         179       1.00      0.80      0.89         5
         180       0.67      1.00      0.80         4
         181       1.00      1.00      1.00         9
         182       0.00      0.00      0.00         9
         183       1.00      1.00      1.00         9
         184       0.80      0.57      0.67         7
         185       0.75      0.75      0.75         4
         186       1.00      0.25      0.40         4
         187       0.75      0.75      0.75         4
         188       0.00      0.00      0.00         4
         189       1.00      1.00      1.00         4
         190       0.89      0.89      0.89         9
         191       0.80      1.00      0.89         4
         192       0.50      0.50      0.50         4
         193       0.00      0.00      0.00         4
         194       0.88      0.78      0.82         9
         195       0.80      1.00      0.89         4
         196       0.75      0.60      0.67         5
         197       0.00      0.00      0.00         4
         198       0.50      0.25      0.33         4
         199       0.75      0.75      0.75         4
         200       1.00      0.80      0.89         5
         201       0.80      1.00      0.89         4
         202       1.00      1.00      1.00         4
         203       1.00      1.00      1.00         4
         204       1.00      0.50      0.67         4
         205       0.75      0.75      0.75         4
         206       0.00      0.00      0.00         4
         207       0.00      0.00      0.00         9
         208       0.50      0.25      0.33         4
         209       0.33      0.25      0.29         4
         210       0.00      0.00      0.00         4
         211       0.33      0.40      0.36         5
         212       0.50      0.75      0.60         4
         213       0.90      1.00      0.95         9
         214       0.07      0.11      0.09         9
         215       0.67      1.00      0.80         4
         216       1.00      1.00      1.00         5
         217       0.57      1.00      0.73         4
         218       1.00      1.00      1.00         4
         219       1.00      0.75      0.86         4
         220       0.50      0.75      0.60         4
         221       0.00      0.00      0.00         4
         222       0.67      1.00      0.80         4
         223       1.00      1.00      1.00         4
         224       0.80      0.89      0.84         9
         225       0.17      0.11      0.13         9
         226       0.50      0.50      0.50         4
         227       0.62      1.00      0.77         5
         228       0.80      1.00      0.89         4
         229       0.67      0.22      0.33         9
         230       0.88      0.78      0.82         9
         231       0.00      0.00      0.00         4
         232       1.00      1.00      1.00         4
         233       0.54      0.78      0.64         9
         234       0.50      0.20      0.29         5
         235       0.50      0.25      0.33         4
         236       0.00      0.00      0.00         4
         237       0.50      0.78      0.61         9
         238       1.00      1.00      1.00         5
         239       0.62      0.89      0.73         9
         240       0.71      1.00      0.83         5
         241       0.67      1.00      0.80         4
         242       1.00      0.75      0.86         4
         243       0.38      0.33      0.35         9
         244       0.89      0.89      0.89         9
         245       0.43      0.75      0.55         4
         246       0.67      0.67      0.67         9
         247       0.07      0.25      0.11         4
         248       0.83      1.00      0.91         5
         249       0.00      0.00      0.00         4
         250       0.80      0.80      0.80         5
         251       1.00      1.00      1.00         4
         252       0.38      0.75      0.50         4
         253       0.89      0.89      0.89         9
         254       0.00      0.00      0.00         4
         255       0.20      0.25      0.22         4
         256       0.00      0.00      0.00         4
         257       1.00      0.67      0.80         9
         258       0.11      0.25      0.15         4
         259       0.78      0.78      0.78         9
         260       0.67      0.80      0.73         5
         261       0.00      0.00      0.00         9
         262       0.57      0.80      0.67         5
         263       1.00      1.00      1.00         4
         264       0.60      1.00      0.75         9
         265       0.00      0.00      0.00         4
         266       0.55      0.67      0.60         9
         267       0.67      1.00      0.80         4
         268       1.00      1.00      1.00         4
         269       0.62      1.00      0.77         5
         270       0.14      0.50      0.22         4
         271       1.00      1.00      1.00         4
         272       0.57      0.80      0.67         5
         273       0.60      0.75      0.67         4
         274       0.80      1.00      0.89         4
         275       0.67      0.50      0.57         4
         276       0.00      0.00      0.00         4
         277       0.00      0.00      0.00         4
         278       0.83      1.00      0.91         5
         279       0.00      0.00      0.00         4
         280       0.00      0.00      0.00         4
         281       0.00      0.00      0.00         4
         282       0.50      0.60      0.55         5
         283       1.00      0.75      0.86         4
         284       0.00      0.00      0.00         4
         285       0.56      0.56      0.56         9
         286       0.40      0.40      0.40         5
         287       0.00      0.00      0.00         4
         288       1.00      1.00      1.00         5
         289       0.10      0.50      0.17         4
         290       0.57      0.80      0.67         5
         291       0.00      0.00      0.00         4
         292       0.00      0.00      0.00         4
         293       1.00      0.80      0.89         5
         294       0.50      1.00      0.67         5
         295       1.00      0.75      0.86         4
         296       0.71      1.00      0.83         5
         297       0.80      1.00      0.89         4
         298       0.60      0.60      0.60         5
         299       0.86      0.67      0.75         9
         300       0.50      0.75      0.60         4
         301       1.00      0.44      0.62         9
         302       0.90      1.00      0.95         9
         303       0.89      0.89      0.89         9
         304       1.00      0.56      0.71         9
         305       0.67      0.80      0.73         5
         306       1.00      0.75      0.86         4
         307       0.67      0.50      0.57         4
         308       0.83      0.56      0.67         9
         309       0.78      0.78      0.78         9
         310       1.00      1.00      1.00         4
         311       0.00      0.00      0.00         4
         312       1.00      0.50      0.67         4
         313       0.80      1.00      0.89         4
         314       1.00      0.75      0.86         4
         315       0.18      0.50      0.27         4
         316       0.86      0.67      0.75         9
         317       0.75      0.75      0.75         4
         318       0.00      0.00      0.00         4
         319       0.60      0.75      0.67         4
         320       1.00      1.00      1.00         9
         321       0.00      0.00      0.00         4
         322       0.80      1.00      0.89         4
         323       1.00      1.00      1.00         4
         324       1.00      0.75      0.86         4
         325       0.80      1.00      0.89         4
         326       0.82      1.00      0.90         9
         327       1.00      0.75      0.86         4
         328       1.00      0.78      0.88         9
         329       0.57      1.00      0.73         4
         330       1.00      1.00      1.00         5
         331       0.10      0.25      0.14         4
         332       0.00      0.00      0.00         4
         333       1.00      0.22      0.36         9
         334       0.00      0.00      0.00         4
         335       0.67      0.50      0.57         4
         336       1.00      0.60      0.75         5
         337       0.55      0.67      0.60         9
         338       1.00      1.00      1.00         5
         339       0.62      0.56      0.59         9
         340       1.00      1.00      1.00         9
         341       1.00      1.00      1.00         4
         342       0.82      1.00      0.90         9
         343       0.83      1.00      0.91         5
         344       0.17      0.50      0.25         4
         345       1.00      1.00      1.00         4
         346       0.57      1.00      0.73         4
         347       1.00      0.56      0.71         9
         348       0.80      1.00      0.89         4
         349       0.30      0.75      0.43         4
         350       0.00      0.00      0.00         4
         351       0.80      1.00      0.89         4
         352       0.64      0.78      0.70         9
         353       0.67      1.00      0.80         4
         354       0.83      1.00      0.91         5
         355       1.00      0.75      0.86         4
         356       1.00      1.00      1.00         4
         357       0.57      0.80      0.67         5
         358       0.67      0.50      0.57         4
         359       0.00      0.00      0.00         4
         360       0.09      0.25      0.13         4
         361       0.00      0.00      0.00         4
         362       0.69      1.00      0.82         9
         363       0.60      0.75      0.67         4
         364       0.90      1.00      0.95         9
         365       0.50      0.50      0.50         4
         366       1.00      0.80      0.89         5
         367       0.55      0.67      0.60         9
         368       0.00      0.00      0.00         9
         369       0.67      0.50      0.57         4
         370       1.00      1.00      1.00         4
         371       1.00      0.75      0.86         4
         372       0.06      0.25      0.10         4
         373       0.75      0.75      0.75         4
         374       0.89      0.89      0.89         9
         375       0.00      0.00      0.00         4
         376       0.80      1.00      0.89         4
         377       1.00      0.89      0.94         9
         378       0.57      1.00      0.73         4
         379       0.00      0.00      0.00         4
         380       0.50      0.25      0.33         4
         381       0.00      0.00      0.00         4
         382       0.60      0.75      0.67         4
         383       0.90      1.00      0.95         9
         384       0.50      0.33      0.40         9
         385       1.00      1.00      1.00         9
         386       0.71      1.00      0.83         5
         387       1.00      1.00      1.00         4
         388       0.67      0.22      0.33         9
         389       1.00      0.60      0.75         5
         390       1.00      0.75      0.86         4
         391       1.00      1.00      1.00         4
         392       1.00      1.00      1.00         5
         393       0.50      0.75      0.60         4
         394       1.00      1.00      1.00         9
         395       0.80      1.00      0.89         4
         396       0.08      0.25      0.12         4
         397       1.00      1.00      1.00         5
         398       1.00      1.00      1.00         4
         399       1.00      1.00      1.00         4
         400       1.00      0.75      0.86         4
         401       0.00      0.00      0.00         4
         402       0.33      0.50      0.40         4
         403       0.90      1.00      0.95         9
         404       0.88      0.78      0.82         9
         405       0.43      0.75      0.55         4
         406       1.00      1.00      1.00         4
         407       0.69      1.00      0.82         9
         408       1.00      1.00      1.00         4
         409       0.44      1.00      0.62         4
         410       0.83      1.00      0.91         5
         411       0.60      0.60      0.60         5
         412       0.25      0.25      0.25         4
         413       1.00      0.20      0.33         5
         414       0.00      0.00      0.00         4
         415       1.00      1.00      1.00         4
         416       1.00      0.89      0.94         9
         417       0.67      0.89      0.76         9
         418       0.88      0.78      0.82         9
         419       1.00      0.80      0.89         5
         420       0.17      0.11      0.13         9
         421       0.50      0.50      0.50         4
         422       1.00      1.00      1.00         5
         423       1.00      0.75      0.86         4
         424       0.33      0.20      0.25         5
         425       1.00      1.00      1.00         4
         426       0.71      0.56      0.63         9
         427       0.00      0.00      0.00         4
         428       0.00      0.00      0.00         4
         429       0.80      0.44      0.57         9
         430       1.00      1.00      1.00         4
         431       0.33      0.25      0.29         4
         432       0.83      1.00      0.91         5
         433       0.90      1.00      0.95         9
         434       0.43      0.75      0.55         4
         435       1.00      0.78      0.88         9
         436       0.50      0.50      0.50         4
         437       1.00      1.00      1.00         4
         438       1.00      0.80      0.89         5
         439       0.40      0.50      0.44         4
         440       0.80      0.80      0.80         5
         441       0.00      0.00      0.00         4
         442       0.00      0.00      0.00         4
         443       0.43      0.75      0.55         4
         444       1.00      1.00      1.00         4
         445       0.57      1.00      0.73         4
         446       0.50      0.67      0.57         9
         447       1.00      0.78      0.88         9
         448       0.67      1.00      0.80         4
         449       0.80      0.89      0.84         9
         450       0.90      1.00      0.95         9
         451       1.00      1.00      1.00         4
         452       0.80      1.00      0.89         4
         453       0.40      0.50      0.44         4
         454       0.00      0.00      0.00         4
         455       1.00      0.75      0.86         4
         456       1.00      1.00      1.00         5
         457       1.00      0.89      0.94         9
         458       1.00      1.00      1.00         4
         459       0.71      0.56      0.63         9
         460       1.00      1.00      1.00         5
         461       0.33      0.25      0.29         4
         462       0.80      1.00      0.89         4
         463       0.50      0.25      0.33         4
         464       1.00      1.00      1.00         9
         465       0.00      0.00      0.00         4
         466       0.50      0.50      0.50         4
         467       0.78      0.78      0.78         9
         468       0.10      0.11      0.11         9
         469       0.83      1.00      0.91         5
         470       1.00      0.75      0.86         4
         471       0.14      0.25      0.18         4
         472       0.67      0.44      0.53         9
         473       0.83      1.00      0.91         5
         474       0.50      0.50      0.50         4
         475       1.00      1.00      1.00         4
         476       0.00      0.00      0.00         4
         477       0.44      0.80      0.57         5
         478       0.00      0.00      0.00         4
         479       0.80      1.00      0.89         4
         480       0.83      1.00      0.91         5
         481       0.40      1.00      0.57         4
         482       0.33      0.40      0.36         5
         483       0.67      0.89      0.76         9
         484       1.00      0.78      0.88         9
         485       0.44      1.00      0.62         4
         486       1.00      1.00      1.00         5
         487       1.00      1.00      1.00         4
         488       0.00      0.00      0.00         4
         489       0.40      0.67      0.50         9
         490       0.80      0.44      0.57         9
         491       0.71      1.00      0.83         5
         492       0.40      0.22      0.29         9
         493       0.75      0.75      0.75         4
         494       1.00      1.00      1.00         4
         495       0.89      0.89      0.89         9
         496       0.89      0.89      0.89         9
         497       0.88      0.78      0.82         9
         498       0.40      0.22      0.29         9
         499       1.00      1.00      1.00         4
         500       1.00      1.00      1.00         4
         501       0.75      0.33      0.46         9
         502       0.20      0.25      0.22         4
         503       0.00      0.00      0.00         4
         504       1.00      0.75      0.86         4
         505       0.62      1.00      0.77         5
         506       1.00      0.50      0.67         4
         507       1.00      1.00      1.00         4
         508       0.33      0.25      0.29         4
         509       0.75      0.75      0.75         4
         510       0.90      1.00      0.95         9
         511       1.00      0.20      0.33         5
         512       0.67      0.80      0.73         5
         513       1.00      0.75      0.86         4
         514       0.00      0.00      0.00         4
         515       1.00      1.00      1.00         4
         516       0.33      0.22      0.27         9
         517       0.00      0.00      0.00         7
         518       0.80      0.44      0.57         9
         519       0.67      0.80      0.73         5
         520       0.80      0.80      0.80         5
         521       0.67      0.50      0.57         4
         522       1.00      0.50      0.67         4
         523       0.80      1.00      0.89         4
         524       1.00      1.00      1.00         9
         525       0.57      1.00      0.73         4
         526       0.50      0.25      0.33         4
         527       1.00      0.80      0.89         5
         528       0.80      1.00      0.89         4
         529       0.89      0.89      0.89         9
         530       0.00      0.00      0.00         4
         531       1.00      0.50      0.67         4
         532       1.00      0.50      0.67         4
         533       0.67      0.80      0.73         5
         534       0.00      0.00      0.00         4
         535       0.25      0.11      0.15         9
         536       0.00      0.00      0.00         4
         537       1.00      1.00      1.00         4
         538       1.00      0.80      0.89         5
         539       0.82      1.00      0.90         9
         540       0.83      1.00      0.91         5
         541       1.00      1.00      1.00         5
         542       1.00      0.60      0.75         5
         543       0.00      0.00      0.00         4
         544       0.80      1.00      0.89         4
         545       1.00      0.50      0.67         4
         546       0.67      0.50      0.57         4
         547       0.00      0.00      0.00         4
         548       1.00      0.75      0.86         4
         549       0.54      0.78      0.64         9
         550       0.40      1.00      0.57         4
         551       0.86      0.67      0.75         9
         552       0.00      0.00      0.00         9
         553       0.50      0.75      0.60         4
         554       0.80      1.00      0.89         4
         555       0.75      0.75      0.75         4
         556       0.50      0.75      0.60         4
         557       0.40      0.50      0.44         4
         558       0.75      1.00      0.86         9
         559       0.60      0.75      0.67         4
         560       1.00      0.89      0.94         9
         561       0.67      1.00      0.80         4
         562       0.83      1.00      0.91         5
         563       1.00      0.50      0.67         4
         564       1.00      1.00      1.00         9
         565       1.00      0.25      0.40         4
         566       1.00      1.00      1.00         4
         567       0.80      1.00      0.89         4
         568       0.80      1.00      0.89         4
         569       1.00      0.80      0.89         5
         570       0.80      0.80      0.80         5
         571       0.60      0.75      0.67         4
         572       0.50      0.22      0.31         9
         573       0.80      1.00      0.89         4
         574       0.75      0.75      0.75         4
         575       0.00      0.00      0.00         5
         576       1.00      1.00      1.00         4
         577       0.11      0.25      0.15         4
         578       0.33      0.25      0.29         4
         579       0.57      1.00      0.73         4
         580       0.00      0.00      0.00         9
         581       0.57      1.00      0.73         4
         582       0.75      0.75      0.75         4
         583       1.00      0.89      0.94         9
         584       1.00      0.75      0.86         4
         585       0.00      0.00      0.00         4
         586       0.00      0.00      0.00         9
         587       0.67      0.50      0.57         4
         588       0.50      0.25      0.33         4
         589       0.71      1.00      0.83         5
         590       0.67      0.67      0.67         9
         591       0.75      0.60      0.67         5
         592       1.00      0.89      0.94         9
         593       0.88      0.78      0.82         9
         594       0.50      0.40      0.44         5
         595       0.67      1.00      0.80         4
         596       0.67      0.67      0.67         9
         597       0.44      0.44      0.44         9
         598       0.89      0.89      0.89         9
         599       0.00      0.00      0.00         4
         600       0.90      1.00      0.95         9
         601       1.00      0.80      0.89         5
         602       0.73      0.89      0.80         9
         603       0.80      1.00      0.89         4
         604       1.00      0.50      0.67         4
         605       0.67      0.89      0.76         9
         606       1.00      1.00      1.00         4
         607       0.07      0.25      0.11         4
         608       0.43      0.75      0.55         4
         609       1.00      0.60      0.75         5
         610       0.00      0.00      0.00         9
         611       0.67      0.50      0.57         4
         612       1.00      0.75      0.86         4
         613       1.00      0.80      0.89         5
         614       0.88      0.78      0.82         9
         615       0.43      0.75      0.55         4
         616       0.67      0.40      0.50         5
         617       0.89      0.89      0.89         9
         618       0.75      1.00      0.86         9
         619       0.80      1.00      0.89         4
         620       0.00      0.00      0.00         4
         621       0.00      0.00      0.00         4
         622       0.67      0.50      0.57         4
         623       0.50      0.25      0.33         4
         624       1.00      1.00      1.00         4
         625       0.80      1.00      0.89         4
         626       1.00      1.00      1.00         4
         627       0.67      1.00      0.80         4
         628       1.00      1.00      1.00         9
         629       0.38      0.75      0.50         4
         630       0.73      0.89      0.80         9
         631       0.75      1.00      0.86         9
         632       0.58      0.78      0.67         9
         633       0.75      0.75      0.75         4
         634       1.00      0.75      0.86         4
         635       0.62      1.00      0.77         5
         636       0.00      0.00      0.00         9
         637       0.83      1.00      0.91         5
         638       0.80      1.00      0.89         4
         639       0.50      0.50      0.50         4
         640       1.00      1.00      1.00         4
         641       1.00      1.00      1.00         4
         642       0.80      0.89      0.84         9
         643       0.25      0.50      0.33         4
         644       0.00      0.00      0.00         4
         645       0.67      1.00      0.80         4
         646       1.00      0.75      0.86         4
         647       0.00      0.00      0.00         4
         648       0.50      0.20      0.29         5
         649       0.75      0.75      0.75         4
         650       0.38      0.75      0.50         4
         651       0.75      0.75      0.75         4
         652       1.00      1.00      1.00         4
         653       0.50      1.00      0.67         4
         654       0.71      0.56      0.63         9
         655       0.75      0.67      0.71         9
         656       1.00      1.00      1.00         4
         657       1.00      0.40      0.57         5
         658       0.73      0.89      0.80         9
         659       0.67      0.50      0.57         4
         660       0.33      0.75      0.46         4
         661       0.83      0.56      0.67         9
         662       0.75      0.75      0.75         4
         663       0.50      0.50      0.50         6
         664       0.00      0.00      0.00         4
         665       1.00      0.50      0.67         4
         666       1.00      1.00      1.00         4
         667       1.00      0.50      0.67         4
         668       0.67      1.00      0.80         4
         669       0.60      0.75      0.67         4
         670       0.88      0.78      0.82         9
         671       0.60      0.75      0.67         4
         672       0.86      0.67      0.75         9
         673       0.67      0.89      0.76         9
         674       0.60      0.75      0.67         4
         675       1.00      0.89      0.94         9
         676       0.67      0.50      0.57         4
         677       1.00      0.40      0.57         5
         678       0.67      1.00      0.80         4
         679       0.38      0.75      0.50         4
         680       1.00      0.40      0.57         5
         681       1.00      1.00      1.00         4
         682       0.60      0.67      0.63         9
         683       0.00      0.00      0.00         4
         684       1.00      1.00      1.00         9
         685       1.00      0.88      0.93         8
         686       1.00      0.78      0.88         9
         687       0.78      0.78      0.78         9
         688       0.00      0.00      0.00         4
         689       1.00      0.80      0.89         5
         690       0.83      1.00      0.91         5
         691       0.64      0.78      0.70         9
         692       0.75      0.75      0.75         4
         693       0.67      1.00      0.80         4
         694       0.50      0.50      0.50         4
         695       0.75      0.75      0.75         4
         696       1.00      0.50      0.67         4
         697       0.00      0.00      0.00         4
         698       0.78      0.78      0.78         9
         699       0.75      0.75      0.75         4
         700       0.88      0.78      0.82         9
         701       0.75      0.75      0.75         4
         702       0.50      0.44      0.47         9
         703       1.00      0.75      0.86         4
         704       0.00      0.00      0.00         4
         705       0.20      0.25      0.22         4
         706       1.00      1.00      1.00         4
         707       0.89      0.89      0.89         9
         708       0.57      0.44      0.50         9
         709       0.82      1.00      0.90         9
         710       0.67      1.00      0.80         4
         711       1.00      0.50      0.67         4
         712       0.71      0.56      0.63         9
         713       0.80      1.00      0.89         4
         714       0.67      0.40      0.50         5
         715       0.00      0.00      0.00         4
         716       0.67      0.50      0.57         4
         717       0.80      1.00      0.89         4
         718       0.75      0.75      0.75         4
         719       0.54      0.78      0.64         9
         720       0.09      0.11      0.10         9
         721       1.00      0.20      0.33         5
         722       0.33      0.25      0.29         4
         723       1.00      0.75      0.86         4
         724       0.78      0.78      0.78         9
         725       1.00      1.00      1.00         4
         726       1.00      1.00      1.00         4
         727       0.60      0.75      0.67         4
         728       0.00      0.00      0.00         4
         729       0.50      0.80      0.62         5
         730       0.00      0.00      0.00         9
         731       0.75      0.75      0.75         4
         732       1.00      1.00      1.00         4
         733       0.89      0.89      0.89         9
         734       1.00      1.00      1.00         4
         735       0.67      0.50      0.57         4
         736       1.00      0.75      0.86         4
         737       0.50      0.25      0.33         4
         738       0.67      0.40      0.50         5
         739       0.80      0.80      0.80         5
         740       0.67      0.50      0.57         4
         741       1.00      0.50      0.67         4
         742       0.60      0.67      0.63         9
         743       0.40      0.50      0.44         4
         744       0.89      0.89      0.89         9
         745       0.33      0.50      0.40         4
         746       0.00      0.00      0.00         4
         747       0.43      0.75      0.55         4
         748       1.00      1.00      1.00         5
         749       0.60      0.60      0.60         5
         750       1.00      0.75      0.86         4
         751       0.00      0.00      0.00         9
         752       1.00      0.56      0.71         9
         753       0.83      1.00      0.91         5
         754       0.00      0.00      0.00         5
         755       0.90      1.00      0.95         9
         756       0.75      0.75      0.75         4
         757       1.00      1.00      1.00         5
         758       0.33      0.25      0.29         4
         759       1.00      0.25      0.40         4
         760       0.50      0.40      0.44         5
         761       0.80      1.00      0.89         4
         762       0.40      0.40      0.40         5
         763       1.00      0.89      0.94         9
         764       1.00      0.75      0.86         4
         765       1.00      0.50      0.67         4
         766       0.67      1.00      0.80         4
         767       0.47      0.78      0.58         9
         768       1.00      0.50      0.67         4
         769       0.80      1.00      0.89         4
         770       1.00      0.75      0.86         4
         771       1.00      1.00      1.00         4
         772       0.89      0.89      0.89         9
         773       1.00      1.00      1.00         9
         774       0.90      1.00      0.95         9
         775       0.43      0.75      0.55         4
         776       0.00      0.00      0.00         5
         777       0.83      1.00      0.91         5
         778       1.00      0.20      0.33         5
         779       0.80      0.44      0.57         9
         780       0.89      0.89      0.89         9
         781       0.00      0.00      0.00         9
         782       0.00      0.00      0.00         4
         783       1.00      0.89      0.94         9
         784       0.71      1.00      0.83         5
         785       0.40      0.50      0.44         4
         786       0.75      0.75      0.75         4
         787       0.90      1.00      0.95         9
         788       1.00      0.25      0.40         4
         789       0.67      0.44      0.53         9
         790       0.00      0.00      0.00         4
         791       1.00      0.75      0.86         4
         792       0.64      1.00      0.78         9
         793       0.08      0.25      0.12         4
         794       1.00      1.00      1.00         4
         795       1.00      0.67      0.80         9
         796       1.00      1.00      1.00         4
         797       1.00      0.75      0.86         4
         798       0.71      1.00      0.83         5
         799       0.00      0.00      0.00         4
         800       0.67      1.00      0.80         4
         801       0.50      0.75      0.60         4
         802       1.00      0.89      0.94         9
         803       1.00      1.00      1.00         4
         804       1.00      0.80      0.89         5
         805       1.00      0.75      0.86         4
         806       0.00      0.00      0.00         9
         807       0.00      0.00      0.00         4
         808       0.56      0.56      0.56         9
         809       1.00      0.50      0.67         4
         810       1.00      0.25      0.40         4
         811       1.00      0.80      0.89         5
         812       0.58      0.78      0.67         9
         813       0.40      0.50      0.44         4
         814       0.75      0.67      0.71         9
         815       0.50      0.25      0.33         4
         816       0.75      0.75      0.75         4
         817       0.75      0.75      0.75         4
         818       0.00      0.00      0.00         4
         819       0.50      0.44      0.47         9
         820       1.00      1.00      1.00         4
         821       0.00      0.00      0.00         4
         822       0.00      0.00      0.00         4
         823       0.20      0.11      0.14         9
         824       1.00      1.00      1.00         5
         825       0.50      0.75      0.60         4
         826       0.75      0.75      0.75         4
         827       0.60      0.75      0.67         4
         828       1.00      0.50      0.67         4
         829       0.67      0.40      0.50         5
         830       0.90      1.00      0.95         9
         831       0.00      0.00      0.00         5
         832       0.80      1.00      0.89         4
         833       0.00      0.00      0.00         5
         834       0.00      0.00      0.00         4
         835       0.75      0.33      0.46         9
         836       1.00      0.25      0.40         4
         837       1.00      0.50      0.67         4
         838       1.00      0.75      0.86         4
         839       0.80      1.00      0.89         4
         840       0.80      1.00      0.89         4
         841       0.00      0.00      0.00         5
         842       0.67      1.00      0.80         4
         843       0.00      0.00      0.00         4
         844       0.53      1.00      0.69         9
         845       1.00      0.86      0.92         7
         846       0.00      0.00      0.00         4
         847       1.00      0.25      0.40         4
         848       0.80      1.00      0.89         4
         849       0.67      0.50      0.57         4
         850       0.50      0.50      0.50         4
         851       0.00      0.00      0.00         9
         852       0.00      0.00      0.00         4
         853       0.80      1.00      0.89         4
         854       0.00      0.00      0.00         4
         855       0.89      0.89      0.89         9
         856       0.00      0.00      0.00         4
         857       1.00      0.89      0.94         9
         858       1.00      0.50      0.67         4
         859       0.90      1.00      0.95         9
         860       1.00      0.20      0.33         5
         861       1.00      0.60      0.75         5
         862       1.00      0.22      0.36         9
         863       0.75      0.75      0.75         4
         864       0.50      0.33      0.40         9
         865       0.00      0.00      0.00         4
         866       0.67      0.44      0.53         9
         867       1.00      0.50      0.67         4
         868       0.00      0.00      0.00         4
         869       1.00      0.20      0.33         5
         870       0.67      0.67      0.67         9
         871       0.25      0.11      0.15         9
         872       1.00      0.88      0.93         8
         873       0.00      0.00      0.00         4
         874       0.89      0.89      0.89         9
         875       0.57      1.00      0.73         4
         876       1.00      0.50      0.67         4
         877       0.25      0.40      0.31         5
         878       0.50      0.25      0.33         4
         879       0.60      0.75      0.67         4
         880       1.00      1.00      1.00         4
         881       0.50      0.20      0.29         5
         882       0.11      0.25      0.15         4
         883       1.00      0.50      0.67         4
         884       0.11      0.89      0.20         9
         885       0.80      0.80      0.80         5
         886       0.80      1.00      0.89         4
         887       0.00      0.00      0.00         4
         888       0.67      0.80      0.73         5
         889       1.00      0.60      0.75         5
         890       0.00      0.00      0.00         4
         891       0.70      0.88      0.78         8
         892       0.08      0.50      0.13         4
         893       0.80      0.80      0.80         5

    accuracy                           0.65      4917
   macro avg       0.65      0.64      0.62      4917
weighted avg       0.67      0.65      0.64      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
