	dataset_config: {'path': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'path_test': '/home/fr27/Documents/pyscript/wisdm/dataset/arff_files/phone/accel/all.csv', 'resize': None, 'pad': None, 'crop': None, 'normalize': None, 'class_order': None, 'extend_channel': None, 'flip': False}
CLASS_ORDER: [647, 468, 592, 539, 141, 807, 872, 380, 162, 605, 710, 778, 134, 368, 418, 709, 49, 474, 328, 348, 754, 352, 359, 560, 720, 590, 6, 702, 757, 321, 300, 459, 653, 128, 121, 442, 432, 447, 90, 68, 558, 595, 658, 264, 731, 248, 598, 518, 280, 269, 266, 96, 562, 768, 263, 59, 770, 295, 230, 697, 661, 533, 683, 20, 393, 690, 210, 787, 174, 607, 125, 488, 93, 577, 660, 739, 692, 293, 221, 290, 665, 617, 113, 275, 809, 889, 35, 282, 105, 790, 816, 738, 112, 262, 286, 16, 333, 668, 688, 336, 31, 611, 42, 546, 394, 483, 85, 388, 520, 199, 357, 532, 308, 756, 803, 537, 736, 249, 614, 455, 788, 462, 46, 865, 784, 644, 417, 753, 715, 48, 491, 862, 538, 680, 198, 649, 379, 135, 77, 63, 829, 802, 183, 79, 201, 869, 339, 65, 463, 888, 399, 725, 51, 493, 793, 273, 327, 322, 859, 303, 119, 153, 667, 373, 703, 296, 830, 160, 104, 569, 167, 503, 631, 404, 475, 513, 591, 634, 98, 74, 232, 825, 666, 473, 775, 516, 851, 137, 780, 593, 808, 32, 84, 486, 169, 122, 772, 238, 749, 561, 469, 716, 759, 139, 371, 436, 195, 478, 409, 347, 723, 377, 799, 304, 670, 648, 120, 86, 798, 718, 669, 15, 820, 317, 832, 576, 292, 374, 822, 283, 239, 434, 575, 454, 140, 136, 632, 192, 824, 346, 760, 211, 610, 53, 890, 620, 700, 358, 698, 618, 229, 18, 307, 745, 627, 57, 746, 645, 587, 99, 194, 641, 173, 111, 8, 236, 484, 406, 364, 132, 424, 849, 14, 494, 685, 834, 742, 191, 806, 608, 687, 351, 601, 356, 758, 320, 0, 81, 737, 639, 606, 551, 297, 602, 831, 325, 477, 636, 332, 39, 330, 663, 334, 75, 586, 268, 797, 801, 401, 38, 453, 369, 870, 856, 794, 61, 880, 633, 514, 771, 250, 62, 528, 552, 387, 464, 504, 877, 547, 711, 487, 254, 600, 572, 172, 408, 863, 411, 400, 378, 50, 878, 452, 301, 306, 526, 354, 841, 619, 207, 695, 384, 209, 506, 431, 156, 734, 335, 776, 272, 108, 585, 218, 54, 398, 43, 402, 353, 783, 629, 157, 274, 728, 179, 451, 391, 193, 821, 425, 196, 107, 568, 795, 659, 501, 882, 180, 316, 225, 733, 509, 774, 838, 564, 570, 630, 27, 138, 392, 579, 344, 370, 25, 881, 873, 212, 106, 588, 444, 130, 415, 252, 495, 542, 410, 69, 376, 2, 845, 142, 553, 460, 763, 386, 441, 115, 109, 36, 517, 530, 5, 724, 559, 868, 203, 835, 278, 545, 11, 609, 197, 543, 492, 60, 382, 810, 185, 117, 188, 256, 842, 833, 149, 523, 7, 747, 255, 529, 311, 155, 205, 17, 701, 228, 245, 131, 287, 721, 643, 792, 101, 796, 150, 331, 312, 684, 726, 145, 360, 55, 73, 500, 819, 217, 338, 505, 855, 127, 118, 536, 270, 548, 885, 367, 161, 419, 426, 186, 361, 24, 498, 189, 471, 677, 554, 886, 527, 324, 178, 302, 420, 800, 437, 80, 769, 342, 102, 258, 224, 813, 707, 314, 271, 674, 892, 177, 625, 175, 223, 237, 67, 597, 456, 893, 267, 823, 817, 583, 867, 730, 874, 887, 761, 407, 696, 818, 549, 323, 416, 535, 158, 672, 626, 708, 396, 603, 604, 646, 299, 37, 82, 152, 143, 412, 265, 815, 429, 699, 740, 345, 439, 565, 791, 78, 216, 88, 294, 858, 837, 23, 510, 621, 329, 864, 257, 556, 582, 445, 235, 305, 682, 681, 284, 871, 341, 190, 226, 124, 767, 675, 403, 144, 126, 450, 525, 766, 422, 555, 717, 497, 655, 499, 490, 433, 421, 635, 826, 204, 72, 750, 662, 129, 476, 259, 94, 182, 71, 438, 76, 110, 640, 857, 458, 852, 612, 279, 440, 91, 220, 343, 485, 291, 876, 215, 741, 481, 686, 764, 846, 318, 40, 563, 52, 812, 714, 676, 722, 4, 281, 47, 29, 383, 242, 176, 466, 397, 405, 289, 390, 628, 678, 544, 840, 531, 247, 19, 785, 673, 827, 170, 814, 534, 148, 853, 446, 883, 89, 850, 472, 41, 891, 461, 260, 372, 443, 64, 350, 70, 355, 375, 507, 103, 860, 743, 704, 241, 511, 550, 729, 691, 656, 428, 578, 811, 58, 786, 482, 208, 385, 449, 56, 244, 202, 168, 679, 508, 521, 22, 97, 848, 213, 762, 805, 1, 689, 26, 732, 594, 884, 114, 693, 839, 596, 123, 365, 642, 735, 395, 765, 712, 45, 315, 496, 234, 489, 744, 615, 650, 3, 522, 567, 243, 159, 706, 414, 147, 83, 277, 566, 448, 844, 253, 751, 705, 652, 773, 261, 727, 465, 664, 622, 637, 574, 214, 206, 515, 187, 480, 10, 413, 541, 92, 95, 467, 319, 584, 28, 638, 349, 309, 21, 366, 861, 671, 524, 171, 12, 599, 502, 116, 389, 251, 154, 337, 512, 470, 66, 423, 276, 879, 231, 200, 33, 782, 875, 854, 828, 713, 240, 779, 581, 657, 613, 435, 789, 227, 34, 381, 836, 571, 133, 804, 165, 519, 100, 163, 9, 479, 30, 233, 781, 573, 184, 326, 151, 430, 246, 288, 719, 589, 13, 285, 181, 748, 313, 847, 166, 146, 310, 340, 362, 557, 457, 866, 624, 44, 219, 777, 540, 623, 616, 843, 164, 580, 363, 298, 87, 752, 755, 222, 427, 651, 694, 654]
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList()
)
======

************************************************************************************************************
Task  0
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.915, TAw acc= 36.8% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.428, TAw acc= 53.5% | *
| Epoch   3, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=2.050, TAw acc= 63.9% | *
| Epoch   4, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.755, TAw acc= 71.5% | *
| Epoch   5, time=  0.2s | Train: skip eval | Valid: time=  0.2s loss=1.527, TAw acc= 72.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 319 train exemplars, time=  0.0s
319
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.488 | TAw acc= 76.3%, forg=  0.0%| TAg acc= 76.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
  )
)
======

************************************************************************************************************
Task  1
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.818, TAw acc= 61.8% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.204, TAw acc= 69.7% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.826, TAw acc= 80.3% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.553, TAw acc= 81.6% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.342, TAw acc= 89.5% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.340, TAw acc= 89.5% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.338, TAw acc= 89.5% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.337, TAw acc= 89.5% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.335, TAw acc= 89.5% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 89.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 499 train exemplars, time=  0.0s
499
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.503 | TAw acc= 88.1%, forg=-11.9%| TAg acc= 75.3%, forg=  1.0% <<<
>>> Test on task  1 : loss=1.387 | TAw acc= 90.4%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  2
************************************************************************************************************
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.219, TAw acc= 40.7% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.409, TAw acc= 51.6% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.038, TAw acc= 56.0% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.806, TAw acc= 78.0% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.592, TAw acc= 73.6% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.589, TAw acc= 73.6% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.586, TAw acc= 75.8% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.583, TAw acc= 75.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.581, TAw acc= 75.8% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=1.578, TAw acc= 75.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 679 train exemplars, time=  0.0s
679
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.336 | TAw acc= 87.6%, forg=  0.5%| TAg acc= 75.8%, forg=  0.5% <<<
>>> Test on task  1 : loss=1.562 | TAw acc= 95.2%, forg= -4.8%| TAg acc= 57.7%, forg= 19.2% <<<
>>> Test on task  2 : loss=1.505 | TAw acc= 80.0%, forg=  0.0%| TAg acc= 70.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  3
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.156, TAw acc= 47.6% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=3.373, TAw acc= 52.4% | *
| Epoch   3, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.820, TAw acc= 54.8% | *
| Epoch   4, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.445, TAw acc= 57.1% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.164, TAw acc= 63.1% | *
| Epoch   1, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.161, TAw acc= 64.3% | *
| Epoch   2, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.159, TAw acc= 64.3% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.157, TAw acc= 64.3% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.154, TAw acc= 64.3% | *
| Epoch   5, time=  0.3s | Train: skip eval | Valid: time=  0.2s loss=2.152, TAw acc= 64.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 859 train exemplars, time=  0.0s
859
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.219 | TAw acc= 88.7%, forg= -0.5%| TAg acc= 80.9%, forg= -4.6% <<<
>>> Test on task  1 : loss=1.368 | TAw acc= 97.1%, forg= -1.9%| TAg acc= 68.3%, forg=  8.7% <<<
>>> Test on task  2 : loss=1.691 | TAw acc= 90.8%, forg=-10.8%| TAg acc= 54.2%, forg= 15.8% <<<
>>> Test on task  3 : loss=1.949 | TAw acc= 70.8%, forg=  0.0%| TAg acc= 50.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  4
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=3.599, TAw acc= 33.3% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.736, TAw acc= 59.0% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.315, TAw acc= 71.8% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.045, TAw acc= 73.1% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.868, TAw acc= 76.9% | *
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.865, TAw acc= 76.9% | *
| Epoch   2, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.862, TAw acc= 76.9% | *
| Epoch   3, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.859, TAw acc= 76.9% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.856, TAw acc= 76.9% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=1.853, TAw acc= 76.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
| Selected 1039 train exemplars, time=  0.0s
1039
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.133 | TAw acc= 87.1%, forg=  1.5%| TAg acc= 80.9%, forg=  0.0% <<<
>>> Test on task  1 : loss=1.240 | TAw acc= 99.0%, forg= -1.9%| TAg acc= 77.9%, forg= -1.0% <<<
>>> Test on task  2 : loss=1.512 | TAw acc= 92.5%, forg= -1.7%| TAg acc= 65.0%, forg=  5.0% <<<
>>> Test on task  3 : loss=1.907 | TAw acc= 85.8%, forg=-15.0%| TAg acc= 43.4%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.895 | TAw acc= 80.4%, forg=  0.0%| TAg acc= 66.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  5
************************************************************************************************************
| Epoch   1, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=4.468, TAw acc= 52.3% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=3.359, TAw acc= 60.5% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.745, TAw acc= 64.0% | *
| Epoch   4, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.299, TAw acc= 66.3% | *
| Epoch   5, time=  0.4s | Train: skip eval | Valid: time=  0.2s loss=2.067, TAw acc= 69.8% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=2.065, TAw acc= 69.8% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.062, TAw acc= 69.8% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.3s loss=2.059, TAw acc= 69.8% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.056, TAw acc= 69.8% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.054, TAw acc= 69.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1205 train exemplars, time=  0.0s
1205
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.062 | TAw acc= 89.2%, forg= -0.5%| TAg acc= 82.5%, forg= -1.5% <<<
>>> Test on task  1 : loss=1.208 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.8%, forg= -1.0% <<<
>>> Test on task  2 : loss=1.462 | TAw acc= 93.3%, forg= -0.8%| TAg acc= 63.3%, forg=  6.7% <<<
>>> Test on task  3 : loss=1.821 | TAw acc= 89.4%, forg= -3.5%| TAg acc= 43.4%, forg=  7.1% <<<
>>> Test on task  4 : loss=1.864 | TAw acc= 90.7%, forg=-10.3%| TAg acc= 55.1%, forg= 11.2% <<<
>>> Test on task  5 : loss=1.895 | TAw acc= 75.9%, forg=  0.0%| TAg acc= 59.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  6
************************************************************************************************************
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=4.197, TAw acc= 39.4% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.705, TAw acc= 52.1% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=2.242, TAw acc= 62.0% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.056, TAw acc= 67.6% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.810, TAw acc= 73.2% | *
| Epoch   1, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.804, TAw acc= 73.2% | *
| Epoch   2, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.798, TAw acc= 73.2% | *
| Epoch   3, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.793, TAw acc= 73.2% | *
| Epoch   4, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.787, TAw acc= 73.2% | *
| Epoch   5, time=  0.5s | Train: skip eval | Valid: time=  0.2s loss=1.782, TAw acc= 73.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1345 train exemplars, time=  0.0s
1345
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.969 | TAw acc= 88.1%, forg=  1.0%| TAg acc= 80.4%, forg=  2.1% <<<
>>> Test on task  1 : loss=1.131 | TAw acc= 98.1%, forg=  1.0%| TAg acc= 79.8%, forg= -1.0% <<<
>>> Test on task  2 : loss=1.400 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 66.7%, forg=  3.3% <<<
>>> Test on task  3 : loss=1.620 | TAw acc= 90.3%, forg= -0.9%| TAg acc= 56.6%, forg= -6.2% <<<
>>> Test on task  4 : loss=1.663 | TAw acc= 91.6%, forg= -0.9%| TAg acc= 68.2%, forg= -1.9% <<<
>>> Test on task  5 : loss=2.033 | TAw acc= 91.4%, forg=-15.5%| TAg acc= 52.6%, forg=  6.9% <<<
>>> Test on task  6 : loss=1.684 | TAw acc= 87.8%, forg=  0.0%| TAg acc= 55.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  7
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=4.488, TAw acc= 38.8% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.053, TAw acc= 55.0% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.390, TAw acc= 70.0% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.034, TAw acc= 77.5% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.735, TAw acc= 81.2% | *
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.733, TAw acc= 81.2% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.731, TAw acc= 81.2% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.728, TAw acc= 81.2% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.726, TAw acc= 81.2% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.724, TAw acc= 81.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1485 train exemplars, time=  0.0s
1485
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.015 | TAw acc= 88.1%, forg=  1.0%| TAg acc= 78.4%, forg=  4.1% <<<
>>> Test on task  1 : loss=1.111 | TAw acc= 96.2%, forg=  2.9%| TAg acc= 79.8%, forg=  0.0% <<<
>>> Test on task  2 : loss=1.380 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 60.0%, forg= 10.0% <<<
>>> Test on task  3 : loss=1.562 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 60.2%, forg= -3.5% <<<
>>> Test on task  4 : loss=1.536 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 68.2%, forg=  0.0% <<<
>>> Test on task  5 : loss=1.756 | TAw acc= 93.1%, forg= -1.7%| TAg acc= 63.8%, forg= -4.3% <<<
>>> Test on task  6 : loss=1.651 | TAw acc= 91.8%, forg= -4.1%| TAg acc= 52.0%, forg=  3.1% <<<
>>> Test on task  7 : loss=1.608 | TAw acc= 85.3%, forg=  0.0%| TAg acc= 56.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  8
************************************************************************************************************
| Epoch   1, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=3.840, TAw acc= 50.6% | *
| Epoch   2, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=2.577, TAw acc= 67.9% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.945, TAw acc= 72.8% | *
| Epoch   4, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.550, TAw acc= 77.8% | *
| Epoch   5, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.326, TAw acc= 77.8% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.323, TAw acc= 77.8% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.320, TAw acc= 77.8% | *
| Epoch   3, time=  0.6s | Train: skip eval | Valid: time=  0.2s loss=1.318, TAw acc= 77.8% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.315, TAw acc= 77.8% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=1.313, TAw acc= 77.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1625 train exemplars, time=  0.0s
1625
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.949 | TAw acc= 89.2%, forg=  0.0%| TAg acc= 78.9%, forg=  3.6% <<<
>>> Test on task  1 : loss=1.037 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.8%, forg=  1.0% <<<
>>> Test on task  2 : loss=1.293 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 71.7%, forg= -1.7% <<<
>>> Test on task  3 : loss=1.449 | TAw acc= 88.5%, forg=  1.8%| TAg acc= 64.6%, forg= -4.4% <<<
>>> Test on task  4 : loss=1.524 | TAw acc= 91.6%, forg=  0.0%| TAg acc= 57.9%, forg= 10.3% <<<
>>> Test on task  5 : loss=1.684 | TAw acc= 94.0%, forg= -0.9%| TAg acc= 66.4%, forg= -2.6% <<<
>>> Test on task  6 : loss=1.487 | TAw acc= 92.9%, forg= -1.0%| TAg acc= 58.2%, forg= -3.1% <<<
>>> Test on task  7 : loss=1.686 | TAw acc= 95.4%, forg=-10.1%| TAg acc= 56.0%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.503 | TAw acc= 80.7%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task  9
************************************************************************************************************
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=5.024, TAw acc= 25.4% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.805, TAw acc= 39.4% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=3.032, TAw acc= 56.3% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.495, TAw acc= 81.7% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.140, TAw acc= 88.7% | *
| Epoch   1, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.137, TAw acc= 88.7% | *
| Epoch   2, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.134, TAw acc= 88.7% | *
| Epoch   3, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.131, TAw acc= 88.7% | *
| Epoch   4, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.128, TAw acc= 88.7% | *
| Epoch   5, time=  0.7s | Train: skip eval | Valid: time=  0.2s loss=2.125, TAw acc= 88.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1765 train exemplars, time=  0.0s
1765
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.965 | TAw acc= 90.7%, forg= -1.5%| TAg acc= 81.4%, forg=  1.0% <<<
>>> Test on task  1 : loss=1.062 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 80.8%, forg= -1.0% <<<
>>> Test on task  2 : loss=1.200 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 67.5%, forg=  4.2% <<<
>>> Test on task  3 : loss=1.359 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 68.1%, forg= -3.5% <<<
>>> Test on task  4 : loss=1.363 | TAw acc= 94.4%, forg= -2.8%| TAg acc= 69.2%, forg= -0.9% <<<
>>> Test on task  5 : loss=1.485 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 67.2%, forg= -0.9% <<<
>>> Test on task  6 : loss=1.323 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 73.5%, forg=-15.3% <<<
>>> Test on task  7 : loss=1.493 | TAw acc= 93.6%, forg=  1.8%| TAg acc= 61.5%, forg= -4.6% <<<
>>> Test on task  8 : loss=1.517 | TAw acc= 96.3%, forg=-15.6%| TAg acc= 73.4%, forg= -4.6% <<<
>>> Test on task  9 : loss=1.751 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 54.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 10
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=4.930, TAw acc= 38.7% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=3.458, TAw acc= 58.7% | *
| Epoch   3, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.664, TAw acc= 73.3% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=2.156, TAw acc= 81.3% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.863, TAw acc= 90.7% | *
| Epoch   1, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.859, TAw acc= 90.7% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.3s loss=1.855, TAw acc= 90.7% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.851, TAw acc= 90.7% | *
| Epoch   4, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.846, TAw acc= 90.7% | *
| Epoch   5, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=1.842, TAw acc= 90.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 1905 train exemplars, time=  0.0s
1905
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.937 | TAw acc= 90.2%, forg=  0.5%| TAg acc= 77.8%, forg=  4.6% <<<
>>> Test on task  1 : loss=1.025 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  6.7% <<<
>>> Test on task  2 : loss=1.225 | TAw acc= 92.5%, forg=  0.8%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.355 | TAw acc= 88.5%, forg=  1.8%| TAg acc= 69.9%, forg= -1.8% <<<
>>> Test on task  4 : loss=1.312 | TAw acc= 96.3%, forg= -1.9%| TAg acc= 71.0%, forg= -1.9% <<<
>>> Test on task  5 : loss=1.418 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 72.4%, forg= -5.2% <<<
>>> Test on task  6 : loss=1.241 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 71.4%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.327 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 67.0%, forg= -5.5% <<<
>>> Test on task  8 : loss=1.438 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 73.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.617 | TAw acc= 91.9%, forg=  1.0%| TAg acc= 56.6%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.811 | TAw acc= 92.2%, forg=  0.0%| TAg acc= 50.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 11
************************************************************************************************************
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=5.204, TAw acc= 48.7% | *
| Epoch   2, time=  0.8s | Train: skip eval | Valid: time=  0.2s loss=3.188, TAw acc= 64.5% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.345, TAw acc= 71.1% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=2.052, TAw acc= 82.9% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.748, TAw acc= 85.5% | *
| Epoch   1, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.746, TAw acc= 85.5% | *
| Epoch   2, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.744, TAw acc= 85.5% | *
| Epoch   3, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.742, TAw acc= 85.5% | *
| Epoch   4, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.740, TAw acc= 85.5% | *
| Epoch   5, time=  0.9s | Train: skip eval | Valid: time=  0.2s loss=1.738, TAw acc= 86.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2045 train exemplars, time=  0.0s
2045
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.977 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 77.3%, forg=  5.2% <<<
>>> Test on task  1 : loss=1.035 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.1%, forg=  7.7% <<<
>>> Test on task  2 : loss=1.273 | TAw acc= 91.7%, forg=  1.7%| TAg acc= 64.2%, forg=  7.5% <<<
>>> Test on task  3 : loss=1.291 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 69.0%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.354 | TAw acc= 95.3%, forg=  0.9%| TAg acc= 70.1%, forg=  0.9% <<<
>>> Test on task  5 : loss=1.384 | TAw acc= 94.8%, forg= -0.9%| TAg acc= 69.8%, forg=  2.6% <<<
>>> Test on task  6 : loss=1.246 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 75.5%, forg= -2.0% <<<
>>> Test on task  7 : loss=1.292 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  1.8% <<<
>>> Test on task  8 : loss=1.363 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 69.7%, forg=  3.7% <<<
>>> Test on task  9 : loss=1.390 | TAw acc= 97.0%, forg= -4.0%| TAg acc= 60.6%, forg= -4.0% <<<
>>> Test on task 10 : loss=1.761 | TAw acc= 97.1%, forg= -4.9%| TAg acc= 49.0%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.694 | TAw acc= 89.4%, forg=  0.0%| TAg acc= 59.6%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 12
************************************************************************************************************
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=5.227, TAw acc= 44.9% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=3.211, TAw acc= 55.1% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=2.194, TAw acc= 67.9% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.746, TAw acc= 74.4% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.527, TAw acc= 85.9% | *
| Epoch   1, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.525, TAw acc= 85.9% | *
| Epoch   2, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.522, TAw acc= 85.9% | *
| Epoch   3, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.520, TAw acc= 85.9% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.517, TAw acc= 85.9% | *
| Epoch   5, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.515, TAw acc= 85.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2185 train exemplars, time=  0.0s
2185
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.938 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 76.8%, forg=  5.7% <<<
>>> Test on task  1 : loss=1.033 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.9%, forg=  3.8% <<<
>>> Test on task  2 : loss=1.256 | TAw acc= 91.7%, forg=  1.7%| TAg acc= 62.5%, forg=  9.2% <<<
>>> Test on task  3 : loss=1.254 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 73.5%, forg= -3.5% <<<
>>> Test on task  4 : loss=1.333 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 72.9%, forg= -1.9% <<<
>>> Test on task  5 : loss=1.372 | TAw acc= 95.7%, forg= -0.9%| TAg acc= 71.6%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.212 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 74.5%, forg=  1.0% <<<
>>> Test on task  7 : loss=1.235 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 63.3%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.245 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.216 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 70.7%, forg=-10.1% <<<
>>> Test on task 10 : loss=1.582 | TAw acc= 99.0%, forg= -2.0%| TAg acc= 53.9%, forg= -3.9% <<<
>>> Test on task 11 : loss=1.844 | TAw acc= 94.2%, forg= -4.8%| TAg acc= 53.8%, forg=  5.8% <<<
>>> Test on task 12 : loss=1.788 | TAw acc= 84.0%, forg=  0.0%| TAg acc= 51.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 13
************************************************************************************************************
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=4.809, TAw acc= 31.2% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=3.131, TAw acc= 56.2% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=2.335, TAw acc= 68.8% | *
| Epoch   4, time=  1.0s | Train: skip eval | Valid: time=  0.2s loss=1.931, TAw acc= 76.0% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.688, TAw acc= 78.1% | *
| Epoch   1, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.686, TAw acc= 78.1% | *
| Epoch   2, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.684, TAw acc= 78.1% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.682, TAw acc= 78.1% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.679, TAw acc= 78.1% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.677, TAw acc= 78.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2325 train exemplars, time=  0.0s
2325
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.951 | TAw acc= 90.2%, forg=  0.5%| TAg acc= 76.8%, forg=  5.7% <<<
>>> Test on task  1 : loss=1.031 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.1%, forg=  8.7% <<<
>>> Test on task  2 : loss=1.323 | TAw acc= 93.3%, forg=  0.0%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.301 | TAw acc= 89.4%, forg=  0.9%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.399 | TAw acc= 91.6%, forg=  4.7%| TAg acc= 67.3%, forg=  5.6% <<<
>>> Test on task  5 : loss=1.397 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 75.9%, forg= -3.4% <<<
>>> Test on task  6 : loss=1.123 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg= -3.1% <<<
>>> Test on task  7 : loss=1.147 | TAw acc= 94.5%, forg=  0.9%| TAg acc= 68.8%, forg= -1.8% <<<
>>> Test on task  8 : loss=1.234 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.223 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 72.7%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.475 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 62.7%, forg= -8.8% <<<
>>> Test on task 11 : loss=1.729 | TAw acc= 98.1%, forg= -3.8%| TAg acc= 58.7%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.956 | TAw acc= 90.6%, forg= -6.6%| TAg acc= 52.8%, forg= -0.9% <<<
>>> Test on task 13 : loss=1.665 | TAw acc= 81.9%, forg=  0.0%| TAg acc= 51.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 14
************************************************************************************************************
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=5.805, TAw acc= 44.0% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=3.790, TAw acc= 58.7% | *
| Epoch   3, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.743, TAw acc= 73.3% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.124, TAw acc= 85.3% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.702, TAw acc= 89.3% | *
| Epoch   1, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.699, TAw acc= 89.3% | *
| Epoch   2, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.696, TAw acc= 89.3% | *
| Epoch   3, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.693, TAw acc= 90.7% | *
| Epoch   4, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.690, TAw acc= 90.7% | *
| Epoch   5, time=  1.1s | Train: skip eval | Valid: time=  0.2s loss=1.687, TAw acc= 90.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2465 train exemplars, time=  0.0s
2465
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.922 | TAw acc= 88.7%, forg=  2.1%| TAg acc= 77.3%, forg=  5.2% <<<
>>> Test on task  1 : loss=0.999 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  5.8% <<<
>>> Test on task  2 : loss=1.251 | TAw acc= 95.8%, forg= -2.5%| TAg acc= 65.0%, forg=  6.7% <<<
>>> Test on task  3 : loss=1.268 | TAw acc= 89.4%, forg=  0.9%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.258 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 75.7%, forg= -2.8% <<<
>>> Test on task  5 : loss=1.298 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 75.0%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.138 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.194 | TAw acc= 95.4%, forg=  0.0%| TAg acc= 65.1%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.325 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.8%, forg=  4.6% <<<
>>> Test on task  9 : loss=1.103 | TAw acc= 94.9%, forg=  2.0%| TAg acc= 77.8%, forg= -5.1% <<<
>>> Test on task 10 : loss=1.370 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 63.7%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.542 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 61.5%, forg= -1.9% <<<
>>> Test on task 12 : loss=1.813 | TAw acc= 92.5%, forg= -1.9%| TAg acc= 50.9%, forg=  1.9% <<<
>>> Test on task 13 : loss=2.092 | TAw acc= 83.5%, forg= -1.6%| TAg acc= 32.3%, forg= 18.9% <<<
>>> Test on task 14 : loss=1.369 | TAw acc= 95.1%, forg=  0.0%| TAg acc= 74.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 15
************************************************************************************************************
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=5.170, TAw acc= 47.3% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=3.257, TAw acc= 63.5% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=2.517, TAw acc= 74.3% | *
| Epoch   4, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=2.071, TAw acc= 79.7% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.828, TAw acc= 83.8% | *
| Epoch   1, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.825, TAw acc= 82.4% | *
| Epoch   2, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.822, TAw acc= 82.4% | *
| Epoch   3, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.819, TAw acc= 82.4% | *
| Epoch   4, time=  1.3s | Train: skip eval | Valid: time=  0.2s loss=1.816, TAw acc= 82.4% | *
| Epoch   5, time=  1.2s | Train: skip eval | Valid: time=  0.2s loss=1.814, TAw acc= 82.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2605 train exemplars, time=  0.0s
2605
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.890 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 77.8%, forg=  4.6% <<<
>>> Test on task  1 : loss=0.999 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  6.7% <<<
>>> Test on task  2 : loss=1.256 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 64.2%, forg=  7.5% <<<
>>> Test on task  3 : loss=1.201 | TAw acc= 89.4%, forg=  0.9%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.259 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 74.8%, forg=  0.9% <<<
>>> Test on task  5 : loss=1.294 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 78.4%, forg= -2.6% <<<
>>> Test on task  6 : loss=1.131 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 77.6%, forg=  1.0% <<<
>>> Test on task  7 : loss=1.190 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 65.1%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.234 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.4%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.072 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 74.7%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.327 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 64.7%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.500 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 57.7%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.712 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 57.5%, forg= -4.7% <<<
>>> Test on task 13 : loss=1.948 | TAw acc= 81.9%, forg=  1.6%| TAg acc= 42.5%, forg=  8.7% <<<
>>> Test on task 14 : loss=1.393 | TAw acc= 97.1%, forg= -1.9%| TAg acc= 66.0%, forg=  8.7% <<<
>>> Test on task 15 : loss=1.606 | TAw acc= 87.0%, forg=  0.0%| TAg acc= 57.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 16
************************************************************************************************************
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=5.757, TAw acc= 41.9% | *
| Epoch   2, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=3.508, TAw acc= 55.4% | *
| Epoch   3, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.606, TAw acc= 66.2% | *
| Epoch   4, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=2.129, TAw acc= 78.4% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.851, TAw acc= 79.7% | *
| Epoch   1, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.845, TAw acc= 79.7% | *
| Epoch   2, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.839, TAw acc= 79.7% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.833, TAw acc= 79.7% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.828, TAw acc= 79.7% | *
| Epoch   5, time=  1.4s | Train: skip eval | Valid: time=  0.2s loss=1.822, TAw acc= 79.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2745 train exemplars, time=  0.0s
2745
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.898 | TAw acc= 90.2%, forg=  0.5%| TAg acc= 76.3%, forg=  6.2% <<<
>>> Test on task  1 : loss=1.007 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.9%, forg=  3.8% <<<
>>> Test on task  2 : loss=1.216 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 65.8%, forg=  5.8% <<<
>>> Test on task  3 : loss=1.186 | TAw acc= 90.3%, forg=  0.0%| TAg acc= 73.5%, forg=  0.0% <<<
>>> Test on task  4 : loss=1.265 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 71.0%, forg=  4.7% <<<
>>> Test on task  5 : loss=1.268 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 76.7%, forg=  1.7% <<<
>>> Test on task  6 : loss=1.125 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 78.6%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.165 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 66.1%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.208 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 74.3%, forg= -0.9% <<<
>>> Test on task  9 : loss=1.111 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 79.8%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.228 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.6%, forg= -6.9% <<<
>>> Test on task 11 : loss=1.378 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 67.3%, forg= -5.8% <<<
>>> Test on task 12 : loss=1.620 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 67.9%, forg=-10.4% <<<
>>> Test on task 13 : loss=1.862 | TAw acc= 82.7%, forg=  0.8%| TAg acc= 47.2%, forg=  3.9% <<<
>>> Test on task 14 : loss=1.228 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 72.8%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.752 | TAw acc= 90.0%, forg= -3.0%| TAg acc= 45.0%, forg= 12.0% <<<
>>> Test on task 16 : loss=1.653 | TAw acc= 86.1%, forg=  0.0%| TAg acc= 56.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 17
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=4.358, TAw acc= 50.0% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=2.272, TAw acc= 76.1% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.447, TAw acc= 94.6% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.184, TAw acc= 95.7% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.030, TAw acc= 95.7% | *
| Epoch   1, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.030, TAw acc= 96.7% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.029, TAw acc= 96.7% | *
| Epoch   3, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.029, TAw acc= 96.7% | *
| Epoch   4, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.028, TAw acc= 96.7% | *
| Epoch   5, time=  1.5s | Train: skip eval | Valid: time=  0.2s loss=1.027, TAw acc= 96.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 2885 train exemplars, time=  0.0s
2885
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.927 | TAw acc= 88.7%, forg=  2.1%| TAg acc= 74.7%, forg=  7.7% <<<
>>> Test on task  1 : loss=1.006 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  6.7% <<<
>>> Test on task  2 : loss=1.249 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 61.7%, forg= 10.0% <<<
>>> Test on task  3 : loss=1.192 | TAw acc= 92.0%, forg= -1.8%| TAg acc= 73.5%, forg=  0.0% <<<
>>> Test on task  4 : loss=1.312 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 71.0%, forg=  4.7% <<<
>>> Test on task  5 : loss=1.314 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 74.1%, forg=  4.3% <<<
>>> Test on task  6 : loss=1.113 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 80.6%, forg= -2.0% <<<
>>> Test on task  7 : loss=1.128 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.0%, forg=  1.8% <<<
>>> Test on task  8 : loss=1.234 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.091 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 79.8%, forg=  0.0% <<<
>>> Test on task 10 : loss=1.221 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.5%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.395 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 68.3%, forg= -1.0% <<<
>>> Test on task 12 : loss=1.595 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 68.9%, forg= -0.9% <<<
>>> Test on task 13 : loss=1.855 | TAw acc= 83.5%, forg=  0.0%| TAg acc= 46.5%, forg=  4.7% <<<
>>> Test on task 14 : loss=1.098 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 79.6%, forg= -4.9% <<<
>>> Test on task 15 : loss=1.689 | TAw acc= 90.0%, forg=  0.0%| TAg acc= 49.0%, forg=  8.0% <<<
>>> Test on task 16 : loss=1.793 | TAw acc= 90.1%, forg= -4.0%| TAg acc= 42.6%, forg= 13.9% <<<
>>> Test on task 17 : loss=1.057 | TAw acc= 96.7%, forg=  0.0%| TAg acc= 76.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 18
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=5.624, TAw acc= 52.4% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.919, TAw acc= 76.2% | *
| Epoch   3, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=2.054, TAw acc= 81.0% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.611, TAw acc= 92.9% | *
| Epoch   5, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.304, TAw acc= 92.9% | *
| Epoch   1, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.301, TAw acc= 92.9% | *
| Epoch   2, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.298, TAw acc= 92.9% | *
| Epoch   3, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.295, TAw acc= 92.9% | *
| Epoch   4, time=  1.6s | Train: skip eval | Valid: time=  0.2s loss=1.293, TAw acc= 92.9% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.290, TAw acc= 92.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 3025 train exemplars, time=  0.1s
3025
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.983 | TAw acc= 88.7%, forg=  2.1%| TAg acc= 73.2%, forg=  9.3% <<<
>>> Test on task  1 : loss=1.046 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 65.4%, forg= 15.4% <<<
>>> Test on task  2 : loss=1.177 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 71.7%, forg=  0.0% <<<
>>> Test on task  3 : loss=1.238 | TAw acc= 90.3%, forg=  1.8%| TAg acc= 67.3%, forg=  6.2% <<<
>>> Test on task  4 : loss=1.252 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 70.1%, forg=  5.6% <<<
>>> Test on task  5 : loss=1.220 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 79.3%, forg= -0.9% <<<
>>> Test on task  6 : loss=1.068 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 78.6%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.187 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 64.2%, forg=  4.6% <<<
>>> Test on task  8 : loss=1.200 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 74.3%, forg=  0.0% <<<
>>> Test on task  9 : loss=1.022 | TAw acc= 93.9%, forg=  3.0%| TAg acc= 78.8%, forg=  1.0% <<<
>>> Test on task 10 : loss=1.186 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 71.6%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.401 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 63.5%, forg=  4.8% <<<
>>> Test on task 12 : loss=1.494 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 70.8%, forg= -1.9% <<<
>>> Test on task 13 : loss=1.747 | TAw acc= 85.0%, forg= -1.6%| TAg acc= 53.5%, forg= -2.4% <<<
>>> Test on task 14 : loss=1.033 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 82.5%, forg= -2.9% <<<
>>> Test on task 15 : loss=1.601 | TAw acc= 92.0%, forg= -2.0%| TAg acc= 54.0%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.665 | TAw acc= 88.1%, forg=  2.0%| TAg acc= 51.5%, forg=  5.0% <<<
>>> Test on task 17 : loss=1.314 | TAw acc= 98.4%, forg= -1.6%| TAg acc= 67.5%, forg=  8.9% <<<
>>> Test on task 18 : loss=1.189 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 73.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 19
************************************************************************************************************
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=4.218, TAw acc= 58.3% | *
| Epoch   2, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=2.648, TAw acc= 75.0% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.783, TAw acc= 89.3% | *
| Epoch   4, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.503, TAw acc= 95.2% | *
| Epoch   5, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 95.2% | *
| Epoch   1, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 95.2% | *
| Epoch   2, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 95.2% | *
| Epoch   3, time=  1.7s | Train: skip eval | Valid: time=  0.2s loss=1.213, TAw acc= 95.2% | *
| Epoch   4, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.212, TAw acc= 95.2% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.212, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 3165 train exemplars, time=  0.0s
3165
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.931 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 72.2%, forg= 10.3% <<<
>>> Test on task  1 : loss=1.021 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 72.1%, forg=  8.7% <<<
>>> Test on task  2 : loss=1.270 | TAw acc= 95.8%, forg=  0.0%| TAg acc= 59.2%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.201 | TAw acc= 90.3%, forg=  1.8%| TAg acc= 72.6%, forg=  0.9% <<<
>>> Test on task  4 : loss=1.437 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 62.6%, forg= 13.1% <<<
>>> Test on task  5 : loss=1.226 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.066 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 79.6%, forg=  1.0% <<<
>>> Test on task  7 : loss=1.101 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.9%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.235 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg= -0.9% <<<
>>> Test on task  9 : loss=1.017 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 78.8%, forg=  1.0% <<<
>>> Test on task 10 : loss=1.176 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg= -2.0% <<<
>>> Test on task 11 : loss=1.366 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 63.5%, forg=  4.8% <<<
>>> Test on task 12 : loss=1.405 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 72.6%, forg= -1.9% <<<
>>> Test on task 13 : loss=1.711 | TAw acc= 85.8%, forg= -0.8%| TAg acc= 53.5%, forg=  0.0% <<<
>>> Test on task 14 : loss=0.939 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 82.5%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.570 | TAw acc= 93.0%, forg= -1.0%| TAg acc= 58.0%, forg= -1.0% <<<
>>> Test on task 16 : loss=1.617 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 52.5%, forg=  4.0% <<<
>>> Test on task 17 : loss=1.225 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 67.5%, forg=  8.9% <<<
>>> Test on task 18 : loss=1.301 | TAw acc= 96.5%, forg= -2.7%| TAg acc= 63.7%, forg=  9.7% <<<
>>> Test on task 19 : loss=1.238 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 20
************************************************************************************************************
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=5.721, TAw acc= 50.7% | *
| Epoch   2, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=3.355, TAw acc= 64.0% | *
| Epoch   3, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=2.319, TAw acc= 81.3% | *
| Epoch   4, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.885, TAw acc= 84.0% | *
| Epoch   5, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.631, TAw acc= 86.7% | *
| Epoch   1, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.629, TAw acc= 86.7% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.626, TAw acc= 86.7% | *
| Epoch   3, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.624, TAw acc= 86.7% | *
| Epoch   4, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.621, TAw acc= 86.7% | *
| Epoch   5, time=  1.8s | Train: skip eval | Valid: time=  0.2s loss=1.619, TAw acc= 86.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
| Selected 3305 train exemplars, time=  0.0s
3305
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.901 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 73.2%, forg=  9.3% <<<
>>> Test on task  1 : loss=1.004 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  6.7% <<<
>>> Test on task  2 : loss=1.236 | TAw acc= 97.5%, forg= -1.7%| TAg acc= 64.2%, forg=  7.5% <<<
>>> Test on task  3 : loss=1.257 | TAw acc= 91.2%, forg=  0.9%| TAg acc= 69.9%, forg=  3.5% <<<
>>> Test on task  4 : loss=1.335 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.3%, forg=  8.4% <<<
>>> Test on task  5 : loss=1.211 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.048 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 80.6%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.163 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.9%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.221 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.4%, forg=  1.8% <<<
>>> Test on task  9 : loss=1.040 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 76.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.135 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.315 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 64.4%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.433 | TAw acc= 91.5%, forg=  0.9%| TAg acc= 75.5%, forg= -2.8% <<<
>>> Test on task 13 : loss=1.790 | TAw acc= 86.6%, forg= -0.8%| TAg acc= 52.8%, forg=  0.8% <<<
>>> Test on task 14 : loss=0.958 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 78.6%, forg=  3.9% <<<
>>> Test on task 15 : loss=1.488 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 62.0%, forg= -4.0% <<<
>>> Test on task 16 : loss=1.536 | TAw acc= 91.1%, forg= -1.0%| TAg acc= 58.4%, forg= -2.0% <<<
>>> Test on task 17 : loss=1.146 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 71.5%, forg=  4.9% <<<
>>> Test on task 18 : loss=1.210 | TAw acc= 97.3%, forg= -0.9%| TAg acc= 64.6%, forg=  8.8% <<<
>>> Test on task 19 : loss=1.488 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 56.6%, forg= 15.0% <<<
>>> Test on task 20 : loss=1.531 | TAw acc= 92.3%, forg=  0.0%| TAg acc= 65.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 21
************************************************************************************************************
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=5.532, TAw acc= 47.7% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=3.326, TAw acc= 64.0% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=2.319, TAw acc= 75.6% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.897, TAw acc= 84.9% | *
| Epoch   5, time=  1.9s | Train: skip eval | Valid: time=  0.2s loss=1.579, TAw acc= 93.0% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.575, TAw acc= 93.0% | *
| Epoch   2, time=  2.0s | Train: skip eval | Valid: time=  0.2s loss=1.572, TAw acc= 93.0% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.568, TAw acc= 93.0% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.566, TAw acc= 93.0% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.564, TAw acc= 93.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3425 train exemplars, time=  0.0s
3425
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.964 | TAw acc= 90.2%, forg=  0.5%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task  1 : loss=1.040 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.2%, forg= 10.6% <<<
>>> Test on task  2 : loss=1.276 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 65.0%, forg=  6.7% <<<
>>> Test on task  3 : loss=1.251 | TAw acc= 91.2%, forg=  0.9%| TAg acc= 69.0%, forg=  4.4% <<<
>>> Test on task  4 : loss=1.331 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 69.2%, forg=  6.5% <<<
>>> Test on task  5 : loss=1.191 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.055 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 79.6%, forg=  1.0% <<<
>>> Test on task  7 : loss=1.130 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 64.2%, forg=  4.6% <<<
>>> Test on task  8 : loss=1.275 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 74.3%, forg=  0.9% <<<
>>> Test on task  9 : loss=1.018 | TAw acc= 98.0%, forg= -1.0%| TAg acc= 76.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.117 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.5%, forg= -2.0% <<<
>>> Test on task 11 : loss=1.271 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg= -1.0% <<<
>>> Test on task 12 : loss=1.381 | TAw acc= 92.5%, forg=  0.0%| TAg acc= 74.5%, forg=  0.9% <<<
>>> Test on task 13 : loss=1.706 | TAw acc= 87.4%, forg= -0.8%| TAg acc= 59.8%, forg= -6.3% <<<
>>> Test on task 14 : loss=0.917 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 81.6%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.528 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 57.0%, forg=  5.0% <<<
>>> Test on task 16 : loss=1.539 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 55.4%, forg=  3.0% <<<
>>> Test on task 17 : loss=1.075 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 71.5%, forg=  4.9% <<<
>>> Test on task 18 : loss=1.162 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 68.1%, forg=  5.3% <<<
>>> Test on task 19 : loss=1.490 | TAw acc= 98.2%, forg= -0.9%| TAg acc= 57.5%, forg= 14.2% <<<
>>> Test on task 20 : loss=1.719 | TAw acc= 93.3%, forg= -1.0%| TAg acc= 59.6%, forg=  5.8% <<<
>>> Test on task 21 : loss=1.300 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 68.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 22
************************************************************************************************************
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=6.248, TAw acc= 25.4% | *
| Epoch   2, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=3.390, TAw acc= 62.0% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.069, TAw acc= 85.9% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.644, TAw acc= 98.6% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.416, TAw acc= 98.6% | *
| Epoch   1, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.415, TAw acc= 98.6% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.414, TAw acc= 98.6% | *
| Epoch   3, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.413, TAw acc= 98.6% | *
| Epoch   4, time=  2.1s | Train: skip eval | Valid: time=  0.2s loss=1.412, TAw acc= 98.6% | *
| Epoch   5, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.411, TAw acc= 98.6% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3525 train exemplars, time=  0.0s
3525
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.874 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 74.7%, forg=  7.7% <<<
>>> Test on task  1 : loss=1.065 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 68.3%, forg= 12.5% <<<
>>> Test on task  2 : loss=1.297 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 66.7%, forg=  5.0% <<<
>>> Test on task  3 : loss=1.282 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 70.8%, forg=  2.7% <<<
>>> Test on task  4 : loss=1.404 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 65.4%, forg= 10.3% <<<
>>> Test on task  5 : loss=1.181 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 81.0%, forg= -1.7% <<<
>>> Test on task  6 : loss=1.104 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.104 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 68.8%, forg=  0.0% <<<
>>> Test on task  8 : loss=1.231 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.1%, forg= -1.8% <<<
>>> Test on task  9 : loss=0.947 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.8%, forg=  1.0% <<<
>>> Test on task 10 : loss=1.098 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.5%, forg=  2.0% <<<
>>> Test on task 11 : loss=1.286 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.3%, forg=  1.9% <<<
>>> Test on task 12 : loss=1.382 | TAw acc= 90.6%, forg=  1.9%| TAg acc= 70.8%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.733 | TAw acc= 86.6%, forg=  0.8%| TAg acc= 61.4%, forg= -1.6% <<<
>>> Test on task 14 : loss=0.909 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 82.5%, forg=  0.0% <<<
>>> Test on task 15 : loss=1.437 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 65.0%, forg= -3.0% <<<
>>> Test on task 16 : loss=1.489 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 59.4%, forg= -1.0% <<<
>>> Test on task 17 : loss=0.963 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 78.0%, forg= -1.6% <<<
>>> Test on task 18 : loss=1.113 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.7%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.290 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.8%, forg=  0.9% <<<
>>> Test on task 20 : loss=1.574 | TAw acc= 92.3%, forg=  1.0%| TAg acc= 61.5%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.744 | TAw acc= 95.7%, forg= -0.9%| TAg acc= 40.0%, forg= 28.7% <<<
>>> Test on task 22 : loss=1.649 | TAw acc= 89.8%, forg=  0.0%| TAg acc= 72.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 23
************************************************************************************************************
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=4.383, TAw acc= 65.2% | *
| Epoch   2, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=2.161, TAw acc= 83.1% | *
| Epoch   3, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.340, TAw acc= 95.5% | *
| Epoch   4, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.107, TAw acc=100.0% | *
| Epoch   5, time=  2.2s | Train: skip eval | Valid: time=  0.2s loss=1.022, TAw acc= 93.3% | *
| Epoch   1, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.018, TAw acc= 93.3% | *
| Epoch   2, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.014, TAw acc= 93.3% | *
| Epoch   3, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.011, TAw acc= 93.3% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.008, TAw acc= 93.3% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.006, TAw acc= 93.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3625 train exemplars, time=  0.0s
3625
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.926 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 74.7%, forg=  7.7% <<<
>>> Test on task  1 : loss=0.988 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  6.7% <<<
>>> Test on task  2 : loss=1.239 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.2%, forg=  7.5% <<<
>>> Test on task  3 : loss=1.256 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 68.1%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.487 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 57.9%, forg= 17.8% <<<
>>> Test on task  5 : loss=1.178 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task  6 : loss=1.100 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 79.6%, forg=  1.0% <<<
>>> Test on task  7 : loss=1.176 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 67.9%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.293 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 73.4%, forg=  3.7% <<<
>>> Test on task  9 : loss=0.987 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.132 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.327 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 64.4%, forg=  4.8% <<<
>>> Test on task 12 : loss=1.409 | TAw acc= 93.4%, forg= -0.9%| TAg acc= 70.8%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.675 | TAw acc= 85.0%, forg=  2.4%| TAg acc= 58.3%, forg=  3.1% <<<
>>> Test on task 14 : loss=0.912 | TAw acc= 97.1%, forg=  0.0%| TAg acc= 79.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.379 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 69.0%, forg= -4.0% <<<
>>> Test on task 16 : loss=1.507 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 59.4%, forg=  0.0% <<<
>>> Test on task 17 : loss=1.070 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 70.7%, forg=  7.3% <<<
>>> Test on task 18 : loss=1.039 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 75.2%, forg= -1.8% <<<
>>> Test on task 19 : loss=1.262 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 66.4%, forg=  5.3% <<<
>>> Test on task 20 : loss=1.547 | TAw acc= 92.3%, forg=  1.0%| TAg acc= 59.6%, forg=  5.8% <<<
>>> Test on task 21 : loss=1.567 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 48.7%, forg= 20.0% <<<
>>> Test on task 22 : loss=2.117 | TAw acc= 91.8%, forg= -2.0%| TAg acc= 36.7%, forg= 35.7% <<<
>>> Test on task 23 : loss=1.046 | TAw acc= 95.0%, forg=  0.0%| TAg acc= 74.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 24
************************************************************************************************************
| Epoch   1, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=5.001, TAw acc= 53.0% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.374, TAw acc= 78.3% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.543, TAw acc= 92.8% | *
| Epoch   4, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.222, TAw acc= 96.4% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 96.4% |
| Epoch   1, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.222, TAw acc= 96.4% | *
| Epoch   2, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.221, TAw acc= 96.4% | *
| Epoch   3, time=  2.3s | Train: skip eval | Valid: time=  0.2s loss=1.220, TAw acc= 96.4% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.220, TAw acc= 96.4% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.219, TAw acc= 96.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3725 train exemplars, time=  0.0s
3725
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.947 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task  1 : loss=0.986 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.1%, forg=  7.7% <<<
>>> Test on task  2 : loss=1.263 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.309 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 68.1%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.458 | TAw acc= 94.4%, forg=  2.8%| TAg acc= 67.3%, forg=  8.4% <<<
>>> Test on task  5 : loss=1.197 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.084 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.104 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 70.6%, forg= -1.8% <<<
>>> Test on task  8 : loss=1.239 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.1%, forg=  0.0% <<<
>>> Test on task  9 : loss=0.934 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 79.8%, forg=  0.0% <<<
>>> Test on task 10 : loss=1.111 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.4%, forg= -1.0% <<<
>>> Test on task 11 : loss=1.305 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 68.3%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.332 | TAw acc= 93.4%, forg=  0.0%| TAg acc= 72.6%, forg=  2.8% <<<
>>> Test on task 13 : loss=1.660 | TAw acc= 86.6%, forg=  0.8%| TAg acc= 58.3%, forg=  3.1% <<<
>>> Test on task 14 : loss=0.865 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 84.5%, forg= -1.9% <<<
>>> Test on task 15 : loss=1.365 | TAw acc= 93.0%, forg=  0.0%| TAg acc= 73.0%, forg= -4.0% <<<
>>> Test on task 16 : loss=1.503 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 60.4%, forg= -1.0% <<<
>>> Test on task 17 : loss=0.994 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 74.0%, forg=  4.1% <<<
>>> Test on task 18 : loss=1.144 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 62.8%, forg= 12.4% <<<
>>> Test on task 19 : loss=1.228 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 72.6%, forg= -0.9% <<<
>>> Test on task 20 : loss=1.441 | TAw acc= 91.3%, forg=  1.9%| TAg acc= 63.5%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.694 | TAw acc= 96.5%, forg= -0.9%| TAg acc= 44.3%, forg= 24.3% <<<
>>> Test on task 22 : loss=1.995 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 52.0%, forg= 20.4% <<<
>>> Test on task 23 : loss=1.693 | TAw acc=100.0%, forg= -5.0%| TAg acc= 40.3%, forg= 34.5% <<<
>>> Test on task 24 : loss=1.444 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 67.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 25
************************************************************************************************************
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=6.640, TAw acc= 39.2% | *
| Epoch   2, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=3.775, TAw acc= 62.0% | *
| Epoch   3, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=2.600, TAw acc= 75.9% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=2.129, TAw acc= 84.8% | *
| Epoch   5, time=  2.4s | Train: skip eval | Valid: time=  0.2s loss=1.750, TAw acc= 89.9% | *
| Epoch   1, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.748, TAw acc= 89.9% | *
| Epoch   2, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.745, TAw acc= 89.9% | *
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.742, TAw acc= 89.9% | *
| Epoch   4, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.740, TAw acc= 89.9% | *
| Epoch   5, time=  2.5s | Train: skip eval | Valid: time=  0.2s loss=1.737, TAw acc= 89.9% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3825 train exemplars, time=  0.0s
3825
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.016 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 67.5%, forg= 14.9% <<<
>>> Test on task  1 : loss=1.031 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.0%, forg=  4.8% <<<
>>> Test on task  2 : loss=1.260 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.303 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 68.1%, forg=  5.3% <<<
>>> Test on task  4 : loss=1.430 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 65.4%, forg= 10.3% <<<
>>> Test on task  5 : loss=1.212 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task  6 : loss=1.081 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 80.6%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.125 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 66.1%, forg=  4.6% <<<
>>> Test on task  8 : loss=1.294 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.931 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 76.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.075 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.5%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.304 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 64.4%, forg=  4.8% <<<
>>> Test on task 12 : loss=1.373 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 69.8%, forg=  5.7% <<<
>>> Test on task 13 : loss=1.658 | TAw acc= 87.4%, forg=  0.0%| TAg acc= 57.5%, forg=  3.9% <<<
>>> Test on task 14 : loss=0.845 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 83.5%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.400 | TAw acc= 94.0%, forg= -1.0%| TAg acc= 69.0%, forg=  4.0% <<<
>>> Test on task 16 : loss=1.465 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 58.4%, forg=  2.0% <<<
>>> Test on task 17 : loss=0.924 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 79.7%, forg= -1.6% <<<
>>> Test on task 18 : loss=1.095 | TAw acc= 96.5%, forg=  0.9%| TAg acc= 75.2%, forg=  0.0% <<<
>>> Test on task 19 : loss=1.179 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 74.3%, forg= -1.8% <<<
>>> Test on task 20 : loss=1.427 | TAw acc= 91.3%, forg=  1.9%| TAg acc= 65.4%, forg=  0.0% <<<
>>> Test on task 21 : loss=1.596 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 47.0%, forg= 21.7% <<<
>>> Test on task 22 : loss=1.877 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 55.1%, forg= 17.3% <<<
>>> Test on task 23 : loss=1.587 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 43.7%, forg= 31.1% <<<
>>> Test on task 24 : loss=2.054 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 24.1%, forg= 43.8% <<<
>>> Test on task 25 : loss=1.502 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 66.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 26
************************************************************************************************************
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=6.072, TAw acc= 46.2% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=3.136, TAw acc= 67.5% | *
| Epoch   3, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.995, TAw acc= 87.5% | *
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.711, TAw acc= 96.2% | *
| Epoch   5, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.438, TAw acc= 96.2% | *
| Epoch   1, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.439, TAw acc= 96.2% | *
| Epoch   2, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.439, TAw acc= 96.2% |
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.440, TAw acc= 96.2% |
| Epoch   4, time=  2.6s | Train: skip eval | Valid: time=  0.2s loss=1.440, TAw acc= 96.2% |
| Epoch   5, time=  2.7s | Train: skip eval | Valid: time=  0.2s loss=1.440, TAw acc= 96.2% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 3925 train exemplars, time=  0.0s
3925
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.066 | TAw acc= 90.2%, forg=  0.5%| TAg acc= 68.0%, forg= 14.4% <<<
>>> Test on task  1 : loss=1.057 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.0%, forg=  6.7% <<<
>>> Test on task  2 : loss=1.346 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 58.3%, forg= 13.3% <<<
>>> Test on task  3 : loss=1.317 | TAw acc= 92.9%, forg= -0.9%| TAg acc= 64.6%, forg=  8.8% <<<
>>> Test on task  4 : loss=1.422 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 67.3%, forg=  8.4% <<<
>>> Test on task  5 : loss=1.219 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.092 | TAw acc= 92.9%, forg=  0.0%| TAg acc= 80.6%, forg=  0.0% <<<
>>> Test on task  7 : loss=1.116 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.8%, forg=  1.8% <<<
>>> Test on task  8 : loss=1.250 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.953 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 80.8%, forg= -1.0% <<<
>>> Test on task 10 : loss=1.101 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 77.5%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.204 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 71.2%, forg= -1.9% <<<
>>> Test on task 12 : loss=1.343 | TAw acc= 90.6%, forg=  2.8%| TAg acc= 68.9%, forg=  6.6% <<<
>>> Test on task 13 : loss=1.670 | TAw acc= 89.0%, forg= -1.6%| TAg acc= 58.3%, forg=  3.1% <<<
>>> Test on task 14 : loss=0.831 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 82.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.459 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 62.0%, forg= 11.0% <<<
>>> Test on task 16 : loss=1.540 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 60.4%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.913 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 81.3%, forg= -1.6% <<<
>>> Test on task 18 : loss=1.071 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.0%, forg= -1.8% <<<
>>> Test on task 19 : loss=1.173 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 77.0%, forg= -2.7% <<<
>>> Test on task 20 : loss=1.350 | TAw acc= 94.2%, forg= -1.0%| TAg acc= 67.3%, forg= -1.9% <<<
>>> Test on task 21 : loss=1.579 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 47.8%, forg= 20.9% <<<
>>> Test on task 22 : loss=1.892 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 53.1%, forg= 19.4% <<<
>>> Test on task 23 : loss=1.470 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 46.2%, forg= 28.6% <<<
>>> Test on task 24 : loss=1.949 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 30.4%, forg= 37.5% <<<
>>> Test on task 25 : loss=2.351 | TAw acc= 93.5%, forg=  2.8%| TAg acc= 21.3%, forg= 45.4% <<<
>>> Test on task 26 : loss=1.260 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 66.1%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 27
************************************************************************************************************
| Epoch   1, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=5.762, TAw acc= 50.0% | *
| Epoch   2, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=2.988, TAw acc= 79.3% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.839, TAw acc= 92.7% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 93.9% | *
| Epoch   5, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.274, TAw acc= 98.8% | *
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.273, TAw acc= 98.8% | *
| Epoch   2, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.272, TAw acc= 98.8% | *
| Epoch   3, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.271, TAw acc= 98.8% | *
| Epoch   4, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.270, TAw acc= 98.8% | *
| Epoch   5, time=  2.8s | Train: skip eval | Valid: time=  0.2s loss=1.269, TAw acc= 98.8% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4025 train exemplars, time=  0.0s
4025
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.009 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 70.6%, forg= 11.9% <<<
>>> Test on task  1 : loss=1.034 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 75.0%, forg=  5.8% <<<
>>> Test on task  2 : loss=1.254 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.5%, forg=  9.2% <<<
>>> Test on task  3 : loss=1.324 | TAw acc= 90.3%, forg=  2.7%| TAg acc= 67.3%, forg=  6.2% <<<
>>> Test on task  4 : loss=1.392 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 70.1%, forg=  5.6% <<<
>>> Test on task  5 : loss=1.229 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task  6 : loss=1.112 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.143 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.9%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.262 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.1%, forg=  0.0% <<<
>>> Test on task  9 : loss=0.887 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 81.8%, forg= -1.0% <<<
>>> Test on task 10 : loss=1.080 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 78.4%, forg=  0.0% <<<
>>> Test on task 11 : loss=1.294 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.3%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.293 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 74.5%, forg=  0.9% <<<
>>> Test on task 13 : loss=1.675 | TAw acc= 85.0%, forg=  3.9%| TAg acc= 57.5%, forg=  3.9% <<<
>>> Test on task 14 : loss=0.895 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 82.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.382 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 71.0%, forg=  2.0% <<<
>>> Test on task 16 : loss=1.531 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 57.4%, forg=  3.0% <<<
>>> Test on task 17 : loss=0.894 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 78.9%, forg=  2.4% <<<
>>> Test on task 18 : loss=1.033 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.162 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task 20 : loss=1.388 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 65.4%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.472 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 56.5%, forg= 12.2% <<<
>>> Test on task 22 : loss=1.768 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 59.2%, forg= 13.3% <<<
>>> Test on task 23 : loss=1.393 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 56.3%, forg= 18.5% <<<
>>> Test on task 24 : loss=1.861 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 38.4%, forg= 29.5% <<<
>>> Test on task 25 : loss=2.116 | TAw acc= 93.5%, forg=  2.8%| TAg acc= 33.3%, forg= 33.3% <<<
>>> Test on task 26 : loss=1.940 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 26.6%, forg= 39.4% <<<
>>> Test on task 27 : loss=1.496 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 65.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 28
************************************************************************************************************
| Epoch   1, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=5.605, TAw acc= 42.3% | *
| Epoch   2, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=3.162, TAw acc= 69.2% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.100, TAw acc= 92.3% | *
| Epoch   4, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.607, TAw acc= 88.5% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.402, TAw acc= 92.3% | *
| Epoch   1, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.400, TAw acc= 92.3% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.398, TAw acc= 92.3% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.397, TAw acc= 92.3% | *
| Epoch   4, time=  2.9s | Train: skip eval | Valid: time=  0.2s loss=1.395, TAw acc= 92.3% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.393, TAw acc= 92.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4125 train exemplars, time=  0.1s
4125
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=0.979 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 70.6%, forg= 11.9% <<<
>>> Test on task  1 : loss=1.056 | TAw acc=100.0%, forg= -1.0%| TAg acc= 68.3%, forg= 12.5% <<<
>>> Test on task  2 : loss=1.247 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 65.0%, forg=  6.7% <<<
>>> Test on task  3 : loss=1.265 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 67.3%, forg=  6.2% <<<
>>> Test on task  4 : loss=1.469 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 67.3%, forg=  8.4% <<<
>>> Test on task  5 : loss=1.238 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task  6 : loss=1.106 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 79.6%, forg=  1.0% <<<
>>> Test on task  7 : loss=1.152 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.9%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.317 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  0.9% <<<
>>> Test on task  9 : loss=0.941 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.210 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 70.6%, forg=  7.8% <<<
>>> Test on task 11 : loss=1.235 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 68.3%, forg=  2.9% <<<
>>> Test on task 12 : loss=1.296 | TAw acc= 90.6%, forg=  2.8%| TAg acc= 71.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.622 | TAw acc= 87.4%, forg=  1.6%| TAg acc= 60.6%, forg=  0.8% <<<
>>> Test on task 14 : loss=0.940 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.438 | TAw acc= 93.0%, forg=  1.0%| TAg acc= 67.0%, forg=  6.0% <<<
>>> Test on task 16 : loss=1.444 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 59.4%, forg=  1.0% <<<
>>> Test on task 17 : loss=0.896 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 78.9%, forg=  2.4% <<<
>>> Test on task 18 : loss=1.086 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 71.7%, forg=  5.3% <<<
>>> Test on task 19 : loss=1.170 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 20 : loss=1.365 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 68.3%, forg= -1.0% <<<
>>> Test on task 21 : loss=1.396 | TAw acc= 95.7%, forg=  0.9%| TAg acc= 59.1%, forg=  9.6% <<<
>>> Test on task 22 : loss=1.734 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 67.3%, forg=  5.1% <<<
>>> Test on task 23 : loss=1.334 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 56.3%, forg= 18.5% <<<
>>> Test on task 24 : loss=1.774 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 44.6%, forg= 23.2% <<<
>>> Test on task 25 : loss=2.035 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 38.0%, forg= 28.7% <<<
>>> Test on task 26 : loss=1.876 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 32.1%, forg= 33.9% <<<
>>> Test on task 27 : loss=2.069 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 40.5%, forg= 25.2% <<<
>>> Test on task 28 : loss=1.248 | TAw acc= 93.5%, forg=  0.0%| TAg acc= 72.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 29
************************************************************************************************************
| Epoch   1, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=5.457, TAw acc= 51.2% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.958, TAw acc= 81.4% | *
| Epoch   3, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.003, TAw acc= 89.5% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.3s loss=1.608, TAw acc= 93.0% | *
| Epoch   5, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.432, TAw acc= 94.2% | *
| Epoch   1, time=  3.3s | Train: skip eval | Valid: time=  0.2s loss=1.429, TAw acc= 94.2% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.426, TAw acc= 94.2% | *
| Epoch   3, time=  3.0s | Train: skip eval | Valid: time=  0.2s loss=1.423, TAw acc= 94.2% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.420, TAw acc= 94.2% | *
| Epoch   5, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.417, TAw acc= 94.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4225 train exemplars, time=  0.0s
4225
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.039 | TAw acc= 88.7%, forg=  2.1%| TAg acc= 71.6%, forg= 10.8% <<<
>>> Test on task  1 : loss=1.053 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.3%, forg= 12.5% <<<
>>> Test on task  2 : loss=1.285 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 62.5%, forg=  9.2% <<<
>>> Test on task  3 : loss=1.331 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 65.5%, forg=  8.0% <<<
>>> Test on task  4 : loss=1.429 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.3%, forg=  8.4% <<<
>>> Test on task  5 : loss=1.190 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 79.3%, forg=  1.7% <<<
>>> Test on task  6 : loss=1.159 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 73.5%, forg=  7.1% <<<
>>> Test on task  7 : loss=1.143 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 67.9%, forg=  2.8% <<<
>>> Test on task  8 : loss=1.312 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.899 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 83.8%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.108 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.5%, forg=  2.0% <<<
>>> Test on task 11 : loss=1.261 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 66.3%, forg=  4.8% <<<
>>> Test on task 12 : loss=1.312 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 73.6%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.650 | TAw acc= 86.6%, forg=  2.4%| TAg acc= 57.5%, forg=  3.9% <<<
>>> Test on task 14 : loss=0.878 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.438 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 72.0%, forg=  1.0% <<<
>>> Test on task 16 : loss=1.450 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 63.4%, forg= -3.0% <<<
>>> Test on task 17 : loss=0.836 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 79.7%, forg=  1.6% <<<
>>> Test on task 18 : loss=1.130 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.272 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 70.8%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.462 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 62.5%, forg=  5.8% <<<
>>> Test on task 21 : loss=1.445 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 58.3%, forg= 10.4% <<<
>>> Test on task 22 : loss=1.715 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 63.3%, forg=  9.2% <<<
>>> Test on task 23 : loss=1.291 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 56.3%, forg= 18.5% <<<
>>> Test on task 24 : loss=1.716 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 52.7%, forg= 15.2% <<<
>>> Test on task 25 : loss=2.050 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 38.0%, forg= 28.7% <<<
>>> Test on task 26 : loss=1.879 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 41.3%, forg= 24.8% <<<
>>> Test on task 27 : loss=1.913 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 47.7%, forg= 18.0% <<<
>>> Test on task 28 : loss=1.659 | TAw acc= 95.3%, forg= -1.9%| TAg acc= 49.5%, forg= 23.4% <<<
>>> Test on task 29 : loss=1.404 | TAw acc= 94.8%, forg=  0.0%| TAg acc= 67.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 30
************************************************************************************************************
| Epoch   1, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=4.852, TAw acc= 59.8% | *
| Epoch   2, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=2.449, TAw acc= 92.0% | *
| Epoch   3, time=  3.2s | Train: skip eval | Valid: time=  0.2s loss=1.621, TAw acc= 95.4% | *
| Epoch   4, time=  3.1s | Train: skip eval | Valid: time=  0.2s loss=1.324, TAw acc= 97.7% | *
| Epoch   5, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.171, TAw acc= 97.7% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.168, TAw acc= 97.7% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.166, TAw acc= 97.7% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.164, TAw acc= 97.7% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.3s loss=1.162, TAw acc= 97.7% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.160, TAw acc= 97.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4325 train exemplars, time=  0.0s
4325
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.024 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 71.1%, forg= 11.3% <<<
>>> Test on task  1 : loss=1.070 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.2%, forg= 10.6% <<<
>>> Test on task  2 : loss=1.245 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.474 | TAw acc= 90.3%, forg=  2.7%| TAg acc= 63.7%, forg=  9.7% <<<
>>> Test on task  4 : loss=1.449 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.2%, forg=  7.5% <<<
>>> Test on task  5 : loss=1.186 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 79.3%, forg=  1.7% <<<
>>> Test on task  6 : loss=1.155 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 75.5%, forg=  5.1% <<<
>>> Test on task  7 : loss=1.125 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.0%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.312 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.1%, forg=  0.0% <<<
>>> Test on task  9 : loss=0.932 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 78.8%, forg=  5.1% <<<
>>> Test on task 10 : loss=1.088 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 74.5%, forg=  3.9% <<<
>>> Test on task 11 : loss=1.271 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 68.3%, forg=  2.9% <<<
>>> Test on task 12 : loss=1.292 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 70.8%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.585 | TAw acc= 85.8%, forg=  3.1%| TAg acc= 60.6%, forg=  0.8% <<<
>>> Test on task 14 : loss=0.870 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 82.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.352 | TAw acc= 93.0%, forg=  1.0%| TAg acc= 70.0%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.473 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 62.4%, forg=  1.0% <<<
>>> Test on task 17 : loss=0.889 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 74.8%, forg=  6.5% <<<
>>> Test on task 18 : loss=1.045 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.160 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 20 : loss=1.429 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 67.3%, forg=  1.0% <<<
>>> Test on task 21 : loss=1.443 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 60.0%, forg=  8.7% <<<
>>> Test on task 22 : loss=1.671 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 67.3%, forg=  5.1% <<<
>>> Test on task 23 : loss=1.529 | TAw acc=100.0%, forg=  0.0%| TAg acc= 48.7%, forg= 26.1% <<<
>>> Test on task 24 : loss=1.708 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 52.7%, forg= 15.2% <<<
>>> Test on task 25 : loss=2.014 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 38.0%, forg= 28.7% <<<
>>> Test on task 26 : loss=1.719 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 45.9%, forg= 20.2% <<<
>>> Test on task 27 : loss=1.915 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 50.5%, forg= 15.3% <<<
>>> Test on task 28 : loss=1.685 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 48.6%, forg= 24.3% <<<
>>> Test on task 29 : loss=1.898 | TAw acc= 93.9%, forg=  0.9%| TAg acc= 49.6%, forg= 18.3% <<<
>>> Test on task 30 : loss=1.213 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 70.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 31
************************************************************************************************************
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=4.321, TAw acc= 62.8% | *
| Epoch   2, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.859, TAw acc= 77.7% | *
| Epoch   3, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.307, TAw acc= 91.5% | *
| Epoch   4, time=  3.4s | Train: skip eval | Valid: time=  0.2s loss=1.199, TAw acc= 95.7% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.935, TAw acc= 94.7% | *
| Epoch   1, time=  3.8s | Train: skip eval | Valid: time=  0.2s loss=0.937, TAw acc= 94.7% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.3s loss=0.939, TAw acc= 94.7% |
| Epoch   3, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.940, TAw acc= 94.7% |
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=0.942, TAw acc= 94.7% |
| Epoch   5, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=0.943, TAw acc= 94.7% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4425 train exemplars, time=  0.0s
4425
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.073 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 66.5%, forg= 16.0% <<<
>>> Test on task  1 : loss=1.083 | TAw acc=100.0%, forg=  0.0%| TAg acc= 71.2%, forg=  9.6% <<<
>>> Test on task  2 : loss=1.308 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.8%, forg= 10.8% <<<
>>> Test on task  3 : loss=1.374 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 61.1%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.439 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.2%, forg=  7.5% <<<
>>> Test on task  5 : loss=1.239 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 79.3%, forg=  1.7% <<<
>>> Test on task  6 : loss=1.321 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 70.4%, forg= 10.2% <<<
>>> Test on task  7 : loss=1.140 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 67.0%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.320 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  0.9% <<<
>>> Test on task  9 : loss=0.899 | TAw acc= 94.9%, forg=  3.0%| TAg acc= 81.8%, forg=  2.0% <<<
>>> Test on task 10 : loss=1.120 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 73.5%, forg=  4.9% <<<
>>> Test on task 11 : loss=1.311 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 12 : loss=1.278 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 74.5%, forg=  0.9% <<<
>>> Test on task 13 : loss=1.750 | TAw acc= 86.6%, forg=  2.4%| TAg acc= 52.0%, forg=  9.4% <<<
>>> Test on task 14 : loss=0.939 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 79.6%, forg=  4.9% <<<
>>> Test on task 15 : loss=1.402 | TAw acc= 90.0%, forg=  4.0%| TAg acc= 67.0%, forg=  6.0% <<<
>>> Test on task 16 : loss=1.476 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 59.4%, forg=  4.0% <<<
>>> Test on task 17 : loss=0.896 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 72.4%, forg=  8.9% <<<
>>> Test on task 18 : loss=1.139 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 70.8%, forg=  6.2% <<<
>>> Test on task 19 : loss=1.147 | TAw acc= 96.5%, forg=  1.8%| TAg acc= 70.8%, forg=  6.2% <<<
>>> Test on task 20 : loss=1.426 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 63.5%, forg=  4.8% <<<
>>> Test on task 21 : loss=1.391 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 59.1%, forg=  9.6% <<<
>>> Test on task 22 : loss=1.712 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 64.3%, forg=  8.2% <<<
>>> Test on task 23 : loss=1.352 | TAw acc=100.0%, forg=  0.0%| TAg acc= 57.1%, forg= 17.6% <<<
>>> Test on task 24 : loss=1.705 | TAw acc= 95.5%, forg= -0.9%| TAg acc= 54.5%, forg= 13.4% <<<
>>> Test on task 25 : loss=2.045 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 38.9%, forg= 27.8% <<<
>>> Test on task 26 : loss=1.755 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 43.1%, forg= 22.9% <<<
>>> Test on task 27 : loss=1.937 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 46.8%, forg= 18.9% <<<
>>> Test on task 28 : loss=1.593 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 53.3%, forg= 19.6% <<<
>>> Test on task 29 : loss=1.827 | TAw acc= 95.7%, forg= -0.9%| TAg acc= 53.9%, forg= 13.9% <<<
>>> Test on task 30 : loss=1.677 | TAw acc= 97.4%, forg= -1.7%| TAg acc= 52.6%, forg= 18.1% <<<
>>> Test on task 31 : loss=1.062 | TAw acc= 92.8%, forg=  0.0%| TAg acc= 75.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 32
************************************************************************************************************
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=5.075, TAw acc= 62.1% | *
| Epoch   2, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=2.703, TAw acc= 80.5% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.759, TAw acc= 89.7% | *
| Epoch   4, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.443, TAw acc= 92.0% | *
| Epoch   5, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.245, TAw acc= 95.4% | *
| Epoch   1, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.243, TAw acc= 95.4% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.241, TAw acc= 95.4% | *
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.240, TAw acc= 95.4% | *
| Epoch   4, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.238, TAw acc= 95.4% | *
| Epoch   5, time=  3.5s | Train: skip eval | Valid: time=  0.2s loss=1.237, TAw acc= 95.4% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4525 train exemplars, time=  0.0s
4525
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.052 | TAw acc= 88.7%, forg=  2.1%| TAg acc= 70.1%, forg= 12.4% <<<
>>> Test on task  1 : loss=1.097 | TAw acc=100.0%, forg=  0.0%| TAg acc= 65.4%, forg= 15.4% <<<
>>> Test on task  2 : loss=1.286 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.398 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 62.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.483 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.2%, forg=  7.5% <<<
>>> Test on task  5 : loss=1.216 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 79.3%, forg=  1.7% <<<
>>> Test on task  6 : loss=1.181 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.202 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 69.7%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.370 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.1%, forg=  0.0% <<<
>>> Test on task  9 : loss=0.903 | TAw acc= 94.9%, forg=  3.0%| TAg acc= 79.8%, forg=  4.0% <<<
>>> Test on task 10 : loss=1.091 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 76.5%, forg=  2.0% <<<
>>> Test on task 11 : loss=1.222 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.2%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.375 | TAw acc= 90.6%, forg=  2.8%| TAg acc= 71.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.663 | TAw acc= 85.8%, forg=  3.1%| TAg acc= 57.5%, forg=  3.9% <<<
>>> Test on task 14 : loss=0.920 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 80.6%, forg=  3.9% <<<
>>> Test on task 15 : loss=1.422 | TAw acc= 91.0%, forg=  3.0%| TAg acc= 70.0%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.449 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 61.4%, forg=  2.0% <<<
>>> Test on task 17 : loss=0.899 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 81.3%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.102 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 73.5%, forg=  3.5% <<<
>>> Test on task 19 : loss=1.149 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 76.1%, forg=  0.9% <<<
>>> Test on task 20 : loss=1.401 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 66.3%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.474 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 56.5%, forg= 12.2% <<<
>>> Test on task 22 : loss=1.700 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 66.3%, forg=  6.1% <<<
>>> Test on task 23 : loss=1.375 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 53.8%, forg= 21.0% <<<
>>> Test on task 24 : loss=1.653 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 59.8%, forg=  8.0% <<<
>>> Test on task 25 : loss=2.047 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 41.7%, forg= 25.0% <<<
>>> Test on task 26 : loss=1.670 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 51.4%, forg= 14.7% <<<
>>> Test on task 27 : loss=1.804 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 55.0%, forg= 10.8% <<<
>>> Test on task 28 : loss=1.596 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 55.1%, forg= 17.8% <<<
>>> Test on task 29 : loss=1.697 | TAw acc= 93.9%, forg=  1.7%| TAg acc= 58.3%, forg=  9.6% <<<
>>> Test on task 30 : loss=1.698 | TAw acc= 98.3%, forg= -0.9%| TAg acc= 50.9%, forg= 19.8% <<<
>>> Test on task 31 : loss=1.676 | TAw acc= 92.0%, forg=  0.8%| TAg acc= 38.4%, forg= 36.8% <<<
>>> Test on task 32 : loss=1.199 | TAw acc= 93.2%, forg=  0.0%| TAg acc= 76.9%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 33
************************************************************************************************************
| Epoch   1, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=6.572, TAw acc= 30.8% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=4.157, TAw acc= 70.5% | *
| Epoch   3, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=2.542, TAw acc= 87.2% | *
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.890, TAw acc= 88.5% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.531, TAw acc= 96.2% | *
| Epoch   1, time=  3.6s | Train: skip eval | Valid: time=  0.2s loss=1.532, TAw acc= 96.2% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.533, TAw acc= 96.2% |
| Epoch   3, time=  3.7s | Train: skip eval | Valid: time=  0.2s loss=1.534, TAw acc= 96.2% |
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.535, TAw acc= 96.2% |
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.535, TAw acc= 96.2% |
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4625 train exemplars, time=  0.0s
4625
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.093 | TAw acc= 90.2%, forg=  0.5%| TAg acc= 67.0%, forg= 15.5% <<<
>>> Test on task  1 : loss=1.133 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.3%, forg= 12.5% <<<
>>> Test on task  2 : loss=1.299 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.0%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.382 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 62.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.444 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 68.2%, forg=  7.5% <<<
>>> Test on task  5 : loss=1.191 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.197 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 78.6%, forg=  2.0% <<<
>>> Test on task  7 : loss=1.178 | TAw acc= 95.4%, forg=  1.8%| TAg acc= 65.1%, forg=  5.5% <<<
>>> Test on task  8 : loss=1.367 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.0%, forg= -0.9% <<<
>>> Test on task  9 : loss=0.841 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 85.9%, forg= -2.0% <<<
>>> Test on task 10 : loss=1.095 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 75.5%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.196 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 68.3%, forg=  2.9% <<<
>>> Test on task 12 : loss=1.412 | TAw acc= 93.4%, forg=  0.0%| TAg acc= 69.8%, forg=  5.7% <<<
>>> Test on task 13 : loss=1.678 | TAw acc= 85.8%, forg=  3.1%| TAg acc= 55.9%, forg=  5.5% <<<
>>> Test on task 14 : loss=0.935 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.427 | TAw acc= 90.0%, forg=  4.0%| TAg acc= 66.0%, forg=  7.0% <<<
>>> Test on task 16 : loss=1.505 | TAw acc= 89.1%, forg=  2.0%| TAg acc= 63.4%, forg=  0.0% <<<
>>> Test on task 17 : loss=0.890 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 78.0%, forg=  3.3% <<<
>>> Test on task 18 : loss=1.051 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.190 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 73.5%, forg=  3.5% <<<
>>> Test on task 20 : loss=1.431 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 66.3%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.476 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 56.5%, forg= 12.2% <<<
>>> Test on task 22 : loss=1.700 | TAw acc= 91.8%, forg=  0.0%| TAg acc= 67.3%, forg=  5.1% <<<
>>> Test on task 23 : loss=1.295 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 60.5%, forg= 14.3% <<<
>>> Test on task 24 : loss=1.602 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 62.5%, forg=  5.4% <<<
>>> Test on task 25 : loss=1.953 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 43.5%, forg= 23.1% <<<
>>> Test on task 26 : loss=1.623 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 50.5%, forg= 15.6% <<<
>>> Test on task 27 : loss=1.861 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 55.9%, forg=  9.9% <<<
>>> Test on task 28 : loss=1.491 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 60.7%, forg= 12.1% <<<
>>> Test on task 29 : loss=1.671 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 57.4%, forg= 10.4% <<<
>>> Test on task 30 : loss=1.697 | TAw acc= 97.4%, forg=  0.9%| TAg acc= 52.6%, forg= 18.1% <<<
>>> Test on task 31 : loss=1.545 | TAw acc= 93.6%, forg= -0.8%| TAg acc= 54.4%, forg= 20.8% <<<
>>> Test on task 32 : loss=1.722 | TAw acc= 94.9%, forg= -1.7%| TAg acc= 58.1%, forg= 18.8% <<<
>>> Test on task 33 : loss=1.246 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 34
************************************************************************************************************
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=5.530, TAw acc= 48.2% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=2.693, TAw acc= 69.9% | *
| Epoch   3, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.904, TAw acc= 84.3% | *
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.581, TAw acc= 89.2% | *
| Epoch   5, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.601, TAw acc= 84.3% |
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.580, TAw acc= 89.2% | *
| Epoch   2, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.578, TAw acc= 89.2% | *
| Epoch   3, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.578, TAw acc= 89.2% | *
| Epoch   4, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.577, TAw acc= 89.2% | *
| Epoch   5, time=  3.9s | Train: skip eval | Valid: time=  0.2s loss=1.576, TAw acc= 89.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4725 train exemplars, time=  0.0s
4725
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.119 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 69.1%, forg= 13.4% <<<
>>> Test on task  1 : loss=1.219 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.3%, forg= 13.5% <<<
>>> Test on task  2 : loss=1.352 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.0%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.494 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 60.2%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.466 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 65.4%, forg= 10.3% <<<
>>> Test on task  5 : loss=1.226 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.236 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 76.5%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.177 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 69.7%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.375 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.888 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.145 | TAw acc= 98.0%, forg=  1.0%| TAg acc= 77.5%, forg=  1.0% <<<
>>> Test on task 11 : loss=1.245 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 68.3%, forg=  2.9% <<<
>>> Test on task 12 : loss=1.314 | TAw acc= 92.5%, forg=  0.9%| TAg acc= 73.6%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.683 | TAw acc= 85.8%, forg=  3.1%| TAg acc= 58.3%, forg=  3.1% <<<
>>> Test on task 14 : loss=0.962 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 80.6%, forg=  3.9% <<<
>>> Test on task 15 : loss=1.361 | TAw acc= 93.0%, forg=  1.0%| TAg acc= 73.0%, forg=  0.0% <<<
>>> Test on task 16 : loss=1.543 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 59.4%, forg=  4.0% <<<
>>> Test on task 17 : loss=0.856 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 77.2%, forg=  4.1% <<<
>>> Test on task 18 : loss=1.134 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.094 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 77.0%, forg=  0.0% <<<
>>> Test on task 20 : loss=1.363 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 65.4%, forg=  2.9% <<<
>>> Test on task 21 : loss=1.481 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 60.9%, forg=  7.8% <<<
>>> Test on task 22 : loss=1.738 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 70.4%, forg=  2.0% <<<
>>> Test on task 23 : loss=1.333 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 62.2%, forg= 12.6% <<<
>>> Test on task 24 : loss=1.572 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 60.7%, forg=  7.1% <<<
>>> Test on task 25 : loss=2.005 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 42.6%, forg= 24.1% <<<
>>> Test on task 26 : loss=1.733 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 46.8%, forg= 19.3% <<<
>>> Test on task 27 : loss=1.907 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 51.4%, forg= 14.4% <<<
>>> Test on task 28 : loss=1.503 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 61.7%, forg= 11.2% <<<
>>> Test on task 29 : loss=1.633 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 61.7%, forg=  6.1% <<<
>>> Test on task 30 : loss=1.624 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 51.7%, forg= 19.0% <<<
>>> Test on task 31 : loss=1.618 | TAw acc= 93.6%, forg=  0.0%| TAg acc= 45.6%, forg= 29.6% <<<
>>> Test on task 32 : loss=1.668 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 56.4%, forg= 20.5% <<<
>>> Test on task 33 : loss=1.651 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 40.2%, forg= 29.0% <<<
>>> Test on task 34 : loss=1.526 | TAw acc= 89.3%, forg=  0.0%| TAg acc= 54.5%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 35
************************************************************************************************************
| Epoch   1, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=5.859, TAw acc= 47.9% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=3.109, TAw acc= 78.1% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.170, TAw acc= 87.7% | *
| Epoch   4, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.821, TAw acc= 90.4% | *
| Epoch   5, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.679, TAw acc= 89.0% | *
| Epoch   1, time=  4.3s | Train: skip eval | Valid: time=  0.3s loss=1.670, TAw acc= 89.0% | *
| Epoch   2, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.663, TAw acc= 89.0% | *
| Epoch   3, time=  4.0s | Train: skip eval | Valid: time=  0.2s loss=1.656, TAw acc= 89.0% | *
| Epoch   4, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=1.650, TAw acc= 89.0% | *
| Epoch   5, time=  4.1s | Train: skip eval | Valid: time=  0.2s loss=1.645, TAw acc= 89.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4825 train exemplars, time=  0.0s
4825
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.084 | TAw acc= 88.7%, forg=  2.1%| TAg acc= 69.6%, forg= 12.9% <<<
>>> Test on task  1 : loss=1.134 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.3%, forg= 12.5% <<<
>>> Test on task  2 : loss=1.314 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 61.7%, forg= 10.0% <<<
>>> Test on task  3 : loss=1.463 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 61.1%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.467 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.2%, forg=  7.5% <<<
>>> Test on task  5 : loss=1.246 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 78.4%, forg=  2.6% <<<
>>> Test on task  6 : loss=1.281 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 77.6%, forg=  3.1% <<<
>>> Test on task  7 : loss=1.135 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 69.7%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.406 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  2.8% <<<
>>> Test on task  9 : loss=0.910 | TAw acc= 97.0%, forg=  1.0%| TAg acc= 78.8%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.164 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 76.5%, forg=  2.0% <<<
>>> Test on task 11 : loss=1.267 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.3%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.323 | TAw acc= 94.3%, forg= -0.9%| TAg acc= 72.6%, forg=  2.8% <<<
>>> Test on task 13 : loss=1.615 | TAw acc= 85.8%, forg=  3.1%| TAg acc= 59.1%, forg=  2.4% <<<
>>> Test on task 14 : loss=0.876 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 83.5%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.381 | TAw acc= 94.0%, forg=  0.0%| TAg acc= 69.0%, forg=  4.0% <<<
>>> Test on task 16 : loss=1.523 | TAw acc= 89.1%, forg=  2.0%| TAg acc= 64.4%, forg= -1.0% <<<
>>> Test on task 17 : loss=0.846 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 80.5%, forg=  0.8% <<<
>>> Test on task 18 : loss=1.108 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 19 : loss=1.161 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 73.5%, forg=  3.5% <<<
>>> Test on task 20 : loss=1.364 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 67.3%, forg=  1.0% <<<
>>> Test on task 21 : loss=1.403 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 60.0%, forg=  8.7% <<<
>>> Test on task 22 : loss=1.678 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 72.4%, forg=  0.0% <<<
>>> Test on task 23 : loss=1.337 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 59.7%, forg= 15.1% <<<
>>> Test on task 24 : loss=1.550 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 59.8%, forg=  8.0% <<<
>>> Test on task 25 : loss=1.981 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 44.4%, forg= 22.2% <<<
>>> Test on task 26 : loss=1.709 | TAw acc= 97.2%, forg= -0.9%| TAg acc= 52.3%, forg= 13.8% <<<
>>> Test on task 27 : loss=1.856 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 58.6%, forg=  7.2% <<<
>>> Test on task 28 : loss=1.478 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 58.9%, forg= 14.0% <<<
>>> Test on task 29 : loss=1.629 | TAw acc= 93.9%, forg=  1.7%| TAg acc= 56.5%, forg= 11.3% <<<
>>> Test on task 30 : loss=1.584 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 53.4%, forg= 17.2% <<<
>>> Test on task 31 : loss=1.518 | TAw acc= 90.4%, forg=  3.2%| TAg acc= 56.0%, forg= 19.2% <<<
>>> Test on task 32 : loss=1.594 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 59.8%, forg= 17.1% <<<
>>> Test on task 33 : loss=1.531 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 47.7%, forg= 21.5% <<<
>>> Test on task 34 : loss=2.298 | TAw acc= 85.7%, forg=  3.6%| TAg acc= 24.1%, forg= 30.4% <<<
>>> Test on task 35 : loss=1.300 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 66.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 36
************************************************************************************************************
| Epoch   1, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=4.904, TAw acc= 45.2% | *
| Epoch   2, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.752, TAw acc= 89.4% | *
| Epoch   3, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=1.208, TAw acc= 97.1% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.992, TAw acc= 97.1% | *
| Epoch   5, time=  4.2s | Train: skip eval | Valid: time=  0.2s loss=0.861, TAw acc= 99.0% | *
| Epoch   1, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=0.860, TAw acc= 99.0% | *
| Epoch   2, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=0.860, TAw acc= 99.0% | *
| Epoch   3, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=0.860, TAw acc= 99.0% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=0.859, TAw acc= 99.0% | *
| Epoch   5, time=  4.3s | Train: skip eval | Valid: time=  0.2s loss=0.859, TAw acc= 99.0% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 4925 train exemplars, time=  0.0s
4925
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.117 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 68.0%, forg= 14.4% <<<
>>> Test on task  1 : loss=1.224 | TAw acc=100.0%, forg=  0.0%| TAg acc= 69.2%, forg= 11.5% <<<
>>> Test on task  2 : loss=1.275 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.8%, forg= 10.8% <<<
>>> Test on task  3 : loss=1.529 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 56.6%, forg= 16.8% <<<
>>> Test on task  4 : loss=1.487 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 66.4%, forg=  9.3% <<<
>>> Test on task  5 : loss=1.201 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.225 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 77.6%, forg=  3.1% <<<
>>> Test on task  7 : loss=1.192 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 69.7%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.397 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 78.0%, forg=  0.0% <<<
>>> Test on task  9 : loss=0.962 | TAw acc= 94.9%, forg=  3.0%| TAg acc= 76.8%, forg=  9.1% <<<
>>> Test on task 10 : loss=1.177 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 74.5%, forg=  3.9% <<<
>>> Test on task 11 : loss=1.242 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 12 : loss=1.338 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 71.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.753 | TAw acc= 86.6%, forg=  2.4%| TAg acc= 54.3%, forg=  7.1% <<<
>>> Test on task 14 : loss=0.905 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 83.5%, forg=  1.0% <<<
>>> Test on task 15 : loss=1.429 | TAw acc= 92.0%, forg=  2.0%| TAg acc= 68.0%, forg=  5.0% <<<
>>> Test on task 16 : loss=1.585 | TAw acc= 89.1%, forg=  2.0%| TAg acc= 61.4%, forg=  3.0% <<<
>>> Test on task 17 : loss=0.935 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 77.2%, forg=  4.1% <<<
>>> Test on task 18 : loss=1.126 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.200 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 20 : loss=1.373 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 66.3%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.388 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 61.7%, forg=  7.0% <<<
>>> Test on task 22 : loss=1.641 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 72.4%, forg=  0.0% <<<
>>> Test on task 23 : loss=1.349 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 60.5%, forg= 14.3% <<<
>>> Test on task 24 : loss=1.600 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 54.5%, forg= 13.4% <<<
>>> Test on task 25 : loss=2.031 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 42.6%, forg= 24.1% <<<
>>> Test on task 26 : loss=1.686 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 50.5%, forg= 15.6% <<<
>>> Test on task 27 : loss=1.892 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 55.9%, forg=  9.9% <<<
>>> Test on task 28 : loss=1.452 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 63.6%, forg=  9.3% <<<
>>> Test on task 29 : loss=1.602 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 60.0%, forg=  7.8% <<<
>>> Test on task 30 : loss=1.591 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 56.0%, forg= 14.7% <<<
>>> Test on task 31 : loss=1.496 | TAw acc= 94.4%, forg= -0.8%| TAg acc= 64.0%, forg= 11.2% <<<
>>> Test on task 32 : loss=1.586 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 65.8%, forg= 11.1% <<<
>>> Test on task 33 : loss=1.495 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 57.0%, forg= 12.1% <<<
>>> Test on task 34 : loss=2.226 | TAw acc= 91.1%, forg= -1.8%| TAg acc= 24.1%, forg= 30.4% <<<
>>> Test on task 35 : loss=1.633 | TAw acc= 90.1%, forg=  0.0%| TAg acc= 55.4%, forg= 10.9% <<<
>>> Test on task 36 : loss=0.976 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 82.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 37
************************************************************************************************************
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=4.763, TAw acc= 60.6% | *
| Epoch   2, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=2.122, TAw acc= 83.0% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.463, TAw acc= 88.3% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.299, TAw acc= 89.4% | *
| Epoch   5, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.156, TAw acc= 91.5% | *
| Epoch   1, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.152, TAw acc= 91.5% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.148, TAw acc= 91.5% | *
| Epoch   3, time=  4.4s | Train: skip eval | Valid: time=  0.2s loss=1.145, TAw acc= 91.5% | *
| Epoch   4, time=  4.5s | Train: skip eval | Valid: time=  0.2s loss=1.142, TAw acc= 91.5% | *
| Epoch   5, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.140, TAw acc= 91.5% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5025 train exemplars, time=  0.0s
5025
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.264 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 64.9%, forg= 17.5% <<<
>>> Test on task  1 : loss=1.234 | TAw acc=100.0%, forg=  0.0%| TAg acc= 66.3%, forg= 14.4% <<<
>>> Test on task  2 : loss=1.275 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.2%, forg=  7.5% <<<
>>> Test on task  3 : loss=1.465 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 60.2%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.485 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 69.2%, forg=  6.5% <<<
>>> Test on task  5 : loss=1.200 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.268 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 75.5%, forg=  5.1% <<<
>>> Test on task  7 : loss=1.187 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.8%, forg=  1.8% <<<
>>> Test on task  8 : loss=1.447 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.925 | TAw acc= 94.9%, forg=  3.0%| TAg acc= 80.8%, forg=  5.1% <<<
>>> Test on task 10 : loss=1.207 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 75.5%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.221 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.3%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.407 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 67.9%, forg=  7.5% <<<
>>> Test on task 13 : loss=1.684 | TAw acc= 87.4%, forg=  1.6%| TAg acc= 59.1%, forg=  2.4% <<<
>>> Test on task 14 : loss=0.907 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.491 | TAw acc= 90.0%, forg=  4.0%| TAg acc= 64.0%, forg=  9.0% <<<
>>> Test on task 16 : loss=1.519 | TAw acc= 89.1%, forg=  2.0%| TAg acc= 63.4%, forg=  1.0% <<<
>>> Test on task 17 : loss=0.847 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 77.2%, forg=  4.1% <<<
>>> Test on task 18 : loss=1.159 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 72.6%, forg=  4.4% <<<
>>> Test on task 19 : loss=1.183 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 74.3%, forg=  2.7% <<<
>>> Test on task 20 : loss=1.396 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 64.4%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.407 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 61.7%, forg=  7.0% <<<
>>> Test on task 22 : loss=1.670 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 71.4%, forg=  1.0% <<<
>>> Test on task 23 : loss=1.341 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 60.5%, forg= 14.3% <<<
>>> Test on task 24 : loss=1.539 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 62.5%, forg=  5.4% <<<
>>> Test on task 25 : loss=2.183 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 35.2%, forg= 31.5% <<<
>>> Test on task 26 : loss=1.735 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 52.3%, forg= 13.8% <<<
>>> Test on task 27 : loss=1.900 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 58.6%, forg=  7.2% <<<
>>> Test on task 28 : loss=1.425 | TAw acc= 95.3%, forg=  0.0%| TAg acc= 61.7%, forg= 11.2% <<<
>>> Test on task 29 : loss=1.573 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 63.5%, forg=  4.3% <<<
>>> Test on task 30 : loss=1.540 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 59.5%, forg= 11.2% <<<
>>> Test on task 31 : loss=1.514 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 52.8%, forg= 22.4% <<<
>>> Test on task 32 : loss=1.534 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 67.5%, forg=  9.4% <<<
>>> Test on task 33 : loss=1.410 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 63.6%, forg=  5.6% <<<
>>> Test on task 34 : loss=2.251 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 26.8%, forg= 27.7% <<<
>>> Test on task 35 : loss=1.589 | TAw acc= 91.1%, forg= -1.0%| TAg acc= 53.5%, forg= 12.9% <<<
>>> Test on task 36 : loss=1.602 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 53.7%, forg= 28.7% <<<
>>> Test on task 37 : loss=1.136 | TAw acc= 91.2%, forg=  0.0%| TAg acc= 67.2%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 38
************************************************************************************************************
| Epoch   1, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=6.530, TAw acc= 40.6% | *
| Epoch   2, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=3.527, TAw acc= 78.3% | *
| Epoch   3, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=2.025, TAw acc= 92.8% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.766, TAw acc= 91.3% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.627, TAw acc= 94.2% | *
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.623, TAw acc= 94.2% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.619, TAw acc= 94.2% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.614, TAw acc= 94.2% | *
| Epoch   4, time=  4.6s | Train: skip eval | Valid: time=  0.2s loss=1.611, TAw acc= 94.2% | *
| Epoch   5, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.607, TAw acc= 94.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5125 train exemplars, time=  0.0s
5125
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.172 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 69.6%, forg= 12.9% <<<
>>> Test on task  1 : loss=1.223 | TAw acc=100.0%, forg=  0.0%| TAg acc= 66.3%, forg= 14.4% <<<
>>> Test on task  2 : loss=1.396 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.0%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.432 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 62.8%, forg= 10.6% <<<
>>> Test on task  4 : loss=1.653 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 58.9%, forg= 16.8% <<<
>>> Test on task  5 : loss=1.253 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 77.6%, forg=  3.4% <<<
>>> Test on task  6 : loss=1.276 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 77.6%, forg=  3.1% <<<
>>> Test on task  7 : loss=1.123 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 72.5%, forg= -1.8% <<<
>>> Test on task  8 : loss=1.448 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.911 | TAw acc= 94.9%, forg=  3.0%| TAg acc= 81.8%, forg=  4.0% <<<
>>> Test on task 10 : loss=1.196 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 73.5%, forg=  4.9% <<<
>>> Test on task 11 : loss=1.256 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 67.3%, forg=  3.8% <<<
>>> Test on task 12 : loss=1.317 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 75.5%, forg=  0.0% <<<
>>> Test on task 13 : loss=1.695 | TAw acc= 86.6%, forg=  2.4%| TAg acc= 58.3%, forg=  3.1% <<<
>>> Test on task 14 : loss=0.940 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 80.6%, forg=  3.9% <<<
>>> Test on task 15 : loss=1.494 | TAw acc= 90.0%, forg=  4.0%| TAg acc= 68.0%, forg=  5.0% <<<
>>> Test on task 16 : loss=1.587 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 60.4%, forg=  4.0% <<<
>>> Test on task 17 : loss=0.830 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 78.9%, forg=  2.4% <<<
>>> Test on task 18 : loss=1.047 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.0%, forg=  0.0% <<<
>>> Test on task 19 : loss=1.051 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 80.5%, forg= -3.5% <<<
>>> Test on task 20 : loss=1.412 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 65.4%, forg=  2.9% <<<
>>> Test on task 21 : loss=1.522 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 60.9%, forg=  7.8% <<<
>>> Test on task 22 : loss=1.696 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 68.4%, forg=  4.1% <<<
>>> Test on task 23 : loss=1.378 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 56.3%, forg= 18.5% <<<
>>> Test on task 24 : loss=1.514 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 61.6%, forg=  6.2% <<<
>>> Test on task 25 : loss=1.996 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 45.4%, forg= 21.3% <<<
>>> Test on task 26 : loss=1.736 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 54.1%, forg= 11.9% <<<
>>> Test on task 27 : loss=1.919 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 59.5%, forg=  6.3% <<<
>>> Test on task 28 : loss=1.379 | TAw acc= 96.3%, forg= -0.9%| TAg acc= 64.5%, forg=  8.4% <<<
>>> Test on task 29 : loss=1.498 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 63.5%, forg=  4.3% <<<
>>> Test on task 30 : loss=1.589 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 56.9%, forg= 13.8% <<<
>>> Test on task 31 : loss=1.475 | TAw acc= 93.6%, forg=  0.8%| TAg acc= 54.4%, forg= 20.8% <<<
>>> Test on task 32 : loss=1.470 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 65.8%, forg= 11.1% <<<
>>> Test on task 33 : loss=1.304 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 72.9%, forg= -3.7% <<<
>>> Test on task 34 : loss=2.070 | TAw acc= 92.0%, forg= -0.9%| TAg acc= 37.5%, forg= 17.0% <<<
>>> Test on task 35 : loss=1.594 | TAw acc= 91.1%, forg=  0.0%| TAg acc= 54.5%, forg= 11.9% <<<
>>> Test on task 36 : loss=1.473 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 61.8%, forg= 20.6% <<<
>>> Test on task 37 : loss=1.775 | TAw acc= 90.4%, forg=  0.8%| TAg acc= 32.0%, forg= 35.2% <<<
>>> Test on task 38 : loss=1.615 | TAw acc= 92.7%, forg=  0.0%| TAg acc= 51.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 39
************************************************************************************************************
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=5.371, TAw acc= 56.6% | *
| Epoch   2, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=2.713, TAw acc= 81.9% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.584, TAw acc= 97.6% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.376, TAw acc= 91.6% | *
| Epoch   5, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.302, TAw acc= 94.0% | *
| Epoch   1, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.296, TAw acc= 94.0% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.291, TAw acc= 94.0% | *
| Epoch   3, time=  4.7s | Train: skip eval | Valid: time=  0.2s loss=1.286, TAw acc= 95.2% | *
| Epoch   4, time=  4.8s | Train: skip eval | Valid: time=  0.2s loss=1.282, TAw acc= 95.2% | *
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.278, TAw acc= 95.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5225 train exemplars, time=  0.0s
5225
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.156 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 71.6%, forg= 10.8% <<<
>>> Test on task  1 : loss=1.163 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.2%, forg= 10.6% <<<
>>> Test on task  2 : loss=1.265 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 60.0%, forg= 11.7% <<<
>>> Test on task  3 : loss=1.488 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 61.9%, forg= 11.5% <<<
>>> Test on task  4 : loss=1.624 | TAw acc= 95.3%, forg=  1.9%| TAg acc= 58.9%, forg= 16.8% <<<
>>> Test on task  5 : loss=1.244 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 76.7%, forg=  4.3% <<<
>>> Test on task  6 : loss=1.374 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 75.5%, forg=  5.1% <<<
>>> Test on task  7 : loss=1.203 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 68.8%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.464 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.921 | TAw acc= 94.9%, forg=  3.0%| TAg acc= 77.8%, forg=  8.1% <<<
>>> Test on task 10 : loss=1.205 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 71.6%, forg=  6.9% <<<
>>> Test on task 11 : loss=1.215 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.2%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.403 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 71.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.725 | TAw acc= 85.8%, forg=  3.1%| TAg acc= 59.1%, forg=  2.4% <<<
>>> Test on task 14 : loss=0.897 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 82.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.469 | TAw acc= 91.0%, forg=  3.0%| TAg acc= 71.0%, forg=  2.0% <<<
>>> Test on task 16 : loss=1.582 | TAw acc= 89.1%, forg=  2.0%| TAg acc= 65.3%, forg= -1.0% <<<
>>> Test on task 17 : loss=0.838 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 79.7%, forg=  1.6% <<<
>>> Test on task 18 : loss=1.094 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 73.5%, forg=  3.5% <<<
>>> Test on task 19 : loss=1.154 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 72.6%, forg=  8.0% <<<
>>> Test on task 20 : loss=1.442 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 59.6%, forg=  8.7% <<<
>>> Test on task 21 : loss=1.514 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 60.0%, forg=  8.7% <<<
>>> Test on task 22 : loss=1.687 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 73.5%, forg= -1.0% <<<
>>> Test on task 23 : loss=1.297 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 63.9%, forg= 10.9% <<<
>>> Test on task 24 : loss=1.589 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 58.0%, forg=  9.8% <<<
>>> Test on task 25 : loss=2.032 | TAw acc= 94.4%, forg=  1.9%| TAg acc= 51.9%, forg= 14.8% <<<
>>> Test on task 26 : loss=1.727 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 53.2%, forg= 12.8% <<<
>>> Test on task 27 : loss=1.898 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 57.7%, forg=  8.1% <<<
>>> Test on task 28 : loss=1.401 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 66.4%, forg=  6.5% <<<
>>> Test on task 29 : loss=1.591 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 59.1%, forg=  8.7% <<<
>>> Test on task 30 : loss=1.546 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 57.8%, forg= 12.9% <<<
>>> Test on task 31 : loss=1.537 | TAw acc= 92.0%, forg=  2.4%| TAg acc= 51.2%, forg= 24.0% <<<
>>> Test on task 32 : loss=1.420 | TAw acc= 94.9%, forg=  0.0%| TAg acc= 71.8%, forg=  5.1% <<<
>>> Test on task 33 : loss=1.316 | TAw acc= 98.1%, forg=  0.0%| TAg acc= 69.2%, forg=  3.7% <<<
>>> Test on task 34 : loss=2.194 | TAw acc= 91.1%, forg=  0.9%| TAg acc= 33.0%, forg= 21.4% <<<
>>> Test on task 35 : loss=1.495 | TAw acc= 93.1%, forg= -2.0%| TAg acc= 58.4%, forg=  7.9% <<<
>>> Test on task 36 : loss=1.406 | TAw acc= 95.6%, forg=  0.0%| TAg acc= 66.2%, forg= 16.2% <<<
>>> Test on task 37 : loss=1.733 | TAw acc= 92.0%, forg= -0.8%| TAg acc= 30.4%, forg= 36.8% <<<
>>> Test on task 38 : loss=2.082 | TAw acc= 93.8%, forg= -1.0%| TAg acc= 32.3%, forg= 18.7% <<<
>>> Test on task 39 : loss=1.463 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 63.4%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 40
************************************************************************************************************
| Epoch   1, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=6.633, TAw acc= 50.0% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=3.507, TAw acc= 80.6% | *
| Epoch   3, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=2.050, TAw acc= 91.7% | *
| Epoch   4, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.471, TAw acc= 97.2% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.342, TAw acc= 97.2% | *
| Epoch   1, time=  5.2s | Train: skip eval | Valid: time=  0.2s loss=1.339, TAw acc= 97.2% | *
| Epoch   2, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.336, TAw acc= 97.2% | *
| Epoch   3, time=  4.9s | Train: skip eval | Valid: time=  0.2s loss=1.333, TAw acc= 97.2% | *
| Epoch   4, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.330, TAw acc= 97.2% | *
| Epoch   5, time=  5.0s | Train: skip eval | Valid: time=  0.2s loss=1.328, TAw acc= 97.2% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5325 train exemplars, time=  0.0s
5325
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.169 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 67.5%, forg= 14.9% <<<
>>> Test on task  1 : loss=1.231 | TAw acc=100.0%, forg=  0.0%| TAg acc= 66.3%, forg= 14.4% <<<
>>> Test on task  2 : loss=1.310 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 64.2%, forg=  7.5% <<<
>>> Test on task  3 : loss=1.543 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 60.2%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.552 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 62.6%, forg= 13.1% <<<
>>> Test on task  5 : loss=1.241 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 79.3%, forg=  1.7% <<<
>>> Test on task  6 : loss=1.319 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 76.5%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.201 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 70.6%, forg=  1.8% <<<
>>> Test on task  8 : loss=1.460 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 76.1%, forg=  1.8% <<<
>>> Test on task  9 : loss=0.932 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 77.8%, forg=  8.1% <<<
>>> Test on task 10 : loss=1.180 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 75.5%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.220 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 70.2%, forg=  1.0% <<<
>>> Test on task 12 : loss=1.357 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 73.6%, forg=  1.9% <<<
>>> Test on task 13 : loss=1.782 | TAw acc= 87.4%, forg=  1.6%| TAg acc= 56.7%, forg=  4.7% <<<
>>> Test on task 14 : loss=0.907 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.434 | TAw acc= 91.0%, forg=  3.0%| TAg acc= 68.0%, forg=  5.0% <<<
>>> Test on task 16 : loss=1.486 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 67.3%, forg= -2.0% <<<
>>> Test on task 17 : loss=0.814 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 79.7%, forg=  1.6% <<<
>>> Test on task 18 : loss=1.157 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 75.2%, forg=  1.8% <<<
>>> Test on task 19 : loss=1.163 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 73.5%, forg=  7.1% <<<
>>> Test on task 20 : loss=1.401 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 64.4%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.482 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 59.1%, forg=  9.6% <<<
>>> Test on task 22 : loss=1.717 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 72.4%, forg=  1.0% <<<
>>> Test on task 23 : loss=1.322 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 63.0%, forg= 11.8% <<<
>>> Test on task 24 : loss=1.504 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 66.1%, forg=  1.8% <<<
>>> Test on task 25 : loss=1.982 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 50.9%, forg= 15.7% <<<
>>> Test on task 26 : loss=1.789 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 53.2%, forg= 12.8% <<<
>>> Test on task 27 : loss=1.977 | TAw acc= 92.8%, forg=  2.7%| TAg acc= 55.9%, forg=  9.9% <<<
>>> Test on task 28 : loss=1.412 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 64.5%, forg=  8.4% <<<
>>> Test on task 29 : loss=1.544 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 64.3%, forg=  3.5% <<<
>>> Test on task 30 : loss=1.513 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 59.5%, forg= 11.2% <<<
>>> Test on task 31 : loss=1.464 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 52.8%, forg= 22.4% <<<
>>> Test on task 32 : loss=1.453 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 72.6%, forg=  4.3% <<<
>>> Test on task 33 : loss=1.274 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 72.0%, forg=  0.9% <<<
>>> Test on task 34 : loss=2.171 | TAw acc= 91.1%, forg=  0.9%| TAg acc= 38.4%, forg= 16.1% <<<
>>> Test on task 35 : loss=1.466 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 58.4%, forg=  7.9% <<<
>>> Test on task 36 : loss=1.371 | TAw acc= 96.3%, forg= -0.7%| TAg acc= 66.2%, forg= 16.2% <<<
>>> Test on task 37 : loss=1.805 | TAw acc= 91.2%, forg=  0.8%| TAg acc= 30.4%, forg= 36.8% <<<
>>> Test on task 38 : loss=1.961 | TAw acc= 93.8%, forg=  0.0%| TAg acc= 39.6%, forg= 11.5% <<<
>>> Test on task 39 : loss=2.023 | TAw acc= 93.8%, forg= -1.8%| TAg acc= 46.4%, forg= 17.0% <<<
>>> Test on task 40 : loss=1.273 | TAw acc= 96.0%, forg=  0.0%| TAg acc= 73.7%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 41
************************************************************************************************************
| Epoch   1, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=5.351, TAw acc= 50.0% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=2.774, TAw acc= 72.0% | *
| Epoch   3, time=  6.0s | Train: skip eval | Valid: time=  0.2s loss=1.723, TAw acc= 92.7% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.503, TAw acc= 95.1% | *
| Epoch   5, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.357, TAw acc= 95.1% | *
| Epoch   1, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.355, TAw acc= 95.1% | *
| Epoch   2, time=  5.1s | Train: skip eval | Valid: time=  0.2s loss=1.353, TAw acc= 95.1% | *
| Epoch   3, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.352, TAw acc= 95.1% | *
| Epoch   4, time=  5.3s | Train: skip eval | Valid: time=  0.2s loss=1.350, TAw acc= 95.1% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.349, TAw acc= 95.1% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5425 train exemplars, time=  0.1s
5425
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.197 | TAw acc= 87.6%, forg=  3.1%| TAg acc= 69.1%, forg= 13.4% <<<
>>> Test on task  1 : loss=1.244 | TAw acc=100.0%, forg=  0.0%| TAg acc= 67.3%, forg= 13.5% <<<
>>> Test on task  2 : loss=1.298 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 63.3%, forg=  8.3% <<<
>>> Test on task  3 : loss=1.479 | TAw acc= 92.0%, forg=  0.9%| TAg acc= 61.1%, forg= 12.4% <<<
>>> Test on task  4 : loss=1.636 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 63.6%, forg= 12.1% <<<
>>> Test on task  5 : loss=1.285 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.292 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 77.6%, forg=  3.1% <<<
>>> Test on task  7 : loss=1.159 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 68.8%, forg=  3.7% <<<
>>> Test on task  8 : loss=1.493 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  2.8% <<<
>>> Test on task  9 : loss=0.943 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 82.8%, forg=  3.0% <<<
>>> Test on task 10 : loss=1.180 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 75.5%, forg=  2.9% <<<
>>> Test on task 11 : loss=1.186 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 12 : loss=1.461 | TAw acc= 91.5%, forg=  2.8%| TAg acc= 70.8%, forg=  4.7% <<<
>>> Test on task 13 : loss=1.749 | TAw acc= 86.6%, forg=  2.4%| TAg acc= 58.3%, forg=  3.1% <<<
>>> Test on task 14 : loss=0.940 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 79.6%, forg=  4.9% <<<
>>> Test on task 15 : loss=1.477 | TAw acc= 92.0%, forg=  2.0%| TAg acc= 70.0%, forg=  3.0% <<<
>>> Test on task 16 : loss=1.627 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 60.4%, forg=  6.9% <<<
>>> Test on task 17 : loss=0.882 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 78.0%, forg=  3.3% <<<
>>> Test on task 18 : loss=1.086 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 77.0%, forg=  0.0% <<<
>>> Test on task 19 : loss=1.168 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 72.6%, forg=  8.0% <<<
>>> Test on task 20 : loss=1.427 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 66.3%, forg=  1.9% <<<
>>> Test on task 21 : loss=1.385 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 64.3%, forg=  4.3% <<<
>>> Test on task 22 : loss=1.791 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 70.4%, forg=  3.1% <<<
>>> Test on task 23 : loss=1.331 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 64.7%, forg= 10.1% <<<
>>> Test on task 24 : loss=1.599 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 60.7%, forg=  7.1% <<<
>>> Test on task 25 : loss=2.020 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 50.0%, forg= 16.7% <<<
>>> Test on task 26 : loss=1.744 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 52.3%, forg= 13.8% <<<
>>> Test on task 27 : loss=2.009 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 53.2%, forg= 12.6% <<<
>>> Test on task 28 : loss=1.448 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 63.6%, forg=  9.3% <<<
>>> Test on task 29 : loss=1.473 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 62.6%, forg=  5.2% <<<
>>> Test on task 30 : loss=1.610 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 59.5%, forg= 11.2% <<<
>>> Test on task 31 : loss=1.402 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 62.4%, forg= 12.8% <<<
>>> Test on task 32 : loss=1.509 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 64.1%, forg= 12.8% <<<
>>> Test on task 33 : loss=1.239 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 71.0%, forg=  1.9% <<<
>>> Test on task 34 : loss=2.109 | TAw acc= 90.2%, forg=  1.8%| TAg acc= 41.1%, forg= 13.4% <<<
>>> Test on task 35 : loss=1.386 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 58.4%, forg=  7.9% <<<
>>> Test on task 36 : loss=1.395 | TAw acc= 95.6%, forg=  0.7%| TAg acc= 64.0%, forg= 18.4% <<<
>>> Test on task 37 : loss=1.718 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 36.0%, forg= 31.2% <<<
>>> Test on task 38 : loss=1.937 | TAw acc= 94.8%, forg= -1.0%| TAg acc= 43.8%, forg=  7.3% <<<
>>> Test on task 39 : loss=1.967 | TAw acc= 94.6%, forg= -0.9%| TAg acc= 44.6%, forg= 18.7% <<<
>>> Test on task 40 : loss=1.617 | TAw acc= 97.0%, forg= -1.0%| TAg acc= 56.6%, forg= 17.2% <<<
>>> Test on task 41 : loss=1.081 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 74.8%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 42
************************************************************************************************************
| Epoch   1, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=6.305, TAw acc= 38.9% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=3.368, TAw acc= 79.2% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=1.940, TAw acc= 88.9% | *
| Epoch   4, time=  5.4s | Train: skip eval | Valid: time=  0.2s loss=1.493, TAw acc= 93.1% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.461, TAw acc= 91.7% | *
| Epoch   1, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.454, TAw acc= 91.7% | *
| Epoch   2, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.448, TAw acc= 91.7% | *
| Epoch   3, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.443, TAw acc= 91.7% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.438, TAw acc= 91.7% | *
| Epoch   5, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.434, TAw acc= 91.7% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5525 train exemplars, time=  0.0s
5525
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.166 | TAw acc= 89.7%, forg=  1.0%| TAg acc= 69.1%, forg= 13.4% <<<
>>> Test on task  1 : loss=1.226 | TAw acc=100.0%, forg=  0.0%| TAg acc= 68.3%, forg= 12.5% <<<
>>> Test on task  2 : loss=1.333 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 59.2%, forg= 12.5% <<<
>>> Test on task  3 : loss=1.479 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 60.2%, forg= 13.3% <<<
>>> Test on task  4 : loss=1.635 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 64.5%, forg= 11.2% <<<
>>> Test on task  5 : loss=1.264 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 80.2%, forg=  0.9% <<<
>>> Test on task  6 : loss=1.327 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 76.5%, forg=  4.1% <<<
>>> Test on task  7 : loss=1.182 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 71.6%, forg=  0.9% <<<
>>> Test on task  8 : loss=1.509 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 75.2%, forg=  2.8% <<<
>>> Test on task  9 : loss=0.987 | TAw acc= 96.0%, forg=  2.0%| TAg acc= 78.8%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.208 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 73.5%, forg=  4.9% <<<
>>> Test on task 11 : loss=1.205 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 12 : loss=1.415 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 72.6%, forg=  2.8% <<<
>>> Test on task 13 : loss=1.712 | TAw acc= 88.2%, forg=  0.8%| TAg acc= 56.7%, forg=  4.7% <<<
>>> Test on task 14 : loss=0.914 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 82.5%, forg=  1.9% <<<
>>> Test on task 15 : loss=1.478 | TAw acc= 91.0%, forg=  3.0%| TAg acc= 71.0%, forg=  2.0% <<<
>>> Test on task 16 : loss=1.543 | TAw acc= 90.1%, forg=  1.0%| TAg acc= 64.4%, forg=  3.0% <<<
>>> Test on task 17 : loss=0.773 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 81.3%, forg=  0.0% <<<
>>> Test on task 18 : loss=1.178 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 73.5%, forg=  3.5% <<<
>>> Test on task 19 : loss=1.151 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 77.9%, forg=  2.7% <<<
>>> Test on task 20 : loss=1.463 | TAw acc= 93.3%, forg=  1.0%| TAg acc= 64.4%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.641 | TAw acc= 96.5%, forg=  0.0%| TAg acc= 56.5%, forg= 12.2% <<<
>>> Test on task 22 : loss=1.780 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 70.4%, forg=  3.1% <<<
>>> Test on task 23 : loss=1.346 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 63.0%, forg= 11.8% <<<
>>> Test on task 24 : loss=1.569 | TAw acc= 94.6%, forg=  0.9%| TAg acc= 61.6%, forg=  6.2% <<<
>>> Test on task 25 : loss=2.022 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 49.1%, forg= 17.6% <<<
>>> Test on task 26 : loss=1.818 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 48.6%, forg= 17.4% <<<
>>> Test on task 27 : loss=1.982 | TAw acc= 93.7%, forg=  1.8%| TAg acc= 55.9%, forg=  9.9% <<<
>>> Test on task 28 : loss=1.425 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 65.4%, forg=  7.5% <<<
>>> Test on task 29 : loss=1.630 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 57.4%, forg= 10.4% <<<
>>> Test on task 30 : loss=1.560 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 57.8%, forg= 12.9% <<<
>>> Test on task 31 : loss=1.477 | TAw acc= 92.8%, forg=  1.6%| TAg acc= 57.6%, forg= 17.6% <<<
>>> Test on task 32 : loss=1.476 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 72.6%, forg=  4.3% <<<
>>> Test on task 33 : loss=1.292 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 71.0%, forg=  1.9% <<<
>>> Test on task 34 : loss=2.120 | TAw acc= 90.2%, forg=  1.8%| TAg acc= 42.9%, forg= 11.6% <<<
>>> Test on task 35 : loss=1.410 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 59.4%, forg=  6.9% <<<
>>> Test on task 36 : loss=1.293 | TAw acc= 95.6%, forg=  0.7%| TAg acc= 66.9%, forg= 15.4% <<<
>>> Test on task 37 : loss=1.750 | TAw acc= 92.0%, forg=  0.0%| TAg acc= 38.4%, forg= 28.8% <<<
>>> Test on task 38 : loss=1.764 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 49.0%, forg=  2.1% <<<
>>> Test on task 39 : loss=1.921 | TAw acc= 94.6%, forg=  0.0%| TAg acc= 52.7%, forg= 10.7% <<<
>>> Test on task 40 : loss=1.557 | TAw acc= 97.0%, forg=  0.0%| TAg acc= 51.5%, forg= 22.2% <<<
>>> Test on task 41 : loss=1.497 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 62.2%, forg= 12.6% <<<
>>> Test on task 42 : loss=1.461 | TAw acc= 89.0%, forg=  0.0%| TAg acc= 55.0%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
======
LLL_Net(
  (model): SimpleMLP(
    (fc1): Linear(in_features=91, out_features=1000, bias=True)
    (fc2): Sequential()
  )
  (heads): ModuleList(
    (0): Linear(in_features=1000, out_features=34, bias=True)
    (1): Linear(in_features=1000, out_features=20, bias=True)
    (2): Linear(in_features=1000, out_features=20, bias=True)
    (3): Linear(in_features=1000, out_features=20, bias=True)
    (4): Linear(in_features=1000, out_features=20, bias=True)
    (5): Linear(in_features=1000, out_features=20, bias=True)
    (6): Linear(in_features=1000, out_features=20, bias=True)
    (7): Linear(in_features=1000, out_features=20, bias=True)
    (8): Linear(in_features=1000, out_features=20, bias=True)
    (9): Linear(in_features=1000, out_features=20, bias=True)
    (10): Linear(in_features=1000, out_features=20, bias=True)
    (11): Linear(in_features=1000, out_features=20, bias=True)
    (12): Linear(in_features=1000, out_features=20, bias=True)
    (13): Linear(in_features=1000, out_features=20, bias=True)
    (14): Linear(in_features=1000, out_features=20, bias=True)
    (15): Linear(in_features=1000, out_features=20, bias=True)
    (16): Linear(in_features=1000, out_features=20, bias=True)
    (17): Linear(in_features=1000, out_features=20, bias=True)
    (18): Linear(in_features=1000, out_features=20, bias=True)
    (19): Linear(in_features=1000, out_features=20, bias=True)
    (20): Linear(in_features=1000, out_features=20, bias=True)
    (21): Linear(in_features=1000, out_features=20, bias=True)
    (22): Linear(in_features=1000, out_features=20, bias=True)
    (23): Linear(in_features=1000, out_features=20, bias=True)
    (24): Linear(in_features=1000, out_features=20, bias=True)
    (25): Linear(in_features=1000, out_features=20, bias=True)
    (26): Linear(in_features=1000, out_features=20, bias=True)
    (27): Linear(in_features=1000, out_features=20, bias=True)
    (28): Linear(in_features=1000, out_features=20, bias=True)
    (29): Linear(in_features=1000, out_features=20, bias=True)
    (30): Linear(in_features=1000, out_features=20, bias=True)
    (31): Linear(in_features=1000, out_features=20, bias=True)
    (32): Linear(in_features=1000, out_features=20, bias=True)
    (33): Linear(in_features=1000, out_features=20, bias=True)
    (34): Linear(in_features=1000, out_features=20, bias=True)
    (35): Linear(in_features=1000, out_features=20, bias=True)
    (36): Linear(in_features=1000, out_features=20, bias=True)
    (37): Linear(in_features=1000, out_features=20, bias=True)
    (38): Linear(in_features=1000, out_features=20, bias=True)
    (39): Linear(in_features=1000, out_features=20, bias=True)
    (40): Linear(in_features=1000, out_features=20, bias=True)
    (41): Linear(in_features=1000, out_features=20, bias=True)
    (42): Linear(in_features=1000, out_features=20, bias=True)
  )
)
======

************************************************************************************************************
Task 43
************************************************************************************************************
| Epoch   1, time=  5.9s | Train: skip eval | Valid: time=  0.2s loss=6.038, TAw acc= 41.9% | *
| Epoch   2, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=3.320, TAw acc= 74.3% | *
| Epoch   3, time=  5.8s | Train: skip eval | Valid: time=  0.2s loss=2.122, TAw acc= 86.5% | *
| Epoch   4, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.549, TAw acc= 95.9% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.310, TAw acc= 95.9% | *
| Epoch   1, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.309, TAw acc= 95.9% | *
| Epoch   2, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.308, TAw acc= 95.9% | *
| Epoch   3, time=  5.5s | Train: skip eval | Valid: time=  0.2s loss=1.307, TAw acc= 97.3% | *
| Epoch   4, time=  5.6s | Train: skip eval | Valid: time=  0.2s loss=1.306, TAw acc= 97.3% | *
| Epoch   5, time=  5.7s | Train: skip eval | Valid: time=  0.2s loss=1.305, TAw acc= 97.3% | *
EXEMPLARS_PER_CLASS: 10
Not enough samples to store. Select all samples instead.	Needed: 10
Not enough samples to store. Select all samples instead.	Needed: 9
Not enough samples to store. Select all samples instead.	Needed: 7
| Selected 5625 train exemplars, time=  0.0s
5625
------------------------------------------------------------------------------------------------------------
>>> Test on task  0 : loss=1.196 | TAw acc= 89.2%, forg=  1.5%| TAg acc= 68.0%, forg= 14.4% <<<
>>> Test on task  1 : loss=1.175 | TAw acc=100.0%, forg=  0.0%| TAg acc= 70.2%, forg= 10.6% <<<
>>> Test on task  2 : loss=1.403 | TAw acc= 97.5%, forg=  0.0%| TAg acc= 61.7%, forg= 10.0% <<<
>>> Test on task  3 : loss=1.485 | TAw acc= 91.2%, forg=  1.8%| TAg acc= 59.3%, forg= 14.2% <<<
>>> Test on task  4 : loss=1.768 | TAw acc= 96.3%, forg=  0.9%| TAg acc= 58.9%, forg= 16.8% <<<
>>> Test on task  5 : loss=1.256 | TAw acc= 95.7%, forg=  0.0%| TAg acc= 81.0%, forg=  0.0% <<<
>>> Test on task  6 : loss=1.353 | TAw acc= 91.8%, forg=  1.0%| TAg acc= 74.5%, forg=  6.1% <<<
>>> Test on task  7 : loss=1.241 | TAw acc= 95.4%, forg=  1.8%| TAg acc= 67.0%, forg=  5.5% <<<
>>> Test on task  8 : loss=1.492 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 77.1%, forg=  0.9% <<<
>>> Test on task  9 : loss=0.957 | TAw acc= 98.0%, forg=  0.0%| TAg acc= 78.8%, forg=  7.1% <<<
>>> Test on task 10 : loss=1.220 | TAw acc= 97.1%, forg=  2.0%| TAg acc= 76.5%, forg=  2.0% <<<
>>> Test on task 11 : loss=1.287 | TAw acc= 97.1%, forg=  1.0%| TAg acc= 69.2%, forg=  1.9% <<<
>>> Test on task 12 : loss=1.458 | TAw acc= 93.4%, forg=  0.9%| TAg acc= 71.7%, forg=  3.8% <<<
>>> Test on task 13 : loss=1.862 | TAw acc= 88.2%, forg=  0.8%| TAg acc= 55.9%, forg=  5.5% <<<
>>> Test on task 14 : loss=0.921 | TAw acc= 96.1%, forg=  1.0%| TAg acc= 81.6%, forg=  2.9% <<<
>>> Test on task 15 : loss=1.472 | TAw acc= 91.0%, forg=  3.0%| TAg acc= 68.0%, forg=  5.0% <<<
>>> Test on task 16 : loss=1.518 | TAw acc= 89.1%, forg=  2.0%| TAg acc= 63.4%, forg=  4.0% <<<
>>> Test on task 17 : loss=0.858 | TAw acc= 98.4%, forg=  0.0%| TAg acc= 79.7%, forg=  1.6% <<<
>>> Test on task 18 : loss=1.126 | TAw acc= 97.3%, forg=  0.0%| TAg acc= 72.6%, forg=  4.4% <<<
>>> Test on task 19 : loss=1.094 | TAw acc= 97.3%, forg=  0.9%| TAg acc= 81.4%, forg= -0.9% <<<
>>> Test on task 20 : loss=1.530 | TAw acc= 92.3%, forg=  1.9%| TAg acc= 64.4%, forg=  3.8% <<<
>>> Test on task 21 : loss=1.571 | TAw acc= 98.3%, forg= -1.7%| TAg acc= 54.8%, forg= 13.9% <<<
>>> Test on task 22 : loss=1.700 | TAw acc= 90.8%, forg=  1.0%| TAg acc= 72.4%, forg=  1.0% <<<
>>> Test on task 23 : loss=1.309 | TAw acc= 99.2%, forg=  0.8%| TAg acc= 63.9%, forg= 10.9% <<<
>>> Test on task 24 : loss=1.533 | TAw acc= 95.5%, forg=  0.0%| TAg acc= 59.8%, forg=  8.0% <<<
>>> Test on task 25 : loss=2.063 | TAw acc= 95.4%, forg=  0.9%| TAg acc= 53.7%, forg= 13.0% <<<
>>> Test on task 26 : loss=1.745 | TAw acc= 97.2%, forg=  0.0%| TAg acc= 54.1%, forg= 11.9% <<<
>>> Test on task 27 : loss=1.981 | TAw acc= 92.8%, forg=  2.7%| TAg acc= 56.8%, forg=  9.0% <<<
>>> Test on task 28 : loss=1.445 | TAw acc= 96.3%, forg=  0.0%| TAg acc= 61.7%, forg= 11.2% <<<
>>> Test on task 29 : loss=1.520 | TAw acc= 94.8%, forg=  0.9%| TAg acc= 63.5%, forg=  4.3% <<<
>>> Test on task 30 : loss=1.658 | TAw acc= 98.3%, forg=  0.0%| TAg acc= 57.8%, forg= 12.9% <<<
>>> Test on task 31 : loss=1.429 | TAw acc= 94.4%, forg=  0.0%| TAg acc= 61.6%, forg= 13.6% <<<
>>> Test on task 32 : loss=1.412 | TAw acc= 94.0%, forg=  0.9%| TAg acc= 76.1%, forg=  0.9% <<<
>>> Test on task 33 : loss=1.276 | TAw acc= 97.2%, forg=  0.9%| TAg acc= 71.0%, forg=  1.9% <<<
>>> Test on task 34 : loss=2.086 | TAw acc= 91.1%, forg=  0.9%| TAg acc= 42.0%, forg= 12.5% <<<
>>> Test on task 35 : loss=1.381 | TAw acc= 93.1%, forg=  0.0%| TAg acc= 59.4%, forg=  6.9% <<<
>>> Test on task 36 : loss=1.350 | TAw acc= 95.6%, forg=  0.7%| TAg acc= 66.2%, forg= 16.2% <<<
>>> Test on task 37 : loss=1.675 | TAw acc= 91.2%, forg=  0.8%| TAg acc= 44.0%, forg= 23.2% <<<
>>> Test on task 38 : loss=1.794 | TAw acc= 93.8%, forg=  1.0%| TAg acc= 46.9%, forg=  4.2% <<<
>>> Test on task 39 : loss=1.791 | TAw acc= 93.8%, forg=  0.9%| TAg acc= 51.8%, forg= 11.6% <<<
>>> Test on task 40 : loss=1.497 | TAw acc= 96.0%, forg=  1.0%| TAg acc= 55.6%, forg= 18.2% <<<
>>> Test on task 41 : loss=1.423 | TAw acc= 99.1%, forg=  0.0%| TAg acc= 63.1%, forg= 11.7% <<<
>>> Test on task 42 : loss=1.952 | TAw acc= 91.0%, forg= -2.0%| TAg acc= 35.0%, forg= 20.0% <<<
>>> Test on task 43 : loss=1.134 | TAw acc= 99.0%, forg=  0.0%| TAg acc= 69.3%, forg=  0.0% <<<
Save at eeil_e5_wisdm_4/wisdm_flex_eeil
************************************************************************************************************
TAw Acc
	 76.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.3% 
	 88.1%  90.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 89.3% 
	 87.6%  95.2%  80.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 87.6% 
	 88.7%  97.1%  90.8%  70.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 86.9% 
	 87.1%  99.0%  92.5%  85.8%  80.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 89.0% 
	 89.2%  99.0%  93.3%  89.4%  90.7%  75.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 89.6% 
	 88.1%  98.1%  93.3%  90.3%  91.6%  91.4%  87.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.5% 
	 88.1%  96.2%  92.5%  90.3%  91.6%  93.1%  91.8%  85.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.1% 
	 89.2%  99.0%  93.3%  88.5%  91.6%  94.0%  92.9%  95.4%  80.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 91.6% 
	 90.7%  99.0%  92.5%  90.3%  94.4%  94.0%  91.8%  93.6%  96.3%  92.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.6% 
	 90.2%  99.0%  92.5%  88.5%  96.3%  94.0%  91.8%  95.4%  97.2%  91.9%  92.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.5% 
	 89.7%  99.0%  91.7%  90.3%  95.3%  94.8%  91.8%  95.4%  97.2%  97.0%  97.1%  89.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.1% 
	 89.7%  99.0%  91.7%  90.3%  96.3%  95.7%  91.8%  94.5%  97.2%  97.0%  99.0%  94.2%  84.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.9% 
	 90.2%  99.0%  93.3%  89.4%  91.6%  95.7%  91.8%  94.5%  97.2%  97.0%  99.0%  98.1%  90.6%  81.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.5% 
	 88.7%  99.0%  95.8%  89.4%  96.3%  94.8%  91.8%  95.4%  96.3%  94.9%  99.0%  98.1%  92.5%  83.5%  95.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.0% 
	 89.7%  99.0%  95.8%  89.4%  96.3%  94.8%  92.9%  96.3%  97.2%  97.0%  99.0%  98.1%  92.5%  81.9%  97.1%  87.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.0% 
	 90.2%  99.0%  95.8%  90.3%  96.3%  95.7%  92.9%  96.3%  97.2%  97.0%  99.0%  98.1%  90.6%  82.7%  97.1%  90.0%  86.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 93.8% 
	 88.7%  99.0%  95.8%  92.0%  96.3%  94.8%  91.8%  96.3%  97.2%  97.0%  99.0%  98.1%  90.6%  83.5%  97.1%  90.0%  90.1%  96.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.1% 
	 88.7%  99.0%  95.8%  90.3%  96.3%  95.7%  92.9%  95.4%  97.2%  93.9%  99.0%  98.1%  90.6%  85.0%  97.1%  92.0%  88.1%  98.4%  93.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.1% 
	 89.2%  99.0%  95.8%  90.3%  96.3%  95.7%  91.8%  96.3%  97.2%  97.0%  99.0%  98.1%  90.6%  85.8%  97.1%  93.0%  90.1%  98.4%  96.5%  97.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 89.2%  99.0%  97.5%  91.2%  96.3%  95.7%  91.8%  96.3%  97.2%  97.0%  99.0%  98.1%  91.5%  86.6%  97.1%  93.0%  91.1%  98.4%  97.3%  97.3%  92.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 90.2%  99.0%  97.5%  91.2%  96.3%  95.7%  91.8%  95.4%  97.2%  98.0%  99.0%  97.1%  92.5%  87.4%  97.1%  93.0%  91.1%  98.4%  97.3%  98.2%  93.3%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 89.2%  99.0%  97.5%  92.0%  97.2%  95.7%  91.8%  96.3%  97.2%  98.0%  99.0%  97.1%  90.6%  86.6%  96.1%  93.0%  91.1%  98.4%  97.3%  97.3%  92.3%  95.7%  89.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 89.7%  99.0%  97.5%  92.0%  96.3%  95.7%  91.8%  96.3%  97.2%  98.0%  99.0%  97.1%  93.4%  85.0%  97.1%  93.0%  91.1%  98.4%  96.5%  97.3%  92.3%  95.7%  91.8%  95.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 89.2%  99.0%  97.5%  92.0%  94.4%  95.7%  91.8%  97.2%  97.2%  98.0%  99.0%  97.1%  93.4%  86.6%  96.1%  93.0%  91.1%  98.4%  97.3%  97.3%  91.3%  96.5%  91.8% 100.0%  94.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 89.2%  99.0%  97.5%  92.0%  95.3%  95.7%  91.8%  96.3%  97.2%  98.0%  99.0%  97.1%  92.5%  87.4%  96.1%  94.0%  91.1%  98.4%  96.5%  97.3%  91.3%  96.5%  90.8%  99.2%  94.6%  96.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 90.2%  99.0%  97.5%  92.9%  95.3%  95.7%  92.9%  96.3%  97.2%  98.0%  99.0%  97.1%  90.6%  89.0%  96.1%  94.0%  90.1%  98.4%  97.3%  97.3%  94.2%  96.5%  91.8%  99.2%  94.6%  93.5%  96.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.2% 
	 89.7%  99.0%  97.5%  90.3%  95.3%  95.7%  91.8%  96.3%  97.2%  96.0%  99.0%  97.1%  92.5%  85.0%  96.1%  94.0%  91.1%  98.4%  97.3%  97.3%  93.3%  96.5%  91.8%  99.2%  94.6%  93.5%  96.3%  95.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 89.2% 100.0%  97.5%  92.0%  95.3%  95.7%  91.8%  96.3%  97.2%  98.0%  99.0%  97.1%  90.6%  87.4%  96.1%  93.0%  91.1%  98.4%  97.3%  97.3%  93.3%  95.7%  91.8%  99.2%  94.6%  95.4%  96.3%  94.6%  93.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 88.7% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  97.2%  97.2%  98.0%  99.0%  97.1%  92.5%  86.6%  96.1%  94.0%  91.1%  98.4%  97.3%  97.3%  92.3%  96.5%  90.8%  99.2%  94.6%  95.4%  96.3%  94.6%  95.3%  94.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.1% 
	 89.2% 100.0%  97.5%  90.3%  96.3%  95.7%  91.8%  96.3%  97.2%  96.0%  99.0%  97.1%  92.5%  85.8%  96.1%  93.0%  91.1%  98.4%  97.3%  97.3%  92.3%  96.5%  91.8% 100.0%  94.6%  95.4%  96.3%  94.6%  95.3%  93.9%  95.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 89.7% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  96.3%  97.2%  94.9%  99.0%  97.1%  92.5%  86.6%  96.1%  90.0%  91.1%  98.4%  97.3%  96.5%  92.3%  96.5%  90.8% 100.0%  95.5%  95.4%  96.3%  94.6%  95.3%  95.7%  97.4%  92.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 88.7% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  96.3%  97.2%  94.9%  99.0%  97.1%  90.6%  85.8%  96.1%  91.0%  90.1%  98.4%  97.3%  97.3%  93.3%  96.5%  91.8%  99.2%  95.5%  96.3%  96.3%  94.6%  95.3%  93.9%  98.3%  92.0%  93.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 90.2% 100.0%  97.5%  92.0%  95.3%  95.7%  91.8%  95.4%  97.2%  98.0%  98.0%  97.1%  93.4%  85.8%  96.1%  90.0%  89.1%  98.4%  97.3%  97.3%  92.3%  96.5%  91.8%  99.2%  95.5%  95.4%  96.3%  94.6%  95.3%  94.8%  97.4%  93.6%  94.9%  98.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 89.7% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  96.3%  97.2%  98.0%  98.0%  97.1%  92.5%  85.8%  96.1%  93.0%  90.1%  98.4%  97.3%  97.3%  93.3%  96.5%  90.8%  99.2%  95.5%  94.4%  96.3%  93.7%  95.3%  94.8%  98.3%  93.6%  94.9%  98.1%  89.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 95.0% 
	 88.7% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  96.3%  97.2%  97.0%  97.1%  97.1%  94.3%  85.8%  96.1%  94.0%  89.1%  98.4%  97.3%  97.3%  93.3%  96.5%  90.8%  99.2%  95.5%  94.4%  97.2%  94.6%  95.3%  93.9%  98.3%  90.4%  94.9%  98.1%  85.7%  90.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 89.2% 100.0%  97.5%  91.2%  95.3%  95.7%  91.8%  96.3%  97.2%  94.9%  97.1%  97.1%  93.4%  86.6%  96.1%  92.0%  89.1%  98.4%  97.3%  97.3%  92.3%  96.5%  90.8%  99.2%  95.5%  94.4%  97.2%  94.6%  95.3%  94.8%  98.3%  94.4%  94.9%  98.1%  91.1%  90.1%  95.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.8% 
	 89.2% 100.0%  97.5%  91.2%  95.3%  95.7%  91.8%  97.2%  97.2%  94.9%  97.1%  97.1%  93.4%  87.4%  96.1%  90.0%  89.1%  98.4%  97.3%  97.3%  92.3%  96.5%  90.8%  99.2%  95.5%  95.4%  97.2%  93.7%  95.3%  94.8%  98.3%  93.6%  94.9%  98.1%  91.1%  91.1%  95.6%  91.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 89.7% 100.0%  97.5%  92.0%  95.3%  95.7%  91.8%  96.3%  97.2%  94.9%  97.1%  97.1%  93.4%  86.6%  96.1%  90.0%  91.1%  98.4%  97.3%  97.3%  93.3%  96.5%  90.8%  99.2%  95.5%  95.4%  97.2%  93.7%  96.3%  94.8%  98.3%  93.6%  94.9%  98.1%  92.0%  91.1%  95.6%  90.4%  92.7%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.7% 
	 89.7% 100.0%  97.5%  92.0%  95.3%  94.8%  91.8%  97.2%  97.2%  94.9%  97.1%  97.1%  93.4%  85.8%  96.1%  91.0%  89.1%  98.4%  97.3%  97.3%  93.3%  96.5%  90.8%  99.2%  95.5%  94.4%  97.2%  93.7%  96.3%  94.8%  98.3%  92.0%  94.9%  98.1%  91.1%  93.1%  95.6%  92.0%  93.8%  92.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 94.6% 
	 89.2% 100.0%  97.5%  92.0%  96.3%  95.7%  91.8%  96.3%  97.2%  96.0%  97.1%  97.1%  93.4%  87.4%  96.1%  91.0%  90.1%  98.4%  97.3%  97.3%  93.3%  96.5%  90.8%  99.2%  95.5%  96.3%  97.2%  92.8%  96.3%  94.8%  98.3%  94.4%  94.0%  97.2%  91.1%  93.1%  96.3%  91.2%  93.8%  93.8%  96.0%   0.0%   0.0%   0.0% 	Avg.: 94.9% 
	 87.6% 100.0%  97.5%  92.0%  96.3%  95.7%  91.8%  96.3%  97.2%  98.0%  97.1%  97.1%  91.5%  86.6%  96.1%  92.0%  90.1%  98.4%  97.3%  97.3%  92.3%  96.5%  90.8%  99.2%  95.5%  95.4%  97.2%  93.7%  96.3%  94.8%  98.3%  94.4%  94.0%  97.2%  90.2%  93.1%  95.6%  92.0%  94.8%  94.6%  97.0%  99.1%   0.0%   0.0% 	Avg.: 95.0% 
	 89.7% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  97.2%  97.2%  96.0%  97.1%  97.1%  93.4%  88.2%  96.1%  91.0%  90.1%  98.4%  97.3%  97.3%  93.3%  96.5%  90.8%  99.2%  94.6%  95.4%  97.2%  93.7%  96.3%  94.8%  98.3%  92.8%  94.0%  97.2%  90.2%  93.1%  95.6%  92.0%  93.8%  94.6%  97.0%  99.1%  89.0%   0.0% 	Avg.: 94.8% 
	 89.2% 100.0%  97.5%  91.2%  96.3%  95.7%  91.8%  95.4%  97.2%  98.0%  97.1%  97.1%  93.4%  88.2%  96.1%  91.0%  89.1%  98.4%  97.3%  97.3%  92.3%  98.3%  90.8%  99.2%  95.5%  95.4%  97.2%  92.8%  96.3%  94.8%  98.3%  94.4%  94.0%  97.2%  91.1%  93.1%  95.6%  91.2%  93.8%  93.8%  96.0%  99.1%  91.0%  99.0% 	Avg.: 94.9% 
************************************************************************************************************
TAg Acc
	 76.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.3% 
	 75.3%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 76.1% 
	 75.8%  57.7%  70.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.8% 
	 80.9%  68.3%  54.2%  50.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 63.5% 
	 80.9%  77.9%  65.0%  43.4%  66.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.7% 
	 82.5%  78.8%  63.3%  43.4%  55.1%  59.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 63.8% 
	 80.4%  79.8%  66.7%  56.6%  68.2%  52.6%  55.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.6% 
	 78.4%  79.8%  60.0%  60.2%  68.2%  63.8%  52.0%  56.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.9% 
	 78.9%  78.8%  71.7%  64.6%  57.9%  66.4%  58.2%  56.0%  68.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.8% 
	 81.4%  80.8%  67.5%  68.1%  69.2%  67.2%  73.5%  61.5%  73.4%  54.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.7% 
	 77.8%  74.0%  63.3%  69.9%  71.0%  72.4%  71.4%  67.0%  73.4%  56.6%  50.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.9% 
	 77.3%  73.1%  64.2%  69.0%  70.1%  69.8%  75.5%  65.1%  69.7%  60.6%  49.0%  59.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.9% 
	 76.8%  76.9%  62.5%  73.5%  72.9%  71.6%  74.5%  63.3%  73.4%  70.7%  53.9%  53.8%  51.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.4% 
	 76.8%  72.1%  63.3%  72.6%  67.3%  75.9%  78.6%  68.8%  73.4%  72.7%  62.7%  58.7%  52.8%  51.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 77.3%  75.0%  65.0%  72.6%  75.7%  75.0%  78.6%  65.1%  68.8%  77.8%  63.7%  61.5%  50.9%  32.3%  74.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 77.8%  74.0%  64.2%  72.6%  74.8%  78.4%  77.6%  65.1%  73.4%  74.7%  64.7%  57.7%  57.5%  42.5%  66.0%  57.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.4% 
	 76.3%  76.9%  65.8%  73.5%  71.0%  76.7%  78.6%  66.1%  74.3%  79.8%  71.6%  67.3%  67.9%  47.2%  72.8%  45.0%  56.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.7% 
	 74.7%  74.0%  61.7%  73.5%  71.0%  74.1%  80.6%  67.0%  74.3%  79.8%  72.5%  68.3%  68.9%  46.5%  79.6%  49.0%  42.6%  76.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.6% 
	 73.2%  65.4%  71.7%  67.3%  70.1%  79.3%  78.6%  64.2%  74.3%  78.8%  71.6%  63.5%  70.8%  53.5%  82.5%  54.0%  51.5%  67.5%  73.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 72.2%  72.1%  59.2%  72.6%  62.6%  78.4%  79.6%  67.9%  75.2%  78.8%  74.5%  63.5%  72.6%  53.5%  82.5%  58.0%  52.5%  67.5%  63.7%  71.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.9% 
	 73.2%  74.0%  64.2%  69.9%  67.3%  78.4%  80.6%  67.9%  73.4%  76.8%  75.5%  64.4%  75.5%  52.8%  78.6%  62.0%  58.4%  71.5%  64.6%  56.6%  65.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.1% 
	 70.1%  70.2%  65.0%  69.0%  69.2%  78.4%  79.6%  64.2%  74.3%  76.8%  77.5%  69.2%  74.5%  59.8%  81.6%  57.0%  55.4%  71.5%  68.1%  57.5%  59.6%  68.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.0% 
	 74.7%  68.3%  66.7%  70.8%  65.4%  81.0%  78.6%  68.8%  77.1%  78.8%  75.5%  67.3%  70.8%  61.4%  82.5%  65.0%  59.4%  78.0%  71.7%  70.8%  61.5%  40.0%  72.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 69.9% 
	 74.7%  74.0%  64.2%  68.1%  57.9%  78.4%  79.6%  67.9%  73.4%  76.8%  74.5%  64.4%  70.8%  58.3%  79.6%  69.0%  59.4%  70.7%  75.2%  66.4%  59.6%  48.7%  36.7%  74.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 67.6% 
	 70.1%  73.1%  63.3%  68.1%  67.3%  80.2%  78.6%  70.6%  77.1%  79.8%  78.4%  68.3%  72.6%  58.3%  84.5%  73.0%  60.4%  74.0%  62.8%  72.6%  63.5%  44.3%  52.0%  40.3%  67.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 68.0% 
	 67.5%  76.0%  63.3%  68.1%  65.4%  78.4%  80.6%  66.1%  75.2%  76.8%  77.5%  64.4%  69.8%  57.5%  83.5%  69.0%  58.4%  79.7%  75.2%  74.3%  65.4%  47.0%  55.1%  43.7%  24.1%  66.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.5% 
	 68.0%  74.0%  58.3%  64.6%  67.3%  80.2%  80.6%  68.8%  75.2%  80.8%  77.5%  71.2%  68.9%  58.3%  82.5%  62.0%  60.4%  81.3%  77.0%  77.0%  67.3%  47.8%  53.1%  46.2%  30.4%  21.3%  66.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.4% 
	 70.6%  75.0%  62.5%  67.3%  70.1%  78.4%  78.6%  67.9%  77.1%  81.8%  78.4%  67.3%  74.5%  57.5%  82.5%  71.0%  57.4%  78.9%  75.2%  75.2%  65.4%  56.5%  59.2%  56.3%  38.4%  33.3%  26.6%  65.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 66.0% 
	 70.6%  68.3%  65.0%  67.3%  67.3%  78.4%  79.6%  67.9%  76.1%  78.8%  70.6%  68.3%  71.7%  60.6%  81.6%  67.0%  59.4%  78.9%  71.7%  74.3%  68.3%  59.1%  67.3%  56.3%  44.6%  38.0%  32.1%  40.5%  72.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.6% 
	 71.6%  68.3%  62.5%  65.5%  67.3%  79.3%  73.5%  67.9%  75.2%  83.8%  76.5%  66.3%  73.6%  57.5%  81.6%  72.0%  63.4%  79.7%  74.3%  70.8%  62.5%  58.3%  63.3%  56.3%  52.7%  38.0%  41.3%  47.7%  49.5%  67.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.6% 
	 71.1%  70.2%  63.3%  63.7%  68.2%  79.3%  75.5%  67.0%  77.1%  78.8%  74.5%  68.3%  70.8%  60.6%  82.5%  70.0%  62.4%  74.8%  74.3%  74.3%  67.3%  60.0%  67.3%  48.7%  52.7%  38.0%  45.9%  50.5%  48.6%  49.6%  70.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.4% 
	 66.5%  71.2%  60.8%  61.1%  68.2%  79.3%  70.4%  67.0%  76.1%  81.8%  73.5%  69.2%  74.5%  52.0%  79.6%  67.0%  59.4%  72.4%  70.8%  70.8%  63.5%  59.1%  64.3%  57.1%  54.5%  38.9%  43.1%  46.8%  53.3%  53.9%  52.6%  75.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.2% 
	 70.1%  65.4%  63.3%  62.8%  68.2%  79.3%  78.6%  69.7%  77.1%  79.8%  76.5%  70.2%  71.7%  57.5%  80.6%  70.0%  61.4%  81.3%  73.5%  76.1%  66.3%  56.5%  66.3%  53.8%  59.8%  41.7%  51.4%  55.0%  55.1%  58.3%  50.9%  38.4%  76.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.6% 
	 67.0%  68.3%  60.0%  62.8%  68.2%  81.0%  78.6%  65.1%  78.0%  85.9%  75.5%  68.3%  69.8%  55.9%  81.6%  66.0%  63.4%  78.0%  74.3%  73.5%  66.3%  56.5%  67.3%  60.5%  62.5%  43.5%  50.5%  55.9%  60.7%  57.4%  52.6%  54.4%  58.1%  69.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.8% 
	 69.1%  67.3%  60.0%  60.2%  65.4%  80.2%  76.5%  69.7%  76.1%  82.8%  77.5%  68.3%  73.6%  58.3%  80.6%  73.0%  59.4%  77.2%  75.2%  77.0%  65.4%  60.9%  70.4%  62.2%  60.7%  42.6%  46.8%  51.4%  61.7%  61.7%  51.7%  45.6%  56.4%  40.2%  54.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.6% 
	 69.6%  68.3%  61.7%  61.1%  68.2%  78.4%  77.6%  69.7%  75.2%  78.8%  76.5%  67.3%  72.6%  59.1%  83.5%  69.0%  64.4%  80.5%  74.3%  73.5%  67.3%  60.0%  72.4%  59.7%  59.8%  44.4%  52.3%  58.6%  58.9%  56.5%  53.4%  56.0%  59.8%  47.7%  24.1%  66.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.6% 
	 68.0%  69.2%  60.8%  56.6%  66.4%  81.0%  77.6%  69.7%  78.0%  76.8%  74.5%  69.2%  71.7%  54.3%  83.5%  68.0%  61.4%  77.2%  75.2%  74.3%  66.3%  61.7%  72.4%  60.5%  54.5%  42.6%  50.5%  55.9%  63.6%  60.0%  56.0%  64.0%  65.8%  57.0%  24.1%  55.4%  82.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 65.0% 
	 64.9%  66.3%  64.2%  60.2%  69.2%  81.0%  75.5%  68.8%  76.1%  80.8%  75.5%  67.3%  67.9%  59.1%  81.6%  64.0%  63.4%  77.2%  72.6%  74.3%  64.4%  61.7%  71.4%  60.5%  62.5%  35.2%  52.3%  58.6%  61.7%  63.5%  59.5%  52.8%  67.5%  63.6%  26.8%  53.5%  53.7%  67.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.4% 
	 69.6%  66.3%  60.0%  62.8%  58.9%  77.6%  77.6%  72.5%  76.1%  81.8%  73.5%  67.3%  75.5%  58.3%  80.6%  68.0%  60.4%  78.9%  77.0%  80.5%  65.4%  60.9%  68.4%  56.3%  61.6%  45.4%  54.1%  59.5%  64.5%  63.5%  56.9%  54.4%  65.8%  72.9%  37.5%  54.5%  61.8%  32.0%  51.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: 64.3% 
	 71.6%  70.2%  60.0%  61.9%  58.9%  76.7%  75.5%  68.8%  76.1%  77.8%  71.6%  70.2%  71.7%  59.1%  82.5%  71.0%  65.3%  79.7%  73.5%  72.6%  59.6%  60.0%  73.5%  63.9%  58.0%  51.9%  53.2%  57.7%  66.4%  59.1%  57.8%  51.2%  71.8%  69.2%  33.0%  58.4%  66.2%  30.4%  32.3%  63.4%   0.0%   0.0%   0.0%   0.0% 	Avg.: 63.8% 
	 67.5%  66.3%  64.2%  60.2%  62.6%  79.3%  76.5%  70.6%  76.1%  77.8%  75.5%  70.2%  73.6%  56.7%  81.6%  68.0%  67.3%  79.7%  75.2%  73.5%  64.4%  59.1%  72.4%  63.0%  66.1%  50.9%  53.2%  55.9%  64.5%  64.3%  59.5%  52.8%  72.6%  72.0%  38.4%  58.4%  66.2%  30.4%  39.6%  46.4%  73.7%   0.0%   0.0%   0.0% 	Avg.: 64.5% 
	 69.1%  67.3%  63.3%  61.1%  63.6%  80.2%  77.6%  68.8%  75.2%  82.8%  75.5%  69.2%  70.8%  58.3%  79.6%  70.0%  60.4%  78.0%  77.0%  72.6%  66.3%  64.3%  70.4%  64.7%  60.7%  50.0%  52.3%  53.2%  63.6%  62.6%  59.5%  62.4%  64.1%  71.0%  41.1%  58.4%  64.0%  36.0%  43.8%  44.6%  56.6%  74.8%   0.0%   0.0% 	Avg.: 64.4% 
	 69.1%  68.3%  59.2%  60.2%  64.5%  80.2%  76.5%  71.6%  75.2%  78.8%  73.5%  69.2%  72.6%  56.7%  82.5%  71.0%  64.4%  81.3%  73.5%  77.9%  64.4%  56.5%  70.4%  63.0%  61.6%  49.1%  48.6%  55.9%  65.4%  57.4%  57.8%  57.6%  72.6%  71.0%  42.9%  59.4%  66.9%  38.4%  49.0%  52.7%  51.5%  62.2%  55.0%   0.0% 	Avg.: 64.1% 
	 68.0%  70.2%  61.7%  59.3%  58.9%  81.0%  74.5%  67.0%  77.1%  78.8%  76.5%  69.2%  71.7%  55.9%  81.6%  68.0%  63.4%  79.7%  72.6%  81.4%  64.4%  54.8%  72.4%  63.9%  59.8%  53.7%  54.1%  56.8%  61.7%  63.5%  57.8%  61.6%  76.1%  71.0%  42.0%  59.4%  66.2%  44.0%  46.9%  51.8%  55.6%  63.1%  35.0%  69.3% 	Avg.: 64.1% 
************************************************************************************************************
TAw Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	-11.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:-11.9% 
	  0.5%  -4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.1% 
	 -0.5%  -1.9% -10.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.4% 
	  1.5%  -1.9%  -1.7% -15.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -4.3% 
	 -0.5%   0.0%  -0.8%  -3.5% -10.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -3.0% 
	  1.0%   1.0%   0.0%  -0.9%  -0.9% -15.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.6% 
	  1.0%   2.9%   0.8%   0.0%   0.0%  -1.7%  -4.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	  0.0%   0.0%   0.0%   1.8%   0.0%  -0.9%  -1.0% -10.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.3% 
	 -1.5%   0.0%   0.8%   0.0%  -2.8%   0.0%   1.0%   1.8% -15.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -1.8% 
	  0.5%   0.0%   0.8%   1.8%  -1.9%   0.0%   1.0%   0.0%  -0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.0%   0.0%   1.7%   0.0%   0.9%  -0.9%   1.0%   0.0%   0.0%  -4.0%  -4.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.5% 
	  1.0%   0.0%   1.7%   0.0%   0.0%  -0.9%   1.0%   0.9%   0.0%   0.0%  -2.0%  -4.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	  0.5%   0.0%   0.0%   0.9%   4.7%   0.0%   1.0%   0.9%   0.0%   0.0%   0.0%  -3.8%  -6.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -0.2% 
	  2.1%   0.0%  -2.5%   0.9%   0.0%   0.9%   1.0%   0.0%   0.9%   2.0%   0.0%   0.0%  -1.9%  -1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  1.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   1.6%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  0.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.9%   0.8%   0.0%  -3.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  2.1%   0.0%   0.0%  -1.8%   0.0%   0.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.9%   0.0%   0.0%   0.0%  -4.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  2.1%   0.0%   0.0%   1.8%   0.0%   0.0%   0.0%   0.9%   0.0%   3.0%   0.0%   0.0%   1.9%  -1.6%   0.0%  -2.0%   2.0%  -1.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.5%   0.0%   0.0%   1.8%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   1.9%  -0.8%   0.0%  -1.0%   0.0%   0.0%  -2.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.1% 
	  1.5%   0.0%  -1.7%   0.9%   0.0%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.9%  -0.8%   0.0%   0.0%  -1.0%   0.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  0.5%   0.0%   0.0%   0.9%   0.0%   0.0%   1.0%   0.9%   0.0%  -1.0%   0.0%   1.0%   0.0%  -0.8%   0.0%   0.0%   0.0%   0.0%   0.0%  -0.9%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.0% 
	  1.5%   0.0%   0.0%   0.0%  -0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   1.0%   1.9%   0.8%   1.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.0%   0.0%   0.0%   0.0%   0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   1.0%  -0.9%   2.4%   0.0%   0.0%   0.0%   0.0%   0.9%   0.9%   1.0%   0.0%  -2.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.3% 
	  1.5%   0.0%   0.0%   0.0%   2.8%   0.0%   1.0%  -0.9%   0.0%   0.0%   0.0%   1.0%   0.0%   0.8%   1.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.9%  -0.9%   0.0%  -5.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.2% 
	  1.5%   0.0%   0.0%   0.0%   1.9%   0.0%   1.0%   0.9%   0.0%   0.0%   0.0%   1.0%   0.9%   0.0%   1.0%  -1.0%   0.0%   0.0%   0.9%   0.9%   1.9%   0.0%   1.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  0.5%   0.0%   0.0%  -0.9%   1.9%   0.0%   0.0%   0.9%   0.0%   0.0%   0.0%   1.0%   2.8%  -1.6%   1.0%   0.0%   1.0%   0.0%   0.0%   0.9%  -1.0%   0.0%   0.0%   0.8%   0.0%   2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.4% 
	  1.0%   0.0%   0.0%   2.7%   1.9%   0.0%   1.0%   0.9%   0.0%   2.0%   0.0%   1.0%   0.9%   3.9%   1.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.0%   0.0%   0.0%   0.8%   0.0%   2.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.5%  -1.0%   0.0%   0.9%   1.9%   0.0%   1.0%   0.9%   0.0%   0.0%   0.0%   1.0%   2.8%   1.6%   1.0%   1.0%   0.0%   0.0%   0.0%   0.9%   1.0%   0.9%   0.0%   0.8%   0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  2.1%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   1.0%   0.9%   2.4%   1.0%   0.0%   0.0%   0.0%   0.0%   0.9%   1.9%   0.0%   1.0%   0.8%   0.0%   0.9%   0.0%   0.9%  -1.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.5%   0.0%   0.0%   2.7%   0.9%   0.0%   1.0%   0.9%   0.0%   2.0%   0.0%   1.0%   0.9%   3.1%   1.0%   1.0%   0.0%   0.0%   0.0%   0.9%   1.9%   0.0%   0.0%   0.0%   0.0%   0.9%   0.0%   0.9%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.0%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   0.9%   0.0%   3.0%   0.0%   1.0%   0.9%   2.4%   1.0%   4.0%   0.0%   0.0%   0.0%   1.8%   1.9%   0.0%   1.0%   0.0%  -0.9%   0.9%   0.0%   0.9%   0.0%  -0.9%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  2.1%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   0.9%   0.0%   3.0%   0.0%   1.0%   2.8%   3.1%   1.0%   3.0%   1.0%   0.0%   0.0%   0.9%   1.0%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.9%   0.0%   1.7%  -0.9%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  0.5%   0.0%   0.0%   0.9%   1.9%   0.0%   1.0%   1.8%   0.0%   0.0%   1.0%   1.0%   0.0%   3.1%   1.0%   4.0%   2.0%   0.0%   0.0%   0.9%   1.9%   0.0%   0.0%   0.8%   0.0%   0.9%   0.0%   0.9%   0.0%   0.9%   0.9%  -0.8%  -1.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.0%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   0.9%   0.0%   0.0%   1.0%   1.0%   0.9%   3.1%   1.0%   1.0%   1.0%   0.0%   0.0%   0.9%   1.0%   0.0%   1.0%   0.8%   0.0%   1.9%   0.0%   1.8%   0.0%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  2.1%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   0.9%   0.0%   1.0%   2.0%   1.0%  -0.9%   3.1%   1.0%   0.0%   2.0%   0.0%   0.0%   0.9%   1.0%   0.0%   1.0%   0.8%   0.0%   1.9%  -0.9%   0.9%   0.0%   1.7%   0.0%   3.2%   0.0%   0.0%   3.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  1.5%   0.0%   0.0%   1.8%   1.9%   0.0%   1.0%   0.9%   0.0%   3.0%   2.0%   1.0%   0.9%   2.4%   1.0%   2.0%   2.0%   0.0%   0.0%   0.9%   1.9%   0.0%   1.0%   0.8%   0.0%   1.9%   0.0%   0.9%   0.0%   0.9%   0.0%  -0.8%   0.0%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.5%   0.0%   0.0%   1.8%   1.9%   0.0%   1.0%   0.0%   0.0%   3.0%   2.0%   1.0%   0.9%   1.6%   1.0%   4.0%   2.0%   0.0%   0.0%   0.9%   1.9%   0.0%   1.0%   0.8%   0.0%   0.9%   0.0%   1.8%   0.0%   0.9%   0.0%   0.8%   0.0%   0.0%   0.0%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.0%   0.0%   0.0%   0.9%   1.9%   0.0%   1.0%   0.9%   0.0%   3.0%   2.0%   1.0%   0.9%   2.4%   1.0%   4.0%   0.0%   0.0%   0.0%   0.9%   1.0%   0.0%   1.0%   0.8%   0.0%   0.9%   0.0%   1.8%  -0.9%   0.9%   0.0%   0.8%   0.0%   0.0%  -0.9%   0.0%   0.0%   0.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.0%   0.0%   0.0%   0.9%   1.9%   0.9%   1.0%   0.0%   0.0%   3.0%   2.0%   1.0%   0.9%   3.1%   1.0%   3.0%   2.0%   0.0%   0.0%   0.9%   1.0%   0.0%   1.0%   0.8%   0.0%   1.9%   0.0%   1.8%   0.0%   0.9%   0.0%   2.4%   0.0%   0.0%   0.9%  -2.0%   0.0%  -0.8%  -1.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.5%   0.0%   0.0%   0.9%   0.9%   0.0%   1.0%   0.9%   0.0%   2.0%   2.0%   1.0%   0.9%   1.6%   1.0%   3.0%   1.0%   0.0%   0.0%   0.9%   1.0%   0.0%   1.0%   0.8%   0.0%   0.0%   0.0%   2.7%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   0.9%   0.0%  -0.7%   0.8%   0.0%  -1.8%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.6% 
	  3.1%   0.0%   0.0%   0.9%   0.9%   0.0%   1.0%   0.9%   0.0%   0.0%   2.0%   1.0%   2.8%   2.4%   1.0%   2.0%   1.0%   0.0%   0.0%   0.9%   1.9%   0.0%   1.0%   0.8%   0.0%   0.9%   0.0%   1.8%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   1.8%   0.0%   0.7%   0.0%  -1.0%  -0.9%  -1.0%   0.0%   0.0%   0.0% 	Avg.:  0.7% 
	  1.0%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   0.0%   0.0%   2.0%   2.0%   1.0%   0.9%   0.8%   1.0%   3.0%   1.0%   0.0%   0.0%   0.9%   1.0%   0.0%   1.0%   0.8%   0.9%   0.9%   0.0%   1.8%   0.0%   0.9%   0.0%   1.6%   0.9%   0.9%   1.8%   0.0%   0.7%   0.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  1.5%   0.0%   0.0%   1.8%   0.9%   0.0%   1.0%   1.8%   0.0%   0.0%   2.0%   1.0%   0.9%   0.8%   1.0%   3.0%   2.0%   0.0%   0.0%   0.9%   1.9%  -1.7%   1.0%   0.8%   0.0%   0.9%   0.0%   2.7%   0.0%   0.9%   0.0%   0.0%   0.9%   0.9%   0.9%   0.0%   0.7%   0.8%   1.0%   0.9%   1.0%   0.0%  -2.0%   0.0% 	Avg.:  0.7% 
************************************************************************************************************
TAg Forg
	  0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 
	  1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.0% 
	  0.5%  19.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.9% 
	 -4.6%   8.7%  15.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.6% 
	  0.0%  -1.0%   5.0%   7.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.8% 
	 -1.5%  -1.0%   6.7%   7.1%  11.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  4.5% 
	  2.1%  -1.0%   3.3%  -6.2%  -1.9%   6.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  4.1%   0.0%  10.0%  -3.5%   0.0%  -4.3%   3.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.3% 
	  3.6%   1.0%  -1.7%  -4.4%  10.3%  -2.6%  -3.1%   0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  1.0%  -1.0%   4.2%  -3.5%  -0.9%  -0.9% -15.3%  -4.6%  -4.6%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.: -2.8% 
	  4.6%   6.7%   8.3%  -1.8%  -1.9%  -5.2%   2.0%  -5.5%   0.0%  -2.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.5% 
	  5.2%   7.7%   7.5%   0.9%   0.9%   2.6%  -2.0%   1.8%   3.7%  -4.0%   1.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.3% 
	  5.7%   3.8%   9.2%  -3.5%  -1.9%   0.9%   1.0%   3.7%   0.0% -10.1%  -3.9%   5.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.9% 
	  5.7%   8.7%   8.3%   0.9%   5.6%  -3.4%  -3.1%  -1.8%   0.0%  -2.0%  -8.8%   1.0%  -0.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  0.8% 
	  5.2%   5.8%   6.7%   0.9%  -2.8%   0.9%   0.0%   3.7%   4.6%  -5.1%  -1.0%  -1.9%   1.9%  18.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.7% 
	  4.6%   6.7%   7.5%   0.9%   0.9%  -2.6%   1.0%   3.7%   0.0%   3.0%  -1.0%   3.8%  -4.7%   8.7%   8.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  2.8% 
	  6.2%   3.8%   5.8%   0.0%   4.7%   1.7%   0.0%   2.8%  -0.9%  -2.0%  -6.9%  -5.8% -10.4%   3.9%   1.9%  12.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  1.1% 
	  7.7%   6.7%  10.0%   0.0%   4.7%   4.3%  -2.0%   1.8%   0.0%   0.0%  -1.0%  -1.0%  -0.9%   4.7%  -4.9%   8.0%  13.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.1% 
	  9.3%  15.4%   0.0%   6.2%   5.6%  -0.9%   2.0%   4.6%   0.0%   1.0%   1.0%   4.8%  -1.9%  -2.4%  -2.9%   3.0%   5.0%   8.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.3% 
	 10.3%   8.7%  12.5%   0.9%  13.1%   0.9%   1.0%   0.9%  -0.9%   1.0%  -2.0%   4.8%  -1.9%   0.0%   0.0%  -1.0%   4.0%   8.9%   9.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.7% 
	  9.3%   6.7%   7.5%   3.5%   8.4%   0.9%   0.0%   0.9%   1.8%   3.0%  -1.0%   3.8%  -2.8%   0.8%   3.9%  -4.0%  -2.0%   4.9%   8.8%  15.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.5% 
	 12.4%  10.6%   6.7%   4.4%   6.5%   0.9%   1.0%   4.6%   0.9%   3.0%  -2.0%  -1.0%   0.9%  -6.3%   1.0%   5.0%   3.0%   4.9%   5.3%  14.2%   5.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.9% 
	  7.7%  12.5%   5.0%   2.7%  10.3%  -1.7%   2.0%   0.0%  -1.8%   1.0%   2.0%   1.9%   4.7%  -1.6%   0.0%  -3.0%  -1.0%  -1.6%   1.8%   0.9%   3.8%  28.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  3.4% 
	  7.7%   6.7%   7.5%   5.3%  17.8%   2.6%   1.0%   0.9%   3.7%   3.0%   2.9%   4.8%   4.7%   3.1%   2.9%  -4.0%   0.0%   7.3%  -1.8%   5.3%   5.8%  20.0%  35.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  6.2% 
	 12.4%   7.7%   8.3%   5.3%   8.4%   0.9%   2.0%  -1.8%   0.0%   0.0%  -1.0%   1.0%   2.8%   3.1%  -1.9%  -4.0%  -1.0%   4.1%  12.4%  -0.9%   1.9%  24.3%  20.4%  34.5%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  5.8% 
	 14.9%   4.8%   8.3%   5.3%  10.3%   2.6%   0.0%   4.6%   1.8%   3.0%   1.0%   4.8%   5.7%   3.9%   1.0%   4.0%   2.0%  -1.6%   0.0%  -1.8%   0.0%  21.7%  17.3%  31.1%  43.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  7.5% 
	 14.4%   6.7%  13.3%   8.8%   8.4%   0.9%   0.0%   1.8%   1.8%  -1.0%   1.0%  -1.9%   6.6%   3.1%   1.9%  11.0%   0.0%  -1.6%  -1.8%  -2.7%  -1.9%  20.9%  19.4%  28.6%  37.5%  45.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 11.9%   5.8%   9.2%   6.2%   5.6%   2.6%   2.0%   2.8%   0.0%  -1.0%   0.0%   3.8%   0.9%   3.9%   1.9%   2.0%   3.0%   2.4%   1.8%   1.8%   1.9%  12.2%  13.3%  18.5%  29.5%  33.3%  39.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.0% 
	 11.9%  12.5%   6.7%   6.2%   8.4%   2.6%   1.0%   2.8%   0.9%   3.0%   7.8%   2.9%   3.8%   0.8%   2.9%   6.0%   1.0%   2.4%   5.3%   2.7%  -1.0%   9.6%   5.1%  18.5%  23.2%  28.7%  33.9%  25.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 10.8%  12.5%   9.2%   8.0%   8.4%   1.7%   7.1%   2.8%   1.8%  -2.0%   2.0%   4.8%   1.9%   3.9%   2.9%   1.0%  -3.0%   1.6%   2.7%   6.2%   5.8%  10.4%   9.2%  18.5%  15.2%  28.7%  24.8%  18.0%  23.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.2% 
	 11.3%  10.6%   8.3%   9.7%   7.5%   1.7%   5.1%   3.7%   0.0%   5.1%   3.9%   2.9%   4.7%   0.8%   1.9%   3.0%   1.0%   6.5%   2.7%   2.7%   1.0%   8.7%   5.1%  26.1%  15.2%  28.7%  20.2%  15.3%  24.3%  18.3%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 16.0%   9.6%  10.8%  12.4%   7.5%   1.7%  10.2%   3.7%   0.9%   2.0%   4.9%   1.9%   0.9%   9.4%   4.9%   6.0%   4.0%   8.9%   6.2%   6.2%   4.8%   9.6%   8.2%  17.6%  13.4%  27.8%  22.9%  18.9%  19.6%  13.9%  18.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.8% 
	 12.4%  15.4%   8.3%  10.6%   7.5%   1.7%   2.0%   0.9%   0.0%   4.0%   2.0%   1.0%   3.8%   3.9%   3.9%   3.0%   2.0%   0.0%   3.5%   0.9%   1.9%  12.2%   6.1%  21.0%   8.0%  25.0%  14.7%  10.8%  17.8%   9.6%  19.8%  36.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 15.5%  12.5%  11.7%  10.6%   7.5%   0.0%   2.0%   5.5%  -0.9%  -2.0%   2.9%   2.9%   5.7%   5.5%   2.9%   7.0%   0.0%   3.3%   2.7%   3.5%   1.9%  12.2%   5.1%  14.3%   5.4%  23.1%  15.6%   9.9%  12.1%  10.4%  18.1%  20.8%  18.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.1% 
	 13.4%  13.5%  11.7%  13.3%  10.3%   0.9%   4.1%   0.9%   1.8%   3.0%   1.0%   2.9%   1.9%   3.1%   3.9%   0.0%   4.0%   4.1%   1.8%   0.0%   2.9%   7.8%   2.0%  12.6%   7.1%  24.1%  19.3%  14.4%  11.2%   6.1%  19.0%  29.6%  20.5%  29.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.9% 
	 12.9%  12.5%  10.0%  12.4%   7.5%   2.6%   3.1%   0.9%   2.8%   7.1%   2.0%   3.8%   2.8%   2.4%   1.0%   4.0%  -1.0%   0.8%   2.7%   3.5%   1.0%   8.7%   0.0%  15.1%   8.0%  22.2%  13.8%   7.2%  14.0%  11.3%  17.2%  19.2%  17.1%  21.5%  30.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.6% 
	 14.4%  11.5%  10.8%  16.8%   9.3%   0.0%   3.1%   0.9%   0.0%   9.1%   3.9%   1.9%   3.8%   7.1%   1.0%   5.0%   3.0%   4.1%   1.8%   2.7%   1.9%   7.0%   0.0%  14.3%  13.4%  24.1%  15.6%   9.9%   9.3%   7.8%  14.7%  11.2%  11.1%  12.1%  30.4%  10.9%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 17.5%  14.4%   7.5%  13.3%   6.5%   0.0%   5.1%   1.8%   1.8%   5.1%   2.9%   3.8%   7.5%   2.4%   2.9%   9.0%   1.0%   4.1%   4.4%   2.7%   3.8%   7.0%   1.0%  14.3%   5.4%  31.5%  13.8%   7.2%  11.2%   4.3%  11.2%  22.4%   9.4%   5.6%  27.7%  12.9%  28.7%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.0% 
	 12.9%  14.4%  11.7%  10.6%  16.8%   3.4%   3.1%  -1.8%   1.8%   4.0%   4.9%   3.8%   0.0%   3.1%   3.9%   5.0%   4.0%   2.4%   0.0%  -3.5%   2.9%   7.8%   4.1%  18.5%   6.2%  21.3%  11.9%   6.3%   8.4%   4.3%  13.8%  20.8%  11.1%  -3.7%  17.0%  11.9%  20.6%  35.2%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.4% 
	 10.8%  10.6%  11.7%  11.5%  16.8%   4.3%   5.1%   3.7%   1.8%   8.1%   6.9%   1.0%   3.8%   2.4%   1.9%   2.0%  -1.0%   1.6%   3.5%   8.0%   8.7%   8.7%  -1.0%  10.9%   9.8%  14.8%  12.8%   8.1%   6.5%   8.7%  12.9%  24.0%   5.1%   3.7%  21.4%   7.9%  16.2%  36.8%  18.7%   0.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  9.0% 
	 14.9%  14.4%   7.5%  13.3%  13.1%   1.7%   4.1%   1.8%   1.8%   8.1%   2.9%   1.0%   1.9%   4.7%   2.9%   5.0%  -2.0%   1.6%   1.8%   7.1%   3.8%   9.6%   1.0%  11.8%   1.8%  15.7%  12.8%   9.9%   8.4%   3.5%  11.2%  22.4%   4.3%   0.9%  16.1%   7.9%  16.2%  36.8%  11.5%  17.0%   0.0%   0.0%   0.0%   0.0% 	Avg.:  8.3% 
	 13.4%  13.5%   8.3%  12.4%  12.1%   0.9%   3.1%   3.7%   2.8%   3.0%   2.9%   1.9%   4.7%   3.1%   4.9%   3.0%   6.9%   3.3%   0.0%   8.0%   1.9%   4.3%   3.1%  10.1%   7.1%  16.7%  13.8%  12.6%   9.3%   5.2%  11.2%  12.8%  12.8%   1.9%  13.4%   7.9%  18.4%  31.2%   7.3%  18.7%  17.2%   0.0%   0.0%   0.0% 	Avg.:  8.5% 
	 13.4%  12.5%  12.5%  13.3%  11.2%   0.9%   4.1%   0.9%   2.8%   7.1%   4.9%   1.9%   2.8%   4.7%   1.9%   2.0%   3.0%   0.0%   3.5%   2.7%   3.8%  12.2%   3.1%  11.8%   6.2%  17.6%  17.4%   9.9%   7.5%  10.4%  12.9%  17.6%   4.3%   1.9%  11.6%   6.9%  15.4%  28.8%   2.1%  10.7%  22.2%  12.6%   0.0%   0.0% 	Avg.:  8.4% 
	 14.4%  10.6%  10.0%  14.2%  16.8%   0.0%   6.1%   5.5%   0.9%   7.1%   2.0%   1.9%   3.8%   5.5%   2.9%   5.0%   4.0%   1.6%   4.4%  -0.9%   3.8%  13.9%   1.0%  10.9%   8.0%  13.0%  11.9%   9.0%  11.2%   4.3%  12.9%  13.6%   0.9%   1.9%  12.5%   6.9%  16.2%  23.2%   4.2%  11.6%  18.2%  11.7%  20.0%   0.0% 	Avg.:  8.3% 
************************************************************************************************************
[Elapsed time = 0.4 h]
Done!

f1_score_micro: 0.6412446613788896
f1_score_macro: 0.6053778157368183
              precision    recall  f1-score   support

           0       0.33      1.00      0.50         4
           1       0.18      0.50      0.27         4
           2       1.00      1.00      1.00         9
           3       0.54      0.78      0.64         9
           4       1.00      0.78      0.88         9
           5       0.12      0.25      0.17         4
           6       0.75      0.75      0.75         4
           7       0.40      1.00      0.57         4
           8       0.20      0.75      0.32         4
           9       0.00      0.00      0.00         9
          10       0.12      0.44      0.20         9
          11       0.67      1.00      0.80         4
          12       0.57      1.00      0.73         4
          13       1.00      0.75      0.86         4
          14       0.60      0.75      0.67         4
          15       0.06      0.11      0.07         9
          16       0.60      1.00      0.75         9
          17       1.00      1.00      1.00         4
          18       0.29      0.50      0.36         4
          19       0.42      1.00      0.59         5
          20       0.00      0.00      0.00         4
          21       0.71      1.00      0.83         5
          22       0.00      0.00      0.00         4
          23       0.00      0.00      0.00         4
          24       0.64      1.00      0.78         9
          25       0.50      0.44      0.47         9
          26       1.00      1.00      1.00         4
          27       1.00      0.89      0.94         9
          28       0.00      0.00      0.00         4
          29       0.80      1.00      0.89         4
          30       1.00      0.80      0.89         5
          31       0.13      0.50      0.21         4
          32       0.75      1.00      0.86         9
          33       0.80      1.00      0.89         4
          34       0.67      1.00      0.80         4
          35       0.25      0.40      0.31         5
          36       0.60      0.75      0.67         4
          37       0.50      0.80      0.62         5
          38       1.00      0.89      0.94         9
          39       1.00      0.75      0.86         4
          40       0.07      0.25      0.11         4
          41       0.57      1.00      0.73         4
          42       0.09      0.33      0.14         9
          43       0.00      0.00      0.00         4
          44       1.00      0.75      0.86         4
          45       1.00      0.75      0.86         4
          46       0.83      1.00      0.91         5
          47       0.40      0.50      0.44         4
          48       1.00      0.75      0.86         4
          49       0.44      1.00      0.62         4
          50       0.25      0.60      0.35         5
          51       1.00      0.89      0.94         9
          52       1.00      0.25      0.40         4
          53       0.69      1.00      0.82         9
          54       0.14      1.00      0.25         4
          55       0.00      0.00      0.00         4
          56       0.78      0.78      0.78         9
          57       0.71      1.00      0.83         5
          58       0.29      0.50      0.36         4
          59       0.56      1.00      0.71         5
          60       0.00      0.00      0.00         9
          61       0.33      0.50      0.40         4
          62       0.80      1.00      0.89         4
          63       1.00      1.00      1.00         5
          64       0.50      0.40      0.44         5
          65       0.22      0.40      0.29         5
          66       0.00      0.00      0.00         4
          67       0.60      0.67      0.63         9
          68       0.64      0.78      0.70         9
          69       0.00      0.00      0.00         9
          70       0.67      1.00      0.80         4
          71       0.67      0.89      0.76         9
          72       1.00      0.89      0.94         9
          73       0.38      0.75      0.50         4
          74       0.10      0.11      0.11         9
          75       0.60      0.67      0.63         9
          76       0.33      0.60      0.43         5
          77       0.29      0.40      0.33         5
          78       0.57      1.00      0.73         4
          79       0.75      1.00      0.86         9
          80       0.57      1.00      0.73         4
          81       0.80      1.00      0.89         4
          82       0.00      0.00      0.00         4
          83       0.60      0.75      0.67         4
          84       0.10      0.25      0.14         4
          85       0.44      0.44      0.44         9
          86       0.60      0.75      0.67         4
          87       1.00      1.00      1.00         4
          88       0.80      1.00      0.89         4
          89       0.62      1.00      0.77         5
          90       0.64      0.78      0.70         9
          91       0.33      0.33      0.33         9
          92       0.00      0.00      0.00         4
          93       0.00      0.00      0.00         4
          94       0.75      0.75      0.75         4
          95       0.18      0.50      0.27         4
          96       0.67      1.00      0.80         4
          97       0.57      1.00      0.73         4
          98       0.00      0.00      0.00         9
          99       1.00      0.75      0.86         4
         100       0.80      1.00      0.89         4
         101       0.00      0.00      0.00         9
         102       1.00      1.00      1.00         9
         103       0.83      1.00      0.91         5
         104       0.62      0.56      0.59         9
         105       0.33      0.25      0.29         4
         106       1.00      1.00      1.00         4
         107       0.70      0.78      0.74         9
         108       0.25      0.75      0.38         4
         109       0.83      1.00      0.91         5
         110       0.14      0.25      0.18         4
         111       0.50      0.75      0.60         4
         112       0.00      0.00      0.00         4
         113       0.00      0.00      0.00         4
         114       0.50      0.50      0.50         4
         115       0.58      0.78      0.67         9
         116       0.45      0.56      0.50         9
         117       1.00      1.00      1.00         5
         118       0.50      0.75      0.60         4
         119       0.17      0.25      0.20         4
         120       0.88      0.78      0.82         9
         121       0.43      0.75      0.55         4
         122       0.80      0.80      0.80         5
         123       0.80      1.00      0.89         4
         124       0.82      1.00      0.90         9
         125       0.57      0.89      0.70         9
         126       0.50      0.60      0.55         5
         127       0.67      1.00      0.80         4
         128       0.80      1.00      0.89         4
         129       0.71      1.00      0.83         5
         130       0.33      0.40      0.36         5
         131       0.71      1.00      0.83         5
         132       0.82      1.00      0.90         9
         133       0.50      1.00      0.67         4
         134       0.75      0.75      0.75         4
         135       1.00      0.75      0.86         4
         136       0.50      1.00      0.67         4
         137       1.00      0.75      0.86         4
         138       0.57      1.00      0.73         4
         139       0.00      0.00      0.00         4
         140       0.60      0.75      0.67         4
         141       1.00      1.00      1.00         4
         142       1.00      0.75      0.86         4
         143       1.00      1.00      1.00         4
         144       1.00      0.80      0.89         5
         145       0.60      0.75      0.67         4
         146       0.64      0.78      0.70         9
         147       0.00      0.00      0.00         4
         148       0.50      0.20      0.29         5
         149       0.88      0.78      0.82         9
         150       1.00      0.88      0.93         8
         151       0.67      1.00      0.80         4
         152       1.00      0.80      0.89         5
         153       0.83      1.00      0.91         5
         154       0.89      0.89      0.89         9
         155       1.00      0.50      0.67         4
         156       1.00      0.75      0.86         4
         157       1.00      0.89      0.94         9
         158       0.00      0.00      0.00         4
         159       1.00      1.00      1.00         4
         160       0.43      0.60      0.50         5
         161       0.57      0.80      0.67         5
         162       0.67      0.50      0.57         4
         163       0.50      0.44      0.47         9
         164       0.60      0.75      0.67         4
         165       1.00      0.78      0.88         9
         166       1.00      1.00      1.00         4
         167       0.00      0.00      0.00         4
         168       1.00      1.00      1.00         5
         169       0.60      0.75      0.67         4
         170       0.67      0.22      0.33         9
         171       1.00      1.00      1.00         4
         172       1.00      1.00      1.00         4
         173       0.75      0.60      0.67         5
         174       0.75      0.75      0.75         4
         175       0.50      0.80      0.62         5
         176       0.83      1.00      0.91         5
         177       0.80      1.00      0.89         4
         178       0.90      1.00      0.95         9
         179       1.00      0.89      0.94         9
         180       0.50      0.56      0.53         9
         181       0.50      0.50      0.50         4
         182       1.00      0.50      0.67         4
         183       0.60      0.75      0.67         4
         184       1.00      0.75      0.86         4
         185       0.80      1.00      0.89         4
         186       1.00      0.40      0.57         5
         187       0.50      1.00      0.67         4
         188       1.00      1.00      1.00         4
         189       0.83      1.00      0.91         5
         190       0.00      0.00      0.00         4
         191       0.82      1.00      0.90         9
         192       1.00      0.25      0.40         4
         193       0.88      0.78      0.82         9
         194       1.00      1.00      1.00         4
         195       0.67      1.00      0.80         4
         196       1.00      1.00      1.00         4
         197       1.00      1.00      1.00         9
         198       0.00      0.00      0.00         5
         199       0.00      0.00      0.00         4
         200       0.44      1.00      0.62         4
         201       0.80      1.00      0.89         4
         202       0.11      0.50      0.17         4
         203       0.73      0.89      0.80         9
         204       0.80      0.80      0.80         5
         205       0.50      1.00      0.67         4
         206       0.80      0.89      0.84         9
         207       0.60      0.75      0.67         4
         208       0.00      0.00      0.00         4
         209       1.00      1.00      1.00         4
         210       0.67      1.00      0.80         4
         211       0.75      0.75      0.75         4
         212       0.67      0.80      0.73         5
         213       0.83      1.00      0.91         5
         214       0.88      0.78      0.82         9
         215       1.00      0.80      0.89         5
         216       0.80      1.00      0.89         4
         217       1.00      0.50      0.67         4
         218       0.80      1.00      0.89         4
         219       0.80      1.00      0.89         4
         220       0.57      1.00      0.73         4
         221       0.00      0.00      0.00         4
         222       0.82      1.00      0.90         9
         223       0.33      0.40      0.36         5
         224       1.00      1.00      1.00         4
         225       0.38      0.75      0.50         4
         226       0.88      0.78      0.82         9
         227       0.40      1.00      0.57         4
         228       0.67      1.00      0.80         4
         229       1.00      0.50      0.67         4
         230       1.00      1.00      1.00         9
         231       0.33      0.25      0.29         4
         232       0.60      0.75      0.67         4
         233       1.00      0.25      0.40         4
         234       0.89      0.89      0.89         9
         235       0.75      0.75      0.75         4
         236       0.60      0.75      0.67         4
         237       0.69      1.00      0.82         9
         238       1.00      0.50      0.67         4
         239       0.82      1.00      0.90         9
         240       0.00      0.00      0.00         4
         241       0.00      0.00      0.00         4
         242       0.17      0.11      0.13         9
         243       0.83      1.00      0.91         5
         244       0.80      0.80      0.80         5
         245       1.00      0.50      0.67         4
         246       0.80      1.00      0.89         4
         247       0.00      0.00      0.00         4
         248       0.67      1.00      0.80         4
         249       0.67      1.00      0.80         4
         250       0.50      1.00      0.67         4
         251       1.00      1.00      1.00         5
         252       0.00      0.00      0.00         4
         253       0.62      1.00      0.77         5
         254       1.00      0.89      0.94         9
         255       1.00      1.00      1.00         5
         256       0.00      0.00      0.00         4
         257       0.57      1.00      0.73         4
         258       0.75      0.67      0.71         9
         259       1.00      0.75      0.86         4
         260       1.00      0.60      0.75         5
         261       0.45      1.00      0.62         5
         262       0.38      0.75      0.50         4
         263       0.00      0.00      0.00         4
         264       1.00      1.00      1.00         9
         265       0.57      1.00      0.73         4
         266       1.00      0.75      0.86         4
         267       0.75      0.75      0.75         4
         268       0.67      0.50      0.57         4
         269       0.50      0.75      0.60         4
         270       0.27      0.67      0.38         6
         271       1.00      0.60      0.75         5
         272       0.00      0.00      0.00         4
         273       0.89      0.89      0.89         9
         274       0.78      0.78      0.78         9
         275       1.00      1.00      1.00         9
         276       0.83      1.00      0.91         5
         277       0.38      0.33      0.35         9
         278       0.00      0.00      0.00         4
         279       0.00      0.00      0.00         9
         280       0.60      0.67      0.63         9
         281       1.00      0.50      0.67         4
         282       0.75      0.75      0.75         4
         283       0.50      0.25      0.33         4
         284       0.00      0.00      0.00         4
         285       0.67      1.00      0.80         4
         286       1.00      1.00      1.00         4
         287       1.00      0.89      0.94         9
         288       0.50      0.44      0.47         9
         289       0.20      0.22      0.21         9
         290       0.17      0.11      0.13         9
         291       1.00      0.75      0.86         4
         292       0.80      1.00      0.89         4
         293       0.62      1.00      0.77         5
         294       1.00      1.00      1.00         4
         295       0.67      1.00      0.80         4
         296       0.67      0.50      0.57         4
         297       0.88      0.78      0.82         9
         298       0.50      1.00      0.67         4
         299       1.00      1.00      1.00         4
         300       0.75      0.75      0.75         4
         301       0.80      1.00      0.89         4
         302       1.00      1.00      1.00         4
         303       0.80      0.89      0.84         9
         304       0.82      1.00      0.90         9
         305       0.25      0.20      0.22         5
         306       0.50      1.00      0.67         5
         307       0.83      1.00      0.91         5
         308       1.00      1.00      1.00         4
         309       1.00      1.00      1.00         4
         310       1.00      0.75      0.86         4
         311       0.80      1.00      0.89         4
         312       0.67      0.44      0.53         9
         313       0.05      0.25      0.08         4
         314       1.00      1.00      1.00         4
         315       0.00      0.00      0.00         4
         316       0.75      0.75      0.75         4
         317       0.80      1.00      0.89         4
         318       0.67      0.44      0.53         9
         319       0.80      1.00      0.89         4
         320       0.25      0.75      0.38         4
         321       0.00      0.00      0.00         4
         322       0.75      0.75      0.75         4
         323       0.83      1.00      0.91         5
         324       0.64      0.78      0.70         9
         325       0.67      0.50      0.57         4
         326       0.75      0.75      0.75         4
         327       1.00      1.00      1.00         4
         328       1.00      0.75      0.86         4
         329       0.67      1.00      0.80         4
         330       1.00      0.89      0.94         9
         331       0.80      1.00      0.89         4
         332       0.67      0.40      0.50         5
         333       0.17      0.14      0.15         7
         334       0.64      1.00      0.78         9
         335       0.12      0.25      0.17         4
         336       0.80      0.89      0.84         9
         337       0.15      0.50      0.24         4
         338       0.83      1.00      0.91         5
         339       0.60      0.75      0.67         4
         340       0.75      0.75      0.75         4
         341       0.67      0.50      0.57         4
         342       0.50      0.75      0.60         4
         343       0.50      0.50      0.50         4
         344       1.00      0.25      0.40         4
         345       0.50      0.50      0.50         4
         346       0.40      0.40      0.40         5
         347       0.88      0.78      0.82         9
         348       0.60      0.43      0.50         7
         349       1.00      1.00      1.00         4
         350       0.43      0.60      0.50         5
         351       0.80      1.00      0.89         4
         352       0.00      0.00      0.00         4
         353       0.00      0.00      0.00         4
         354       0.50      0.78      0.61         9
         355       0.80      1.00      0.89         4
         356       0.64      0.78      0.70         9
         357       1.00      0.75      0.86         4
         358       0.67      1.00      0.80         4
         359       1.00      0.78      0.88         9
         360       1.00      0.75      0.86         4
         361       0.88      0.78      0.82         9
         362       0.57      0.80      0.67         5
         363       0.67      1.00      0.80         4
         364       1.00      1.00      1.00         5
         365       0.88      0.78      0.82         9
         366       1.00      1.00      1.00         5
         367       0.33      1.00      0.50         4
         368       0.67      1.00      0.80         4
         369       0.75      1.00      0.86         9
         370       0.50      0.50      0.50         4
         371       0.80      0.44      0.57         9
         372       0.64      0.78      0.70         9
         373       0.33      0.25      0.29         4
         374       1.00      0.80      0.89         5
         375       0.78      0.78      0.78         9
         376       1.00      0.78      0.88         9
         377       1.00      0.89      0.94         9
         378       0.44      1.00      0.62         4
         379       1.00      1.00      1.00         5
         380       1.00      0.75      0.86         4
         381       0.20      0.25      0.22         4
         382       1.00      1.00      1.00         5
         383       0.00      0.00      0.00         9
         384       0.20      0.25      0.22         4
         385       1.00      1.00      1.00         4
         386       0.50      0.50      0.50         4
         387       0.80      0.89      0.84         9
         388       1.00      1.00      1.00         4
         389       0.80      1.00      0.89         4
         390       0.10      0.25      0.14         4
         391       1.00      1.00      1.00         4
         392       0.78      0.78      0.78         9
         393       0.75      0.75      0.75         4
         394       0.43      0.75      0.55         4
         395       0.67      1.00      0.80         4
         396       1.00      0.89      0.94         9
         397       1.00      0.25      0.40         4
         398       0.60      1.00      0.75         9
         399       0.90      1.00      0.95         9
         400       1.00      0.78      0.88         9
         401       0.57      1.00      0.73         4
         402       1.00      1.00      1.00         4
         403       0.80      1.00      0.89         4
         404       1.00      1.00      1.00         4
         405       0.13      0.50      0.21         4
         406       1.00      1.00      1.00         5
         407       0.71      0.56      0.63         9
         408       1.00      0.80      0.89         5
         409       0.75      0.75      0.75         4
         410       0.60      0.60      0.60         5
         411       1.00      0.75      0.86         4
         412       0.44      1.00      0.62         4
         413       0.60      0.67      0.63         9
         414       0.00      0.00      0.00         4
         415       0.71      1.00      0.83         5
         416       0.50      0.75      0.60         4
         417       1.00      1.00      1.00         4
         418       0.62      1.00      0.77         5
         419       1.00      0.44      0.62         9
         420       1.00      0.89      0.94         9
         421       0.00      0.00      0.00         4
         422       0.60      0.60      0.60         5
         423       1.00      0.75      0.86         4
         424       0.44      0.44      0.44         9
         425       0.17      0.25      0.20         4
         426       0.00      0.00      0.00         4
         427       0.80      1.00      0.89         4
         428       0.60      0.75      0.67         4
         429       0.80      1.00      0.89         4
         430       0.83      1.00      0.91         5
         431       0.67      1.00      0.80         4
         432       0.07      0.25      0.11         4
         433       0.75      0.67      0.71         9
         434       0.40      0.40      0.40         5
         435       1.00      1.00      1.00         9
         436       0.75      0.75      0.75         4
         437       0.00      0.00      0.00         4
         438       0.00      0.00      0.00         4
         439       0.00      0.00      0.00         9
         440       0.82      1.00      0.90         9
         441       0.83      1.00      0.91         5
         442       0.60      0.67      0.63         9
         443       0.00      0.00      0.00         4
         444       1.00      0.50      0.67         4
         445       0.00      0.00      0.00         4
         446       0.67      0.50      0.57         4
         447       0.67      0.80      0.73         5
         448       1.00      0.11      0.20         9
         449       0.50      0.50      0.50         4
         450       1.00      0.60      0.75         5
         451       1.00      1.00      1.00         4
         452       0.56      1.00      0.71         5
         453       0.60      0.67      0.63         9
         454       1.00      1.00      1.00         5
         455       1.00      0.80      0.89         5
         456       0.67      0.80      0.73         5
         457       0.78      0.78      0.78         9
         458       0.00      0.00      0.00         4
         459       0.67      0.80      0.73         5
         460       0.75      0.60      0.67         5
         461       1.00      1.00      1.00         4
         462       0.60      0.60      0.60         5
         463       0.50      0.50      0.50         4
         464       0.82      1.00      0.90         9
         465       1.00      1.00      1.00         4
         466       1.00      1.00      1.00         4
         467       0.33      0.25      0.29         4
         468       0.75      0.60      0.67         5
         469       1.00      0.80      0.89         5
         470       0.43      0.75      0.55         4
         471       1.00      0.75      0.86         4
         472       1.00      0.25      0.40         4
         473       0.23      0.75      0.35         4
         474       0.00      0.00      0.00         4
         475       0.27      0.33      0.30         9
         476       0.60      0.75      0.67         4
         477       1.00      0.89      0.94         9
         478       0.00      0.00      0.00         4
         479       1.00      1.00      1.00         5
         480       0.75      0.75      0.75         4
         481       1.00      0.20      0.33         5
         482       0.75      0.75      0.75         4
         483       0.67      0.67      0.67         9
         484       0.88      0.78      0.82         9
         485       0.00      0.00      0.00         4
         486       0.00      0.00      0.00         4
         487       0.75      0.75      0.75         4
         488       0.80      1.00      0.89         4
         489       0.67      0.67      0.67         9
         490       1.00      0.78      0.88         9
         491       1.00      1.00      1.00         5
         492       1.00      0.89      0.94         9
         493       1.00      0.80      0.89         5
         494       0.00      0.00      0.00         4
         495       1.00      0.50      0.67         4
         496       0.25      0.25      0.25         4
         497       1.00      0.75      0.86         4
         498       0.00      0.00      0.00         4
         499       0.80      0.89      0.84         9
         500       1.00      1.00      1.00         5
         501       0.86      0.67      0.75         9
         502       1.00      1.00      1.00         9
         503       0.75      0.75      0.75         4
         504       0.50      0.50      0.50         4
         505       0.80      0.44      0.57         9
         506       0.00      0.00      0.00         4
         507       0.80      0.89      0.84         9
         508       0.50      0.25      0.33         4
         509       1.00      1.00      1.00         5
         510       0.00      0.00      0.00         4
         511       1.00      0.50      0.67         4
         512       0.45      0.56      0.50         9
         513       1.00      0.75      0.86         4
         514       0.67      0.50      0.57         4
         515       1.00      0.67      0.80         9
         516       1.00      0.80      0.89         5
         517       0.00      0.00      0.00         4
         518       1.00      0.11      0.20         9
         519       0.40      0.22      0.29         9
         520       0.09      0.11      0.10         9
         521       0.50      0.50      0.50         4
         522       0.75      0.75      0.75         4
         523       0.44      1.00      0.62         4
         524       1.00      0.80      0.89         5
         525       1.00      1.00      1.00         4
         526       1.00      1.00      1.00         4
         527       1.00      0.50      0.67         4
         528       0.67      0.50      0.57         4
         529       1.00      1.00      1.00         4
         530       0.83      1.00      0.91         5
         531       0.43      0.75      0.55         4
         532       0.00      0.00      0.00         4
         533       1.00      0.56      0.71         9
         534       0.50      1.00      0.67         4
         535       0.75      0.75      0.75         4
         536       1.00      0.25      0.40         4
         537       0.50      0.25      0.33         4
         538       1.00      1.00      1.00         4
         539       0.50      0.25      0.33         4
         540       0.80      1.00      0.89         4
         541       0.75      0.67      0.71         9
         542       0.67      1.00      0.80         4
         543       0.00      0.00      0.00         4
         544       1.00      0.25      0.40         4
         545       0.70      0.88      0.78         8
         546       0.60      0.75      0.67         4
         547       0.73      0.89      0.80         9
         548       0.29      0.22      0.25         9
         549       0.40      0.44      0.42         9
         550       0.00      0.00      0.00         4
         551       0.80      1.00      0.89         4
         552       0.67      0.50      0.57         4
         553       0.00      0.00      0.00         9
         554       0.78      0.78      0.78         9
         555       0.75      0.67      0.71         9
         556       1.00      0.40      0.57         5
         557       1.00      0.20      0.33         5
         558       1.00      0.75      0.86         4
         559       1.00      0.75      0.86         4
         560       0.80      1.00      0.89         4
         561       0.75      0.75      0.75         4
         562       1.00      1.00      1.00         9
         563       0.00      0.00      0.00         4
         564       0.50      0.75      0.60         4
         565       0.00      0.00      0.00         4
         566       0.00      0.00      0.00         4
         567       0.80      0.80      0.80         5
         568       0.50      0.20      0.29         5
         569       1.00      0.80      0.89         5
         570       0.30      0.33      0.32         9
         571       0.67      0.40      0.50         5
         572       0.80      0.44      0.57         9
         573       1.00      1.00      1.00         4
         574       1.00      0.80      0.89         5
         575       0.43      0.75      0.55         4
         576       1.00      0.22      0.36         9
         577       0.00      0.00      0.00         4
         578       1.00      0.89      0.94         9
         579       1.00      1.00      1.00         4
         580       0.00      0.00      0.00         4
         581       0.75      1.00      0.86         9
         582       0.67      0.50      0.57         4
         583       0.80      0.80      0.80         5
         584       0.00      0.00      0.00         4
         585       0.00      0.00      0.00         4
         586       1.00      0.25      0.40         4
         587       0.83      0.56      0.67         9
         588       1.00      0.75      0.86         4
         589       0.80      1.00      0.89         4
         590       0.40      0.50      0.44         4
         591       1.00      1.00      1.00         4
         592       0.60      0.75      0.67         4
         593       0.89      0.89      0.89         9
         594       0.73      0.89      0.80         9
         595       0.73      0.89      0.80         9
         596       0.67      0.50      0.57         4
         597       1.00      1.00      1.00         4
         598       0.00      0.00      0.00         4
         599       1.00      1.00      1.00         4
         600       0.67      0.50      0.57         4
         601       1.00      1.00      1.00         5
         602       1.00      1.00      1.00         9
         603       0.50      0.25      0.33         4
         604       1.00      1.00      1.00         4
         605       0.75      0.33      0.46         9
         606       0.00      0.00      0.00         4
         607       0.00      0.00      0.00         4
         608       1.00      0.50      0.67         4
         609       1.00      1.00      1.00         8
         610       0.00      0.00      0.00         9
         611       1.00      0.50      0.67         4
         612       0.89      0.89      0.89         9
         613       0.38      0.75      0.50         4
         614       0.56      0.56      0.56         9
         615       0.60      0.33      0.43         9
         616       0.80      1.00      0.89         4
         617       1.00      0.50      0.67         4
         618       1.00      1.00      1.00         4
         619       0.57      1.00      0.73         4
         620       1.00      1.00      1.00         5
         621       0.75      0.75      0.75         4
         622       1.00      0.50      0.67         4
         623       0.00      0.00      0.00         4
         624       1.00      0.78      0.88         9
         625       1.00      0.11      0.20         9
         626       0.67      1.00      0.80         4
         627       0.67      0.67      0.67         9
         628       1.00      1.00      1.00         9
         629       0.00      0.00      0.00         4
         630       0.73      0.89      0.80         9
         631       0.00      0.00      0.00         4
         632       0.00      0.00      0.00         4
         633       0.00      0.00      0.00         4
         634       1.00      0.75      0.86         4
         635       0.60      0.75      0.67         4
         636       0.75      0.67      0.71         9
         637       1.00      0.89      0.94         9
         638       0.50      0.22      0.31         9
         639       1.00      0.40      0.57         5
         640       0.50      0.89      0.64         9
         641       1.00      0.67      0.80         9
         642       0.80      1.00      0.89         4
         643       1.00      0.75      0.86         4
         644       0.60      0.33      0.43         9
         645       0.40      0.50      0.44         4
         646       0.50      0.44      0.47         9
         647       0.67      0.80      0.73         5
         648       0.00      0.00      0.00         5
         649       1.00      1.00      1.00         4
         650       0.89      0.89      0.89         9
         651       0.50      0.60      0.55         5
         652       1.00      1.00      1.00         4
         653       0.00      0.00      0.00         5
         654       0.80      0.80      0.80         5
         655       1.00      0.67      0.80         9
         656       0.80      1.00      0.89         4
         657       1.00      1.00      1.00         4
         658       1.00      0.75      0.86         4
         659       1.00      1.00      1.00         9
         660       1.00      1.00      1.00         4
         661       1.00      0.75      0.86         4
         662       0.54      0.78      0.64         9
         663       0.75      0.75      0.75         4
         664       0.75      0.75      0.75         4
         665       0.50      0.75      0.60         4
         666       1.00      0.25      0.40         4
         667       1.00      0.89      0.94         9
         668       0.67      0.89      0.76         9
         669       1.00      0.25      0.40         4
         670       0.83      0.56      0.67         9
         671       0.88      0.78      0.82         9
         672       1.00      0.60      0.75         5
         673       1.00      0.75      0.86         4
         674       1.00      0.78      0.88         9
         675       1.00      1.00      1.00         4
         676       1.00      0.89      0.94         9
         677       1.00      0.75      0.86         4
         678       0.33      0.25      0.29         4
         679       1.00      0.75      0.86         4
         680       1.00      1.00      1.00         5
         681       0.60      0.75      0.67         4
         682       1.00      1.00      1.00         4
         683       0.50      0.75      0.60         4
         684       0.75      0.75      0.75         4
         685       0.60      0.75      0.67         4
         686       1.00      0.89      0.94         9
         687       1.00      0.50      0.67         4
         688       1.00      0.20      0.33         5
         689       0.80      0.89      0.84         9
         690       0.88      0.78      0.82         9
         691       0.00      0.00      0.00         4
         692       0.00      0.00      0.00         4
         693       1.00      0.75      0.86         4
         694       1.00      0.89      0.94         9
         695       0.00      0.00      0.00         4
         696       1.00      0.60      0.75         5
         697       0.75      0.75      0.75         4
         698       1.00      1.00      1.00         4
         699       1.00      0.25      0.40         4
         700       0.00      0.00      0.00         4
         701       0.50      0.50      0.50         4
         702       0.00      0.00      0.00         4
         703       0.86      0.67      0.75         9
         704       0.25      0.11      0.15         9
         705       0.40      0.22      0.29         9
         706       0.00      0.00      0.00         4
         707       1.00      1.00      1.00         5
         708       0.80      1.00      0.89         4
         709       0.38      0.33      0.35         9
         710       0.00      0.00      0.00         9
         711       0.00      0.00      0.00         4
         712       0.29      0.50      0.36         4
         713       1.00      0.75      0.86         4
         714       1.00      1.00      1.00         9
         715       0.89      0.89      0.89         9
         716       0.00      0.00      0.00         4
         717       0.00      0.00      0.00         4
         718       0.60      0.75      0.67         4
         719       0.57      0.80      0.67         5
         720       1.00      1.00      1.00         4
         721       1.00      0.20      0.33         5
         722       1.00      0.75      0.86         4
         723       0.50      0.20      0.29         5
         724       0.80      1.00      0.89         4
         725       0.00      0.00      0.00         4
         726       1.00      0.78      0.88         9
         727       1.00      1.00      1.00         4
         728       1.00      0.80      0.89         5
         729       0.67      0.50      0.57         4
         730       0.00      0.00      0.00         4
         731       0.50      0.20      0.29         5
         732       0.00      0.00      0.00         4
         733       1.00      1.00      1.00         5
         734       0.80      0.44      0.57         9
         735       1.00      1.00      1.00         9
         736       1.00      1.00      1.00         4
         737       1.00      0.78      0.88         9
         738       1.00      0.33      0.50         9
         739       0.00      0.00      0.00         4
         740       1.00      0.78      0.88         9
         741       0.67      0.22      0.33         9
         742       0.50      0.60      0.55         5
         743       0.89      0.89      0.89         9
         744       1.00      0.40      0.57         5
         745       0.71      0.56      0.63         9
         746       0.88      0.78      0.82         9
         747       0.83      1.00      0.91         5
         748       0.50      0.75      0.60         4
         749       1.00      0.80      0.89         5
         750       0.88      0.78      0.82         9
         751       1.00      0.60      0.75         5
         752       1.00      0.60      0.75         5
         753       0.80      1.00      0.89         4
         754       0.55      0.67      0.60         9
         755       0.60      0.75      0.67         4
         756       0.75      0.60      0.67         5
         757       1.00      0.20      0.33         5
         758       1.00      1.00      1.00         5
         759       0.50      0.11      0.18         9
         760       0.50      0.25      0.33         4
         761       1.00      0.56      0.71         9
         762       0.00      0.00      0.00         4
         763       0.00      0.00      0.00         9
         764       0.00      0.00      0.00         4
         765       0.60      0.33      0.43         9
         766       1.00      0.78      0.88         9
         767       0.00      0.00      0.00         4
         768       1.00      1.00      1.00         4
         769       0.50      0.89      0.64         9
         770       1.00      0.25      0.40         4
         771       0.43      0.60      0.50         5
         772       0.80      0.80      0.80         5
         773       0.00      0.00      0.00         9
         774       0.60      0.60      0.60         5
         775       0.67      1.00      0.80         4
         776       0.00      0.00      0.00         4
         777       1.00      0.25      0.40         4
         778       1.00      0.60      0.75         5
         779       0.75      0.60      0.67         5
         780       0.67      1.00      0.80         4
         781       0.80      0.44      0.57         9
         782       0.00      0.00      0.00         4
         783       0.00      0.00      0.00         4
         784       0.43      0.75      0.55         4
         785       1.00      0.20      0.33         5
         786       0.50      0.25      0.33         4
         787       1.00      0.75      0.86         4
         788       0.00      0.00      0.00         4
         789       0.00      0.00      0.00         4
         790       0.00      0.00      0.00         5
         791       1.00      1.00      1.00         9
         792       1.00      1.00      1.00         5
         793       1.00      0.25      0.40         4
         794       0.82      1.00      0.90         9
         795       0.00      0.00      0.00         4
         796       1.00      1.00      1.00         4
         797       1.00      0.78      0.88         9
         798       0.67      0.22      0.33         9
         799       0.00      0.00      0.00         4
         800       1.00      1.00      1.00         4
         801       0.78      0.78      0.78         9
         802       0.67      0.50      0.57         4
         803       0.50      0.75      0.60         4
         804       0.00      0.00      0.00         4
         805       0.00      0.00      0.00         4
         806       0.17      0.25      0.20         4
         807       1.00      0.50      0.67         4
         808       1.00      0.20      0.33         5
         809       1.00      0.25      0.40         4
         810       0.62      0.56      0.59         9
         811       0.83      1.00      0.91         5
         812       1.00      1.00      1.00         4
         813       1.00      0.11      0.20         9
         814       1.00      0.25      0.40         4
         815       0.40      0.50      0.44         4
         816       1.00      1.00      1.00         4
         817       0.89      0.89      0.89         9
         818       0.00      0.00      0.00         4
         819       0.50      1.00      0.67         4
         820       1.00      0.50      0.67         4
         821       0.50      0.50      0.50         4
         822       0.80      1.00      0.89         4
         823       0.80      1.00      0.89         4
         824       1.00      1.00      1.00         4
         825       0.00      0.00      0.00         4
         826       1.00      1.00      1.00         4
         827       1.00      0.29      0.44         7
         828       1.00      0.78      0.88         9
         829       0.50      0.75      0.60         4
         830       1.00      0.50      0.67         4
         831       0.00      0.00      0.00         9
         832       1.00      0.20      0.33         5
         833       1.00      0.25      0.40         4
         834       1.00      1.00      1.00         9
         835       0.67      0.50      0.57         4
         836       0.82      1.00      0.90         9
         837       1.00      0.67      0.80         9
         838       0.78      0.78      0.78         9
         839       0.89      0.89      0.89         9
         840       0.90      1.00      0.95         9
         841       0.00      0.00      0.00         4
         842       0.00      0.00      0.00         4
         843       1.00      0.25      0.40         4
         844       1.00      0.20      0.33         5
         845       0.00      0.00      0.00         4
         846       0.80      1.00      0.89         4
         847       0.00      0.00      0.00         4
         848       1.00      0.75      0.86         4
         849       0.00      0.00      0.00         4
         850       1.00      0.75      0.86         4
         851       0.67      1.00      0.80         4
         852       0.75      0.75      0.75         4
         853       1.00      0.25      0.40         4
         854       1.00      0.20      0.33         5
         855       0.00      0.00      0.00         4
         856       0.83      1.00      0.91         5
         857       0.62      0.56      0.59         9
         858       1.00      0.25      0.40         4
         859       0.50      0.22      0.31         9
         860       0.00      0.00      0.00         4
         861       1.00      1.00      1.00         4
         862       0.80      1.00      0.89         4
         863       0.00      0.00      0.00         4
         864       0.00      0.00      0.00         4
         865       0.67      0.40      0.50         5
         866       1.00      0.40      0.57         5
         867       1.00      0.40      0.57         5
         868       0.00      0.00      0.00         4
         869       0.88      0.78      0.82         9
         870       0.00      0.00      0.00         4
         871       0.00      0.00      0.00         4
         872       0.00      0.00      0.00         4
         873       0.00      0.00      0.00         4
         874       0.67      1.00      0.80         4
         875       0.28      1.00      0.44         9
         876       1.00      0.75      0.86         4
         877       0.29      0.50      0.36         4
         878       0.41      1.00      0.58         9
         879       1.00      0.75      0.86         4
         880       1.00      0.75      0.86         4
         881       0.62      0.89      0.73         9
         882       0.29      0.50      0.36         4
         883       0.80      1.00      0.89         4
         884       0.00      0.00      0.00         4
         885       0.40      0.40      0.40         5
         886       1.00      0.75      0.86         4
         887       0.50      1.00      0.67         4
         888       0.00      0.00      0.00         4
         889       0.80      0.89      0.84         9
         890       0.00      0.00      0.00         4
         891       0.25      0.50      0.33         4
         892       1.00      1.00      1.00         4
         893       0.00      0.00      0.00         4

    accuracy                           0.64      4917
   macro avg       0.64      0.63      0.61      4917
weighted avg       0.66      0.64      0.62      4917

torch.Size([4917, 91]) torch.Size([4917])
Parameters: 986894
Task parameters: {0: 126034, 1: 146054, 2: 166074, 3: 186094, 4: 206114, 5: 226134, 6: 246154, 7: 266174, 8: 286194, 9: 306214, 10: 326234, 11: 346254, 12: 366274, 13: 386294, 14: 406314, 15: 426334, 16: 446354, 17: 466374, 18: 486394, 19: 506414, 20: 526434, 21: 546454, 22: 566474, 23: 586494, 24: 606514, 25: 626534, 26: 646554, 27: 666574, 28: 686594, 29: 706614, 30: 726634, 31: 746654, 32: 766674, 33: 786694, 34: 806714, 35: 826734, 36: 846754, 37: 866774, 38: 886794, 39: 906814, 40: 926834, 41: 946854, 42: 966874, 43: 986894}
