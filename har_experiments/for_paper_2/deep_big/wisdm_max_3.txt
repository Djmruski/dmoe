CLASS_ORDER: [237, 344, 641, 118, 667, 417, 767, 57, 623, 490, 115, 719, 350, 244, 278, 676, 657, 372, 379, 501, 203, 477, 508, 251, 445, 495, 617, 803, 537, 83, 738, 22, 602, 381, 463, 225, 683, 271, 769, 762, 139, 813, 823, 882, 268, 500, 649, 408, 855, 439, 153, 677, 190, 510, 8, 743, 356, 393, 213, 248, 852, 130, 341, 216, 12, 747, 170, 487, 24, 760, 424, 889, 636, 542, 80, 250, 854, 162, 821, 534, 709, 635, 529, 43, 476, 780, 300, 494, 515, 579, 10, 400, 792, 324, 159, 532, 503, 454, 843, 165, 277, 590, 116, 441, 848, 156, 35, 530, 316, 546, 92, 196, 472, 9, 99, 385, 207, 876, 722, 446, 140, 122, 75, 111, 66, 421, 180, 357, 711, 459, 95, 673, 484, 48, 415, 49, 416, 626, 664, 764, 226, 42, 282, 713, 725, 732, 78, 258, 674, 6, 239, 830, 184, 36, 735, 264, 675, 234, 281, 874, 493, 101, 411, 705, 583, 29, 560, 65, 644, 691, 367, 731, 507, 431, 155, 382, 420, 261, 134, 51, 172, 84, 680, 329, 58, 543, 249, 369, 198, 224, 210, 535, 565, 605, 497, 840, 776, 545, 363, 591, 655, 521, 343, 757, 20, 628, 199, 288, 370, 110, 883, 630, 567, 772, 345, 741, 716, 73, 181, 453, 390, 761, 132, 670, 284, 7, 33, 720, 797, 456, 141, 728, 154, 79, 781, 737, 246, 516, 734, 746, 212, 582, 717, 893, 412, 351, 478, 317, 17, 544, 517, 243, 672, 39, 696, 586, 331, 592, 806, 599, 19, 373, 392, 642, 16, 860, 426, 232, 612, 853, 304, 94, 318, 361, 195, 646, 68, 85, 818, 754, 631, 595, 228, 322, 339, 702, 714, 348, 837, 729, 451, 887, 380, 126, 353, 354, 5, 56, 511, 654, 167, 730, 527, 60, 634, 333, 593, 335, 844, 90, 102, 615, 179, 347, 332, 660, 519, 270, 695, 205, 15, 368, 525, 25, 575, 236, 298, 401, 548, 63, 336, 505, 397, 828, 751, 231, 208, 561, 23, 518, 796, 541, 74, 93, 833, 366, 323, 619, 174, 54, 526, 455, 878, 791, 531, 648, 295, 117, 440, 629, 242, 272, 616, 61, 138, 570, 765, 252, 359, 686, 302, 669, 355, 314, 245, 763, 142, 492, 146, 469, 148, 71, 398, 100, 585, 299, 287, 161, 788, 473, 622, 851, 790, 349, 678, 464, 715, 533, 64, 308, 832, 260, 637, 891, 661, 778, 50, 219, 574, 888, 506, 460, 279, 286, 704, 784, 352, 885, 37, 779, 863, 11, 794, 783, 814, 403, 104, 53, 866, 879, 498, 91, 799, 137, 739, 787, 293, 123, 688, 127, 177, 374, 540, 875, 864, 160, 253, 596, 858, 256, 187, 611, 338, 633, 865, 643, 470, 217, 632, 551, 485, 143, 386, 88, 371, 413, 38, 684, 736, 698, 97, 491, 744, 44, 603, 326, 197, 144, 121, 164, 444, 240, 499, 820, 128, 513, 509, 881, 877, 227, 566, 829, 311, 789, 310, 482, 692, 562, 274, 653, 489, 777, 152, 465, 87, 151, 457, 4, 358, 202, 209, 584, 827, 280, 334, 45, 365, 418, 119, 294, 229, 694, 536, 512, 69, 96, 775, 211, 409, 425, 689, 569, 647, 448, 222, 391, 406, 163, 188, 708, 573, 873, 183, 552, 346, 771, 67, 28, 651, 422, 645, 362, 377, 867, 430, 21, 750, 815, 755, 685, 679, 106, 40, 607, 14, 410, 690, 131, 480, 668, 27, 614, 396, 175, 608, 782, 218, 624, 825, 206, 749, 659, 804, 105, 467, 442, 812, 808, 81, 555, 235, 523, 758, 437, 259, 124, 387, 3, 600, 215, 452, 427, 795, 880, 576, 273, 186, 773, 405, 640, 458, 214, 449, 220, 466, 872, 46, 327, 435, 307, 496, 462, 340, 176, 727, 432, 395, 276, 315, 587, 269, 265, 267, 701, 419, 819, 811, 433, 488, 26, 399, 524, 598, 133, 192, 613, 103, 13, 538, 125, 504, 201, 230, 296, 375, 414, 342, 774, 204, 539, 194, 571, 835, 554, 884, 697, 817, 120, 639, 740, 147, 383, 839, 559, 32, 189, 724, 824, 550, 558, 312, 262, 423, 98, 793, 59, 847, 285, 305, 588, 753, 522, 557, 255, 665, 650, 157, 671, 41, 868, 759, 658, 687, 306, 394, 856, 627, 474, 770, 70, 145, 86, 723, 200, 652, 325, 108, 886, 785, 638, 577, 337, 257, 241, 168, 291, 733, 89, 564, 752, 297, 862, 275, 816, 221, 434, 113, 328, 135, 283, 826, 303, 34, 223, 436, 549, 859, 321, 547, 388, 171, 193, 376, 185, 72, 31, 182, 604, 47, 568, 2, 18, 718, 756, 845, 831, 520, 556, 481, 404, 594, 870, 656, 609, 112, 320, 849, 528, 158, 290, 82, 621, 389, 726, 786, 289, 364, 699, 662, 107, 514, 721, 553, 580, 857, 76, 610, 461, 742, 384, 486, 890, 475, 0, 129, 834, 581, 766, 169, 871, 407, 173, 666, 682, 798, 502, 1, 836, 700, 850, 693, 254, 712, 578, 77, 149, 601, 263, 768, 471, 479, 468, 109, 150, 846, 707, 62, 892, 402, 330, 428, 378, 178, 842, 822, 805, 443, 55, 447, 292, 136, 309, 663, 30, 247, 706, 802, 810, 618, 620, 710, 52, 450, 563, 861, 841, 869, 809, 745, 483, 589, 429, 625, 572, 266, 233, 838, 301, 703, 114, 597, 438, 681, 191, 801, 166, 807, 238, 360, 319, 748, 313, 606, 800]
class_group: [(237, 344, 641, 118, 667, 417, 767, 57, 623, 490, 115, 719, 350, 244, 278, 676, 657, 372, 379, 501, 203, 477, 508, 251, 445, 495, 617, 803, 537, 83, 738, 22, 602, 381), (463, 225, 683, 271, 769, 762, 139, 813, 823, 882, 268, 500, 649, 408, 855, 439, 153, 677, 190, 510), (8, 743, 356, 393, 213, 248, 852, 130, 341, 216, 12, 747, 170, 487, 24, 760, 424, 889, 636, 542), (80, 250, 854, 162, 821, 534, 709, 635, 529, 43, 476, 780, 300, 494, 515, 579, 10, 400, 792, 324), (159, 532, 503, 454, 843, 165, 277, 590, 116, 441, 848, 156, 35, 530, 316, 546, 92, 196, 472, 9), (99, 385, 207, 876, 722, 446, 140, 122, 75, 111, 66, 421, 180, 357, 711, 459, 95, 673, 484, 48), (415, 49, 416, 626, 664, 764, 226, 42, 282, 713, 725, 732, 78, 258, 674, 6, 239, 830, 184, 36), (735, 264, 675, 234, 281, 874, 493, 101, 411, 705, 583, 29, 560, 65, 644, 691, 367, 731, 507, 431), (155, 382, 420, 261, 134, 51, 172, 84, 680, 329, 58, 543, 249, 369, 198, 224, 210, 535, 565, 605), (497, 840, 776, 545, 363, 591, 655, 521, 343, 757, 20, 628, 199, 288, 370, 110, 883, 630, 567, 772), (345, 741, 716, 73, 181, 453, 390, 761, 132, 670, 284, 7, 33, 720, 797, 456, 141, 728, 154, 79), (781, 737, 246, 516, 734, 746, 212, 582, 717, 893, 412, 351, 478, 317, 17, 544, 517, 243, 672, 39), (696, 586, 331, 592, 806, 599, 19, 373, 392, 642, 16, 860, 426, 232, 612, 853, 304, 94, 318, 361), (195, 646, 68, 85, 818, 754, 631, 595, 228, 322, 339, 702, 714, 348, 837, 729, 451, 887, 380, 126), (353, 354, 5, 56, 511, 654, 167, 730, 527, 60, 634, 333, 593, 335, 844, 90, 102, 615, 179, 347), (332, 660, 519, 270, 695, 205, 15, 368, 525, 25, 575, 236, 298, 401, 548, 63, 336, 505, 397, 828), (751, 231, 208, 561, 23, 518, 796, 541, 74, 93, 833, 366, 323, 619, 174, 54, 526, 455, 878, 791), (531, 648, 295, 117, 440, 629, 242, 272, 616, 61, 138, 570, 765, 252, 359, 686, 302, 669, 355, 314), (245, 763, 142, 492, 146, 469, 148, 71, 398, 100, 585, 299, 287, 161, 788, 473, 622, 851, 790, 349), (678, 464, 715, 533, 64, 308, 832, 260, 637, 891, 661, 778, 50, 219, 574, 888, 506, 460, 279, 286), (704, 784, 352, 885, 37, 779, 863, 11, 794, 783, 814, 403, 104, 53, 866, 879, 498, 91, 799, 137), (739, 787, 293, 123, 688, 127, 177, 374, 540, 875, 864, 160, 253, 596, 858, 256, 187, 611, 338, 633), (865, 643, 470, 217, 632, 551, 485, 143, 386, 88, 371, 413, 38, 684, 736, 698, 97, 491, 744, 44), (603, 326, 197, 144, 121, 164, 444, 240, 499, 820, 128, 513, 509, 881, 877, 227, 566, 829, 311, 789), (310, 482, 692, 562, 274, 653, 489, 777, 152, 465, 87, 151, 457, 4, 358, 202, 209, 584, 827, 280), (334, 45, 365, 418, 119, 294, 229, 694, 536, 512, 69, 96, 775, 211, 409, 425, 689, 569, 647, 448), (222, 391, 406, 163, 188, 708, 573, 873, 183, 552, 346, 771, 67, 28, 651, 422, 645, 362, 377, 867), (430, 21, 750, 815, 755, 685, 679, 106, 40, 607, 14, 410, 690, 131, 480, 668, 27, 614, 396, 175), (608, 782, 218, 624, 825, 206, 749, 659, 804, 105, 467, 442, 812, 808, 81, 555, 235, 523, 758, 437), (259, 124, 387, 3, 600, 215, 452, 427, 795, 880, 576, 273, 186, 773, 405, 640, 458, 214, 449, 220), (466, 872, 46, 327, 435, 307, 496, 462, 340, 176, 727, 432, 395, 276, 315, 587, 269, 265, 267, 701), (419, 819, 811, 433, 488, 26, 399, 524, 598, 133, 192, 613, 103, 13, 538, 125, 504, 201, 230, 296), (375, 414, 342, 774, 204, 539, 194, 571, 835, 554, 884, 697, 817, 120, 639, 740, 147, 383, 839, 559), (32, 189, 724, 824, 550, 558, 312, 262, 423, 98, 793, 59, 847, 285, 305, 588, 753, 522, 557, 255), (665, 650, 157, 671, 41, 868, 759, 658, 687, 306, 394, 856, 627, 474, 770, 70, 145, 86, 723, 200), (652, 325, 108, 886, 785, 638, 577, 337, 257, 241, 168, 291, 733, 89, 564, 752, 297, 862, 275, 816), (221, 434, 113, 328, 135, 283, 826, 303, 34, 223, 436, 549, 859, 321, 547, 388, 171, 193, 376, 185), (72, 31, 182, 604, 47, 568, 2, 18, 718, 756, 845, 831, 520, 556, 481, 404, 594, 870, 656, 609), (112, 320, 849, 528, 158, 290, 82, 621, 389, 726, 786, 289, 364, 699, 662, 107, 514, 721, 553, 580), (857, 76, 610, 461, 742, 384, 486, 890, 475, 0, 129, 834, 581, 766, 169, 871, 407, 173, 666, 682), (798, 502, 1, 836, 700, 850, 693, 254, 712, 578, 77, 149, 601, 263, 768, 471, 479, 468, 109, 150), (846, 707, 62, 892, 402, 330, 428, 378, 178, 842, 822, 805, 443, 55, 447, 292, 136, 309, 663, 30), (247, 706, 802, 810, 618, 620, 710, 52, 450, 563, 861, 841, 869, 809, 745, 483, 589, 429, 625, 572), (266, 233, 838, 301, 703, 114, 597, 438, 681, 191, 801, 166, 807, 238, 360, 319, 748, 313, 606, 800)]
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]
Polling GMM for: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}
STEP-1	Epoch: 10/50	loss: 1.6772	step1_train_accuracy: 57.5269
STEP-1	Epoch: 20/50	loss: 0.9031	step1_train_accuracy: 85.6631
STEP-1	Epoch: 30/50	loss: 0.5493	step1_train_accuracy: 91.3979
STEP-1	Epoch: 40/50	loss: 0.3813	step1_train_accuracy: 95.6989
STEP-1	Epoch: 50/50	loss: 0.2880	step1_train_accuracy: 96.2366
FINISH STEP 1
Task-1	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.1201	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 40/200	classification_loss: 0.1107	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 60/200	classification_loss: 0.1041	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 80/200	classification_loss: 0.0996	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 100/200	classification_loss: 0.0959	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 120/200	classification_loss: 0.0956	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 140/200	classification_loss: 0.0948	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 160/200	classification_loss: 0.0905	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 180/200	classification_loss: 0.0909	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
STEP-2	Epoch: 200/200	classification_loss: 0.0902	gate_loss: 0.0000	step2_classification_accuracy: 95.7721	step_2_gate_accuracy: 100.0000
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 93.5252	gate_accuracy: 100.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 100.0000


[34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]
Polling GMM for: {34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53}
STEP-1	Epoch: 10/50	loss: 1.9478	step1_train_accuracy: 53.0945
STEP-1	Epoch: 20/50	loss: 0.9682	step1_train_accuracy: 78.8274
STEP-1	Epoch: 30/50	loss: 0.5968	step1_train_accuracy: 86.6450
STEP-1	Epoch: 40/50	loss: 0.4638	step1_train_accuracy: 90.8795
STEP-1	Epoch: 50/50	loss: 0.3302	step1_train_accuracy: 93.4853
FINISH STEP 1
Task-2	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.2083	gate_loss: 0.1277	step2_classification_accuracy: 91.9560	step_2_gate_accuracy: 97.6852
STEP-2	Epoch: 40/200	classification_loss: 0.1879	gate_loss: 0.0580	step2_classification_accuracy: 92.3032	step_2_gate_accuracy: 98.3796
STEP-2	Epoch: 60/200	classification_loss: 0.1769	gate_loss: 0.0443	step2_classification_accuracy: 92.2454	step_2_gate_accuracy: 98.2639
STEP-2	Epoch: 80/200	classification_loss: 0.1555	gate_loss: 0.0356	step2_classification_accuracy: 92.4769	step_2_gate_accuracy: 98.4954
STEP-2	Epoch: 100/200	classification_loss: 0.1547	gate_loss: 0.0326	step2_classification_accuracy: 92.4769	step_2_gate_accuracy: 98.6111
STEP-2	Epoch: 120/200	classification_loss: 0.1535	gate_loss: 0.0299	step2_classification_accuracy: 92.9398	step_2_gate_accuracy: 99.0741
STEP-2	Epoch: 140/200	classification_loss: 0.1425	gate_loss: 0.0267	step2_classification_accuracy: 93.0556	step_2_gate_accuracy: 99.0741
STEP-2	Epoch: 160/200	classification_loss: 0.1412	gate_loss: 0.0263	step2_classification_accuracy: 92.7662	step_2_gate_accuracy: 98.9583
STEP-2	Epoch: 180/200	classification_loss: 0.1381	gate_loss: 0.0243	step2_classification_accuracy: 93.1713	step_2_gate_accuracy: 99.3634
STEP-2	Epoch: 200/200	classification_loss: 0.1358	gate_loss: 0.0242	step2_classification_accuracy: 92.7662	step_2_gate_accuracy: 99.0162
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 91.3669	gate_accuracy: 97.1223
	Task-1	val_accuracy: 83.1169	gate_accuracy: 96.1039
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 96.7593


[54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]
Polling GMM for: {54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73}
STEP-1	Epoch: 10/50	loss: 1.6664	step1_train_accuracy: 69.0667
STEP-1	Epoch: 20/50	loss: 0.6621	step1_train_accuracy: 90.6667
STEP-1	Epoch: 30/50	loss: 0.3371	step1_train_accuracy: 96.8000
STEP-1	Epoch: 40/50	loss: 0.2068	step1_train_accuracy: 98.4000
STEP-1	Epoch: 50/50	loss: 0.1417	step1_train_accuracy: 98.6667
FINISH STEP 1
Task-3	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.1863	gate_loss: 0.1694	step2_classification_accuracy: 93.1204	step_2_gate_accuracy: 97.0107
STEP-2	Epoch: 40/200	classification_loss: 0.1407	gate_loss: 0.0593	step2_classification_accuracy: 93.9394	step_2_gate_accuracy: 98.5258
STEP-2	Epoch: 60/200	classification_loss: 0.1424	gate_loss: 0.0416	step2_classification_accuracy: 94.1851	step_2_gate_accuracy: 98.9762
STEP-2	Epoch: 80/200	classification_loss: 0.1167	gate_loss: 0.0299	step2_classification_accuracy: 94.4308	step_2_gate_accuracy: 99.0172
STEP-2	Epoch: 100/200	classification_loss: 0.1172	gate_loss: 0.0268	step2_classification_accuracy: 94.3898	step_2_gate_accuracy: 98.8943
STEP-2	Epoch: 120/200	classification_loss: 0.1177	gate_loss: 0.0244	step2_classification_accuracy: 94.2260	step_2_gate_accuracy: 99.0582
STEP-2	Epoch: 140/200	classification_loss: 0.1063	gate_loss: 0.0209	step2_classification_accuracy: 94.5536	step_2_gate_accuracy: 99.1810
STEP-2	Epoch: 160/200	classification_loss: 0.1062	gate_loss: 0.0207	step2_classification_accuracy: 94.3898	step_2_gate_accuracy: 98.9762
STEP-2	Epoch: 180/200	classification_loss: 0.2508	gate_loss: 0.0753	step2_classification_accuracy: 93.8984	step_2_gate_accuracy: 98.4029
STEP-2	Epoch: 200/200	classification_loss: 0.1052	gate_loss: 0.0188	step2_classification_accuracy: 94.5946	step_2_gate_accuracy: 99.3038
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 90.6475	gate_accuracy: 94.9640
	Task-1	val_accuracy: 83.1169	gate_accuracy: 93.5065
	Task-2	val_accuracy: 96.8085	gate_accuracy: 97.8723
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 95.4839


[74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]
Polling GMM for: {74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93}
STEP-1	Epoch: 10/50	loss: 1.9759	step1_train_accuracy: 63.1016
STEP-1	Epoch: 20/50	loss: 0.8454	step1_train_accuracy: 81.5508
STEP-1	Epoch: 30/50	loss: 0.4721	step1_train_accuracy: 93.0481
STEP-1	Epoch: 40/50	loss: 0.3105	step1_train_accuracy: 96.2567
STEP-1	Epoch: 50/50	loss: 0.2163	step1_train_accuracy: 98.6631
FINISH STEP 1
Task-4	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.2452	gate_loss: 0.2293	step2_classification_accuracy: 91.4561	step_2_gate_accuracy: 93.9162
STEP-2	Epoch: 40/200	classification_loss: 0.1878	gate_loss: 0.0975	step2_classification_accuracy: 92.6529	step_2_gate_accuracy: 96.7753
STEP-2	Epoch: 60/200	classification_loss: 0.1651	gate_loss: 0.0707	step2_classification_accuracy: 93.0186	step_2_gate_accuracy: 97.4402
STEP-2	Epoch: 80/200	classification_loss: 0.1557	gate_loss: 0.0590	step2_classification_accuracy: 93.1848	step_2_gate_accuracy: 97.5066
STEP-2	Epoch: 100/200	classification_loss: 0.1503	gate_loss: 0.0522	step2_classification_accuracy: 93.7168	step_2_gate_accuracy: 98.2048
STEP-2	Epoch: 120/200	classification_loss: 0.1337	gate_loss: 0.0443	step2_classification_accuracy: 93.9495	step_2_gate_accuracy: 98.4043
STEP-2	Epoch: 140/200	classification_loss: 0.1303	gate_loss: 0.0422	step2_classification_accuracy: 93.9827	step_2_gate_accuracy: 98.3710
STEP-2	Epoch: 160/200	classification_loss: 0.1226	gate_loss: 0.0384	step2_classification_accuracy: 94.2154	step_2_gate_accuracy: 98.7035
STEP-2	Epoch: 180/200	classification_loss: 0.1174	gate_loss: 0.0355	step2_classification_accuracy: 94.3152	step_2_gate_accuracy: 98.7035
STEP-2	Epoch: 200/200	classification_loss: 0.1147	gate_loss: 0.0332	step2_classification_accuracy: 94.4814	step_2_gate_accuracy: 98.7699
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 89.9281	gate_accuracy: 92.8058
	Task-1	val_accuracy: 76.6234	gate_accuracy: 88.3117
	Task-2	val_accuracy: 94.6809	gate_accuracy: 95.7447
	Task-3	val_accuracy: 85.1064	gate_accuracy: 87.2340
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 91.3366


[ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]
Polling GMM for: {94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113}
STEP-1	Epoch: 10/50	loss: 2.3368	step1_train_accuracy: 40.0000
STEP-1	Epoch: 20/50	loss: 1.1894	step1_train_accuracy: 78.7302
STEP-1	Epoch: 30/50	loss: 0.6797	step1_train_accuracy: 91.7460
STEP-1	Epoch: 40/50	loss: 0.4308	step1_train_accuracy: 95.8730
STEP-1	Epoch: 50/50	loss: 0.2965	step1_train_accuracy: 96.8254
FINISH STEP 1
Task-5	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.2620	gate_loss: 0.2805	step2_classification_accuracy: 90.2094	step_2_gate_accuracy: 92.3599
STEP-2	Epoch: 40/200	classification_loss: 0.1792	gate_loss: 0.1127	step2_classification_accuracy: 92.8410	step_2_gate_accuracy: 96.1800
STEP-2	Epoch: 60/200	classification_loss: 0.1649	gate_loss: 0.0788	step2_classification_accuracy: 93.0108	step_2_gate_accuracy: 97.0006
STEP-2	Epoch: 80/200	classification_loss: 0.1458	gate_loss: 0.0633	step2_classification_accuracy: 93.6333	step_2_gate_accuracy: 97.6514
STEP-2	Epoch: 100/200	classification_loss: 0.1384	gate_loss: 0.0527	step2_classification_accuracy: 93.9162	step_2_gate_accuracy: 97.9344
STEP-2	Epoch: 120/200	classification_loss: 0.1262	gate_loss: 0.0444	step2_classification_accuracy: 94.3973	step_2_gate_accuracy: 98.3871
STEP-2	Epoch: 140/200	classification_loss: 0.1130	gate_loss: 0.0379	step2_classification_accuracy: 94.6237	step_2_gate_accuracy: 98.6984
STEP-2	Epoch: 160/200	classification_loss: 0.1116	gate_loss: 0.0359	step2_classification_accuracy: 94.6520	step_2_gate_accuracy: 98.7267
STEP-2	Epoch: 180/200	classification_loss: 0.1093	gate_loss: 0.0348	step2_classification_accuracy: 94.7368	step_2_gate_accuracy: 98.8115
STEP-2	Epoch: 200/200	classification_loss: 0.1073	gate_loss: 0.0325	step2_classification_accuracy: 94.7368	step_2_gate_accuracy: 98.8964
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 90.6475	gate_accuracy: 95.6835
	Task-1	val_accuracy: 79.2208	gate_accuracy: 87.0130
	Task-2	val_accuracy: 87.2340	gate_accuracy: 90.4255
	Task-3	val_accuracy: 79.7872	gate_accuracy: 81.9149
	Task-4	val_accuracy: 91.1392	gate_accuracy: 93.6709
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 90.2691


[114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]
Polling GMM for: {114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133}
STEP-1	Epoch: 10/50	loss: 2.5330	step1_train_accuracy: 49.2857
STEP-1	Epoch: 20/50	loss: 1.2252	step1_train_accuracy: 78.5714
STEP-1	Epoch: 30/50	loss: 0.6436	step1_train_accuracy: 95.3571
STEP-1	Epoch: 40/50	loss: 0.3847	step1_train_accuracy: 97.1429
STEP-1	Epoch: 50/50	loss: 0.2548	step1_train_accuracy: 98.2143
FINISH STEP 1
Task-6	STARTING STEP 2
CLASS COUNTER: Counter({0: 34, 1: 34, 2: 34, 3: 34, 4: 34, 5: 34, 6: 34, 7: 34, 8: 34, 9: 34, 10: 34, 11: 34, 12: 34, 13: 34, 14: 34, 15: 34, 16: 34, 17: 34, 18: 34, 19: 34, 20: 34, 21: 34, 22: 34, 23: 34, 24: 34, 25: 34, 26: 34, 27: 34, 28: 34, 29: 34, 30: 34, 31: 34, 32: 34, 33: 34, 34: 34, 35: 34, 36: 34, 37: 34, 38: 34, 39: 34, 40: 34, 41: 34, 42: 34, 43: 34, 44: 34, 45: 34, 46: 34, 47: 34, 48: 34, 49: 34, 50: 34, 51: 34, 52: 34, 53: 34, 54: 34, 55: 34, 56: 34, 57: 34, 58: 34, 59: 34, 60: 34, 61: 34, 62: 34, 63: 34, 64: 34, 65: 34, 66: 34, 67: 34, 68: 34, 69: 34, 70: 34, 71: 34, 72: 34, 73: 34, 74: 34, 75: 34, 76: 34, 77: 34, 78: 34, 79: 34, 80: 34, 81: 34, 82: 34, 83: 34, 84: 34, 85: 34, 86: 34, 87: 34, 88: 34, 89: 34, 90: 34, 91: 34, 92: 34, 93: 34, 94: 34, 95: 34, 96: 34, 97: 34, 98: 34, 99: 34, 100: 34, 101: 34, 102: 34, 103: 34, 104: 34, 105: 34, 106: 34, 107: 34, 108: 34, 109: 34, 110: 34, 111: 34, 112: 34, 113: 34, 114: 34, 115: 34, 116: 34, 117: 34, 118: 34, 119: 34, 120: 34, 121: 34, 122: 34, 123: 34, 124: 34, 125: 34, 126: 34, 127: 34, 128: 34, 129: 34, 130: 34, 131: 34, 132: 34, 133: 34})
STEP-2	Epoch: 20/200	classification_loss: 0.2502	gate_loss: 0.2495	step2_classification_accuracy: 91.2423	step_2_gate_accuracy: 93.1738
STEP-2	Epoch: 40/200	classification_loss: 0.1902	gate_loss: 0.1189	step2_classification_accuracy: 93.2177	step_2_gate_accuracy: 96.1589
STEP-2	Epoch: 60/200	classification_loss: 0.1574	gate_loss: 0.0800	step2_classification_accuracy: 94.2932	step_2_gate_accuracy: 97.1247
STEP-2	Epoch: 80/200	classification_loss: 0.1408	gate_loss: 0.0670	step2_classification_accuracy: 94.4908	step_2_gate_accuracy: 97.3881
STEP-2	Epoch: 100/200	classification_loss: 0.1343	gate_loss: 0.0579	step2_classification_accuracy: 94.6225	step_2_gate_accuracy: 97.6295
STEP-2	Epoch: 120/200	classification_loss: 0.1305	gate_loss: 0.0544	step2_classification_accuracy: 94.7103	step_2_gate_accuracy: 97.8929
STEP-2	Epoch: 140/200	classification_loss: 0.1155	gate_loss: 0.0453	step2_classification_accuracy: 95.0834	step_2_gate_accuracy: 98.2660
STEP-2	Epoch: 160/200	classification_loss: 0.1115	gate_loss: 0.0420	step2_classification_accuracy: 95.2151	step_2_gate_accuracy: 98.3758
STEP-2	Epoch: 180/200	classification_loss: 0.1084	gate_loss: 0.0411	step2_classification_accuracy: 95.5663	step_2_gate_accuracy: 98.4416
STEP-2	Epoch: 200/200	classification_loss: 0.1148	gate_loss: 0.0408	step2_classification_accuracy: 95.5443	step_2_gate_accuracy: 98.4197
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 86.3309	gate_accuracy: 92.0863
	Task-1	val_accuracy: 74.0260	gate_accuracy: 84.4156
	Task-2	val_accuracy: 87.2340	gate_accuracy: 87.2340
	Task-3	val_accuracy: 78.7234	gate_accuracy: 81.9149
	Task-4	val_accuracy: 88.6076	gate_accuracy: 93.6709
	Task-5	val_accuracy: 90.0000	gate_accuracy: 92.8571
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 88.7884


[134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]
Polling GMM for: {134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153}
STEP-1	Epoch: 10/50	loss: 2.1770	step1_train_accuracy: 46.2541
STEP-1	Epoch: 20/50	loss: 0.9472	step1_train_accuracy: 85.9935
STEP-1	Epoch: 30/50	loss: 0.5147	step1_train_accuracy: 95.1140
STEP-1	Epoch: 40/50	loss: 0.3290	step1_train_accuracy: 96.7427
STEP-1	Epoch: 50/50	loss: 0.2434	step1_train_accuracy: 96.7427
FINISH STEP 1
Task-7	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.2718	gate_loss: 0.3684	step2_classification_accuracy: 90.2150	step_2_gate_accuracy: 89.6776
STEP-2	Epoch: 40/200	classification_loss: 0.2005	gate_loss: 0.1427	step2_classification_accuracy: 92.5213	step_2_gate_accuracy: 95.5441
STEP-2	Epoch: 60/200	classification_loss: 0.1686	gate_loss: 0.0942	step2_classification_accuracy: 93.1706	step_2_gate_accuracy: 96.7980
STEP-2	Epoch: 80/200	classification_loss: 0.1453	gate_loss: 0.0711	step2_classification_accuracy: 93.9543	step_2_gate_accuracy: 97.4250
STEP-2	Epoch: 100/200	classification_loss: 0.1395	gate_loss: 0.0611	step2_classification_accuracy: 94.1335	step_2_gate_accuracy: 98.0519
STEP-2	Epoch: 120/200	classification_loss: 0.1265	gate_loss: 0.0519	step2_classification_accuracy: 94.6932	step_2_gate_accuracy: 98.0072
STEP-2	Epoch: 140/200	classification_loss: 0.1185	gate_loss: 0.0478	step2_classification_accuracy: 94.9172	step_2_gate_accuracy: 98.2087
STEP-2	Epoch: 160/200	classification_loss: 0.1097	gate_loss: 0.0412	step2_classification_accuracy: 95.0739	step_2_gate_accuracy: 98.5893
STEP-2	Epoch: 180/200	classification_loss: 0.1073	gate_loss: 0.0386	step2_classification_accuracy: 95.2530	step_2_gate_accuracy: 98.6565
STEP-2	Epoch: 200/200	classification_loss: 0.1048	gate_loss: 0.0356	step2_classification_accuracy: 95.3874	step_2_gate_accuracy: 98.9476
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.8921	gate_accuracy: 89.2086
	Task-1	val_accuracy: 74.0260	gate_accuracy: 83.1169
	Task-2	val_accuracy: 91.4894	gate_accuracy: 92.5532
	Task-3	val_accuracy: 79.7872	gate_accuracy: 82.9787
	Task-4	val_accuracy: 86.0759	gate_accuracy: 89.8734
	Task-5	val_accuracy: 88.5714	gate_accuracy: 88.5714
	Task-6	val_accuracy: 93.5065	gate_accuracy: 88.3117
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 87.9365


[154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]
Polling GMM for: {154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173}
STEP-1	Epoch: 10/50	loss: 2.2506	step1_train_accuracy: 54.9206
STEP-1	Epoch: 20/50	loss: 0.8838	step1_train_accuracy: 86.3492
STEP-1	Epoch: 30/50	loss: 0.4310	step1_train_accuracy: 94.6032
STEP-1	Epoch: 40/50	loss: 0.2747	step1_train_accuracy: 96.5079
STEP-1	Epoch: 50/50	loss: 0.2037	step1_train_accuracy: 97.4603
FINISH STEP 1
Task-8	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.3254	gate_loss: 0.3716	step2_classification_accuracy: 88.5632	step_2_gate_accuracy: 89.4061
STEP-2	Epoch: 40/200	classification_loss: 0.2322	gate_loss: 0.1721	step2_classification_accuracy: 91.6284	step_2_gate_accuracy: 94.3103
STEP-2	Epoch: 60/200	classification_loss: 0.1955	gate_loss: 0.1241	step2_classification_accuracy: 92.4138	step_2_gate_accuracy: 95.5556
STEP-2	Epoch: 80/200	classification_loss: 0.1809	gate_loss: 0.1025	step2_classification_accuracy: 92.6245	step_2_gate_accuracy: 96.1111
STEP-2	Epoch: 100/200	classification_loss: 0.1629	gate_loss: 0.0854	step2_classification_accuracy: 93.1226	step_2_gate_accuracy: 96.5134
STEP-2	Epoch: 120/200	classification_loss: 0.1538	gate_loss: 0.0780	step2_classification_accuracy: 93.4674	step_2_gate_accuracy: 96.6858
STEP-2	Epoch: 140/200	classification_loss: 0.1455	gate_loss: 0.0709	step2_classification_accuracy: 93.6207	step_2_gate_accuracy: 97.2222
STEP-2	Epoch: 160/200	classification_loss: 0.1387	gate_loss: 0.0653	step2_classification_accuracy: 93.8506	step_2_gate_accuracy: 97.3180
STEP-2	Epoch: 180/200	classification_loss: 0.1333	gate_loss: 0.0615	step2_classification_accuracy: 94.1762	step_2_gate_accuracy: 97.5479
STEP-2	Epoch: 200/200	classification_loss: 0.1337	gate_loss: 0.0611	step2_classification_accuracy: 94.0038	step_2_gate_accuracy: 97.3755
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.8921	gate_accuracy: 90.6475
	Task-1	val_accuracy: 70.1299	gate_accuracy: 80.5195
	Task-2	val_accuracy: 80.8511	gate_accuracy: 86.1702
	Task-3	val_accuracy: 71.2766	gate_accuracy: 75.5319
	Task-4	val_accuracy: 84.8101	gate_accuracy: 89.8734
	Task-5	val_accuracy: 88.5714	gate_accuracy: 91.4286
	Task-6	val_accuracy: 88.3117	gate_accuracy: 84.4156
	Task-7	val_accuracy: 74.6835	gate_accuracy: 77.2152
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 84.7673


[174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]
Polling GMM for: {174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193}
STEP-1	Epoch: 10/50	loss: 2.1364	step1_train_accuracy: 41.1043
STEP-1	Epoch: 20/50	loss: 1.1327	step1_train_accuracy: 73.0061
STEP-1	Epoch: 30/50	loss: 0.7167	step1_train_accuracy: 81.5951
STEP-1	Epoch: 40/50	loss: 0.4693	step1_train_accuracy: 93.8650
STEP-1	Epoch: 50/50	loss: 0.3296	step1_train_accuracy: 95.3988
FINISH STEP 1
Task-9	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.3425	gate_loss: 0.3913	step2_classification_accuracy: 88.7928	step_2_gate_accuracy: 88.6764
STEP-2	Epoch: 40/200	classification_loss: 0.2512	gate_loss: 0.1817	step2_classification_accuracy: 90.8547	step_2_gate_accuracy: 93.6648
STEP-2	Epoch: 60/200	classification_loss: 0.2169	gate_loss: 0.1335	step2_classification_accuracy: 91.7193	step_2_gate_accuracy: 94.8952
STEP-2	Epoch: 80/200	classification_loss: 0.1933	gate_loss: 0.1079	step2_classification_accuracy: 92.5175	step_2_gate_accuracy: 95.7599
STEP-2	Epoch: 100/200	classification_loss: 0.1812	gate_loss: 0.0956	step2_classification_accuracy: 92.9664	step_2_gate_accuracy: 96.1423
STEP-2	Epoch: 120/200	classification_loss: 0.1696	gate_loss: 0.0850	step2_classification_accuracy: 93.1992	step_2_gate_accuracy: 96.3252
STEP-2	Epoch: 140/200	classification_loss: 0.1671	gate_loss: 0.0830	step2_classification_accuracy: 93.2657	step_2_gate_accuracy: 96.4749
STEP-2	Epoch: 160/200	classification_loss: 0.1554	gate_loss: 0.0732	step2_classification_accuracy: 93.7978	step_2_gate_accuracy: 97.0901
STEP-2	Epoch: 180/200	classification_loss: 0.1962	gate_loss: 0.1023	step2_classification_accuracy: 93.2657	step_2_gate_accuracy: 96.3252
STEP-2	Epoch: 200/200	classification_loss: 0.1457	gate_loss: 0.0650	step2_classification_accuracy: 93.9308	step_2_gate_accuracy: 97.2564
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.8921	gate_accuracy: 91.3669
	Task-1	val_accuracy: 70.1299	gate_accuracy: 77.9221
	Task-2	val_accuracy: 85.1064	gate_accuracy: 85.1064
	Task-3	val_accuracy: 74.4681	gate_accuracy: 79.7872
	Task-4	val_accuracy: 87.3418	gate_accuracy: 91.1392
	Task-5	val_accuracy: 90.0000	gate_accuracy: 90.0000
	Task-6	val_accuracy: 92.2078	gate_accuracy: 84.4156
	Task-7	val_accuracy: 74.6835	gate_accuracy: 75.9494
	Task-8	val_accuracy: 82.9268	gate_accuracy: 85.3659
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 84.9557


[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]
Polling GMM for: {194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213}
STEP-1	Epoch: 10/50	loss: 2.1997	step1_train_accuracy: 49.1749
STEP-1	Epoch: 20/50	loss: 0.9532	step1_train_accuracy: 84.1584
STEP-1	Epoch: 30/50	loss: 0.5839	step1_train_accuracy: 90.4290
STEP-1	Epoch: 40/50	loss: 0.3667	step1_train_accuracy: 93.7294
STEP-1	Epoch: 50/50	loss: 0.2743	step1_train_accuracy: 95.3795
FINISH STEP 1
Task-10	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.4150	gate_loss: 0.4497	step2_classification_accuracy: 86.0251	step_2_gate_accuracy: 86.0105
STEP-2	Epoch: 40/200	classification_loss: 0.3056	gate_loss: 0.2213	step2_classification_accuracy: 88.8289	step_2_gate_accuracy: 92.3335
STEP-2	Epoch: 60/200	classification_loss: 0.2608	gate_loss: 0.1650	step2_classification_accuracy: 89.9825	step_2_gate_accuracy: 93.6770
STEP-2	Epoch: 80/200	classification_loss: 0.2326	gate_loss: 0.1355	step2_classification_accuracy: 90.9025	step_2_gate_accuracy: 94.6846
STEP-2	Epoch: 100/200	classification_loss: 0.2080	gate_loss: 0.1152	step2_classification_accuracy: 91.6326	step_2_gate_accuracy: 95.4731
STEP-2	Epoch: 120/200	classification_loss: 0.1923	gate_loss: 0.1042	step2_classification_accuracy: 91.7932	step_2_gate_accuracy: 95.8820
STEP-2	Epoch: 140/200	classification_loss: 0.1856	gate_loss: 0.0956	step2_classification_accuracy: 92.2167	step_2_gate_accuracy: 96.1887
STEP-2	Epoch: 160/200	classification_loss: 0.1814	gate_loss: 0.0909	step2_classification_accuracy: 92.4650	step_2_gate_accuracy: 96.4223
STEP-2	Epoch: 180/200	classification_loss: 0.1701	gate_loss: 0.0842	step2_classification_accuracy: 92.7132	step_2_gate_accuracy: 96.7874
STEP-2	Epoch: 200/200	classification_loss: 0.1679	gate_loss: 0.0817	step2_classification_accuracy: 92.8008	step_2_gate_accuracy: 96.7290
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.5755	gate_accuracy: 87.0504
	Task-1	val_accuracy: 63.6364	gate_accuracy: 79.2208
	Task-2	val_accuracy: 85.1064	gate_accuracy: 89.3617
	Task-3	val_accuracy: 70.2128	gate_accuracy: 72.3404
	Task-4	val_accuracy: 81.0127	gate_accuracy: 84.8101
	Task-5	val_accuracy: 91.4286	gate_accuracy: 91.4286
	Task-6	val_accuracy: 87.0130	gate_accuracy: 83.1169
	Task-7	val_accuracy: 77.2152	gate_accuracy: 75.9494
	Task-8	val_accuracy: 81.7073	gate_accuracy: 81.7073
	Task-9	val_accuracy: 71.0526	gate_accuracy: 71.0526
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.8916


[214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]
Polling GMM for: {214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233}
STEP-1	Epoch: 10/50	loss: 2.2599	step1_train_accuracy: 58.0838
STEP-1	Epoch: 20/50	loss: 0.8920	step1_train_accuracy: 85.3293
STEP-1	Epoch: 30/50	loss: 0.4440	step1_train_accuracy: 96.4072
STEP-1	Epoch: 40/50	loss: 0.2803	step1_train_accuracy: 98.8024
STEP-1	Epoch: 50/50	loss: 0.1903	step1_train_accuracy: 98.8024
FINISH STEP 1
Task-11	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.3907	gate_loss: 0.4118	step2_classification_accuracy: 86.4543	step_2_gate_accuracy: 87.1018
STEP-2	Epoch: 40/200	classification_loss: 0.2870	gate_loss: 0.2107	step2_classification_accuracy: 89.3939	step_2_gate_accuracy: 92.3595
STEP-2	Epoch: 60/200	classification_loss: 0.2480	gate_loss: 0.1600	step2_classification_accuracy: 90.6630	step_2_gate_accuracy: 93.3955
STEP-2	Epoch: 80/200	classification_loss: 0.2176	gate_loss: 0.1351	step2_classification_accuracy: 91.5436	step_2_gate_accuracy: 94.5480
STEP-2	Epoch: 100/200	classification_loss: 0.2009	gate_loss: 0.1175	step2_classification_accuracy: 92.3465	step_2_gate_accuracy: 95.2603
STEP-2	Epoch: 120/200	classification_loss: 0.1828	gate_loss: 0.1045	step2_classification_accuracy: 92.6185	step_2_gate_accuracy: 95.6358
STEP-2	Epoch: 140/200	classification_loss: 0.1768	gate_loss: 0.0973	step2_classification_accuracy: 92.8257	step_2_gate_accuracy: 95.8819
STEP-2	Epoch: 160/200	classification_loss: 0.1608	gate_loss: 0.0892	step2_classification_accuracy: 93.4473	step_2_gate_accuracy: 96.3222
STEP-2	Epoch: 180/200	classification_loss: 0.1552	gate_loss: 0.0839	step2_classification_accuracy: 93.6804	step_2_gate_accuracy: 96.3869
STEP-2	Epoch: 200/200	classification_loss: 0.1546	gate_loss: 0.0820	step2_classification_accuracy: 93.6156	step_2_gate_accuracy: 96.5682
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 84.1727	gate_accuracy: 89.2086
	Task-1	val_accuracy: 66.2338	gate_accuracy: 80.5195
	Task-2	val_accuracy: 78.7234	gate_accuracy: 80.8511
	Task-3	val_accuracy: 73.4043	gate_accuracy: 78.7234
	Task-4	val_accuracy: 84.8101	gate_accuracy: 89.8734
	Task-5	val_accuracy: 85.7143	gate_accuracy: 88.5714
	Task-6	val_accuracy: 87.0130	gate_accuracy: 81.8182
	Task-7	val_accuracy: 67.0886	gate_accuracy: 70.8861
	Task-8	val_accuracy: 79.2683	gate_accuracy: 80.4878
	Task-9	val_accuracy: 77.6316	gate_accuracy: 76.3158
	Task-10	val_accuracy: 92.7711	gate_accuracy: 91.5663
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 82.9474


[234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]
Polling GMM for: {234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253}
STEP-1	Epoch: 10/50	loss: 2.3973	step1_train_accuracy: 54.4872
STEP-1	Epoch: 20/50	loss: 0.9287	step1_train_accuracy: 85.5769
STEP-1	Epoch: 30/50	loss: 0.4409	step1_train_accuracy: 95.8333
STEP-1	Epoch: 40/50	loss: 0.2739	step1_train_accuracy: 97.7564
STEP-1	Epoch: 50/50	loss: 0.1921	step1_train_accuracy: 98.7179
FINISH STEP 1
Task-12	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.4425	gate_loss: 0.4736	step2_classification_accuracy: 85.3716	step_2_gate_accuracy: 84.8302
STEP-2	Epoch: 40/200	classification_loss: 0.3332	gate_loss: 0.2490	step2_classification_accuracy: 88.6319	step_2_gate_accuracy: 91.1171
STEP-2	Epoch: 60/200	classification_loss: 0.2758	gate_loss: 0.1819	step2_classification_accuracy: 89.9360	step_2_gate_accuracy: 92.9257
STEP-2	Epoch: 80/200	classification_loss: 0.2363	gate_loss: 0.1456	step2_classification_accuracy: 91.1294	step_2_gate_accuracy: 94.3036
STEP-2	Epoch: 100/200	classification_loss: 0.2201	gate_loss: 0.1289	step2_classification_accuracy: 91.4739	step_2_gate_accuracy: 95.0418
STEP-2	Epoch: 120/200	classification_loss: 0.2086	gate_loss: 0.1182	step2_classification_accuracy: 92.0153	step_2_gate_accuracy: 95.3494
STEP-2	Epoch: 140/200	classification_loss: 0.1922	gate_loss: 0.1057	step2_classification_accuracy: 92.4336	step_2_gate_accuracy: 95.8538
STEP-2	Epoch: 160/200	classification_loss: 0.1795	gate_loss: 0.0970	step2_classification_accuracy: 92.8273	step_2_gate_accuracy: 96.2598
STEP-2	Epoch: 180/200	classification_loss: 0.1758	gate_loss: 0.0925	step2_classification_accuracy: 92.8888	step_2_gate_accuracy: 96.3583
STEP-2	Epoch: 200/200	classification_loss: 0.1703	gate_loss: 0.0885	step2_classification_accuracy: 93.1594	step_2_gate_accuracy: 96.4567
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.5755	gate_accuracy: 87.0504
	Task-1	val_accuracy: 61.0390	gate_accuracy: 74.0260
	Task-2	val_accuracy: 80.8511	gate_accuracy: 85.1064
	Task-3	val_accuracy: 73.4043	gate_accuracy: 74.4681
	Task-4	val_accuracy: 79.7468	gate_accuracy: 84.8101
	Task-5	val_accuracy: 84.2857	gate_accuracy: 84.2857
	Task-6	val_accuracy: 85.7143	gate_accuracy: 83.1169
	Task-7	val_accuracy: 70.8861	gate_accuracy: 72.1519
	Task-8	val_accuracy: 79.2683	gate_accuracy: 84.1463
	Task-9	val_accuracy: 69.7368	gate_accuracy: 69.7368
	Task-10	val_accuracy: 91.5663	gate_accuracy: 91.5663
	Task-11	val_accuracy: 85.8974	gate_accuracy: 84.6154
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 81.6148


[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]
Polling GMM for: {254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273}
STEP-1	Epoch: 10/50	loss: 1.9675	step1_train_accuracy: 56.3584
STEP-1	Epoch: 20/50	loss: 0.8478	step1_train_accuracy: 80.3468
STEP-1	Epoch: 30/50	loss: 0.4996	step1_train_accuracy: 91.9075
STEP-1	Epoch: 40/50	loss: 0.3413	step1_train_accuracy: 93.9306
STEP-1	Epoch: 50/50	loss: 0.2566	step1_train_accuracy: 95.9538
FINISH STEP 1
Task-13	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.4666	gate_loss: 0.5036	step2_classification_accuracy: 84.6943	step_2_gate_accuracy: 83.4968
STEP-2	Epoch: 40/200	classification_loss: 0.3335	gate_loss: 0.2651	step2_classification_accuracy: 87.9448	step_2_gate_accuracy: 90.4881
STEP-2	Epoch: 60/200	classification_loss: 0.2823	gate_loss: 0.1992	step2_classification_accuracy: 89.2564	step_2_gate_accuracy: 92.1077
STEP-2	Epoch: 80/200	classification_loss: 0.2592	gate_loss: 0.1707	step2_classification_accuracy: 90.3057	step_2_gate_accuracy: 93.4078
STEP-2	Epoch: 100/200	classification_loss: 0.2331	gate_loss: 0.1479	step2_classification_accuracy: 91.2067	step_2_gate_accuracy: 94.0237
STEP-2	Epoch: 120/200	classification_loss: 0.2099	gate_loss: 0.1322	step2_classification_accuracy: 91.7541	step_2_gate_accuracy: 94.6738
STEP-2	Epoch: 140/200	classification_loss: 0.1995	gate_loss: 0.1210	step2_classification_accuracy: 92.0392	step_2_gate_accuracy: 95.0616
STEP-2	Epoch: 160/200	classification_loss: 0.2013	gate_loss: 0.1205	step2_classification_accuracy: 92.0164	step_2_gate_accuracy: 94.9133
STEP-2	Epoch: 180/200	classification_loss: 0.1825	gate_loss: 0.1081	step2_classification_accuracy: 92.7806	step_2_gate_accuracy: 95.6775
STEP-2	Epoch: 200/200	classification_loss: 0.1737	gate_loss: 0.1020	step2_classification_accuracy: 92.9402	step_2_gate_accuracy: 95.6889
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 77.6978	gate_accuracy: 87.0504
	Task-1	val_accuracy: 62.3377	gate_accuracy: 76.6234
	Task-2	val_accuracy: 85.1064	gate_accuracy: 82.9787
	Task-3	val_accuracy: 62.7660	gate_accuracy: 69.1489
	Task-4	val_accuracy: 83.5443	gate_accuracy: 86.0759
	Task-5	val_accuracy: 84.2857	gate_accuracy: 84.2857
	Task-6	val_accuracy: 90.9091	gate_accuracy: 83.1169
	Task-7	val_accuracy: 64.5570	gate_accuracy: 64.5570
	Task-8	val_accuracy: 76.8293	gate_accuracy: 82.9268
	Task-9	val_accuracy: 69.7368	gate_accuracy: 67.1053
	Task-10	val_accuracy: 90.3614	gate_accuracy: 91.5663
	Task-11	val_accuracy: 88.4615	gate_accuracy: 78.2051
	Task-12	val_accuracy: 81.6092	gate_accuracy: 80.4598
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.9103


[274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]
Polling GMM for: {274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293}
STEP-1	Epoch: 10/50	loss: 1.8943	step1_train_accuracy: 69.3989
STEP-1	Epoch: 20/50	loss: 0.7312	step1_train_accuracy: 86.0656
STEP-1	Epoch: 30/50	loss: 0.3496	step1_train_accuracy: 97.5410
STEP-1	Epoch: 40/50	loss: 0.2083	step1_train_accuracy: 98.6339
STEP-1	Epoch: 50/50	loss: 0.1465	step1_train_accuracy: 99.1803
FINISH STEP 1
Task-14	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.4646	gate_loss: 0.5049	step2_classification_accuracy: 84.5982	step_2_gate_accuracy: 84.2368
STEP-2	Epoch: 40/200	classification_loss: 0.3414	gate_loss: 0.2575	step2_classification_accuracy: 88.5523	step_2_gate_accuracy: 90.7632
STEP-2	Epoch: 60/200	classification_loss: 0.2829	gate_loss: 0.1912	step2_classification_accuracy: 89.6152	step_2_gate_accuracy: 93.0272
STEP-2	Epoch: 80/200	classification_loss: 0.2549	gate_loss: 0.1627	step2_classification_accuracy: 90.5825	step_2_gate_accuracy: 93.6862
STEP-2	Epoch: 100/200	classification_loss: 0.2289	gate_loss: 0.1410	step2_classification_accuracy: 91.5072	step_2_gate_accuracy: 94.7917
STEP-2	Epoch: 120/200	classification_loss: 0.2206	gate_loss: 0.1303	step2_classification_accuracy: 91.8155	step_2_gate_accuracy: 94.9086
STEP-2	Epoch: 140/200	classification_loss: 0.2045	gate_loss: 0.1189	step2_classification_accuracy: 92.1025	step_2_gate_accuracy: 95.2806
STEP-2	Epoch: 160/200	classification_loss: 0.1949	gate_loss: 0.1083	step2_classification_accuracy: 92.5808	step_2_gate_accuracy: 95.8014
STEP-2	Epoch: 180/200	classification_loss: 0.1829	gate_loss: 0.1021	step2_classification_accuracy: 92.6977	step_2_gate_accuracy: 96.0034
STEP-2	Epoch: 200/200	classification_loss: 0.1817	gate_loss: 0.0988	step2_classification_accuracy: 92.8571	step_2_gate_accuracy: 95.9821
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.9784	gate_accuracy: 85.6115
	Task-1	val_accuracy: 55.8442	gate_accuracy: 71.4286
	Task-2	val_accuracy: 79.7872	gate_accuracy: 82.9787
	Task-3	val_accuracy: 71.2766	gate_accuracy: 75.5319
	Task-4	val_accuracy: 82.2785	gate_accuracy: 83.5443
	Task-5	val_accuracy: 84.2857	gate_accuracy: 85.7143
	Task-6	val_accuracy: 90.9091	gate_accuracy: 85.7143
	Task-7	val_accuracy: 73.4177	gate_accuracy: 78.4810
	Task-8	val_accuracy: 75.6098	gate_accuracy: 79.2683
	Task-9	val_accuracy: 68.4211	gate_accuracy: 69.7368
	Task-10	val_accuracy: 92.7711	gate_accuracy: 90.3614
	Task-11	val_accuracy: 76.9231	gate_accuracy: 73.0769
	Task-12	val_accuracy: 80.4598	gate_accuracy: 78.1609
	Task-13	val_accuracy: 84.7826	gate_accuracy: 79.3478
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 80.1988


[294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]
Polling GMM for: {294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313}
STEP-1	Epoch: 10/50	loss: 2.7921	step1_train_accuracy: 47.0588
STEP-1	Epoch: 20/50	loss: 1.1283	step1_train_accuracy: 79.4118
STEP-1	Epoch: 30/50	loss: 0.6076	step1_train_accuracy: 90.0735
STEP-1	Epoch: 40/50	loss: 0.4077	step1_train_accuracy: 94.8529
STEP-1	Epoch: 50/50	loss: 0.2806	step1_train_accuracy: 97.7941
FINISH STEP 1
Task-15	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.4913	gate_loss: 0.5549	step2_classification_accuracy: 83.8323	step_2_gate_accuracy: 82.2399
STEP-2	Epoch: 40/200	classification_loss: 0.3506	gate_loss: 0.2843	step2_classification_accuracy: 88.0255	step_2_gate_accuracy: 90.0425
STEP-2	Epoch: 60/200	classification_loss: 0.2924	gate_loss: 0.2100	step2_classification_accuracy: 89.8301	step_2_gate_accuracy: 91.9639
STEP-2	Epoch: 80/200	classification_loss: 0.2572	gate_loss: 0.1715	step2_classification_accuracy: 91.0616	step_2_gate_accuracy: 93.5563
STEP-2	Epoch: 100/200	classification_loss: 0.2325	gate_loss: 0.1516	step2_classification_accuracy: 91.4013	step_2_gate_accuracy: 93.9384
STEP-2	Epoch: 120/200	classification_loss: 0.2230	gate_loss: 0.1396	step2_classification_accuracy: 91.6030	step_2_gate_accuracy: 94.3737
STEP-2	Epoch: 140/200	classification_loss: 0.2003	gate_loss: 0.1218	step2_classification_accuracy: 92.4947	step_2_gate_accuracy: 95.2336
STEP-2	Epoch: 160/200	classification_loss: 0.1929	gate_loss: 0.1161	step2_classification_accuracy: 92.6858	step_2_gate_accuracy: 95.3185
STEP-2	Epoch: 180/200	classification_loss: 0.1883	gate_loss: 0.1109	step2_classification_accuracy: 92.7389	step_2_gate_accuracy: 95.6900
STEP-2	Epoch: 200/200	classification_loss: 0.1831	gate_loss: 0.1057	step2_classification_accuracy: 92.8344	step_2_gate_accuracy: 95.8068
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.1367	gate_accuracy: 88.4892
	Task-1	val_accuracy: 59.7403	gate_accuracy: 76.6234
	Task-2	val_accuracy: 86.1702	gate_accuracy: 86.1702
	Task-3	val_accuracy: 69.1489	gate_accuracy: 72.3404
	Task-4	val_accuracy: 78.4810	gate_accuracy: 84.8101
	Task-5	val_accuracy: 82.8571	gate_accuracy: 85.7143
	Task-6	val_accuracy: 83.1169	gate_accuracy: 80.5195
	Task-7	val_accuracy: 62.0253	gate_accuracy: 63.2911
	Task-8	val_accuracy: 74.3902	gate_accuracy: 73.1707
	Task-9	val_accuracy: 61.8421	gate_accuracy: 60.5263
	Task-10	val_accuracy: 92.7711	gate_accuracy: 90.3614
	Task-11	val_accuracy: 82.0513	gate_accuracy: 79.4872
	Task-12	val_accuracy: 80.4598	gate_accuracy: 81.6092
	Task-13	val_accuracy: 86.9565	gate_accuracy: 84.7826
	Task-14	val_accuracy: 79.4118	gate_accuracy: 77.9412
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 79.6078


[314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]
Polling GMM for: {314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333}
STEP-1	Epoch: 10/50	loss: 2.7427	step1_train_accuracy: 59.8513
STEP-1	Epoch: 20/50	loss: 0.8322	step1_train_accuracy: 91.8216
STEP-1	Epoch: 30/50	loss: 0.4058	step1_train_accuracy: 97.0260
STEP-1	Epoch: 40/50	loss: 0.2631	step1_train_accuracy: 98.5130
STEP-1	Epoch: 50/50	loss: 0.1922	step1_train_accuracy: 98.5130
FINISH STEP 1
Task-16	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.5208	gate_loss: 0.5609	step2_classification_accuracy: 83.3271	step_2_gate_accuracy: 82.1388
STEP-2	Epoch: 40/200	classification_loss: 0.3625	gate_loss: 0.2973	step2_classification_accuracy: 88.0052	step_2_gate_accuracy: 89.4835
STEP-2	Epoch: 60/200	classification_loss: 0.3086	gate_loss: 0.2244	step2_classification_accuracy: 89.2403	step_2_gate_accuracy: 91.6542
STEP-2	Epoch: 80/200	classification_loss: 0.2703	gate_loss: 0.1882	step2_classification_accuracy: 90.4285	step_2_gate_accuracy: 93.0857
STEP-2	Epoch: 100/200	classification_loss: 0.2414	gate_loss: 0.1622	step2_classification_accuracy: 91.2425	step_2_gate_accuracy: 93.7968
STEP-2	Epoch: 120/200	classification_loss: 0.2256	gate_loss: 0.1456	step2_classification_accuracy: 91.7945	step_2_gate_accuracy: 94.2178
STEP-2	Epoch: 140/200	classification_loss: 0.2122	gate_loss: 0.1354	step2_classification_accuracy: 92.2811	step_2_gate_accuracy: 94.7137
STEP-2	Epoch: 160/200	classification_loss: 0.2032	gate_loss: 0.1276	step2_classification_accuracy: 92.4682	step_2_gate_accuracy: 95.1534
STEP-2	Epoch: 180/200	classification_loss: 0.1932	gate_loss: 0.1217	step2_classification_accuracy: 92.5992	step_2_gate_accuracy: 95.3219
STEP-2	Epoch: 200/200	classification_loss: 0.1868	gate_loss: 0.1136	step2_classification_accuracy: 92.8892	step_2_gate_accuracy: 95.4996
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 80.5755	gate_accuracy: 89.9281
	Task-1	val_accuracy: 50.6494	gate_accuracy: 66.2338
	Task-2	val_accuracy: 75.5319	gate_accuracy: 74.4681
	Task-3	val_accuracy: 65.9574	gate_accuracy: 70.2128
	Task-4	val_accuracy: 82.2785	gate_accuracy: 86.0759
	Task-5	val_accuracy: 82.8571	gate_accuracy: 82.8571
	Task-6	val_accuracy: 84.4156	gate_accuracy: 81.8182
	Task-7	val_accuracy: 65.8228	gate_accuracy: 69.6203
	Task-8	val_accuracy: 75.6098	gate_accuracy: 81.7073
	Task-9	val_accuracy: 68.4211	gate_accuracy: 69.7368
	Task-10	val_accuracy: 91.5663	gate_accuracy: 90.3614
	Task-11	val_accuracy: 76.9231	gate_accuracy: 75.6410
	Task-12	val_accuracy: 85.0575	gate_accuracy: 80.4598
	Task-13	val_accuracy: 84.7826	gate_accuracy: 83.6957
	Task-14	val_accuracy: 82.3529	gate_accuracy: 77.9412
	Task-15	val_accuracy: 73.1343	gate_accuracy: 73.1343
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.9121


[334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]
Polling GMM for: {334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353}
STEP-1	Epoch: 10/50	loss: 2.5334	step1_train_accuracy: 50.1458
STEP-1	Epoch: 20/50	loss: 0.9531	step1_train_accuracy: 82.7988
STEP-1	Epoch: 30/50	loss: 0.4535	step1_train_accuracy: 93.0029
STEP-1	Epoch: 40/50	loss: 0.2945	step1_train_accuracy: 94.4606
STEP-1	Epoch: 50/50	loss: 0.2276	step1_train_accuracy: 96.2099
FINISH STEP 1
Task-17	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.5245	gate_loss: 0.5842	step2_classification_accuracy: 83.0067	step_2_gate_accuracy: 81.3118
STEP-2	Epoch: 40/200	classification_loss: 0.3794	gate_loss: 0.3107	step2_classification_accuracy: 86.9968	step_2_gate_accuracy: 88.7270
STEP-2	Epoch: 60/200	classification_loss: 0.3133	gate_loss: 0.2318	step2_classification_accuracy: 88.9389	step_2_gate_accuracy: 91.3489
STEP-2	Epoch: 80/200	classification_loss: 0.2770	gate_loss: 0.1944	step2_classification_accuracy: 89.9806	step_2_gate_accuracy: 92.4082
STEP-2	Epoch: 100/200	classification_loss: 0.2517	gate_loss: 0.1703	step2_classification_accuracy: 90.7751	step_2_gate_accuracy: 93.4057
STEP-2	Epoch: 120/200	classification_loss: 0.2348	gate_loss: 0.1519	step2_classification_accuracy: 91.2782	step_2_gate_accuracy: 94.1296
STEP-2	Epoch: 140/200	classification_loss: 0.2189	gate_loss: 0.1414	step2_classification_accuracy: 91.5960	step_2_gate_accuracy: 94.3856
STEP-2	Epoch: 160/200	classification_loss: 0.2056	gate_loss: 0.1281	step2_classification_accuracy: 92.3464	step_2_gate_accuracy: 94.9770
STEP-2	Epoch: 180/200	classification_loss: 0.1991	gate_loss: 0.1232	step2_classification_accuracy: 92.3111	step_2_gate_accuracy: 95.0124
STEP-2	Epoch: 200/200	classification_loss: 0.1901	gate_loss: 0.1159	step2_classification_accuracy: 92.6907	step_2_gate_accuracy: 95.6480
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 81.2950	gate_accuracy: 89.9281
	Task-1	val_accuracy: 51.9481	gate_accuracy: 63.6364
	Task-2	val_accuracy: 78.7234	gate_accuracy: 82.9787
	Task-3	val_accuracy: 67.0213	gate_accuracy: 75.5319
	Task-4	val_accuracy: 77.2152	gate_accuracy: 81.0127
	Task-5	val_accuracy: 82.8571	gate_accuracy: 84.2857
	Task-6	val_accuracy: 83.1169	gate_accuracy: 83.1169
	Task-7	val_accuracy: 53.1646	gate_accuracy: 54.4304
	Task-8	val_accuracy: 79.2683	gate_accuracy: 80.4878
	Task-9	val_accuracy: 67.1053	gate_accuracy: 67.1053
	Task-10	val_accuracy: 89.1566	gate_accuracy: 89.1566
	Task-11	val_accuracy: 80.7692	gate_accuracy: 75.6410
	Task-12	val_accuracy: 79.3103	gate_accuracy: 79.3103
	Task-13	val_accuracy: 85.8696	gate_accuracy: 85.8696
	Task-14	val_accuracy: 77.9412	gate_accuracy: 79.4118
	Task-15	val_accuracy: 77.6119	gate_accuracy: 76.1194
	Task-16	val_accuracy: 75.5814	gate_accuracy: 76.7442
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 78.5714


[354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]
Polling GMM for: {354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373}
STEP-1	Epoch: 10/50	loss: 2.1302	step1_train_accuracy: 52.7950
STEP-1	Epoch: 20/50	loss: 0.8761	step1_train_accuracy: 82.2981
STEP-1	Epoch: 30/50	loss: 0.4580	step1_train_accuracy: 97.5155
STEP-1	Epoch: 40/50	loss: 0.2970	step1_train_accuracy: 98.4472
STEP-1	Epoch: 50/50	loss: 0.2007	step1_train_accuracy: 98.4472
FINISH STEP 1
Task-18	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.5567	gate_loss: 0.5954	step2_classification_accuracy: 82.8710	step_2_gate_accuracy: 81.1330
STEP-2	Epoch: 40/200	classification_loss: 0.3849	gate_loss: 0.3184	step2_classification_accuracy: 87.3663	step_2_gate_accuracy: 88.8620
STEP-2	Epoch: 60/200	classification_loss: 0.3142	gate_loss: 0.2362	step2_classification_accuracy: 89.2630	step_2_gate_accuracy: 91.5107
STEP-2	Epoch: 80/200	classification_loss: 0.2779	gate_loss: 0.1971	step2_classification_accuracy: 90.0819	step_2_gate_accuracy: 92.5635
STEP-2	Epoch: 100/200	classification_loss: 0.2499	gate_loss: 0.1701	step2_classification_accuracy: 91.0010	step_2_gate_accuracy: 93.4659
STEP-2	Epoch: 120/200	classification_loss: 0.2289	gate_loss: 0.1523	step2_classification_accuracy: 91.6945	step_2_gate_accuracy: 94.1093
STEP-2	Epoch: 140/200	classification_loss: 0.2133	gate_loss: 0.1400	step2_classification_accuracy: 92.2126	step_2_gate_accuracy: 94.5688
STEP-2	Epoch: 160/200	classification_loss: 0.2055	gate_loss: 0.1312	step2_classification_accuracy: 92.3797	step_2_gate_accuracy: 95.0201
STEP-2	Epoch: 180/200	classification_loss: 0.1950	gate_loss: 0.1230	step2_classification_accuracy: 92.7390	step_2_gate_accuracy: 95.2958
STEP-2	Epoch: 200/200	classification_loss: 0.1922	gate_loss: 0.1171	step2_classification_accuracy: 92.6220	step_2_gate_accuracy: 95.3626
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 78.4173	gate_accuracy: 84.8921
	Task-1	val_accuracy: 57.1429	gate_accuracy: 68.8312
	Task-2	val_accuracy: 74.4681	gate_accuracy: 78.7234
	Task-3	val_accuracy: 69.1489	gate_accuracy: 73.4043
	Task-4	val_accuracy: 78.4810	gate_accuracy: 82.2785
	Task-5	val_accuracy: 78.5714	gate_accuracy: 81.4286
	Task-6	val_accuracy: 83.1169	gate_accuracy: 75.3247
	Task-7	val_accuracy: 63.2911	gate_accuracy: 64.5570
	Task-8	val_accuracy: 74.3902	gate_accuracy: 75.6098
	Task-9	val_accuracy: 63.1579	gate_accuracy: 71.0526
	Task-10	val_accuracy: 95.1807	gate_accuracy: 95.1807
	Task-11	val_accuracy: 75.6410	gate_accuracy: 74.3590
	Task-12	val_accuracy: 80.4598	gate_accuracy: 77.0115
	Task-13	val_accuracy: 83.6957	gate_accuracy: 81.5217
	Task-14	val_accuracy: 85.2941	gate_accuracy: 85.2941
	Task-15	val_accuracy: 74.6269	gate_accuracy: 74.6269
	Task-16	val_accuracy: 68.6047	gate_accuracy: 66.2791
	Task-17	val_accuracy: 77.5000	gate_accuracy: 73.7500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 77.1883


[374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]
Polling GMM for: {374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393}
STEP-1	Epoch: 10/50	loss: 2.0329	step1_train_accuracy: 49.7110
STEP-1	Epoch: 20/50	loss: 0.8073	step1_train_accuracy: 90.1734
STEP-1	Epoch: 30/50	loss: 0.4419	step1_train_accuracy: 95.9538
STEP-1	Epoch: 40/50	loss: 0.2997	step1_train_accuracy: 96.5318
STEP-1	Epoch: 50/50	loss: 0.2142	step1_train_accuracy: 97.1098
FINISH STEP 1
Task-19	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.5834	gate_loss: 0.6385	step2_classification_accuracy: 81.9205	step_2_gate_accuracy: 80.3130
STEP-2	Epoch: 40/200	classification_loss: 0.4126	gate_loss: 0.3374	step2_classification_accuracy: 86.4467	step_2_gate_accuracy: 88.4602
STEP-2	Epoch: 60/200	classification_loss: 0.3424	gate_loss: 0.2533	step2_classification_accuracy: 88.3418	step_2_gate_accuracy: 90.7953
STEP-2	Epoch: 80/200	classification_loss: 0.3005	gate_loss: 0.2096	step2_classification_accuracy: 89.6531	step_2_gate_accuracy: 92.4619
STEP-2	Epoch: 100/200	classification_loss: 0.2732	gate_loss: 0.1836	step2_classification_accuracy: 90.3553	step_2_gate_accuracy: 93.0118
STEP-2	Epoch: 120/200	classification_loss: 0.2498	gate_loss: 0.1645	step2_classification_accuracy: 90.9306	step_2_gate_accuracy: 93.8325
STEP-2	Epoch: 140/200	classification_loss: 0.2310	gate_loss: 0.1474	step2_classification_accuracy: 91.5905	step_2_gate_accuracy: 94.5516
STEP-2	Epoch: 160/200	classification_loss: 0.2251	gate_loss: 0.1407	step2_classification_accuracy: 91.6920	step_2_gate_accuracy: 94.4924
STEP-2	Epoch: 180/200	classification_loss: 0.2138	gate_loss: 0.1291	step2_classification_accuracy: 91.9797	step_2_gate_accuracy: 94.9662
STEP-2	Epoch: 200/200	classification_loss: 0.2016	gate_loss: 0.1241	step2_classification_accuracy: 92.1658	step_2_gate_accuracy: 95.2284
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 72.6619	gate_accuracy: 81.2950
	Task-1	val_accuracy: 59.7403	gate_accuracy: 71.4286
	Task-2	val_accuracy: 72.3404	gate_accuracy: 77.6596
	Task-3	val_accuracy: 70.2128	gate_accuracy: 76.5957
	Task-4	val_accuracy: 77.2152	gate_accuracy: 81.0127
	Task-5	val_accuracy: 80.0000	gate_accuracy: 77.1429
	Task-6	val_accuracy: 85.7143	gate_accuracy: 85.7143
	Task-7	val_accuracy: 58.2278	gate_accuracy: 54.4304
	Task-8	val_accuracy: 74.3902	gate_accuracy: 78.0488
	Task-9	val_accuracy: 64.4737	gate_accuracy: 68.4211
	Task-10	val_accuracy: 93.9759	gate_accuracy: 92.7711
	Task-11	val_accuracy: 79.4872	gate_accuracy: 76.9231
	Task-12	val_accuracy: 79.3103	gate_accuracy: 77.0115
	Task-13	val_accuracy: 85.8696	gate_accuracy: 84.7826
	Task-14	val_accuracy: 72.0588	gate_accuracy: 73.5294
	Task-15	val_accuracy: 73.1343	gate_accuracy: 70.1493
	Task-16	val_accuracy: 70.9302	gate_accuracy: 67.4419
	Task-17	val_accuracy: 78.7500	gate_accuracy: 78.7500
	Task-18	val_accuracy: 83.7209	gate_accuracy: 83.7209
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 77.0389


[394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]
Polling GMM for: {394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413}
STEP-1	Epoch: 10/50	loss: 2.4015	step1_train_accuracy: 55.4839
STEP-1	Epoch: 20/50	loss: 1.0056	step1_train_accuracy: 81.6129
STEP-1	Epoch: 30/50	loss: 0.5619	step1_train_accuracy: 87.4194
STEP-1	Epoch: 40/50	loss: 0.3659	step1_train_accuracy: 94.5161
STEP-1	Epoch: 50/50	loss: 0.2616	step1_train_accuracy: 95.1613
FINISH STEP 1
Task-20	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.5982	gate_loss: 0.6568	step2_classification_accuracy: 81.3929	step_2_gate_accuracy: 79.5572
STEP-2	Epoch: 40/200	classification_loss: 0.4336	gate_loss: 0.3583	step2_classification_accuracy: 86.0145	step_2_gate_accuracy: 87.6973
STEP-2	Epoch: 60/200	classification_loss: 0.3582	gate_loss: 0.2724	step2_classification_accuracy: 88.0193	step_2_gate_accuracy: 90.3784
STEP-2	Epoch: 80/200	classification_loss: 0.3082	gate_loss: 0.2203	step2_classification_accuracy: 89.1948	step_2_gate_accuracy: 91.9726
STEP-2	Epoch: 100/200	classification_loss: 0.2893	gate_loss: 0.1981	step2_classification_accuracy: 89.8953	step_2_gate_accuracy: 92.7456
STEP-2	Epoch: 120/200	classification_loss: 0.2644	gate_loss: 0.1772	step2_classification_accuracy: 90.7971	step_2_gate_accuracy: 93.4380
STEP-2	Epoch: 140/200	classification_loss: 0.2484	gate_loss: 0.1603	step2_classification_accuracy: 91.1111	step_2_gate_accuracy: 94.0982
STEP-2	Epoch: 160/200	classification_loss: 0.2420	gate_loss: 0.1560	step2_classification_accuracy: 91.2721	step_2_gate_accuracy: 94.2029
STEP-2	Epoch: 180/200	classification_loss: 0.2231	gate_loss: 0.1428	step2_classification_accuracy: 91.8519	step_2_gate_accuracy: 94.7746
STEP-2	Epoch: 200/200	classification_loss: 0.2154	gate_loss: 0.1353	step2_classification_accuracy: 91.9163	step_2_gate_accuracy: 94.7504
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.1007	gate_accuracy: 81.2950
	Task-1	val_accuracy: 49.3506	gate_accuracy: 63.6364
	Task-2	val_accuracy: 75.5319	gate_accuracy: 74.4681
	Task-3	val_accuracy: 67.0213	gate_accuracy: 71.2766
	Task-4	val_accuracy: 81.0127	gate_accuracy: 82.2785
	Task-5	val_accuracy: 77.1429	gate_accuracy: 81.4286
	Task-6	val_accuracy: 81.8182	gate_accuracy: 76.6234
	Task-7	val_accuracy: 59.4937	gate_accuracy: 56.9620
	Task-8	val_accuracy: 79.2683	gate_accuracy: 79.2683
	Task-9	val_accuracy: 60.5263	gate_accuracy: 67.1053
	Task-10	val_accuracy: 90.3614	gate_accuracy: 86.7470
	Task-11	val_accuracy: 75.6410	gate_accuracy: 74.3590
	Task-12	val_accuracy: 78.1609	gate_accuracy: 73.5632
	Task-13	val_accuracy: 85.8696	gate_accuracy: 81.5217
	Task-14	val_accuracy: 77.9412	gate_accuracy: 77.9412
	Task-15	val_accuracy: 73.1343	gate_accuracy: 70.1493
	Task-16	val_accuracy: 67.4419	gate_accuracy: 67.4419
	Task-17	val_accuracy: 76.2500	gate_accuracy: 73.7500
	Task-18	val_accuracy: 83.7209	gate_accuracy: 81.3953
	Task-19	val_accuracy: 63.6364	gate_accuracy: 63.6364
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.5661


[414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]
Polling GMM for: {414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433}
STEP-1	Epoch: 10/50	loss: 2.6043	step1_train_accuracy: 48.4756
STEP-1	Epoch: 20/50	loss: 0.8493	step1_train_accuracy: 89.3293
STEP-1	Epoch: 30/50	loss: 0.3781	step1_train_accuracy: 96.3415
STEP-1	Epoch: 40/50	loss: 0.2337	step1_train_accuracy: 97.5610
STEP-1	Epoch: 50/50	loss: 0.1823	step1_train_accuracy: 98.1707
FINISH STEP 1
Task-21	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6043	gate_loss: 0.6706	step2_classification_accuracy: 81.4670	step_2_gate_accuracy: 79.7389
STEP-2	Epoch: 40/200	classification_loss: 0.4272	gate_loss: 0.3569	step2_classification_accuracy: 86.4055	step_2_gate_accuracy: 88.1336
STEP-2	Epoch: 60/200	classification_loss: 0.3592	gate_loss: 0.2705	step2_classification_accuracy: 87.9877	step_2_gate_accuracy: 90.5530
STEP-2	Epoch: 80/200	classification_loss: 0.3187	gate_loss: 0.2299	step2_classification_accuracy: 89.1551	step_2_gate_accuracy: 91.6359
STEP-2	Epoch: 100/200	classification_loss: 0.2969	gate_loss: 0.2058	step2_classification_accuracy: 89.8233	step_2_gate_accuracy: 92.4885
STEP-2	Epoch: 120/200	classification_loss: 0.2657	gate_loss: 0.1813	step2_classification_accuracy: 90.6298	step_2_gate_accuracy: 93.2488
STEP-2	Epoch: 140/200	classification_loss: 0.2498	gate_loss: 0.1688	step2_classification_accuracy: 91.0215	step_2_gate_accuracy: 93.6790
STEP-2	Epoch: 160/200	classification_loss: 0.2458	gate_loss: 0.1628	step2_classification_accuracy: 91.2289	step_2_gate_accuracy: 93.7250
STEP-2	Epoch: 180/200	classification_loss: 0.2299	gate_loss: 0.1488	step2_classification_accuracy: 91.6590	step_2_gate_accuracy: 94.1705
STEP-2	Epoch: 200/200	classification_loss: 0.2205	gate_loss: 0.1414	step2_classification_accuracy: 92.1736	step_2_gate_accuracy: 94.6083
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 79.1367	gate_accuracy: 84.8921
	Task-1	val_accuracy: 55.8442	gate_accuracy: 67.5325
	Task-2	val_accuracy: 70.2128	gate_accuracy: 72.3404
	Task-3	val_accuracy: 70.2128	gate_accuracy: 73.4043
	Task-4	val_accuracy: 79.7468	gate_accuracy: 87.3418
	Task-5	val_accuracy: 82.8571	gate_accuracy: 78.5714
	Task-6	val_accuracy: 79.2208	gate_accuracy: 75.3247
	Task-7	val_accuracy: 56.9620	gate_accuracy: 51.8987
	Task-8	val_accuracy: 71.9512	gate_accuracy: 70.7317
	Task-9	val_accuracy: 60.5263	gate_accuracy: 65.7895
	Task-10	val_accuracy: 91.5663	gate_accuracy: 89.1566
	Task-11	val_accuracy: 73.0769	gate_accuracy: 70.5128
	Task-12	val_accuracy: 82.7586	gate_accuracy: 77.0115
	Task-13	val_accuracy: 82.6087	gate_accuracy: 80.4348
	Task-14	val_accuracy: 75.0000	gate_accuracy: 72.0588
	Task-15	val_accuracy: 73.1343	gate_accuracy: 73.1343
	Task-16	val_accuracy: 74.4186	gate_accuracy: 70.9302
	Task-17	val_accuracy: 77.5000	gate_accuracy: 75.0000
	Task-18	val_accuracy: 84.8837	gate_accuracy: 81.3953
	Task-19	val_accuracy: 62.3377	gate_accuracy: 59.7403
	Task-20	val_accuracy: 80.4878	gate_accuracy: 82.9268
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.7861


[434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]
Polling GMM for: {434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453}
STEP-1	Epoch: 10/50	loss: 2.5081	step1_train_accuracy: 55.8659
STEP-1	Epoch: 20/50	loss: 0.9452	step1_train_accuracy: 85.1955
STEP-1	Epoch: 30/50	loss: 0.4438	step1_train_accuracy: 94.6927
STEP-1	Epoch: 40/50	loss: 0.2623	step1_train_accuracy: 98.3240
STEP-1	Epoch: 50/50	loss: 0.1844	step1_train_accuracy: 98.3240
FINISH STEP 1
Task-22	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.6241	gate_loss: 0.6638	step2_classification_accuracy: 80.6167	step_2_gate_accuracy: 79.3502
STEP-2	Epoch: 40/200	classification_loss: 0.4441	gate_loss: 0.3713	step2_classification_accuracy: 85.7104	step_2_gate_accuracy: 87.0182
STEP-2	Epoch: 60/200	classification_loss: 0.3636	gate_loss: 0.2792	step2_classification_accuracy: 87.5826	step_2_gate_accuracy: 89.8678
STEP-2	Epoch: 80/200	classification_loss: 0.3229	gate_loss: 0.2339	step2_classification_accuracy: 88.9730	step_2_gate_accuracy: 91.6506
STEP-2	Epoch: 100/200	classification_loss: 0.2939	gate_loss: 0.2040	step2_classification_accuracy: 89.7715	step_2_gate_accuracy: 92.4628
STEP-2	Epoch: 120/200	classification_loss: 0.2808	gate_loss: 0.1897	step2_classification_accuracy: 90.1294	step_2_gate_accuracy: 92.9653
STEP-2	Epoch: 140/200	classification_loss: 0.2588	gate_loss: 0.1738	step2_classification_accuracy: 90.8728	step_2_gate_accuracy: 93.6674
STEP-2	Epoch: 160/200	classification_loss: 0.2454	gate_loss: 0.1615	step2_classification_accuracy: 91.0862	step_2_gate_accuracy: 93.7913
STEP-2	Epoch: 180/200	classification_loss: 0.2347	gate_loss: 0.1511	step2_classification_accuracy: 91.4441	step_2_gate_accuracy: 94.4452
STEP-2	Epoch: 200/200	classification_loss: 0.2261	gate_loss: 0.1446	step2_classification_accuracy: 91.7952	step_2_gate_accuracy: 94.6999
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.5036	gate_accuracy: 79.8561
	Task-1	val_accuracy: 42.8571	gate_accuracy: 55.8442
	Task-2	val_accuracy: 73.4043	gate_accuracy: 76.5957
	Task-3	val_accuracy: 64.8936	gate_accuracy: 68.0851
	Task-4	val_accuracy: 74.6835	gate_accuracy: 79.7468
	Task-5	val_accuracy: 87.1429	gate_accuracy: 87.1429
	Task-6	val_accuracy: 87.0130	gate_accuracy: 85.7143
	Task-7	val_accuracy: 60.7595	gate_accuracy: 56.9620
	Task-8	val_accuracy: 59.7561	gate_accuracy: 59.7561
	Task-9	val_accuracy: 63.1579	gate_accuracy: 68.4211
	Task-10	val_accuracy: 91.5663	gate_accuracy: 91.5663
	Task-11	val_accuracy: 76.9231	gate_accuracy: 73.0769
	Task-12	val_accuracy: 78.1609	gate_accuracy: 77.0115
	Task-13	val_accuracy: 84.7826	gate_accuracy: 82.6087
	Task-14	val_accuracy: 77.9412	gate_accuracy: 76.4706
	Task-15	val_accuracy: 76.1194	gate_accuracy: 74.6269
	Task-16	val_accuracy: 74.4186	gate_accuracy: 70.9302
	Task-17	val_accuracy: 72.5000	gate_accuracy: 67.5000
	Task-18	val_accuracy: 83.7209	gate_accuracy: 86.0465
	Task-19	val_accuracy: 59.7403	gate_accuracy: 64.9351
	Task-20	val_accuracy: 75.6098	gate_accuracy: 74.3902
	Task-21	val_accuracy: 81.1111	gate_accuracy: 83.3333
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 74.8237


[454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]
Polling GMM for: {454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473}
STEP-1	Epoch: 10/50	loss: 2.5916	step1_train_accuracy: 49.7059
STEP-1	Epoch: 20/50	loss: 0.8725	step1_train_accuracy: 82.9412
STEP-1	Epoch: 30/50	loss: 0.4417	step1_train_accuracy: 94.7059
STEP-1	Epoch: 40/50	loss: 0.2627	step1_train_accuracy: 97.9412
STEP-1	Epoch: 50/50	loss: 0.1728	step1_train_accuracy: 98.5294
FINISH STEP 1
Task-23	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6387	gate_loss: 0.7138	step2_classification_accuracy: 80.2391	step_2_gate_accuracy: 77.9887
STEP-2	Epoch: 40/200	classification_loss: 0.4482	gate_loss: 0.3873	step2_classification_accuracy: 85.6540	step_2_gate_accuracy: 86.9128
STEP-2	Epoch: 60/200	classification_loss: 0.3707	gate_loss: 0.2860	step2_classification_accuracy: 87.8903	step_2_gate_accuracy: 89.8734
STEP-2	Epoch: 80/200	classification_loss: 0.3305	gate_loss: 0.2411	step2_classification_accuracy: 88.8186	step_2_gate_accuracy: 91.4838
STEP-2	Epoch: 100/200	classification_loss: 0.2843	gate_loss: 0.2024	step2_classification_accuracy: 89.9648	step_2_gate_accuracy: 92.5668
STEP-2	Epoch: 120/200	classification_loss: 0.2664	gate_loss: 0.1847	step2_classification_accuracy: 90.4501	step_2_gate_accuracy: 93.1505
STEP-2	Epoch: 140/200	classification_loss: 0.2517	gate_loss: 0.1698	step2_classification_accuracy: 91.0338	step_2_gate_accuracy: 93.6568
STEP-2	Epoch: 160/200	classification_loss: 0.2347	gate_loss: 0.1553	step2_classification_accuracy: 91.5893	step_2_gate_accuracy: 94.1421
STEP-2	Epoch: 180/200	classification_loss: 0.2246	gate_loss: 0.1460	step2_classification_accuracy: 91.9691	step_2_gate_accuracy: 94.4444
STEP-2	Epoch: 200/200	classification_loss: 0.2202	gate_loss: 0.1419	step2_classification_accuracy: 91.9831	step_2_gate_accuracy: 94.8172
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 75.5396	gate_accuracy: 85.6115
	Task-1	val_accuracy: 55.8442	gate_accuracy: 68.8312
	Task-2	val_accuracy: 70.2128	gate_accuracy: 72.3404
	Task-3	val_accuracy: 70.2128	gate_accuracy: 72.3404
	Task-4	val_accuracy: 69.6203	gate_accuracy: 74.6835
	Task-5	val_accuracy: 80.0000	gate_accuracy: 85.7143
	Task-6	val_accuracy: 81.8182	gate_accuracy: 77.9221
	Task-7	val_accuracy: 58.2278	gate_accuracy: 56.9620
	Task-8	val_accuracy: 64.6341	gate_accuracy: 65.8537
	Task-9	val_accuracy: 65.7895	gate_accuracy: 69.7368
	Task-10	val_accuracy: 91.5663	gate_accuracy: 89.1566
	Task-11	val_accuracy: 76.9231	gate_accuracy: 69.2308
	Task-12	val_accuracy: 78.1609	gate_accuracy: 72.4138
	Task-13	val_accuracy: 84.7826	gate_accuracy: 80.4348
	Task-14	val_accuracy: 76.4706	gate_accuracy: 75.0000
	Task-15	val_accuracy: 73.1343	gate_accuracy: 77.6119
	Task-16	val_accuracy: 70.9302	gate_accuracy: 67.4419
	Task-17	val_accuracy: 76.2500	gate_accuracy: 72.5000
	Task-18	val_accuracy: 80.2326	gate_accuracy: 80.2326
	Task-19	val_accuracy: 58.4416	gate_accuracy: 55.8442
	Task-20	val_accuracy: 74.3902	gate_accuracy: 76.8293
	Task-21	val_accuracy: 68.8889	gate_accuracy: 66.6667
	Task-22	val_accuracy: 71.7647	gate_accuracy: 70.5882
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.5477


[474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]
Polling GMM for: {474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493}
STEP-1	Epoch: 10/50	loss: 2.3326	step1_train_accuracy: 56.8750
STEP-1	Epoch: 20/50	loss: 0.8837	step1_train_accuracy: 84.0625
STEP-1	Epoch: 30/50	loss: 0.4613	step1_train_accuracy: 92.1875
STEP-1	Epoch: 40/50	loss: 0.3096	step1_train_accuracy: 95.9375
STEP-1	Epoch: 50/50	loss: 0.2296	step1_train_accuracy: 97.5000
FINISH STEP 1
Task-24	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.6432	gate_loss: 0.6986	step2_classification_accuracy: 80.5864	step_2_gate_accuracy: 78.3662
STEP-2	Epoch: 40/200	classification_loss: 0.4579	gate_loss: 0.3919	step2_classification_accuracy: 85.9606	step_2_gate_accuracy: 86.9335
STEP-2	Epoch: 60/200	classification_loss: 0.3854	gate_loss: 0.3045	step2_classification_accuracy: 87.5343	step_2_gate_accuracy: 89.0884
STEP-2	Epoch: 80/200	classification_loss: 0.3319	gate_loss: 0.2467	step2_classification_accuracy: 88.8011	step_2_gate_accuracy: 91.1584
STEP-2	Epoch: 100/200	classification_loss: 0.2980	gate_loss: 0.2127	step2_classification_accuracy: 89.7545	step_2_gate_accuracy: 92.1510
STEP-2	Epoch: 120/200	classification_loss: 0.2820	gate_loss: 0.1961	step2_classification_accuracy: 90.4336	step_2_gate_accuracy: 92.9346
STEP-2	Epoch: 140/200	classification_loss: 0.2626	gate_loss: 0.1793	step2_classification_accuracy: 90.8189	step_2_gate_accuracy: 93.4308
STEP-2	Epoch: 160/200	classification_loss: 0.2486	gate_loss: 0.1651	step2_classification_accuracy: 91.2498	step_2_gate_accuracy: 93.9271
STEP-2	Epoch: 180/200	classification_loss: 0.2400	gate_loss: 0.1577	step2_classification_accuracy: 91.5633	step_2_gate_accuracy: 94.0969
STEP-2	Epoch: 200/200	classification_loss: 0.2290	gate_loss: 0.1493	step2_classification_accuracy: 92.0269	step_2_gate_accuracy: 94.5344
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.3813	gate_accuracy: 81.2950
	Task-1	val_accuracy: 51.9481	gate_accuracy: 66.2338
	Task-2	val_accuracy: 70.2128	gate_accuracy: 72.3404
	Task-3	val_accuracy: 63.8298	gate_accuracy: 67.0213
	Task-4	val_accuracy: 68.3544	gate_accuracy: 70.8861
	Task-5	val_accuracy: 77.1429	gate_accuracy: 82.8571
	Task-6	val_accuracy: 79.2208	gate_accuracy: 72.7273
	Task-7	val_accuracy: 54.4304	gate_accuracy: 51.8987
	Task-8	val_accuracy: 73.1707	gate_accuracy: 74.3902
	Task-9	val_accuracy: 56.5789	gate_accuracy: 59.2105
	Task-10	val_accuracy: 91.5663	gate_accuracy: 91.5663
	Task-11	val_accuracy: 79.4872	gate_accuracy: 75.6410
	Task-12	val_accuracy: 81.6092	gate_accuracy: 75.8621
	Task-13	val_accuracy: 80.4348	gate_accuracy: 78.2609
	Task-14	val_accuracy: 77.9412	gate_accuracy: 73.5294
	Task-15	val_accuracy: 77.6119	gate_accuracy: 71.6418
	Task-16	val_accuracy: 72.0930	gate_accuracy: 69.7674
	Task-17	val_accuracy: 76.2500	gate_accuracy: 75.0000
	Task-18	val_accuracy: 84.8837	gate_accuracy: 82.5581
	Task-19	val_accuracy: 58.4416	gate_accuracy: 61.0390
	Task-20	val_accuracy: 80.4878	gate_accuracy: 78.0488
	Task-21	val_accuracy: 67.7778	gate_accuracy: 68.8889
	Task-22	val_accuracy: 68.2353	gate_accuracy: 69.4118
	Task-23	val_accuracy: 70.0000	gate_accuracy: 73.7500
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.9582


[494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]
Polling GMM for: {494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513}
STEP-1	Epoch: 10/50	loss: 2.9140	step1_train_accuracy: 47.2028
STEP-1	Epoch: 20/50	loss: 1.1035	step1_train_accuracy: 78.3217
STEP-1	Epoch: 30/50	loss: 0.6160	step1_train_accuracy: 88.1119
STEP-1	Epoch: 40/50	loss: 0.4308	step1_train_accuracy: 93.3566
STEP-1	Epoch: 50/50	loss: 0.3450	step1_train_accuracy: 94.0559
FINISH STEP 1
Task-25	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.6963	gate_loss: 0.7661	step2_classification_accuracy: 78.8197	step_2_gate_accuracy: 76.1154
STEP-2	Epoch: 40/200	classification_loss: 0.4943	gate_loss: 0.4260	step2_classification_accuracy: 84.4877	step_2_gate_accuracy: 85.7198
STEP-2	Epoch: 60/200	classification_loss: 0.4052	gate_loss: 0.3201	step2_classification_accuracy: 86.7380	step_2_gate_accuracy: 88.8262
STEP-2	Epoch: 80/200	classification_loss: 0.3531	gate_loss: 0.2630	step2_classification_accuracy: 88.4241	step_2_gate_accuracy: 90.6485
STEP-2	Epoch: 100/200	classification_loss: 0.3226	gate_loss: 0.2294	step2_classification_accuracy: 89.0921	step_2_gate_accuracy: 91.5953
STEP-2	Epoch: 120/200	classification_loss: 0.3064	gate_loss: 0.2125	step2_classification_accuracy: 89.5914	step_2_gate_accuracy: 92.2114
STEP-2	Epoch: 140/200	classification_loss: 0.2770	gate_loss: 0.1885	step2_classification_accuracy: 90.3956	step_2_gate_accuracy: 93.1842
STEP-2	Epoch: 160/200	classification_loss: 0.2687	gate_loss: 0.1803	step2_classification_accuracy: 90.6096	step_2_gate_accuracy: 93.4241
STEP-2	Epoch: 180/200	classification_loss: 0.2507	gate_loss: 0.1656	step2_classification_accuracy: 91.0441	step_2_gate_accuracy: 93.9689
STEP-2	Epoch: 200/200	classification_loss: 0.2396	gate_loss: 0.1570	step2_classification_accuracy: 91.5240	step_2_gate_accuracy: 94.2348
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.2230	gate_accuracy: 79.1367
	Task-1	val_accuracy: 51.9481	gate_accuracy: 63.6364
	Task-2	val_accuracy: 71.2766	gate_accuracy: 74.4681
	Task-3	val_accuracy: 69.1489	gate_accuracy: 72.3404
	Task-4	val_accuracy: 70.8861	gate_accuracy: 78.4810
	Task-5	val_accuracy: 77.1429	gate_accuracy: 78.5714
	Task-6	val_accuracy: 79.2208	gate_accuracy: 76.6234
	Task-7	val_accuracy: 51.8987	gate_accuracy: 51.8987
	Task-8	val_accuracy: 67.0732	gate_accuracy: 67.0732
	Task-9	val_accuracy: 67.1053	gate_accuracy: 76.3158
	Task-10	val_accuracy: 91.5663	gate_accuracy: 90.3614
	Task-11	val_accuracy: 78.2051	gate_accuracy: 76.9231
	Task-12	val_accuracy: 74.7126	gate_accuracy: 67.8161
	Task-13	val_accuracy: 80.4348	gate_accuracy: 76.0870
	Task-14	val_accuracy: 82.3529	gate_accuracy: 79.4118
	Task-15	val_accuracy: 74.6269	gate_accuracy: 70.1493
	Task-16	val_accuracy: 70.9302	gate_accuracy: 68.6047
	Task-17	val_accuracy: 72.5000	gate_accuracy: 68.7500
	Task-18	val_accuracy: 86.0465	gate_accuracy: 83.7209
	Task-19	val_accuracy: 57.1429	gate_accuracy: 57.1429
	Task-20	val_accuracy: 75.6098	gate_accuracy: 76.8293
	Task-21	val_accuracy: 73.3333	gate_accuracy: 72.2222
	Task-22	val_accuracy: 75.2941	gate_accuracy: 76.4706
	Task-23	val_accuracy: 70.0000	gate_accuracy: 72.5000
	Task-24	val_accuracy: 59.1549	gate_accuracy: 66.1972
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.1121


[514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]
Polling GMM for: {514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533}
STEP-1	Epoch: 10/50	loss: 2.4117	step1_train_accuracy: 56.8452
STEP-1	Epoch: 20/50	loss: 0.9516	step1_train_accuracy: 85.7143
STEP-1	Epoch: 30/50	loss: 0.4179	step1_train_accuracy: 93.4524
STEP-1	Epoch: 40/50	loss: 0.2595	step1_train_accuracy: 94.6429
STEP-1	Epoch: 50/50	loss: 0.1842	step1_train_accuracy: 97.3214
FINISH STEP 1
Task-26	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.6923	gate_loss: 0.7237	step2_classification_accuracy: 78.7980	step_2_gate_accuracy: 77.5398
STEP-2	Epoch: 40/200	classification_loss: 0.4945	gate_loss: 0.4206	step2_classification_accuracy: 84.1819	step_2_gate_accuracy: 85.7210
STEP-2	Epoch: 60/200	classification_loss: 0.4050	gate_loss: 0.3178	step2_classification_accuracy: 86.8329	step_2_gate_accuracy: 88.8109
STEP-2	Epoch: 80/200	classification_loss: 0.3575	gate_loss: 0.2687	step2_classification_accuracy: 87.8219	step_2_gate_accuracy: 90.2212
STEP-2	Epoch: 100/200	classification_loss: 0.3277	gate_loss: 0.2384	step2_classification_accuracy: 88.7582	step_2_gate_accuracy: 91.1224
STEP-2	Epoch: 120/200	classification_loss: 0.3035	gate_loss: 0.2162	step2_classification_accuracy: 89.5833	step_2_gate_accuracy: 91.9534
STEP-2	Epoch: 140/200	classification_loss: 0.2882	gate_loss: 0.2023	step2_classification_accuracy: 89.8759	step_2_gate_accuracy: 92.5562
STEP-2	Epoch: 160/200	classification_loss: 0.2720	gate_loss: 0.1888	step2_classification_accuracy: 90.3207	step_2_gate_accuracy: 92.9073
STEP-2	Epoch: 180/200	classification_loss: 0.2636	gate_loss: 0.1799	step2_classification_accuracy: 90.6191	step_2_gate_accuracy: 93.2467
STEP-2	Epoch: 200/200	classification_loss: 0.2556	gate_loss: 0.1740	step2_classification_accuracy: 90.8532	step_2_gate_accuracy: 93.4750
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.2590	gate_accuracy: 83.4532
	Task-1	val_accuracy: 45.4545	gate_accuracy: 57.1429
	Task-2	val_accuracy: 76.5957	gate_accuracy: 77.6596
	Task-3	val_accuracy: 71.2766	gate_accuracy: 76.5957
	Task-4	val_accuracy: 77.2152	gate_accuracy: 77.2152
	Task-5	val_accuracy: 80.0000	gate_accuracy: 81.4286
	Task-6	val_accuracy: 80.5195	gate_accuracy: 83.1169
	Task-7	val_accuracy: 54.4304	gate_accuracy: 51.8987
	Task-8	val_accuracy: 62.1951	gate_accuracy: 68.2927
	Task-9	val_accuracy: 65.7895	gate_accuracy: 65.7895
	Task-10	val_accuracy: 89.1566	gate_accuracy: 87.9518
	Task-11	val_accuracy: 74.3590	gate_accuracy: 71.7949
	Task-12	val_accuracy: 78.1609	gate_accuracy: 70.1149
	Task-13	val_accuracy: 77.1739	gate_accuracy: 75.0000
	Task-14	val_accuracy: 82.3529	gate_accuracy: 77.9412
	Task-15	val_accuracy: 73.1343	gate_accuracy: 70.1493
	Task-16	val_accuracy: 74.4186	gate_accuracy: 75.5814
	Task-17	val_accuracy: 71.2500	gate_accuracy: 72.5000
	Task-18	val_accuracy: 81.3953	gate_accuracy: 80.2326
	Task-19	val_accuracy: 58.4416	gate_accuracy: 62.3377
	Task-20	val_accuracy: 80.4878	gate_accuracy: 73.1707
	Task-21	val_accuracy: 72.2222	gate_accuracy: 66.6667
	Task-22	val_accuracy: 69.4118	gate_accuracy: 70.5882
	Task-23	val_accuracy: 72.5000	gate_accuracy: 75.0000
	Task-24	val_accuracy: 59.1549	gate_accuracy: 63.3803
	Task-25	val_accuracy: 83.3333	gate_accuracy: 85.7143
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 73.5090


[534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]
Polling GMM for: {534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553}
STEP-1	Epoch: 10/50	loss: 2.6809	step1_train_accuracy: 53.0159
STEP-1	Epoch: 20/50	loss: 0.8649	step1_train_accuracy: 86.6667
STEP-1	Epoch: 30/50	loss: 0.4136	step1_train_accuracy: 97.1429
STEP-1	Epoch: 40/50	loss: 0.2576	step1_train_accuracy: 98.7302
STEP-1	Epoch: 50/50	loss: 0.1799	step1_train_accuracy: 99.0476
FINISH STEP 1
Task-27	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.7039	gate_loss: 0.7348	step2_classification_accuracy: 78.9711	step_2_gate_accuracy: 77.3297
STEP-2	Epoch: 40/200	classification_loss: 0.5031	gate_loss: 0.4289	step2_classification_accuracy: 84.1437	step_2_gate_accuracy: 85.1647
STEP-2	Epoch: 60/200	classification_loss: 0.4160	gate_loss: 0.3308	step2_classification_accuracy: 86.4734	step_2_gate_accuracy: 88.5323
STEP-2	Epoch: 80/200	classification_loss: 0.3694	gate_loss: 0.2818	step2_classification_accuracy: 87.9287	step_2_gate_accuracy: 90.0835
STEP-2	Epoch: 100/200	classification_loss: 0.3377	gate_loss: 0.2495	step2_classification_accuracy: 88.7128	step_2_gate_accuracy: 90.9352
STEP-2	Epoch: 120/200	classification_loss: 0.3137	gate_loss: 0.2297	step2_classification_accuracy: 89.1979	step_2_gate_accuracy: 91.7644
STEP-2	Epoch: 140/200	classification_loss: 0.2996	gate_loss: 0.2150	step2_classification_accuracy: 89.6379	step_2_gate_accuracy: 92.1988
STEP-2	Epoch: 160/200	classification_loss: 0.2784	gate_loss: 0.1949	step2_classification_accuracy: 90.2978	step_2_gate_accuracy: 92.8644
STEP-2	Epoch: 180/200	classification_loss: 0.2870	gate_loss: 0.2004	step2_classification_accuracy: 90.1625	step_2_gate_accuracy: 92.6895
STEP-2	Epoch: 200/200	classification_loss: 0.2579	gate_loss: 0.1779	step2_classification_accuracy: 90.9183	step_2_gate_accuracy: 93.4792
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.8201	gate_accuracy: 79.8561
	Task-1	val_accuracy: 54.5455	gate_accuracy: 67.5325
	Task-2	val_accuracy: 78.7234	gate_accuracy: 79.7872
	Task-3	val_accuracy: 68.0851	gate_accuracy: 73.4043
	Task-4	val_accuracy: 69.6203	gate_accuracy: 70.8861
	Task-5	val_accuracy: 84.2857	gate_accuracy: 84.2857
	Task-6	val_accuracy: 83.1169	gate_accuracy: 70.1299
	Task-7	val_accuracy: 54.4304	gate_accuracy: 53.1646
	Task-8	val_accuracy: 65.8537	gate_accuracy: 64.6341
	Task-9	val_accuracy: 68.4211	gate_accuracy: 71.0526
	Task-10	val_accuracy: 85.5422	gate_accuracy: 83.1325
	Task-11	val_accuracy: 76.9231	gate_accuracy: 73.0769
	Task-12	val_accuracy: 77.0115	gate_accuracy: 72.4138
	Task-13	val_accuracy: 83.6957	gate_accuracy: 81.5217
	Task-14	val_accuracy: 73.5294	gate_accuracy: 70.5882
	Task-15	val_accuracy: 74.6269	gate_accuracy: 73.1343
	Task-16	val_accuracy: 75.5814	gate_accuracy: 73.2558
	Task-17	val_accuracy: 80.0000	gate_accuracy: 76.2500
	Task-18	val_accuracy: 82.5581	gate_accuracy: 80.2326
	Task-19	val_accuracy: 51.9481	gate_accuracy: 53.2468
	Task-20	val_accuracy: 80.4878	gate_accuracy: 76.8293
	Task-21	val_accuracy: 72.2222	gate_accuracy: 71.1111
	Task-22	val_accuracy: 71.7647	gate_accuracy: 70.5882
	Task-23	val_accuracy: 71.2500	gate_accuracy: 75.0000
	Task-24	val_accuracy: 53.5211	gate_accuracy: 61.9718
	Task-25	val_accuracy: 79.7619	gate_accuracy: 80.9524
	Task-26	val_accuracy: 65.8228	gate_accuracy: 63.2911
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 72.6583


[554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]
Polling GMM for: {554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573}
STEP-1	Epoch: 10/50	loss: 2.7273	step1_train_accuracy: 53.8217
STEP-1	Epoch: 20/50	loss: 1.0331	step1_train_accuracy: 80.2548
STEP-1	Epoch: 30/50	loss: 0.5617	step1_train_accuracy: 92.0382
STEP-1	Epoch: 40/50	loss: 0.3531	step1_train_accuracy: 95.5414
STEP-1	Epoch: 50/50	loss: 0.2416	step1_train_accuracy: 96.4968
FINISH STEP 1
Task-28	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.7304	gate_loss: 0.7887	step2_classification_accuracy: 78.3354	step_2_gate_accuracy: 75.8907
STEP-2	Epoch: 40/200	classification_loss: 0.5311	gate_loss: 0.4575	step2_classification_accuracy: 83.3708	step_2_gate_accuracy: 84.5397
STEP-2	Epoch: 60/200	classification_loss: 0.4407	gate_loss: 0.3499	step2_classification_accuracy: 85.6187	step_2_gate_accuracy: 87.5913
STEP-2	Epoch: 80/200	classification_loss: 0.3919	gate_loss: 0.2957	step2_classification_accuracy: 87.0855	step_2_gate_accuracy: 89.4571
STEP-2	Epoch: 100/200	classification_loss: 0.3490	gate_loss: 0.2557	step2_classification_accuracy: 88.3556	step_2_gate_accuracy: 90.8508
STEP-2	Epoch: 120/200	classification_loss: 0.3320	gate_loss: 0.2377	step2_classification_accuracy: 88.6422	step_2_gate_accuracy: 91.4747
STEP-2	Epoch: 140/200	classification_loss: 0.3140	gate_loss: 0.2228	step2_classification_accuracy: 89.2042	step_2_gate_accuracy: 91.8343
STEP-2	Epoch: 160/200	classification_loss: 0.3020	gate_loss: 0.2096	step2_classification_accuracy: 89.6482	step_2_gate_accuracy: 92.3064
STEP-2	Epoch: 180/200	classification_loss: 0.2822	gate_loss: 0.1931	step2_classification_accuracy: 89.9236	step_2_gate_accuracy: 93.0033
STEP-2	Epoch: 200/200	classification_loss: 0.2792	gate_loss: 0.1916	step2_classification_accuracy: 90.0472	step_2_gate_accuracy: 92.7279
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.2590	gate_accuracy: 84.8921
	Task-1	val_accuracy: 51.9481	gate_accuracy: 63.6364
	Task-2	val_accuracy: 71.2766	gate_accuracy: 74.4681
	Task-3	val_accuracy: 64.8936	gate_accuracy: 71.2766
	Task-4	val_accuracy: 78.4810	gate_accuracy: 82.2785
	Task-5	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-6	val_accuracy: 79.2208	gate_accuracy: 74.0260
	Task-7	val_accuracy: 51.8987	gate_accuracy: 43.0380
	Task-8	val_accuracy: 67.0732	gate_accuracy: 64.6341
	Task-9	val_accuracy: 76.3158	gate_accuracy: 76.3158
	Task-10	val_accuracy: 84.3373	gate_accuracy: 77.1084
	Task-11	val_accuracy: 74.3590	gate_accuracy: 73.0769
	Task-12	val_accuracy: 79.3103	gate_accuracy: 74.7126
	Task-13	val_accuracy: 78.2609	gate_accuracy: 77.1739
	Task-14	val_accuracy: 76.4706	gate_accuracy: 72.0588
	Task-15	val_accuracy: 70.1493	gate_accuracy: 67.1642
	Task-16	val_accuracy: 70.9302	gate_accuracy: 72.0930
	Task-17	val_accuracy: 75.0000	gate_accuracy: 72.5000
	Task-18	val_accuracy: 83.7209	gate_accuracy: 76.7442
	Task-19	val_accuracy: 58.4416	gate_accuracy: 59.7403
	Task-20	val_accuracy: 79.2683	gate_accuracy: 76.8293
	Task-21	val_accuracy: 70.0000	gate_accuracy: 66.6667
	Task-22	val_accuracy: 64.7059	gate_accuracy: 63.5294
	Task-23	val_accuracy: 72.5000	gate_accuracy: 72.5000
	Task-24	val_accuracy: 60.5634	gate_accuracy: 66.1972
	Task-25	val_accuracy: 83.3333	gate_accuracy: 82.1429
	Task-26	val_accuracy: 63.2911	gate_accuracy: 67.0886
	Task-27	val_accuracy: 58.9744	gate_accuracy: 67.9487
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 71.8534


[574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]
Polling GMM for: {574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593}
STEP-1	Epoch: 10/50	loss: 2.5009	step1_train_accuracy: 55.5901
STEP-1	Epoch: 20/50	loss: 0.8830	step1_train_accuracy: 84.7826
STEP-1	Epoch: 30/50	loss: 0.4325	step1_train_accuracy: 91.6149
STEP-1	Epoch: 40/50	loss: 0.3081	step1_train_accuracy: 91.9255
STEP-1	Epoch: 50/50	loss: 0.2368	step1_train_accuracy: 92.8571
FINISH STEP 1
Task-29	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30, 514: 30, 515: 30, 516: 30, 517: 30, 518: 30, 519: 30, 520: 30, 521: 30, 522: 30, 523: 30, 524: 30, 525: 30, 526: 30, 527: 30, 528: 30, 529: 30, 530: 30, 531: 30, 532: 30, 533: 30, 534: 30, 535: 30, 536: 30, 537: 30, 538: 30, 539: 30, 540: 30, 541: 30, 542: 30, 543: 30, 544: 30, 545: 30, 546: 30, 547: 30, 548: 30, 549: 30, 550: 30, 551: 30, 552: 30, 553: 30, 554: 30, 555: 30, 556: 30, 557: 30, 558: 30, 559: 30, 560: 30, 561: 30, 562: 30, 563: 30, 564: 30, 565: 30, 566: 30, 567: 30, 568: 30, 569: 30, 570: 30, 571: 30, 572: 30, 573: 30, 574: 30, 575: 30, 576: 30, 577: 30, 578: 30, 579: 30, 580: 30, 581: 30, 582: 30, 583: 30, 584: 30, 585: 30, 586: 30, 587: 30, 588: 30, 589: 30, 590: 30, 591: 30, 592: 30, 593: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.7634	gate_loss: 0.8344	step2_classification_accuracy: 77.5084	step_2_gate_accuracy: 74.6016
STEP-2	Epoch: 40/200	classification_loss: 0.5609	gate_loss: 0.4869	step2_classification_accuracy: 82.8676	step_2_gate_accuracy: 83.8272
STEP-2	Epoch: 60/200	classification_loss: 0.4718	gate_loss: 0.3786	step2_classification_accuracy: 85.1122	step_2_gate_accuracy: 86.8350
STEP-2	Epoch: 80/200	classification_loss: 0.4114	gate_loss: 0.3162	step2_classification_accuracy: 86.5600	step_2_gate_accuracy: 88.9226
STEP-2	Epoch: 100/200	classification_loss: 0.3693	gate_loss: 0.2750	step2_classification_accuracy: 87.7722	step_2_gate_accuracy: 90.1403
STEP-2	Epoch: 120/200	classification_loss: 0.3438	gate_loss: 0.2503	step2_classification_accuracy: 88.3782	step_2_gate_accuracy: 90.9540
STEP-2	Epoch: 140/200	classification_loss: 0.3192	gate_loss: 0.2277	step2_classification_accuracy: 89.2312	step_2_gate_accuracy: 91.8070
STEP-2	Epoch: 160/200	classification_loss: 0.3144	gate_loss: 0.2217	step2_classification_accuracy: 89.4332	step_2_gate_accuracy: 91.8126
STEP-2	Epoch: 180/200	classification_loss: 0.2888	gate_loss: 0.2012	step2_classification_accuracy: 89.9439	step_2_gate_accuracy: 92.5814
STEP-2	Epoch: 200/200	classification_loss: 0.2885	gate_loss: 0.2010	step2_classification_accuracy: 89.9102	step_2_gate_accuracy: 92.5140
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.5036	gate_accuracy: 77.6978
	Task-1	val_accuracy: 45.4545	gate_accuracy: 51.9481
	Task-2	val_accuracy: 75.5319	gate_accuracy: 69.1489
	Task-3	val_accuracy: 72.3404	gate_accuracy: 74.4681
	Task-4	val_accuracy: 73.4177	gate_accuracy: 70.8861
	Task-5	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-6	val_accuracy: 74.0260	gate_accuracy: 76.6234
	Task-7	val_accuracy: 50.6329	gate_accuracy: 49.3671
	Task-8	val_accuracy: 62.1951	gate_accuracy: 59.7561
	Task-9	val_accuracy: 57.8947	gate_accuracy: 57.8947
	Task-10	val_accuracy: 87.9518	gate_accuracy: 86.7470
	Task-11	val_accuracy: 70.5128	gate_accuracy: 67.9487
	Task-12	val_accuracy: 72.4138	gate_accuracy: 66.6667
	Task-13	val_accuracy: 76.0870	gate_accuracy: 70.6522
	Task-14	val_accuracy: 70.5882	gate_accuracy: 69.1176
	Task-15	val_accuracy: 68.6567	gate_accuracy: 68.6567
	Task-16	val_accuracy: 73.2558	gate_accuracy: 74.4186
	Task-17	val_accuracy: 81.2500	gate_accuracy: 75.0000
	Task-18	val_accuracy: 83.7209	gate_accuracy: 83.7209
	Task-19	val_accuracy: 54.5455	gate_accuracy: 51.9481
	Task-20	val_accuracy: 75.6098	gate_accuracy: 71.9512
	Task-21	val_accuracy: 66.6667	gate_accuracy: 67.7778
	Task-22	val_accuracy: 69.4118	gate_accuracy: 69.4118
	Task-23	val_accuracy: 75.0000	gate_accuracy: 73.7500
	Task-24	val_accuracy: 57.7465	gate_accuracy: 64.7887
	Task-25	val_accuracy: 71.4286	gate_accuracy: 75.0000
	Task-26	val_accuracy: 63.2911	gate_accuracy: 59.4937
	Task-27	val_accuracy: 66.6667	gate_accuracy: 71.7949
	Task-28	val_accuracy: 43.2099	gate_accuracy: 46.9136
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.7630


[594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]
Polling GMM for: {594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613}
STEP-1	Epoch: 10/50	loss: 2.8104	step1_train_accuracy: 39.5105
STEP-1	Epoch: 20/50	loss: 1.0912	step1_train_accuracy: 77.9720
STEP-1	Epoch: 30/50	loss: 0.6213	step1_train_accuracy: 89.8601
STEP-1	Epoch: 40/50	loss: 0.4026	step1_train_accuracy: 94.4056
STEP-1	Epoch: 50/50	loss: 0.2843	step1_train_accuracy: 96.1538
FINISH STEP 1
Task-30	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.7815	gate_loss: 0.8209	step2_classification_accuracy: 76.6260	step_2_gate_accuracy: 74.7662
STEP-2	Epoch: 40/200	classification_loss: 0.5773	gate_loss: 0.4992	step2_classification_accuracy: 82.5628	step_2_gate_accuracy: 83.3824
STEP-2	Epoch: 60/200	classification_loss: 0.4789	gate_loss: 0.3841	step2_classification_accuracy: 84.9322	step_2_gate_accuracy: 86.7868
STEP-2	Epoch: 80/200	classification_loss: 0.4292	gate_loss: 0.3330	step2_classification_accuracy: 85.8779	step_2_gate_accuracy: 88.1002
STEP-2	Epoch: 100/200	classification_loss: 0.3917	gate_loss: 0.2947	step2_classification_accuracy: 87.0600	step_2_gate_accuracy: 89.2981
STEP-2	Epoch: 120/200	classification_loss: 0.3588	gate_loss: 0.2642	step2_classification_accuracy: 88.0162	step_2_gate_accuracy: 90.4382
STEP-2	Epoch: 140/200	classification_loss: 0.3574	gate_loss: 0.2623	step2_classification_accuracy: 87.8796	step_2_gate_accuracy: 90.6956
STEP-2	Epoch: 160/200	classification_loss: 0.3245	gate_loss: 0.2313	step2_classification_accuracy: 88.7517	step_2_gate_accuracy: 91.5467
STEP-2	Epoch: 180/200	classification_loss: 0.3101	gate_loss: 0.2190	step2_classification_accuracy: 88.9566	step_2_gate_accuracy: 92.0616
STEP-2	Epoch: 200/200	classification_loss: 0.2961	gate_loss: 0.2092	step2_classification_accuracy: 89.4715	step_2_gate_accuracy: 92.3821
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 73.3813	gate_accuracy: 80.5755
	Task-1	val_accuracy: 57.1429	gate_accuracy: 62.3377
	Task-2	val_accuracy: 68.0851	gate_accuracy: 62.7660
	Task-3	val_accuracy: 65.9574	gate_accuracy: 71.2766
	Task-4	val_accuracy: 68.3544	gate_accuracy: 69.6203
	Task-5	val_accuracy: 75.7143	gate_accuracy: 72.8571
	Task-6	val_accuracy: 81.8182	gate_accuracy: 76.6234
	Task-7	val_accuracy: 56.9620	gate_accuracy: 50.6329
	Task-8	val_accuracy: 63.4146	gate_accuracy: 62.1951
	Task-9	val_accuracy: 57.8947	gate_accuracy: 64.4737
	Task-10	val_accuracy: 89.1566	gate_accuracy: 87.9518
	Task-11	val_accuracy: 75.6410	gate_accuracy: 69.2308
	Task-12	val_accuracy: 60.9195	gate_accuracy: 55.1724
	Task-13	val_accuracy: 83.6957	gate_accuracy: 79.3478
	Task-14	val_accuracy: 73.5294	gate_accuracy: 75.0000
	Task-15	val_accuracy: 71.6418	gate_accuracy: 64.1791
	Task-16	val_accuracy: 72.0930	gate_accuracy: 67.4419
	Task-17	val_accuracy: 72.5000	gate_accuracy: 68.7500
	Task-18	val_accuracy: 84.8837	gate_accuracy: 79.0698
	Task-19	val_accuracy: 55.8442	gate_accuracy: 57.1429
	Task-20	val_accuracy: 76.8293	gate_accuracy: 73.1707
	Task-21	val_accuracy: 70.0000	gate_accuracy: 68.8889
	Task-22	val_accuracy: 68.2353	gate_accuracy: 67.0588
	Task-23	val_accuracy: 72.5000	gate_accuracy: 71.2500
	Task-24	val_accuracy: 53.5211	gate_accuracy: 61.9718
	Task-25	val_accuracy: 72.6190	gate_accuracy: 75.0000
	Task-26	val_accuracy: 64.5570	gate_accuracy: 64.5570
	Task-27	val_accuracy: 62.8205	gate_accuracy: 67.9487
	Task-28	val_accuracy: 49.3827	gate_accuracy: 54.3210
	Task-29	val_accuracy: 56.3380	gate_accuracy: 66.1972
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.6084


[614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]
Polling GMM for: {614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633}
STEP-1	Epoch: 10/50	loss: 3.1782	step1_train_accuracy: 43.9560
STEP-1	Epoch: 20/50	loss: 1.0754	step1_train_accuracy: 82.7839
STEP-1	Epoch: 30/50	loss: 0.5840	step1_train_accuracy: 93.0403
STEP-1	Epoch: 40/50	loss: 0.4063	step1_train_accuracy: 94.8718
STEP-1	Epoch: 50/50	loss: 0.3008	step1_train_accuracy: 96.3370
FINISH STEP 1
Task-31	STARTING STEP 2
CLASS COUNTER: Counter({0: 25, 1: 25, 2: 25, 3: 25, 4: 25, 5: 25, 6: 25, 7: 25, 8: 25, 9: 25, 10: 25, 11: 25, 12: 25, 13: 25, 14: 25, 15: 25, 16: 25, 17: 25, 18: 25, 19: 25, 20: 25, 21: 25, 22: 25, 23: 25, 24: 25, 25: 25, 26: 25, 27: 25, 28: 25, 29: 25, 30: 25, 31: 25, 32: 25, 33: 25, 34: 25, 35: 25, 36: 25, 37: 25, 38: 25, 39: 25, 40: 25, 41: 25, 42: 25, 43: 25, 44: 25, 45: 25, 46: 25, 47: 25, 48: 25, 49: 25, 50: 25, 51: 25, 52: 25, 53: 25, 54: 25, 55: 25, 56: 25, 57: 25, 58: 25, 59: 25, 60: 25, 61: 25, 62: 25, 63: 25, 64: 25, 65: 25, 66: 25, 67: 25, 68: 25, 69: 25, 70: 25, 71: 25, 72: 25, 73: 25, 74: 25, 75: 25, 76: 25, 77: 25, 78: 25, 79: 25, 80: 25, 81: 25, 82: 25, 83: 25, 84: 25, 85: 25, 86: 25, 87: 25, 88: 25, 89: 25, 90: 25, 91: 25, 92: 25, 93: 25, 94: 25, 95: 25, 96: 25, 97: 25, 98: 25, 99: 25, 100: 25, 101: 25, 102: 25, 103: 25, 104: 25, 105: 25, 106: 25, 107: 25, 108: 25, 109: 25, 110: 25, 111: 25, 112: 25, 113: 25, 114: 25, 115: 25, 116: 25, 117: 25, 118: 25, 119: 25, 120: 25, 121: 25, 122: 25, 123: 25, 124: 25, 125: 25, 126: 25, 127: 25, 128: 25, 129: 25, 130: 25, 131: 25, 132: 25, 133: 25, 134: 25, 135: 25, 136: 25, 137: 25, 138: 25, 139: 25, 140: 25, 141: 25, 142: 25, 143: 25, 144: 25, 145: 25, 146: 25, 147: 25, 148: 25, 149: 25, 150: 25, 151: 25, 152: 25, 153: 25, 154: 25, 155: 25, 156: 25, 157: 25, 158: 25, 159: 25, 160: 25, 161: 25, 162: 25, 163: 25, 164: 25, 165: 25, 166: 25, 167: 25, 168: 25, 169: 25, 170: 25, 171: 25, 172: 25, 173: 25, 174: 25, 175: 25, 176: 25, 177: 25, 178: 25, 179: 25, 180: 25, 181: 25, 182: 25, 183: 25, 184: 25, 185: 25, 186: 25, 187: 25, 188: 25, 189: 25, 190: 25, 191: 25, 192: 25, 193: 25, 194: 25, 195: 25, 196: 25, 197: 25, 198: 25, 199: 25, 200: 25, 201: 25, 202: 25, 203: 25, 204: 25, 205: 25, 206: 25, 207: 25, 208: 25, 209: 25, 210: 25, 211: 25, 212: 25, 213: 25, 214: 25, 215: 25, 216: 25, 217: 25, 218: 25, 219: 25, 220: 25, 221: 25, 222: 25, 223: 25, 224: 25, 225: 25, 226: 25, 227: 25, 228: 25, 229: 25, 230: 25, 231: 25, 232: 25, 233: 25, 234: 25, 235: 25, 236: 25, 237: 25, 238: 25, 239: 25, 240: 25, 241: 25, 242: 25, 243: 25, 244: 25, 245: 25, 246: 25, 247: 25, 248: 25, 249: 25, 250: 25, 251: 25, 252: 25, 253: 25, 254: 25, 255: 25, 256: 25, 257: 25, 258: 25, 259: 25, 260: 25, 261: 25, 262: 25, 263: 25, 264: 25, 265: 25, 266: 25, 267: 25, 268: 25, 269: 25, 270: 25, 271: 25, 272: 25, 273: 25, 274: 25, 275: 25, 276: 25, 277: 25, 278: 25, 279: 25, 280: 25, 281: 25, 282: 25, 283: 25, 284: 25, 285: 25, 286: 25, 287: 25, 288: 25, 289: 25, 290: 25, 291: 25, 292: 25, 293: 25, 294: 25, 295: 25, 296: 25, 297: 25, 298: 25, 299: 25, 300: 25, 301: 25, 302: 25, 303: 25, 304: 25, 305: 25, 306: 25, 307: 25, 308: 25, 309: 25, 310: 25, 311: 25, 312: 25, 313: 25, 314: 25, 315: 25, 316: 25, 317: 25, 318: 25, 319: 25, 320: 25, 321: 25, 322: 25, 323: 25, 324: 25, 325: 25, 326: 25, 327: 25, 328: 25, 329: 25, 330: 25, 331: 25, 332: 25, 333: 25, 334: 25, 335: 25, 336: 25, 337: 25, 338: 25, 339: 25, 340: 25, 341: 25, 342: 25, 343: 25, 344: 25, 345: 25, 346: 25, 347: 25, 348: 25, 349: 25, 350: 25, 351: 25, 352: 25, 353: 25, 354: 25, 355: 25, 356: 25, 357: 25, 358: 25, 359: 25, 360: 25, 361: 25, 362: 25, 363: 25, 364: 25, 365: 25, 366: 25, 367: 25, 368: 25, 369: 25, 370: 25, 371: 25, 372: 25, 373: 25, 374: 25, 375: 25, 376: 25, 377: 25, 378: 25, 379: 25, 380: 25, 381: 25, 382: 25, 383: 25, 384: 25, 385: 25, 386: 25, 387: 25, 388: 25, 389: 25, 390: 25, 391: 25, 392: 25, 393: 25, 394: 25, 395: 25, 396: 25, 397: 25, 398: 25, 399: 25, 400: 25, 401: 25, 402: 25, 403: 25, 404: 25, 405: 25, 406: 25, 407: 25, 408: 25, 409: 25, 410: 25, 411: 25, 412: 25, 413: 25, 414: 25, 415: 25, 416: 25, 417: 25, 418: 25, 419: 25, 420: 25, 421: 25, 422: 25, 423: 25, 424: 25, 425: 25, 426: 25, 427: 25, 428: 25, 429: 25, 430: 25, 431: 25, 432: 25, 433: 25, 434: 25, 435: 25, 436: 25, 437: 25, 438: 25, 439: 25, 440: 25, 441: 25, 442: 25, 443: 25, 444: 25, 445: 25, 446: 25, 447: 25, 448: 25, 449: 25, 450: 25, 451: 25, 452: 25, 453: 25, 454: 25, 455: 25, 456: 25, 457: 25, 458: 25, 459: 25, 460: 25, 461: 25, 462: 25, 463: 25, 464: 25, 465: 25, 466: 25, 467: 25, 468: 25, 469: 25, 470: 25, 471: 25, 472: 25, 473: 25, 474: 25, 475: 25, 476: 25, 477: 25, 478: 25, 479: 25, 480: 25, 481: 25, 482: 25, 483: 25, 484: 25, 485: 25, 486: 25, 487: 25, 488: 25, 489: 25, 490: 25, 491: 25, 492: 25, 493: 25, 494: 25, 495: 25, 496: 25, 497: 25, 498: 25, 499: 25, 500: 25, 501: 25, 502: 25, 503: 25, 504: 25, 505: 25, 506: 25, 507: 25, 508: 25, 509: 25, 510: 25, 511: 25, 512: 25, 513: 25, 514: 25, 515: 25, 516: 25, 517: 25, 518: 25, 519: 25, 520: 25, 521: 25, 522: 25, 523: 25, 524: 25, 525: 25, 526: 25, 527: 25, 528: 25, 529: 25, 530: 25, 531: 25, 532: 25, 533: 25, 534: 25, 535: 25, 536: 25, 537: 25, 538: 25, 539: 25, 540: 25, 541: 25, 542: 25, 543: 25, 544: 25, 545: 25, 546: 25, 547: 25, 548: 25, 549: 25, 550: 25, 551: 25, 552: 25, 553: 25, 554: 25, 555: 25, 556: 25, 557: 25, 558: 25, 559: 25, 560: 25, 561: 25, 562: 25, 563: 25, 564: 25, 565: 25, 566: 25, 567: 25, 568: 25, 569: 25, 570: 25, 571: 25, 572: 25, 573: 25, 574: 25, 575: 25, 576: 25, 577: 25, 578: 25, 579: 25, 580: 25, 581: 25, 582: 25, 583: 25, 584: 25, 585: 25, 586: 25, 587: 25, 588: 25, 589: 25, 590: 25, 591: 25, 592: 25, 593: 25, 594: 25, 595: 25, 596: 25, 597: 25, 598: 25, 599: 25, 600: 25, 601: 25, 602: 25, 603: 25, 604: 25, 605: 25, 606: 25, 607: 25, 608: 25, 609: 25, 610: 25, 611: 25, 612: 25, 613: 25, 614: 25, 615: 25, 616: 25, 617: 25, 618: 25, 619: 25, 620: 25, 621: 25, 622: 25, 623: 25, 624: 25, 625: 25, 626: 25, 627: 25, 628: 25, 629: 25, 630: 25, 631: 25, 632: 25, 633: 25})
STEP-2	Epoch: 20/200	classification_loss: 0.8310	gate_loss: 0.9754	step2_classification_accuracy: 75.3123	step_2_gate_accuracy: 70.1640
STEP-2	Epoch: 40/200	classification_loss: 0.6100	gate_loss: 0.5513	step2_classification_accuracy: 81.3312	step_2_gate_accuracy: 81.8738
STEP-2	Epoch: 60/200	classification_loss: 0.4940	gate_loss: 0.4070	step2_classification_accuracy: 84.3281	step_2_gate_accuracy: 86.0252
STEP-2	Epoch: 80/200	classification_loss: 0.4303	gate_loss: 0.3364	step2_classification_accuracy: 86.1009	step_2_gate_accuracy: 88.3407
STEP-2	Epoch: 100/200	classification_loss: 0.3944	gate_loss: 0.2948	step2_classification_accuracy: 87.1356	step_2_gate_accuracy: 89.4700
STEP-2	Epoch: 120/200	classification_loss: 0.3625	gate_loss: 0.2685	step2_classification_accuracy: 87.9495	step_2_gate_accuracy: 90.5804
STEP-2	Epoch: 140/200	classification_loss: 0.3441	gate_loss: 0.2489	step2_classification_accuracy: 88.3785	step_2_gate_accuracy: 91.2808
STEP-2	Epoch: 160/200	classification_loss: 0.3306	gate_loss: 0.2350	step2_classification_accuracy: 88.7950	step_2_gate_accuracy: 91.5142
STEP-2	Epoch: 180/200	classification_loss: 0.3079	gate_loss: 0.2146	step2_classification_accuracy: 89.2934	step_2_gate_accuracy: 92.2965
STEP-2	Epoch: 200/200	classification_loss: 0.2982	gate_loss: 0.2071	step2_classification_accuracy: 89.5773	step_2_gate_accuracy: 92.6688
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 76.2590	gate_accuracy: 83.4532
	Task-1	val_accuracy: 50.6494	gate_accuracy: 58.4416
	Task-2	val_accuracy: 71.2766	gate_accuracy: 73.4043
	Task-3	val_accuracy: 64.8936	gate_accuracy: 71.2766
	Task-4	val_accuracy: 72.1519	gate_accuracy: 69.6203
	Task-5	val_accuracy: 82.8571	gate_accuracy: 81.4286
	Task-6	val_accuracy: 71.4286	gate_accuracy: 66.2338
	Task-7	val_accuracy: 51.8987	gate_accuracy: 43.0380
	Task-8	val_accuracy: 67.0732	gate_accuracy: 62.1951
	Task-9	val_accuracy: 60.5263	gate_accuracy: 68.4211
	Task-10	val_accuracy: 86.7470	gate_accuracy: 84.3373
	Task-11	val_accuracy: 71.7949	gate_accuracy: 67.9487
	Task-12	val_accuracy: 67.8161	gate_accuracy: 63.2184
	Task-13	val_accuracy: 77.1739	gate_accuracy: 78.2609
	Task-14	val_accuracy: 76.4706	gate_accuracy: 76.4706
	Task-15	val_accuracy: 65.6716	gate_accuracy: 64.1791
	Task-16	val_accuracy: 63.9535	gate_accuracy: 60.4651
	Task-17	val_accuracy: 68.7500	gate_accuracy: 66.2500
	Task-18	val_accuracy: 82.5581	gate_accuracy: 74.4186
	Task-19	val_accuracy: 63.6364	gate_accuracy: 67.5325
	Task-20	val_accuracy: 75.6098	gate_accuracy: 76.8293
	Task-21	val_accuracy: 73.3333	gate_accuracy: 71.1111
	Task-22	val_accuracy: 75.2941	gate_accuracy: 72.9412
	Task-23	val_accuracy: 67.5000	gate_accuracy: 65.0000
	Task-24	val_accuracy: 47.8873	gate_accuracy: 50.7042
	Task-25	val_accuracy: 75.0000	gate_accuracy: 76.1905
	Task-26	val_accuracy: 69.6203	gate_accuracy: 72.1519
	Task-27	val_accuracy: 66.6667	gate_accuracy: 70.5128
	Task-28	val_accuracy: 49.3827	gate_accuracy: 58.0247
	Task-29	val_accuracy: 50.7042	gate_accuracy: 59.1549
	Task-30	val_accuracy: 92.6471	gate_accuracy: 94.1176
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 69.6457


[634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]
Polling GMM for: {634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653}
STEP-1	Epoch: 10/50	loss: 2.1074	step1_train_accuracy: 54.5714
STEP-1	Epoch: 20/50	loss: 0.8772	step1_train_accuracy: 77.7143
STEP-1	Epoch: 30/50	loss: 0.4999	step1_train_accuracy: 92.2857
STEP-1	Epoch: 40/50	loss: 0.3396	step1_train_accuracy: 95.4286
STEP-1	Epoch: 50/50	loss: 0.2529	step1_train_accuracy: 96.0000
FINISH STEP 1
Task-32	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.7699	gate_loss: 0.8107	step2_classification_accuracy: 77.2888	step_2_gate_accuracy: 74.6846
STEP-2	Epoch: 40/200	classification_loss: 0.5633	gate_loss: 0.4874	step2_classification_accuracy: 82.4924	step_2_gate_accuracy: 83.7013
STEP-2	Epoch: 60/200	classification_loss: 0.4771	gate_loss: 0.3860	step2_classification_accuracy: 84.4562	step_2_gate_accuracy: 86.6829
STEP-2	Epoch: 80/200	classification_loss: 0.4328	gate_loss: 0.3318	step2_classification_accuracy: 85.7511	step_2_gate_accuracy: 88.2645
STEP-2	Epoch: 100/200	classification_loss: 0.3889	gate_loss: 0.2914	step2_classification_accuracy: 86.9123	step_2_gate_accuracy: 89.6646
STEP-2	Epoch: 120/200	classification_loss: 0.3656	gate_loss: 0.2687	step2_classification_accuracy: 87.3375	step_2_gate_accuracy: 90.1854
STEP-2	Epoch: 140/200	classification_loss: 0.3466	gate_loss: 0.2491	step2_classification_accuracy: 88.2502	step_2_gate_accuracy: 90.9451
STEP-2	Epoch: 160/200	classification_loss: 0.3316	gate_loss: 0.2337	step2_classification_accuracy: 88.4604	step_2_gate_accuracy: 91.4134
STEP-2	Epoch: 180/200	classification_loss: 0.3199	gate_loss: 0.2246	step2_classification_accuracy: 88.5512	step_2_gate_accuracy: 91.7909
STEP-2	Epoch: 200/200	classification_loss: 0.3052	gate_loss: 0.2118	step2_classification_accuracy: 89.1246	step_2_gate_accuracy: 92.1923
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 67.6259	gate_accuracy: 75.5396
	Task-1	val_accuracy: 55.8442	gate_accuracy: 63.6364
	Task-2	val_accuracy: 64.8936	gate_accuracy: 61.7021
	Task-3	val_accuracy: 67.0213	gate_accuracy: 64.8936
	Task-4	val_accuracy: 72.1519	gate_accuracy: 69.6203
	Task-5	val_accuracy: 75.7143	gate_accuracy: 71.4286
	Task-6	val_accuracy: 74.0260	gate_accuracy: 68.8312
	Task-7	val_accuracy: 51.8987	gate_accuracy: 46.8354
	Task-8	val_accuracy: 62.1951	gate_accuracy: 63.4146
	Task-9	val_accuracy: 53.9474	gate_accuracy: 57.8947
	Task-10	val_accuracy: 90.3614	gate_accuracy: 86.7470
	Task-11	val_accuracy: 71.7949	gate_accuracy: 65.3846
	Task-12	val_accuracy: 71.2644	gate_accuracy: 55.1724
	Task-13	val_accuracy: 85.8696	gate_accuracy: 85.8696
	Task-14	val_accuracy: 80.8824	gate_accuracy: 73.5294
	Task-15	val_accuracy: 70.1493	gate_accuracy: 71.6418
	Task-16	val_accuracy: 63.9535	gate_accuracy: 65.1163
	Task-17	val_accuracy: 72.5000	gate_accuracy: 66.2500
	Task-18	val_accuracy: 87.2093	gate_accuracy: 83.7209
	Task-19	val_accuracy: 59.7403	gate_accuracy: 57.1429
	Task-20	val_accuracy: 84.1463	gate_accuracy: 81.7073
	Task-21	val_accuracy: 63.3333	gate_accuracy: 61.1111
	Task-22	val_accuracy: 71.7647	gate_accuracy: 70.5882
	Task-23	val_accuracy: 70.0000	gate_accuracy: 71.2500
	Task-24	val_accuracy: 52.1127	gate_accuracy: 56.3380
	Task-25	val_accuracy: 79.7619	gate_accuracy: 83.3333
	Task-26	val_accuracy: 68.3544	gate_accuracy: 67.0886
	Task-27	val_accuracy: 67.9487	gate_accuracy: 70.5128
	Task-28	val_accuracy: 48.1481	gate_accuracy: 50.6173
	Task-29	val_accuracy: 49.2958	gate_accuracy: 60.5634
	Task-30	val_accuracy: 88.2353	gate_accuracy: 91.1765
	Task-31	val_accuracy: 77.2727	gate_accuracy: 81.8182
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.9498


[654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]
Polling GMM for: {654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673}
STEP-1	Epoch: 10/50	loss: 2.3595	step1_train_accuracy: 54.0541
STEP-1	Epoch: 20/50	loss: 0.8558	step1_train_accuracy: 85.4054
STEP-1	Epoch: 30/50	loss: 0.4237	step1_train_accuracy: 95.4054
STEP-1	Epoch: 40/50	loss: 0.2695	step1_train_accuracy: 97.0270
STEP-1	Epoch: 50/50	loss: 0.1947	step1_train_accuracy: 97.5676
FINISH STEP 1
Task-33	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.8100	gate_loss: 0.8472	step2_classification_accuracy: 76.1267	step_2_gate_accuracy: 73.9707
STEP-2	Epoch: 40/200	classification_loss: 0.5976	gate_loss: 0.5197	step2_classification_accuracy: 81.8667	step_2_gate_accuracy: 82.6224
STEP-2	Epoch: 60/200	classification_loss: 0.4928	gate_loss: 0.4011	step2_classification_accuracy: 84.7088	step_2_gate_accuracy: 86.3084
STEP-2	Epoch: 80/200	classification_loss: 0.4373	gate_loss: 0.3383	step2_classification_accuracy: 85.8262	step_2_gate_accuracy: 88.3067
STEP-2	Epoch: 100/200	classification_loss: 0.4012	gate_loss: 0.3028	step2_classification_accuracy: 86.7025	step_2_gate_accuracy: 89.2850
STEP-2	Epoch: 120/200	classification_loss: 0.3701	gate_loss: 0.2723	step2_classification_accuracy: 87.6947	step_2_gate_accuracy: 90.2726
STEP-2	Epoch: 140/200	classification_loss: 0.3527	gate_loss: 0.2571	step2_classification_accuracy: 88.2140	step_2_gate_accuracy: 90.8012
STEP-2	Epoch: 160/200	classification_loss: 0.3327	gate_loss: 0.2378	step2_classification_accuracy: 88.7472	step_2_gate_accuracy: 91.2556
STEP-2	Epoch: 180/200	classification_loss: 0.3192	gate_loss: 0.2263	step2_classification_accuracy: 89.0254	step_2_gate_accuracy: 91.8490
STEP-2	Epoch: 200/200	classification_loss: 0.3103	gate_loss: 0.2173	step2_classification_accuracy: 89.0208	step_2_gate_accuracy: 92.0716
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 71.2230	gate_accuracy: 83.4532
	Task-1	val_accuracy: 50.6494	gate_accuracy: 58.4416
	Task-2	val_accuracy: 74.4681	gate_accuracy: 75.5319
	Task-3	val_accuracy: 69.1489	gate_accuracy: 67.0213
	Task-4	val_accuracy: 69.6203	gate_accuracy: 74.6835
	Task-5	val_accuracy: 78.5714	gate_accuracy: 78.5714
	Task-6	val_accuracy: 79.2208	gate_accuracy: 67.5325
	Task-7	val_accuracy: 51.8987	gate_accuracy: 46.8354
	Task-8	val_accuracy: 62.1951	gate_accuracy: 60.9756
	Task-9	val_accuracy: 60.5263	gate_accuracy: 56.5789
	Task-10	val_accuracy: 89.1566	gate_accuracy: 80.7229
	Task-11	val_accuracy: 64.1026	gate_accuracy: 52.5641
	Task-12	val_accuracy: 67.8161	gate_accuracy: 60.9195
	Task-13	val_accuracy: 85.8696	gate_accuracy: 83.6957
	Task-14	val_accuracy: 76.4706	gate_accuracy: 70.5882
	Task-15	val_accuracy: 71.6418	gate_accuracy: 71.6418
	Task-16	val_accuracy: 74.4186	gate_accuracy: 70.9302
	Task-17	val_accuracy: 73.7500	gate_accuracy: 71.2500
	Task-18	val_accuracy: 81.3953	gate_accuracy: 80.2326
	Task-19	val_accuracy: 62.3377	gate_accuracy: 58.4416
	Task-20	val_accuracy: 80.4878	gate_accuracy: 81.7073
	Task-21	val_accuracy: 62.2222	gate_accuracy: 63.3333
	Task-22	val_accuracy: 75.2941	gate_accuracy: 69.4118
	Task-23	val_accuracy: 71.2500	gate_accuracy: 77.5000
	Task-24	val_accuracy: 49.2958	gate_accuracy: 54.9296
	Task-25	val_accuracy: 75.0000	gate_accuracy: 72.6190
	Task-26	val_accuracy: 68.3544	gate_accuracy: 68.3544
	Task-27	val_accuracy: 62.8205	gate_accuracy: 67.9487
	Task-28	val_accuracy: 56.7901	gate_accuracy: 60.4938
	Task-29	val_accuracy: 56.3380	gate_accuracy: 66.1972
	Task-30	val_accuracy: 88.2353	gate_accuracy: 91.1765
	Task-31	val_accuracy: 78.4091	gate_accuracy: 86.3636
	Task-32	val_accuracy: 69.5652	gate_accuracy: 70.6522
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.1471


[674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]
Polling GMM for: {674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693}
STEP-1	Epoch: 10/50	loss: 2.3709	step1_train_accuracy: 58.0737
STEP-1	Epoch: 20/50	loss: 0.7751	step1_train_accuracy: 82.4363
STEP-1	Epoch: 30/50	loss: 0.4294	step1_train_accuracy: 91.2181
STEP-1	Epoch: 40/50	loss: 0.3076	step1_train_accuracy: 93.7677
STEP-1	Epoch: 50/50	loss: 0.2458	step1_train_accuracy: 94.3343
FINISH STEP 1
Task-34	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.8310	gate_loss: 0.8525	step2_classification_accuracy: 75.6799	step_2_gate_accuracy: 73.7302
STEP-2	Epoch: 40/200	classification_loss: 0.6186	gate_loss: 0.5364	step2_classification_accuracy: 81.2140	step_2_gate_accuracy: 82.0155
STEP-2	Epoch: 60/200	classification_loss: 0.5173	gate_loss: 0.4189	step2_classification_accuracy: 83.7716	step_2_gate_accuracy: 85.6718
STEP-2	Epoch: 80/200	classification_loss: 0.4583	gate_loss: 0.3557	step2_classification_accuracy: 85.1315	step_2_gate_accuracy: 87.4009
STEP-2	Epoch: 100/200	classification_loss: 0.4203	gate_loss: 0.3143	step2_classification_accuracy: 86.0861	step_2_gate_accuracy: 88.7833
STEP-2	Epoch: 120/200	classification_loss: 0.3902	gate_loss: 0.2859	step2_classification_accuracy: 87.0317	step_2_gate_accuracy: 89.6479
STEP-2	Epoch: 140/200	classification_loss: 0.3747	gate_loss: 0.2714	step2_classification_accuracy: 87.2208	step_2_gate_accuracy: 90.0441
STEP-2	Epoch: 160/200	classification_loss: 0.3544	gate_loss: 0.2538	step2_classification_accuracy: 87.9053	step_2_gate_accuracy: 90.8051
STEP-2	Epoch: 180/200	classification_loss: 0.3450	gate_loss: 0.2429	step2_classification_accuracy: 88.1079	step_2_gate_accuracy: 91.0077
STEP-2	Epoch: 200/200	classification_loss: 0.3297	gate_loss: 0.2319	step2_classification_accuracy: 88.5131	step_2_gate_accuracy: 91.6381
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.1007	gate_accuracy: 81.2950
	Task-1	val_accuracy: 54.5455	gate_accuracy: 61.0390
	Task-2	val_accuracy: 74.4681	gate_accuracy: 74.4681
	Task-3	val_accuracy: 59.5745	gate_accuracy: 67.0213
	Task-4	val_accuracy: 73.4177	gate_accuracy: 72.1519
	Task-5	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-6	val_accuracy: 72.7273	gate_accuracy: 66.2338
	Task-7	val_accuracy: 53.1646	gate_accuracy: 49.3671
	Task-8	val_accuracy: 63.4146	gate_accuracy: 60.9756
	Task-9	val_accuracy: 61.8421	gate_accuracy: 65.7895
	Task-10	val_accuracy: 85.5422	gate_accuracy: 81.9277
	Task-11	val_accuracy: 73.0769	gate_accuracy: 66.6667
	Task-12	val_accuracy: 66.6667	gate_accuracy: 63.2184
	Task-13	val_accuracy: 84.7826	gate_accuracy: 81.5217
	Task-14	val_accuracy: 75.0000	gate_accuracy: 72.0588
	Task-15	val_accuracy: 68.6567	gate_accuracy: 65.6716
	Task-16	val_accuracy: 72.0930	gate_accuracy: 72.0930
	Task-17	val_accuracy: 70.0000	gate_accuracy: 65.0000
	Task-18	val_accuracy: 77.9070	gate_accuracy: 70.9302
	Task-19	val_accuracy: 55.8442	gate_accuracy: 61.0390
	Task-20	val_accuracy: 70.7317	gate_accuracy: 70.7317
	Task-21	val_accuracy: 61.1111	gate_accuracy: 58.8889
	Task-22	val_accuracy: 75.2941	gate_accuracy: 76.4706
	Task-23	val_accuracy: 72.5000	gate_accuracy: 77.5000
	Task-24	val_accuracy: 60.5634	gate_accuracy: 67.6056
	Task-25	val_accuracy: 77.3810	gate_accuracy: 78.5714
	Task-26	val_accuracy: 62.0253	gate_accuracy: 63.2911
	Task-27	val_accuracy: 61.5385	gate_accuracy: 67.9487
	Task-28	val_accuracy: 48.1481	gate_accuracy: 54.3210
	Task-29	val_accuracy: 60.5634	gate_accuracy: 73.2394
	Task-30	val_accuracy: 86.7647	gate_accuracy: 82.3529
	Task-31	val_accuracy: 77.2727	gate_accuracy: 82.9545
	Task-32	val_accuracy: 77.1739	gate_accuracy: 77.1739
	Task-33	val_accuracy: 73.8636	gate_accuracy: 75.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 70.4416


[694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]
Polling GMM for: {694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713}
STEP-1	Epoch: 10/50	loss: 1.9446	step1_train_accuracy: 66.3934
STEP-1	Epoch: 20/50	loss: 0.7412	step1_train_accuracy: 86.8852
STEP-1	Epoch: 30/50	loss: 0.3664	step1_train_accuracy: 96.1749
STEP-1	Epoch: 40/50	loss: 0.2335	step1_train_accuracy: 98.0874
STEP-1	Epoch: 50/50	loss: 0.1593	step1_train_accuracy: 99.1803
FINISH STEP 1
Task-35	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.8575	gate_loss: 0.8870	step2_classification_accuracy: 75.1379	step_2_gate_accuracy: 72.5575
STEP-2	Epoch: 40/200	classification_loss: 0.6390	gate_loss: 0.5545	step2_classification_accuracy: 80.7147	step_2_gate_accuracy: 81.5975
STEP-2	Epoch: 60/200	classification_loss: 0.5308	gate_loss: 0.4328	step2_classification_accuracy: 83.3843	step_2_gate_accuracy: 84.9928
STEP-2	Epoch: 80/200	classification_loss: 0.4769	gate_loss: 0.3736	step2_classification_accuracy: 84.8485	step_2_gate_accuracy: 86.9833
STEP-2	Epoch: 100/200	classification_loss: 0.4312	gate_loss: 0.3298	step2_classification_accuracy: 85.9392	step_2_gate_accuracy: 88.0740
STEP-2	Epoch: 120/200	classification_loss: 0.4394	gate_loss: 0.3371	step2_classification_accuracy: 85.6379	step_2_gate_accuracy: 87.7939
STEP-2	Epoch: 140/200	classification_loss: 0.3805	gate_loss: 0.2814	step2_classification_accuracy: 87.2973	step_2_gate_accuracy: 90.0051
STEP-2	Epoch: 160/200	classification_loss: 0.3678	gate_loss: 0.2684	step2_classification_accuracy: 87.5477	step_2_gate_accuracy: 90.2682
STEP-2	Epoch: 180/200	classification_loss: 0.3531	gate_loss: 0.2552	step2_classification_accuracy: 88.0570	step_2_gate_accuracy: 90.7351
STEP-2	Epoch: 200/200	classification_loss: 0.3425	gate_loss: 0.2468	step2_classification_accuracy: 88.4178	step_2_gate_accuracy: 91.0704
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 70.5036	gate_accuracy: 79.1367
	Task-1	val_accuracy: 53.2468	gate_accuracy: 61.0390
	Task-2	val_accuracy: 71.2766	gate_accuracy: 68.0851
	Task-3	val_accuracy: 59.5745	gate_accuracy: 63.8298
	Task-4	val_accuracy: 74.6835	gate_accuracy: 73.4177
	Task-5	val_accuracy: 80.0000	gate_accuracy: 75.7143
	Task-6	val_accuracy: 71.4286	gate_accuracy: 63.6364
	Task-7	val_accuracy: 62.0253	gate_accuracy: 53.1646
	Task-8	val_accuracy: 58.5366	gate_accuracy: 52.4390
	Task-9	val_accuracy: 63.1579	gate_accuracy: 68.4211
	Task-10	val_accuracy: 85.5422	gate_accuracy: 83.1325
	Task-11	val_accuracy: 74.3590	gate_accuracy: 67.9487
	Task-12	val_accuracy: 67.8161	gate_accuracy: 65.5172
	Task-13	val_accuracy: 83.6957	gate_accuracy: 78.2609
	Task-14	val_accuracy: 70.5882	gate_accuracy: 67.6471
	Task-15	val_accuracy: 73.1343	gate_accuracy: 73.1343
	Task-16	val_accuracy: 62.7907	gate_accuracy: 58.1395
	Task-17	val_accuracy: 66.2500	gate_accuracy: 62.5000
	Task-18	val_accuracy: 82.5581	gate_accuracy: 75.5814
	Task-19	val_accuracy: 55.8442	gate_accuracy: 57.1429
	Task-20	val_accuracy: 78.0488	gate_accuracy: 74.3902
	Task-21	val_accuracy: 67.7778	gate_accuracy: 64.4444
	Task-22	val_accuracy: 72.9412	gate_accuracy: 71.7647
	Task-23	val_accuracy: 73.7500	gate_accuracy: 73.7500
	Task-24	val_accuracy: 54.9296	gate_accuracy: 60.5634
	Task-25	val_accuracy: 75.0000	gate_accuracy: 79.7619
	Task-26	val_accuracy: 59.4937	gate_accuracy: 56.9620
	Task-27	val_accuracy: 61.5385	gate_accuracy: 66.6667
	Task-28	val_accuracy: 41.9753	gate_accuracy: 45.6790
	Task-29	val_accuracy: 49.2958	gate_accuracy: 64.7887
	Task-30	val_accuracy: 91.1765	gate_accuracy: 89.7059
	Task-31	val_accuracy: 78.4091	gate_accuracy: 82.9545
	Task-32	val_accuracy: 68.4783	gate_accuracy: 65.2174
	Task-33	val_accuracy: 71.5909	gate_accuracy: 73.8636
	Task-34	val_accuracy: 65.9341	gate_accuracy: 78.0220
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.7133


[714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]
Polling GMM for: {714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733}
STEP-1	Epoch: 10/50	loss: 2.0461	step1_train_accuracy: 56.6038
STEP-1	Epoch: 20/50	loss: 0.8593	step1_train_accuracy: 81.6712
STEP-1	Epoch: 30/50	loss: 0.4622	step1_train_accuracy: 91.1051
STEP-1	Epoch: 40/50	loss: 0.3105	step1_train_accuracy: 94.3396
STEP-1	Epoch: 50/50	loss: 0.2309	step1_train_accuracy: 96.7655
FINISH STEP 1
Task-36	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.8702	gate_loss: 0.9226	step2_classification_accuracy: 74.8382	step_2_gate_accuracy: 72.0581
STEP-2	Epoch: 40/200	classification_loss: 0.6402	gate_loss: 0.5664	step2_classification_accuracy: 80.7348	step_2_gate_accuracy: 81.3607
STEP-2	Epoch: 60/200	classification_loss: 0.5314	gate_loss: 0.4404	step2_classification_accuracy: 83.4724	step_2_gate_accuracy: 84.9540
STEP-2	Epoch: 80/200	classification_loss: 0.4747	gate_loss: 0.3781	step2_classification_accuracy: 84.9242	step_2_gate_accuracy: 86.7635
STEP-2	Epoch: 100/200	classification_loss: 0.4322	gate_loss: 0.3328	step2_classification_accuracy: 85.9205	step_2_gate_accuracy: 87.9896
STEP-2	Epoch: 120/200	classification_loss: 0.4003	gate_loss: 0.3016	step2_classification_accuracy: 86.8486	step_2_gate_accuracy: 89.0710
STEP-2	Epoch: 140/200	classification_loss: 0.3792	gate_loss: 0.2819	step2_classification_accuracy: 87.5213	step_2_gate_accuracy: 89.8459
STEP-2	Epoch: 160/200	classification_loss: 0.3662	gate_loss: 0.2675	step2_classification_accuracy: 87.7895	step_2_gate_accuracy: 90.3610
STEP-2	Epoch: 180/200	classification_loss: 0.3468	gate_loss: 0.2512	step2_classification_accuracy: 88.3260	step_2_gate_accuracy: 90.7570
STEP-2	Epoch: 200/200	classification_loss: 0.3383	gate_loss: 0.2408	step2_classification_accuracy: 88.5686	step_2_gate_accuracy: 91.1870
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.4676	gate_accuracy: 76.9784
	Task-1	val_accuracy: 53.2468	gate_accuracy: 61.0390
	Task-2	val_accuracy: 60.6383	gate_accuracy: 59.5745
	Task-3	val_accuracy: 65.9574	gate_accuracy: 69.1489
	Task-4	val_accuracy: 68.3544	gate_accuracy: 70.8861
	Task-5	val_accuracy: 68.5714	gate_accuracy: 71.4286
	Task-6	val_accuracy: 76.6234	gate_accuracy: 66.2338
	Task-7	val_accuracy: 55.6962	gate_accuracy: 54.4304
	Task-8	val_accuracy: 60.9756	gate_accuracy: 60.9756
	Task-9	val_accuracy: 63.1579	gate_accuracy: 67.1053
	Task-10	val_accuracy: 83.1325	gate_accuracy: 83.1325
	Task-11	val_accuracy: 80.7692	gate_accuracy: 73.0769
	Task-12	val_accuracy: 70.1149	gate_accuracy: 67.8161
	Task-13	val_accuracy: 80.4348	gate_accuracy: 75.0000
	Task-14	val_accuracy: 76.4706	gate_accuracy: 70.5882
	Task-15	val_accuracy: 73.1343	gate_accuracy: 68.6567
	Task-16	val_accuracy: 60.4651	gate_accuracy: 55.8140
	Task-17	val_accuracy: 77.5000	gate_accuracy: 66.2500
	Task-18	val_accuracy: 79.0698	gate_accuracy: 74.4186
	Task-19	val_accuracy: 54.5455	gate_accuracy: 51.9481
	Task-20	val_accuracy: 74.3902	gate_accuracy: 73.1707
	Task-21	val_accuracy: 64.4444	gate_accuracy: 56.6667
	Task-22	val_accuracy: 68.2353	gate_accuracy: 71.7647
	Task-23	val_accuracy: 67.5000	gate_accuracy: 65.0000
	Task-24	val_accuracy: 56.3380	gate_accuracy: 56.3380
	Task-25	val_accuracy: 76.1905	gate_accuracy: 78.5714
	Task-26	val_accuracy: 65.8228	gate_accuracy: 63.2911
	Task-27	val_accuracy: 66.6667	gate_accuracy: 67.9487
	Task-28	val_accuracy: 46.9136	gate_accuracy: 48.1481
	Task-29	val_accuracy: 46.4789	gate_accuracy: 56.3380
	Task-30	val_accuracy: 89.7059	gate_accuracy: 88.2353
	Task-31	val_accuracy: 81.8182	gate_accuracy: 85.2273
	Task-32	val_accuracy: 68.4783	gate_accuracy: 68.4783
	Task-33	val_accuracy: 72.7273	gate_accuracy: 81.8182
	Task-34	val_accuracy: 63.7363	gate_accuracy: 69.2308
	Task-35	val_accuracy: 69.8925	gate_accuracy: 73.1183
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.2487


[734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]
Polling GMM for: {734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753}
STEP-1	Epoch: 10/50	loss: 3.9521	step1_train_accuracy: 48.7179
STEP-1	Epoch: 20/50	loss: 1.4249	step1_train_accuracy: 74.3590
STEP-1	Epoch: 30/50	loss: 0.6366	step1_train_accuracy: 89.3773
STEP-1	Epoch: 40/50	loss: 0.4114	step1_train_accuracy: 93.0403
STEP-1	Epoch: 50/50	loss: 0.3056	step1_train_accuracy: 94.1392
FINISH STEP 1
Task-37	STARTING STEP 2
CLASS COUNTER: Counter({0: 29, 1: 29, 2: 29, 3: 29, 4: 29, 5: 29, 6: 29, 7: 29, 8: 29, 9: 29, 10: 29, 11: 29, 12: 29, 13: 29, 14: 29, 15: 29, 16: 29, 17: 29, 18: 29, 19: 29, 20: 29, 21: 29, 22: 29, 23: 29, 24: 29, 25: 29, 26: 29, 27: 29, 28: 29, 29: 29, 30: 29, 31: 29, 32: 29, 33: 29, 34: 29, 35: 29, 36: 29, 37: 29, 38: 29, 39: 29, 40: 29, 41: 29, 42: 29, 43: 29, 44: 29, 45: 29, 46: 29, 47: 29, 48: 29, 49: 29, 50: 29, 51: 29, 52: 29, 53: 29, 54: 29, 55: 29, 56: 29, 57: 29, 58: 29, 59: 29, 60: 29, 61: 29, 62: 29, 63: 29, 64: 29, 65: 29, 66: 29, 67: 29, 68: 29, 69: 29, 70: 29, 71: 29, 72: 29, 73: 29, 74: 29, 75: 29, 76: 29, 77: 29, 78: 29, 79: 29, 80: 29, 81: 29, 82: 29, 83: 29, 84: 29, 85: 29, 86: 29, 87: 29, 88: 29, 89: 29, 90: 29, 91: 29, 92: 29, 93: 29, 94: 29, 95: 29, 96: 29, 97: 29, 98: 29, 99: 29, 100: 29, 101: 29, 102: 29, 103: 29, 104: 29, 105: 29, 106: 29, 107: 29, 108: 29, 109: 29, 110: 29, 111: 29, 112: 29, 113: 29, 114: 29, 115: 29, 116: 29, 117: 29, 118: 29, 119: 29, 120: 29, 121: 29, 122: 29, 123: 29, 124: 29, 125: 29, 126: 29, 127: 29, 128: 29, 129: 29, 130: 29, 131: 29, 132: 29, 133: 29, 134: 29, 135: 29, 136: 29, 137: 29, 138: 29, 139: 29, 140: 29, 141: 29, 142: 29, 143: 29, 144: 29, 145: 29, 146: 29, 147: 29, 148: 29, 149: 29, 150: 29, 151: 29, 152: 29, 153: 29, 154: 29, 155: 29, 156: 29, 157: 29, 158: 29, 159: 29, 160: 29, 161: 29, 162: 29, 163: 29, 164: 29, 165: 29, 166: 29, 167: 29, 168: 29, 169: 29, 170: 29, 171: 29, 172: 29, 173: 29, 174: 29, 175: 29, 176: 29, 177: 29, 178: 29, 179: 29, 180: 29, 181: 29, 182: 29, 183: 29, 184: 29, 185: 29, 186: 29, 187: 29, 188: 29, 189: 29, 190: 29, 191: 29, 192: 29, 193: 29, 194: 29, 195: 29, 196: 29, 197: 29, 198: 29, 199: 29, 200: 29, 201: 29, 202: 29, 203: 29, 204: 29, 205: 29, 206: 29, 207: 29, 208: 29, 209: 29, 210: 29, 211: 29, 212: 29, 213: 29, 214: 29, 215: 29, 216: 29, 217: 29, 218: 29, 219: 29, 220: 29, 221: 29, 222: 29, 223: 29, 224: 29, 225: 29, 226: 29, 227: 29, 228: 29, 229: 29, 230: 29, 231: 29, 232: 29, 233: 29, 234: 29, 235: 29, 236: 29, 237: 29, 238: 29, 239: 29, 240: 29, 241: 29, 242: 29, 243: 29, 244: 29, 245: 29, 246: 29, 247: 29, 248: 29, 249: 29, 250: 29, 251: 29, 252: 29, 253: 29, 254: 29, 255: 29, 256: 29, 257: 29, 258: 29, 259: 29, 260: 29, 261: 29, 262: 29, 263: 29, 264: 29, 265: 29, 266: 29, 267: 29, 268: 29, 269: 29, 270: 29, 271: 29, 272: 29, 273: 29, 274: 29, 275: 29, 276: 29, 277: 29, 278: 29, 279: 29, 280: 29, 281: 29, 282: 29, 283: 29, 284: 29, 285: 29, 286: 29, 287: 29, 288: 29, 289: 29, 290: 29, 291: 29, 292: 29, 293: 29, 294: 29, 295: 29, 296: 29, 297: 29, 298: 29, 299: 29, 300: 29, 301: 29, 302: 29, 303: 29, 304: 29, 305: 29, 306: 29, 307: 29, 308: 29, 309: 29, 310: 29, 311: 29, 312: 29, 313: 29, 314: 29, 315: 29, 316: 29, 317: 29, 318: 29, 319: 29, 320: 29, 321: 29, 322: 29, 323: 29, 324: 29, 325: 29, 326: 29, 327: 29, 328: 29, 329: 29, 330: 29, 331: 29, 332: 29, 333: 29, 334: 29, 335: 29, 336: 29, 337: 29, 338: 29, 339: 29, 340: 29, 341: 29, 342: 29, 343: 29, 344: 29, 345: 29, 346: 29, 347: 29, 348: 29, 349: 29, 350: 29, 351: 29, 352: 29, 353: 29, 354: 29, 355: 29, 356: 29, 357: 29, 358: 29, 359: 29, 360: 29, 361: 29, 362: 29, 363: 29, 364: 29, 365: 29, 366: 29, 367: 29, 368: 29, 369: 29, 370: 29, 371: 29, 372: 29, 373: 29, 374: 29, 375: 29, 376: 29, 377: 29, 378: 29, 379: 29, 380: 29, 381: 29, 382: 29, 383: 29, 384: 29, 385: 29, 386: 29, 387: 29, 388: 29, 389: 29, 390: 29, 391: 29, 392: 29, 393: 29, 394: 29, 395: 29, 396: 29, 397: 29, 398: 29, 399: 29, 400: 29, 401: 29, 402: 29, 403: 29, 404: 29, 405: 29, 406: 29, 407: 29, 408: 29, 409: 29, 410: 29, 411: 29, 412: 29, 413: 29, 414: 29, 415: 29, 416: 29, 417: 29, 418: 29, 419: 29, 420: 29, 421: 29, 422: 29, 423: 29, 424: 29, 425: 29, 426: 29, 427: 29, 428: 29, 429: 29, 430: 29, 431: 29, 432: 29, 433: 29, 434: 29, 435: 29, 436: 29, 437: 29, 438: 29, 439: 29, 440: 29, 441: 29, 442: 29, 443: 29, 444: 29, 445: 29, 446: 29, 447: 29, 448: 29, 449: 29, 450: 29, 451: 29, 452: 29, 453: 29, 454: 29, 455: 29, 456: 29, 457: 29, 458: 29, 459: 29, 460: 29, 461: 29, 462: 29, 463: 29, 464: 29, 465: 29, 466: 29, 467: 29, 468: 29, 469: 29, 470: 29, 471: 29, 472: 29, 473: 29, 474: 29, 475: 29, 476: 29, 477: 29, 478: 29, 479: 29, 480: 29, 481: 29, 482: 29, 483: 29, 484: 29, 485: 29, 486: 29, 487: 29, 488: 29, 489: 29, 490: 29, 491: 29, 492: 29, 493: 29, 494: 29, 495: 29, 496: 29, 497: 29, 498: 29, 499: 29, 500: 29, 501: 29, 502: 29, 503: 29, 504: 29, 505: 29, 506: 29, 507: 29, 508: 29, 509: 29, 510: 29, 511: 29, 512: 29, 513: 29, 514: 29, 515: 29, 516: 29, 517: 29, 518: 29, 519: 29, 520: 29, 521: 29, 522: 29, 523: 29, 524: 29, 525: 29, 526: 29, 527: 29, 528: 29, 529: 29, 530: 29, 531: 29, 532: 29, 533: 29, 534: 29, 535: 29, 536: 29, 537: 29, 538: 29, 539: 29, 540: 29, 541: 29, 542: 29, 543: 29, 544: 29, 545: 29, 546: 29, 547: 29, 548: 29, 549: 29, 550: 29, 551: 29, 552: 29, 553: 29, 554: 29, 555: 29, 556: 29, 557: 29, 558: 29, 559: 29, 560: 29, 561: 29, 562: 29, 563: 29, 564: 29, 565: 29, 566: 29, 567: 29, 568: 29, 569: 29, 570: 29, 571: 29, 572: 29, 573: 29, 574: 29, 575: 29, 576: 29, 577: 29, 578: 29, 579: 29, 580: 29, 581: 29, 582: 29, 583: 29, 584: 29, 585: 29, 586: 29, 587: 29, 588: 29, 589: 29, 590: 29, 591: 29, 592: 29, 593: 29, 594: 29, 595: 29, 596: 29, 597: 29, 598: 29, 599: 29, 600: 29, 601: 29, 602: 29, 603: 29, 604: 29, 605: 29, 606: 29, 607: 29, 608: 29, 609: 29, 610: 29, 611: 29, 612: 29, 613: 29, 614: 29, 615: 29, 616: 29, 617: 29, 618: 29, 619: 29, 620: 29, 621: 29, 622: 29, 623: 29, 624: 29, 625: 29, 626: 29, 627: 29, 628: 29, 629: 29, 630: 29, 631: 29, 632: 29, 633: 29, 634: 29, 635: 29, 636: 29, 637: 29, 638: 29, 639: 29, 640: 29, 641: 29, 642: 29, 643: 29, 644: 29, 645: 29, 646: 29, 647: 29, 648: 29, 649: 29, 650: 29, 651: 29, 652: 29, 653: 29, 654: 29, 655: 29, 656: 29, 657: 29, 658: 29, 659: 29, 660: 29, 661: 29, 662: 29, 663: 29, 664: 29, 665: 29, 666: 29, 667: 29, 668: 29, 669: 29, 670: 29, 671: 29, 672: 29, 673: 29, 674: 29, 675: 29, 676: 29, 677: 29, 678: 29, 679: 29, 680: 29, 681: 29, 682: 29, 683: 29, 684: 29, 685: 29, 686: 29, 687: 29, 688: 29, 689: 29, 690: 29, 691: 29, 692: 29, 693: 29, 694: 29, 695: 29, 696: 29, 697: 29, 698: 29, 699: 29, 700: 29, 701: 29, 702: 29, 703: 29, 704: 29, 705: 29, 706: 29, 707: 29, 708: 29, 709: 29, 710: 29, 711: 29, 712: 29, 713: 29, 714: 29, 715: 29, 716: 29, 717: 29, 718: 29, 719: 29, 720: 29, 721: 29, 722: 29, 723: 29, 724: 29, 725: 29, 726: 29, 727: 29, 728: 29, 729: 29, 730: 29, 731: 29, 732: 29, 733: 29, 734: 29, 735: 29, 736: 29, 737: 29, 738: 29, 739: 29, 740: 29, 741: 29, 742: 29, 743: 29, 744: 29, 745: 29, 746: 29, 747: 29, 748: 29, 749: 29, 750: 29, 751: 29, 752: 29, 753: 29})
STEP-2	Epoch: 20/200	classification_loss: 0.9147	gate_loss: 0.9907	step2_classification_accuracy: 73.8864	step_2_gate_accuracy: 70.0997
STEP-2	Epoch: 40/200	classification_loss: 0.6763	gate_loss: 0.6084	step2_classification_accuracy: 80.1656	step_2_gate_accuracy: 80.2021
STEP-2	Epoch: 60/200	classification_loss: 0.5699	gate_loss: 0.4793	step2_classification_accuracy: 82.7723	step_2_gate_accuracy: 83.9294
STEP-2	Epoch: 80/200	classification_loss: 0.5032	gate_loss: 0.4059	step2_classification_accuracy: 84.4919	step_2_gate_accuracy: 86.0926
STEP-2	Epoch: 100/200	classification_loss: 0.4623	gate_loss: 0.3608	step2_classification_accuracy: 85.5529	step_2_gate_accuracy: 87.4325
STEP-2	Epoch: 120/200	classification_loss: 0.4232	gate_loss: 0.3215	step2_classification_accuracy: 86.1200	step_2_gate_accuracy: 88.6856
STEP-2	Epoch: 140/200	classification_loss: 0.3846	gate_loss: 0.2886	step2_classification_accuracy: 87.5011	step_2_gate_accuracy: 89.8244
STEP-2	Epoch: 160/200	classification_loss: 0.3806	gate_loss: 0.2810	step2_classification_accuracy: 87.4829	step_2_gate_accuracy: 90.1994
STEP-2	Epoch: 180/200	classification_loss: 0.3582	gate_loss: 0.2636	step2_classification_accuracy: 88.0728	step_2_gate_accuracy: 90.5104
STEP-2	Epoch: 200/200	classification_loss: 0.3456	gate_loss: 0.2498	step2_classification_accuracy: 88.4615	step_2_gate_accuracy: 90.9997
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.0647	gate_accuracy: 79.1367
	Task-1	val_accuracy: 53.2468	gate_accuracy: 66.2338
	Task-2	val_accuracy: 60.6383	gate_accuracy: 58.5106
	Task-3	val_accuracy: 65.9574	gate_accuracy: 65.9574
	Task-4	val_accuracy: 70.8861	gate_accuracy: 64.5570
	Task-5	val_accuracy: 75.7143	gate_accuracy: 74.2857
	Task-6	val_accuracy: 76.6234	gate_accuracy: 74.0260
	Task-7	val_accuracy: 53.1646	gate_accuracy: 51.8987
	Task-8	val_accuracy: 67.0732	gate_accuracy: 69.5122
	Task-9	val_accuracy: 57.8947	gate_accuracy: 60.5263
	Task-10	val_accuracy: 90.3614	gate_accuracy: 81.9277
	Task-11	val_accuracy: 78.2051	gate_accuracy: 70.5128
	Task-12	val_accuracy: 71.2644	gate_accuracy: 67.8161
	Task-13	val_accuracy: 78.2609	gate_accuracy: 79.3478
	Task-14	val_accuracy: 64.7059	gate_accuracy: 63.2353
	Task-15	val_accuracy: 65.6716	gate_accuracy: 65.6716
	Task-16	val_accuracy: 68.6047	gate_accuracy: 65.1163
	Task-17	val_accuracy: 73.7500	gate_accuracy: 68.7500
	Task-18	val_accuracy: 84.8837	gate_accuracy: 82.5581
	Task-19	val_accuracy: 54.5455	gate_accuracy: 50.6494
	Task-20	val_accuracy: 80.4878	gate_accuracy: 78.0488
	Task-21	val_accuracy: 56.6667	gate_accuracy: 48.8889
	Task-22	val_accuracy: 65.8824	gate_accuracy: 61.1765
	Task-23	val_accuracy: 71.2500	gate_accuracy: 70.0000
	Task-24	val_accuracy: 54.9296	gate_accuracy: 57.7465
	Task-25	val_accuracy: 77.3810	gate_accuracy: 73.8095
	Task-26	val_accuracy: 64.5570	gate_accuracy: 62.0253
	Task-27	val_accuracy: 57.6923	gate_accuracy: 62.8205
	Task-28	val_accuracy: 49.3827	gate_accuracy: 58.0247
	Task-29	val_accuracy: 45.0704	gate_accuracy: 53.5211
	Task-30	val_accuracy: 88.2353	gate_accuracy: 88.2353
	Task-31	val_accuracy: 73.8636	gate_accuracy: 77.2727
	Task-32	val_accuracy: 66.3043	gate_accuracy: 66.3043
	Task-33	val_accuracy: 71.5909	gate_accuracy: 76.1364
	Task-34	val_accuracy: 60.4396	gate_accuracy: 67.0330
	Task-35	val_accuracy: 68.8172	gate_accuracy: 73.1183
	Task-36	val_accuracy: 60.2941	gate_accuracy: 69.1176
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.9412


[754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]
Polling GMM for: {754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773}
STEP-1	Epoch: 10/50	loss: 2.7035	step1_train_accuracy: 52.5223
STEP-1	Epoch: 20/50	loss: 1.0214	step1_train_accuracy: 75.6677
STEP-1	Epoch: 30/50	loss: 0.5148	step1_train_accuracy: 96.4392
STEP-1	Epoch: 40/50	loss: 0.3061	step1_train_accuracy: 98.8131
STEP-1	Epoch: 50/50	loss: 0.1989	step1_train_accuracy: 98.8131
FINISH STEP 1
Task-38	STARTING STEP 2
CLASS COUNTER: Counter({0: 32, 1: 32, 2: 32, 3: 32, 4: 32, 5: 32, 6: 32, 7: 32, 8: 32, 9: 32, 10: 32, 11: 32, 12: 32, 13: 32, 14: 32, 15: 32, 16: 32, 17: 32, 18: 32, 19: 32, 20: 32, 21: 32, 22: 32, 23: 32, 24: 32, 25: 32, 26: 32, 27: 32, 28: 32, 29: 32, 30: 32, 31: 32, 32: 32, 33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 48: 32, 49: 32, 50: 32, 51: 32, 52: 32, 53: 32, 54: 32, 55: 32, 56: 32, 57: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 65: 32, 66: 32, 67: 32, 68: 32, 69: 32, 70: 32, 71: 32, 72: 32, 73: 32, 74: 32, 75: 32, 76: 32, 77: 32, 78: 32, 79: 32, 80: 32, 81: 32, 82: 32, 83: 32, 84: 32, 85: 32, 86: 32, 87: 32, 88: 32, 89: 32, 90: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 97: 32, 98: 32, 99: 32, 100: 32, 101: 32, 102: 32, 103: 32, 104: 32, 105: 32, 106: 32, 107: 32, 108: 32, 109: 32, 110: 32, 111: 32, 112: 32, 113: 32, 114: 32, 115: 32, 116: 32, 117: 32, 118: 32, 119: 32, 120: 32, 121: 32, 122: 32, 123: 32, 124: 32, 125: 32, 126: 32, 127: 32, 128: 32, 129: 32, 130: 32, 131: 32, 132: 32, 133: 32, 134: 32, 135: 32, 136: 32, 137: 32, 138: 32, 139: 32, 140: 32, 141: 32, 142: 32, 143: 32, 144: 32, 145: 32, 146: 32, 147: 32, 148: 32, 149: 32, 150: 32, 151: 32, 152: 32, 153: 32, 154: 32, 155: 32, 156: 32, 157: 32, 158: 32, 159: 32, 160: 32, 161: 32, 162: 32, 163: 32, 164: 32, 165: 32, 166: 32, 167: 32, 168: 32, 169: 32, 170: 32, 171: 32, 172: 32, 173: 32, 174: 32, 175: 32, 176: 32, 177: 32, 178: 32, 179: 32, 180: 32, 181: 32, 182: 32, 183: 32, 184: 32, 185: 32, 186: 32, 187: 32, 188: 32, 189: 32, 190: 32, 191: 32, 192: 32, 193: 32, 194: 32, 195: 32, 196: 32, 197: 32, 198: 32, 199: 32, 200: 32, 201: 32, 202: 32, 203: 32, 204: 32, 205: 32, 206: 32, 207: 32, 208: 32, 209: 32, 210: 32, 211: 32, 212: 32, 213: 32, 214: 32, 215: 32, 216: 32, 217: 32, 218: 32, 219: 32, 220: 32, 221: 32, 222: 32, 223: 32, 224: 32, 225: 32, 226: 32, 227: 32, 228: 32, 229: 32, 230: 32, 231: 32, 232: 32, 233: 32, 234: 32, 235: 32, 236: 32, 237: 32, 238: 32, 239: 32, 240: 32, 241: 32, 242: 32, 243: 32, 244: 32, 245: 32, 246: 32, 247: 32, 248: 32, 249: 32, 250: 32, 251: 32, 252: 32, 253: 32, 254: 32, 255: 32, 256: 32, 257: 32, 258: 32, 259: 32, 260: 32, 261: 32, 262: 32, 263: 32, 264: 32, 265: 32, 266: 32, 267: 32, 268: 32, 269: 32, 270: 32, 271: 32, 272: 32, 273: 32, 274: 32, 275: 32, 276: 32, 277: 32, 278: 32, 279: 32, 280: 32, 281: 32, 282: 32, 283: 32, 284: 32, 285: 32, 286: 32, 287: 32, 288: 32, 289: 32, 290: 32, 291: 32, 292: 32, 293: 32, 294: 32, 295: 32, 296: 32, 297: 32, 298: 32, 299: 32, 300: 32, 301: 32, 302: 32, 303: 32, 304: 32, 305: 32, 306: 32, 307: 32, 308: 32, 309: 32, 310: 32, 311: 32, 312: 32, 313: 32, 314: 32, 315: 32, 316: 32, 317: 32, 318: 32, 319: 32, 320: 32, 321: 32, 322: 32, 323: 32, 324: 32, 325: 32, 326: 32, 327: 32, 328: 32, 329: 32, 330: 32, 331: 32, 332: 32, 333: 32, 334: 32, 335: 32, 336: 32, 337: 32, 338: 32, 339: 32, 340: 32, 341: 32, 342: 32, 343: 32, 344: 32, 345: 32, 346: 32, 347: 32, 348: 32, 349: 32, 350: 32, 351: 32, 352: 32, 353: 32, 354: 32, 355: 32, 356: 32, 357: 32, 358: 32, 359: 32, 360: 32, 361: 32, 362: 32, 363: 32, 364: 32, 365: 32, 366: 32, 367: 32, 368: 32, 369: 32, 370: 32, 371: 32, 372: 32, 373: 32, 374: 32, 375: 32, 376: 32, 377: 32, 378: 32, 379: 32, 380: 32, 381: 32, 382: 32, 383: 32, 384: 32, 385: 32, 386: 32, 387: 32, 388: 32, 389: 32, 390: 32, 391: 32, 392: 32, 393: 32, 394: 32, 395: 32, 396: 32, 397: 32, 398: 32, 399: 32, 400: 32, 401: 32, 402: 32, 403: 32, 404: 32, 405: 32, 406: 32, 407: 32, 408: 32, 409: 32, 410: 32, 411: 32, 412: 32, 413: 32, 414: 32, 415: 32, 416: 32, 417: 32, 418: 32, 419: 32, 420: 32, 421: 32, 422: 32, 423: 32, 424: 32, 425: 32, 426: 32, 427: 32, 428: 32, 429: 32, 430: 32, 431: 32, 432: 32, 433: 32, 434: 32, 435: 32, 436: 32, 437: 32, 438: 32, 439: 32, 440: 32, 441: 32, 442: 32, 443: 32, 444: 32, 445: 32, 446: 32, 447: 32, 448: 32, 449: 32, 450: 32, 451: 32, 452: 32, 453: 32, 454: 32, 455: 32, 456: 32, 457: 32, 458: 32, 459: 32, 460: 32, 461: 32, 462: 32, 463: 32, 464: 32, 465: 32, 466: 32, 467: 32, 468: 32, 469: 32, 470: 32, 471: 32, 472: 32, 473: 32, 474: 32, 475: 32, 476: 32, 477: 32, 478: 32, 479: 32, 480: 32, 481: 32, 482: 32, 483: 32, 484: 32, 485: 32, 486: 32, 487: 32, 488: 32, 489: 32, 490: 32, 491: 32, 492: 32, 493: 32, 494: 32, 495: 32, 496: 32, 497: 32, 498: 32, 499: 32, 500: 32, 501: 32, 502: 32, 503: 32, 504: 32, 505: 32, 506: 32, 507: 32, 508: 32, 509: 32, 510: 32, 511: 32, 512: 32, 513: 32, 514: 32, 515: 32, 516: 32, 517: 32, 518: 32, 519: 32, 520: 32, 521: 32, 522: 32, 523: 32, 524: 32, 525: 32, 526: 32, 527: 32, 528: 32, 529: 32, 530: 32, 531: 32, 532: 32, 533: 32, 534: 32, 535: 32, 536: 32, 537: 32, 538: 32, 539: 32, 540: 32, 541: 32, 542: 32, 543: 32, 544: 32, 545: 32, 546: 32, 547: 32, 548: 32, 549: 32, 550: 32, 551: 32, 552: 32, 553: 32, 554: 32, 555: 32, 556: 32, 557: 32, 558: 32, 559: 32, 560: 32, 561: 32, 562: 32, 563: 32, 564: 32, 565: 32, 566: 32, 567: 32, 568: 32, 569: 32, 570: 32, 571: 32, 572: 32, 573: 32, 574: 32, 575: 32, 576: 32, 577: 32, 578: 32, 579: 32, 580: 32, 581: 32, 582: 32, 583: 32, 584: 32, 585: 32, 586: 32, 587: 32, 588: 32, 589: 32, 590: 32, 591: 32, 592: 32, 593: 32, 594: 32, 595: 32, 596: 32, 597: 32, 598: 32, 599: 32, 600: 32, 601: 32, 602: 32, 603: 32, 604: 32, 605: 32, 606: 32, 607: 32, 608: 32, 609: 32, 610: 32, 611: 32, 612: 32, 613: 32, 614: 32, 615: 32, 616: 32, 617: 32, 618: 32, 619: 32, 620: 32, 621: 32, 622: 32, 623: 32, 624: 32, 625: 32, 626: 32, 627: 32, 628: 32, 629: 32, 630: 32, 631: 32, 632: 32, 633: 32, 634: 32, 635: 32, 636: 32, 637: 32, 638: 32, 639: 32, 640: 32, 641: 32, 642: 32, 643: 32, 644: 32, 645: 32, 646: 32, 647: 32, 648: 32, 649: 32, 650: 32, 651: 32, 652: 32, 653: 32, 654: 32, 655: 32, 656: 32, 657: 32, 658: 32, 659: 32, 660: 32, 661: 32, 662: 32, 663: 32, 664: 32, 665: 32, 666: 32, 667: 32, 668: 32, 669: 32, 670: 32, 671: 32, 672: 32, 673: 32, 674: 32, 675: 32, 676: 32, 677: 32, 678: 32, 679: 32, 680: 32, 681: 32, 682: 32, 683: 32, 684: 32, 685: 32, 686: 32, 687: 32, 688: 32, 689: 32, 690: 32, 691: 32, 692: 32, 693: 32, 694: 32, 695: 32, 696: 32, 697: 32, 698: 32, 699: 32, 700: 32, 701: 32, 702: 32, 703: 32, 704: 32, 705: 32, 706: 32, 707: 32, 708: 32, 709: 32, 710: 32, 711: 32, 712: 32, 713: 32, 714: 32, 715: 32, 716: 32, 717: 32, 718: 32, 719: 32, 720: 32, 721: 32, 722: 32, 723: 32, 724: 32, 725: 32, 726: 32, 727: 32, 728: 32, 729: 32, 730: 32, 731: 32, 732: 32, 733: 32, 734: 32, 735: 32, 736: 32, 737: 32, 738: 32, 739: 32, 740: 32, 741: 32, 742: 32, 743: 32, 744: 32, 745: 32, 746: 32, 747: 32, 748: 32, 749: 32, 750: 32, 751: 32, 752: 32, 753: 32, 754: 32, 755: 32, 756: 32, 757: 32, 758: 32, 759: 32, 760: 32, 761: 32, 762: 32, 763: 32, 764: 32, 765: 32, 766: 32, 767: 32, 768: 32, 769: 32, 770: 32, 771: 32, 772: 32, 773: 32})
STEP-2	Epoch: 20/200	classification_loss: 0.8788	gate_loss: 0.9176	step2_classification_accuracy: 74.6205	step_2_gate_accuracy: 71.8629
STEP-2	Epoch: 40/200	classification_loss: 0.6615	gate_loss: 0.5847	step2_classification_accuracy: 80.2285	step_2_gate_accuracy: 80.5757
STEP-2	Epoch: 60/200	classification_loss: 0.5557	gate_loss: 0.4657	step2_classification_accuracy: 82.8246	step_2_gate_accuracy: 83.9188
STEP-2	Epoch: 80/200	classification_loss: 0.4991	gate_loss: 0.4036	step2_classification_accuracy: 84.2539	step_2_gate_accuracy: 85.9415
STEP-2	Epoch: 100/200	classification_loss: 0.4563	gate_loss: 0.3622	step2_classification_accuracy: 85.2390	step_2_gate_accuracy: 87.0922
STEP-2	Epoch: 120/200	classification_loss: 0.4354	gate_loss: 0.3390	step2_classification_accuracy: 85.9173	step_2_gate_accuracy: 87.9037
STEP-2	Epoch: 140/200	classification_loss: 0.4125	gate_loss: 0.3196	step2_classification_accuracy: 86.4503	step_2_gate_accuracy: 88.7395
STEP-2	Epoch: 160/200	classification_loss: 0.3925	gate_loss: 0.2982	step2_classification_accuracy: 87.1164	step_2_gate_accuracy: 89.4259
STEP-2	Epoch: 180/200	classification_loss: 0.3694	gate_loss: 0.2757	step2_classification_accuracy: 87.5969	step_2_gate_accuracy: 90.1001
STEP-2	Epoch: 200/200	classification_loss: 0.3584	gate_loss: 0.2647	step2_classification_accuracy: 87.9078	step_2_gate_accuracy: 90.3706
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 69.0647	gate_accuracy: 76.2590
	Task-1	val_accuracy: 59.7403	gate_accuracy: 72.7273
	Task-2	val_accuracy: 71.2766	gate_accuracy: 65.9574
	Task-3	val_accuracy: 65.9574	gate_accuracy: 69.1489
	Task-4	val_accuracy: 73.4177	gate_accuracy: 77.2152
	Task-5	val_accuracy: 71.4286	gate_accuracy: 65.7143
	Task-6	val_accuracy: 80.5195	gate_accuracy: 77.9221
	Task-7	val_accuracy: 53.1646	gate_accuracy: 49.3671
	Task-8	val_accuracy: 64.6341	gate_accuracy: 60.9756
	Task-9	val_accuracy: 61.8421	gate_accuracy: 55.2632
	Task-10	val_accuracy: 90.3614	gate_accuracy: 84.3373
	Task-11	val_accuracy: 65.3846	gate_accuracy: 55.1282
	Task-12	val_accuracy: 67.8161	gate_accuracy: 62.0690
	Task-13	val_accuracy: 72.8261	gate_accuracy: 64.1304
	Task-14	val_accuracy: 75.0000	gate_accuracy: 66.1765
	Task-15	val_accuracy: 70.1493	gate_accuracy: 67.1642
	Task-16	val_accuracy: 73.2558	gate_accuracy: 72.0930
	Task-17	val_accuracy: 72.5000	gate_accuracy: 66.2500
	Task-18	val_accuracy: 82.5581	gate_accuracy: 75.5814
	Task-19	val_accuracy: 61.0390	gate_accuracy: 53.2468
	Task-20	val_accuracy: 80.4878	gate_accuracy: 79.2683
	Task-21	val_accuracy: 63.3333	gate_accuracy: 58.8889
	Task-22	val_accuracy: 68.2353	gate_accuracy: 67.0588
	Task-23	val_accuracy: 72.5000	gate_accuracy: 76.2500
	Task-24	val_accuracy: 57.7465	gate_accuracy: 61.9718
	Task-25	val_accuracy: 79.7619	gate_accuracy: 80.9524
	Task-26	val_accuracy: 62.0253	gate_accuracy: 64.5570
	Task-27	val_accuracy: 62.8205	gate_accuracy: 66.6667
	Task-28	val_accuracy: 46.9136	gate_accuracy: 51.8519
	Task-29	val_accuracy: 54.9296	gate_accuracy: 61.9718
	Task-30	val_accuracy: 88.2353	gate_accuracy: 89.7059
	Task-31	val_accuracy: 78.4091	gate_accuracy: 84.0909
	Task-32	val_accuracy: 70.6522	gate_accuracy: 73.9130
	Task-33	val_accuracy: 78.4091	gate_accuracy: 77.2727
	Task-34	val_accuracy: 64.8352	gate_accuracy: 69.2308
	Task-35	val_accuracy: 68.8172	gate_accuracy: 73.1183
	Task-36	val_accuracy: 55.8824	gate_accuracy: 73.5294
	Task-37	val_accuracy: 58.3333	gate_accuracy: 60.7143
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 68.8295


[774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]
Polling GMM for: {774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793}
STEP-1	Epoch: 10/50	loss: 1.9392	step1_train_accuracy: 57.0621
STEP-1	Epoch: 20/50	loss: 0.7103	step1_train_accuracy: 88.7006
STEP-1	Epoch: 30/50	loss: 0.4415	step1_train_accuracy: 95.7627
STEP-1	Epoch: 40/50	loss: 0.3122	step1_train_accuracy: 96.0452
STEP-1	Epoch: 50/50	loss: 0.2269	step1_train_accuracy: 97.4576
FINISH STEP 1
Task-39	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9174	gate_loss: 0.9889	step2_classification_accuracy: 73.6288	step_2_gate_accuracy: 70.2365
STEP-2	Epoch: 40/200	classification_loss: 0.6913	gate_loss: 0.6193	step2_classification_accuracy: 79.4467	step_2_gate_accuracy: 80.0154
STEP-2	Epoch: 60/200	classification_loss: 0.5825	gate_loss: 0.4929	step2_classification_accuracy: 82.1809	step_2_gate_accuracy: 83.3956
STEP-2	Epoch: 80/200	classification_loss: 0.5207	gate_loss: 0.4272	step2_classification_accuracy: 83.7125	step_2_gate_accuracy: 85.1223
STEP-2	Epoch: 100/200	classification_loss: 0.4718	gate_loss: 0.3743	step2_classification_accuracy: 85.0532	step_2_gate_accuracy: 86.8286
STEP-2	Epoch: 120/200	classification_loss: 0.4404	gate_loss: 0.3427	step2_classification_accuracy: 85.9348	step_2_gate_accuracy: 87.8606
STEP-2	Epoch: 140/200	classification_loss: 0.4201	gate_loss: 0.3241	step2_classification_accuracy: 86.3127	step_2_gate_accuracy: 88.4334
STEP-2	Epoch: 160/200	classification_loss: 0.3985	gate_loss: 0.3005	step2_classification_accuracy: 86.9708	step_2_gate_accuracy: 89.3760
STEP-2	Epoch: 180/200	classification_loss: 0.3878	gate_loss: 0.2926	step2_classification_accuracy: 87.4177	step_2_gate_accuracy: 89.5019
STEP-2	Epoch: 200/200	classification_loss: 0.3668	gate_loss: 0.2697	step2_classification_accuracy: 87.8240	step_2_gate_accuracy: 90.4810
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 74.1007	gate_accuracy: 81.2950
	Task-1	val_accuracy: 45.4545	gate_accuracy: 54.5455
	Task-2	val_accuracy: 73.4043	gate_accuracy: 65.9574
	Task-3	val_accuracy: 61.7021	gate_accuracy: 61.7021
	Task-4	val_accuracy: 70.8861	gate_accuracy: 72.1519
	Task-5	val_accuracy: 74.2857	gate_accuracy: 68.5714
	Task-6	val_accuracy: 75.3247	gate_accuracy: 67.5325
	Task-7	val_accuracy: 55.6962	gate_accuracy: 50.6329
	Task-8	val_accuracy: 59.7561	gate_accuracy: 63.4146
	Task-9	val_accuracy: 56.5789	gate_accuracy: 57.8947
	Task-10	val_accuracy: 84.3373	gate_accuracy: 80.7229
	Task-11	val_accuracy: 61.5385	gate_accuracy: 57.6923
	Task-12	val_accuracy: 66.6667	gate_accuracy: 64.3678
	Task-13	val_accuracy: 84.7826	gate_accuracy: 73.9130
	Task-14	val_accuracy: 69.1176	gate_accuracy: 67.6471
	Task-15	val_accuracy: 68.6567	gate_accuracy: 62.6866
	Task-16	val_accuracy: 63.9535	gate_accuracy: 61.6279
	Task-17	val_accuracy: 70.0000	gate_accuracy: 67.5000
	Task-18	val_accuracy: 82.5581	gate_accuracy: 76.7442
	Task-19	val_accuracy: 55.8442	gate_accuracy: 53.2468
	Task-20	val_accuracy: 74.3902	gate_accuracy: 65.8537
	Task-21	val_accuracy: 62.2222	gate_accuracy: 53.3333
	Task-22	val_accuracy: 70.5882	gate_accuracy: 65.8824
	Task-23	val_accuracy: 68.7500	gate_accuracy: 72.5000
	Task-24	val_accuracy: 50.7042	gate_accuracy: 52.1127
	Task-25	val_accuracy: 76.1905	gate_accuracy: 73.8095
	Task-26	val_accuracy: 64.5570	gate_accuracy: 65.8228
	Task-27	val_accuracy: 57.6923	gate_accuracy: 61.5385
	Task-28	val_accuracy: 49.3827	gate_accuracy: 55.5556
	Task-29	val_accuracy: 54.9296	gate_accuracy: 63.3803
	Task-30	val_accuracy: 82.3529	gate_accuracy: 82.3529
	Task-31	val_accuracy: 81.8182	gate_accuracy: 81.8182
	Task-32	val_accuracy: 79.3478	gate_accuracy: 76.0870
	Task-33	val_accuracy: 76.1364	gate_accuracy: 80.6818
	Task-34	val_accuracy: 54.9451	gate_accuracy: 60.4396
	Task-35	val_accuracy: 73.1183	gate_accuracy: 75.2688
	Task-36	val_accuracy: 66.1765	gate_accuracy: 75.0000
	Task-37	val_accuracy: 52.3810	gate_accuracy: 60.7143
	Task-38	val_accuracy: 75.0000	gate_accuracy: 77.2727
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.2958


[794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]
Polling GMM for: {794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813}
STEP-1	Epoch: 10/50	loss: 2.7089	step1_train_accuracy: 56.5868
STEP-1	Epoch: 20/50	loss: 0.9427	step1_train_accuracy: 76.9461
STEP-1	Epoch: 30/50	loss: 0.3823	step1_train_accuracy: 94.0120
STEP-1	Epoch: 40/50	loss: 0.2417	step1_train_accuracy: 96.7066
STEP-1	Epoch: 50/50	loss: 0.1791	step1_train_accuracy: 98.5030
FINISH STEP 1
Task-40	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33, 714: 33, 715: 33, 716: 33, 717: 33, 718: 33, 719: 33, 720: 33, 721: 33, 722: 33, 723: 33, 724: 33, 725: 33, 726: 33, 727: 33, 728: 33, 729: 33, 730: 33, 731: 33, 732: 33, 733: 33, 734: 33, 735: 33, 736: 33, 737: 33, 738: 33, 739: 33, 740: 33, 741: 33, 742: 33, 743: 33, 744: 33, 745: 33, 746: 33, 747: 33, 748: 33, 749: 33, 750: 33, 751: 33, 752: 33, 753: 33, 754: 33, 755: 33, 756: 33, 757: 33, 758: 33, 759: 33, 760: 33, 761: 33, 762: 33, 763: 33, 764: 33, 765: 33, 766: 33, 767: 33, 768: 33, 769: 33, 770: 33, 771: 33, 772: 33, 773: 33, 774: 33, 775: 33, 776: 33, 777: 33, 778: 33, 779: 33, 780: 33, 781: 33, 782: 33, 783: 33, 784: 33, 785: 33, 786: 33, 787: 33, 788: 33, 789: 33, 790: 33, 791: 33, 792: 33, 793: 33, 794: 33, 795: 33, 796: 33, 797: 33, 798: 33, 799: 33, 800: 33, 801: 33, 802: 33, 803: 33, 804: 33, 805: 33, 806: 33, 807: 33, 808: 33, 809: 33, 810: 33, 811: 33, 812: 33, 813: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.9317	gate_loss: 0.9539	step2_classification_accuracy: 73.3564	step_2_gate_accuracy: 70.5309
STEP-2	Epoch: 40/200	classification_loss: 0.7131	gate_loss: 0.6315	step2_classification_accuracy: 79.0075	step_2_gate_accuracy: 79.3091
STEP-2	Epoch: 60/200	classification_loss: 0.6130	gate_loss: 0.5150	step2_classification_accuracy: 81.5427	step_2_gate_accuracy: 82.6521
STEP-2	Epoch: 80/200	classification_loss: 0.5462	gate_loss: 0.4443	step2_classification_accuracy: 83.0169	step_2_gate_accuracy: 84.4874
STEP-2	Epoch: 100/200	classification_loss: 0.5108	gate_loss: 0.4055	step2_classification_accuracy: 84.2454	step_2_gate_accuracy: 85.9541
STEP-2	Epoch: 120/200	classification_loss: 0.4725	gate_loss: 0.3708	step2_classification_accuracy: 85.0272	step_2_gate_accuracy: 86.9407
STEP-2	Epoch: 140/200	classification_loss: 0.5183	gate_loss: 0.3974	step2_classification_accuracy: 84.1970	step_2_gate_accuracy: 85.8946
STEP-2	Epoch: 160/200	classification_loss: 0.4213	gate_loss: 0.3190	step2_classification_accuracy: 86.3376	step_2_gate_accuracy: 88.9472
STEP-2	Epoch: 180/200	classification_loss: 0.4069	gate_loss: 0.3089	step2_classification_accuracy: 86.7322	step_2_gate_accuracy: 89.0552
STEP-2	Epoch: 200/200	classification_loss: 0.5105	gate_loss: 0.4170	step2_classification_accuracy: 84.0481	step_2_gate_accuracy: 85.3883
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 59.7122	gate_accuracy: 76.9784
	Task-1	val_accuracy: 53.2468	gate_accuracy: 64.9351
	Task-2	val_accuracy: 72.3404	gate_accuracy: 69.1489
	Task-3	val_accuracy: 62.7660	gate_accuracy: 64.8936
	Task-4	val_accuracy: 73.4177	gate_accuracy: 75.9494
	Task-5	val_accuracy: 80.0000	gate_accuracy: 80.0000
	Task-6	val_accuracy: 75.3247	gate_accuracy: 66.2338
	Task-7	val_accuracy: 56.9620	gate_accuracy: 44.3038
	Task-8	val_accuracy: 59.7561	gate_accuracy: 60.9756
	Task-9	val_accuracy: 61.8421	gate_accuracy: 53.9474
	Task-10	val_accuracy: 86.7470	gate_accuracy: 84.3373
	Task-11	val_accuracy: 75.6410	gate_accuracy: 66.6667
	Task-12	val_accuracy: 64.3678	gate_accuracy: 52.8736
	Task-13	val_accuracy: 76.0870	gate_accuracy: 75.0000
	Task-14	val_accuracy: 70.5882	gate_accuracy: 66.1765
	Task-15	val_accuracy: 74.6269	gate_accuracy: 68.6567
	Task-16	val_accuracy: 76.7442	gate_accuracy: 73.2558
	Task-17	val_accuracy: 75.0000	gate_accuracy: 67.5000
	Task-18	val_accuracy: 76.7442	gate_accuracy: 76.7442
	Task-19	val_accuracy: 55.8442	gate_accuracy: 53.2468
	Task-20	val_accuracy: 80.4878	gate_accuracy: 80.4878
	Task-21	val_accuracy: 60.0000	gate_accuracy: 60.0000
	Task-22	val_accuracy: 65.8824	gate_accuracy: 60.0000
	Task-23	val_accuracy: 72.5000	gate_accuracy: 72.5000
	Task-24	val_accuracy: 49.2958	gate_accuracy: 56.3380
	Task-25	val_accuracy: 70.2381	gate_accuracy: 63.0952
	Task-26	val_accuracy: 67.0886	gate_accuracy: 63.2911
	Task-27	val_accuracy: 67.9487	gate_accuracy: 66.6667
	Task-28	val_accuracy: 50.6173	gate_accuracy: 53.0864
	Task-29	val_accuracy: 45.0704	gate_accuracy: 57.7465
	Task-30	val_accuracy: 88.2353	gate_accuracy: 83.8235
	Task-31	val_accuracy: 79.5455	gate_accuracy: 82.9545
	Task-32	val_accuracy: 71.7391	gate_accuracy: 63.0435
	Task-33	val_accuracy: 75.0000	gate_accuracy: 76.1364
	Task-34	val_accuracy: 48.3516	gate_accuracy: 50.5495
	Task-35	val_accuracy: 73.1183	gate_accuracy: 75.2688
	Task-36	val_accuracy: 51.4706	gate_accuracy: 66.1765
	Task-37	val_accuracy: 58.3333	gate_accuracy: 63.0952
	Task-38	val_accuracy: 70.4545	gate_accuracy: 78.4091
	Task-39	val_accuracy: 68.6747	gate_accuracy: 69.8795
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.3303


[814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]
Polling GMM for: {814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833}
STEP-1	Epoch: 10/50	loss: 2.3742	step1_train_accuracy: 50.8251
STEP-1	Epoch: 20/50	loss: 0.9466	step1_train_accuracy: 82.1782
STEP-1	Epoch: 30/50	loss: 0.5315	step1_train_accuracy: 93.0693
STEP-1	Epoch: 40/50	loss: 0.3685	step1_train_accuracy: 94.0594
STEP-1	Epoch: 50/50	loss: 0.2750	step1_train_accuracy: 96.3696
FINISH STEP 1
Task-41	STARTING STEP 2
CLASS COUNTER: Counter({0: 31, 1: 31, 2: 31, 3: 31, 4: 31, 5: 31, 6: 31, 7: 31, 8: 31, 9: 31, 10: 31, 11: 31, 12: 31, 13: 31, 14: 31, 15: 31, 16: 31, 17: 31, 18: 31, 19: 31, 20: 31, 21: 31, 22: 31, 23: 31, 24: 31, 25: 31, 26: 31, 27: 31, 28: 31, 29: 31, 30: 31, 31: 31, 32: 31, 33: 31, 34: 31, 35: 31, 36: 31, 37: 31, 38: 31, 39: 31, 40: 31, 41: 31, 42: 31, 43: 31, 44: 31, 45: 31, 46: 31, 47: 31, 48: 31, 49: 31, 50: 31, 51: 31, 52: 31, 53: 31, 54: 31, 55: 31, 56: 31, 57: 31, 58: 31, 59: 31, 60: 31, 61: 31, 62: 31, 63: 31, 64: 31, 65: 31, 66: 31, 67: 31, 68: 31, 69: 31, 70: 31, 71: 31, 72: 31, 73: 31, 74: 31, 75: 31, 76: 31, 77: 31, 78: 31, 79: 31, 80: 31, 81: 31, 82: 31, 83: 31, 84: 31, 85: 31, 86: 31, 87: 31, 88: 31, 89: 31, 90: 31, 91: 31, 92: 31, 93: 31, 94: 31, 95: 31, 96: 31, 97: 31, 98: 31, 99: 31, 100: 31, 101: 31, 102: 31, 103: 31, 104: 31, 105: 31, 106: 31, 107: 31, 108: 31, 109: 31, 110: 31, 111: 31, 112: 31, 113: 31, 114: 31, 115: 31, 116: 31, 117: 31, 118: 31, 119: 31, 120: 31, 121: 31, 122: 31, 123: 31, 124: 31, 125: 31, 126: 31, 127: 31, 128: 31, 129: 31, 130: 31, 131: 31, 132: 31, 133: 31, 134: 31, 135: 31, 136: 31, 137: 31, 138: 31, 139: 31, 140: 31, 141: 31, 142: 31, 143: 31, 144: 31, 145: 31, 146: 31, 147: 31, 148: 31, 149: 31, 150: 31, 151: 31, 152: 31, 153: 31, 154: 31, 155: 31, 156: 31, 157: 31, 158: 31, 159: 31, 160: 31, 161: 31, 162: 31, 163: 31, 164: 31, 165: 31, 166: 31, 167: 31, 168: 31, 169: 31, 170: 31, 171: 31, 172: 31, 173: 31, 174: 31, 175: 31, 176: 31, 177: 31, 178: 31, 179: 31, 180: 31, 181: 31, 182: 31, 183: 31, 184: 31, 185: 31, 186: 31, 187: 31, 188: 31, 189: 31, 190: 31, 191: 31, 192: 31, 193: 31, 194: 31, 195: 31, 196: 31, 197: 31, 198: 31, 199: 31, 200: 31, 201: 31, 202: 31, 203: 31, 204: 31, 205: 31, 206: 31, 207: 31, 208: 31, 209: 31, 210: 31, 211: 31, 212: 31, 213: 31, 214: 31, 215: 31, 216: 31, 217: 31, 218: 31, 219: 31, 220: 31, 221: 31, 222: 31, 223: 31, 224: 31, 225: 31, 226: 31, 227: 31, 228: 31, 229: 31, 230: 31, 231: 31, 232: 31, 233: 31, 234: 31, 235: 31, 236: 31, 237: 31, 238: 31, 239: 31, 240: 31, 241: 31, 242: 31, 243: 31, 244: 31, 245: 31, 246: 31, 247: 31, 248: 31, 249: 31, 250: 31, 251: 31, 252: 31, 253: 31, 254: 31, 255: 31, 256: 31, 257: 31, 258: 31, 259: 31, 260: 31, 261: 31, 262: 31, 263: 31, 264: 31, 265: 31, 266: 31, 267: 31, 268: 31, 269: 31, 270: 31, 271: 31, 272: 31, 273: 31, 274: 31, 275: 31, 276: 31, 277: 31, 278: 31, 279: 31, 280: 31, 281: 31, 282: 31, 283: 31, 284: 31, 285: 31, 286: 31, 287: 31, 288: 31, 289: 31, 290: 31, 291: 31, 292: 31, 293: 31, 294: 31, 295: 31, 296: 31, 297: 31, 298: 31, 299: 31, 300: 31, 301: 31, 302: 31, 303: 31, 304: 31, 305: 31, 306: 31, 307: 31, 308: 31, 309: 31, 310: 31, 311: 31, 312: 31, 313: 31, 314: 31, 315: 31, 316: 31, 317: 31, 318: 31, 319: 31, 320: 31, 321: 31, 322: 31, 323: 31, 324: 31, 325: 31, 326: 31, 327: 31, 328: 31, 329: 31, 330: 31, 331: 31, 332: 31, 333: 31, 334: 31, 335: 31, 336: 31, 337: 31, 338: 31, 339: 31, 340: 31, 341: 31, 342: 31, 343: 31, 344: 31, 345: 31, 346: 31, 347: 31, 348: 31, 349: 31, 350: 31, 351: 31, 352: 31, 353: 31, 354: 31, 355: 31, 356: 31, 357: 31, 358: 31, 359: 31, 360: 31, 361: 31, 362: 31, 363: 31, 364: 31, 365: 31, 366: 31, 367: 31, 368: 31, 369: 31, 370: 31, 371: 31, 372: 31, 373: 31, 374: 31, 375: 31, 376: 31, 377: 31, 378: 31, 379: 31, 380: 31, 381: 31, 382: 31, 383: 31, 384: 31, 385: 31, 386: 31, 387: 31, 388: 31, 389: 31, 390: 31, 391: 31, 392: 31, 393: 31, 394: 31, 395: 31, 396: 31, 397: 31, 398: 31, 399: 31, 400: 31, 401: 31, 402: 31, 403: 31, 404: 31, 405: 31, 406: 31, 407: 31, 408: 31, 409: 31, 410: 31, 411: 31, 412: 31, 413: 31, 414: 31, 415: 31, 416: 31, 417: 31, 418: 31, 419: 31, 420: 31, 421: 31, 422: 31, 423: 31, 424: 31, 425: 31, 426: 31, 427: 31, 428: 31, 429: 31, 430: 31, 431: 31, 432: 31, 433: 31, 434: 31, 435: 31, 436: 31, 437: 31, 438: 31, 439: 31, 440: 31, 441: 31, 442: 31, 443: 31, 444: 31, 445: 31, 446: 31, 447: 31, 448: 31, 449: 31, 450: 31, 451: 31, 452: 31, 453: 31, 454: 31, 455: 31, 456: 31, 457: 31, 458: 31, 459: 31, 460: 31, 461: 31, 462: 31, 463: 31, 464: 31, 465: 31, 466: 31, 467: 31, 468: 31, 469: 31, 470: 31, 471: 31, 472: 31, 473: 31, 474: 31, 475: 31, 476: 31, 477: 31, 478: 31, 479: 31, 480: 31, 481: 31, 482: 31, 483: 31, 484: 31, 485: 31, 486: 31, 487: 31, 488: 31, 489: 31, 490: 31, 491: 31, 492: 31, 493: 31, 494: 31, 495: 31, 496: 31, 497: 31, 498: 31, 499: 31, 500: 31, 501: 31, 502: 31, 503: 31, 504: 31, 505: 31, 506: 31, 507: 31, 508: 31, 509: 31, 510: 31, 511: 31, 512: 31, 513: 31, 514: 31, 515: 31, 516: 31, 517: 31, 518: 31, 519: 31, 520: 31, 521: 31, 522: 31, 523: 31, 524: 31, 525: 31, 526: 31, 527: 31, 528: 31, 529: 31, 530: 31, 531: 31, 532: 31, 533: 31, 534: 31, 535: 31, 536: 31, 537: 31, 538: 31, 539: 31, 540: 31, 541: 31, 542: 31, 543: 31, 544: 31, 545: 31, 546: 31, 547: 31, 548: 31, 549: 31, 550: 31, 551: 31, 552: 31, 553: 31, 554: 31, 555: 31, 556: 31, 557: 31, 558: 31, 559: 31, 560: 31, 561: 31, 562: 31, 563: 31, 564: 31, 565: 31, 566: 31, 567: 31, 568: 31, 569: 31, 570: 31, 571: 31, 572: 31, 573: 31, 574: 31, 575: 31, 576: 31, 577: 31, 578: 31, 579: 31, 580: 31, 581: 31, 582: 31, 583: 31, 584: 31, 585: 31, 586: 31, 587: 31, 588: 31, 589: 31, 590: 31, 591: 31, 592: 31, 593: 31, 594: 31, 595: 31, 596: 31, 597: 31, 598: 31, 599: 31, 600: 31, 601: 31, 602: 31, 603: 31, 604: 31, 605: 31, 606: 31, 607: 31, 608: 31, 609: 31, 610: 31, 611: 31, 612: 31, 613: 31, 614: 31, 615: 31, 616: 31, 617: 31, 618: 31, 619: 31, 620: 31, 621: 31, 622: 31, 623: 31, 624: 31, 625: 31, 626: 31, 627: 31, 628: 31, 629: 31, 630: 31, 631: 31, 632: 31, 633: 31, 634: 31, 635: 31, 636: 31, 637: 31, 638: 31, 639: 31, 640: 31, 641: 31, 642: 31, 643: 31, 644: 31, 645: 31, 646: 31, 647: 31, 648: 31, 649: 31, 650: 31, 651: 31, 652: 31, 653: 31, 654: 31, 655: 31, 656: 31, 657: 31, 658: 31, 659: 31, 660: 31, 661: 31, 662: 31, 663: 31, 664: 31, 665: 31, 666: 31, 667: 31, 668: 31, 669: 31, 670: 31, 671: 31, 672: 31, 673: 31, 674: 31, 675: 31, 676: 31, 677: 31, 678: 31, 679: 31, 680: 31, 681: 31, 682: 31, 683: 31, 684: 31, 685: 31, 686: 31, 687: 31, 688: 31, 689: 31, 690: 31, 691: 31, 692: 31, 693: 31, 694: 31, 695: 31, 696: 31, 697: 31, 698: 31, 699: 31, 700: 31, 701: 31, 702: 31, 703: 31, 704: 31, 705: 31, 706: 31, 707: 31, 708: 31, 709: 31, 710: 31, 711: 31, 712: 31, 713: 31, 714: 31, 715: 31, 716: 31, 717: 31, 718: 31, 719: 31, 720: 31, 721: 31, 722: 31, 723: 31, 724: 31, 725: 31, 726: 31, 727: 31, 728: 31, 729: 31, 730: 31, 731: 31, 732: 31, 733: 31, 734: 31, 735: 31, 736: 31, 737: 31, 738: 31, 739: 31, 740: 31, 741: 31, 742: 31, 743: 31, 744: 31, 745: 31, 746: 31, 747: 31, 748: 31, 749: 31, 750: 31, 751: 31, 752: 31, 753: 31, 754: 31, 755: 31, 756: 31, 757: 31, 758: 31, 759: 31, 760: 31, 761: 31, 762: 31, 763: 31, 764: 31, 765: 31, 766: 31, 767: 31, 768: 31, 769: 31, 770: 31, 771: 31, 772: 31, 773: 31, 774: 31, 775: 31, 776: 31, 777: 31, 778: 31, 779: 31, 780: 31, 781: 31, 782: 31, 783: 31, 784: 31, 785: 31, 786: 31, 787: 31, 788: 31, 789: 31, 790: 31, 791: 31, 792: 31, 793: 31, 794: 31, 795: 31, 796: 31, 797: 31, 798: 31, 799: 31, 800: 31, 801: 31, 802: 31, 803: 31, 804: 31, 805: 31, 806: 31, 807: 31, 808: 31, 809: 31, 810: 31, 811: 31, 812: 31, 813: 31, 814: 31, 815: 31, 816: 31, 817: 31, 818: 31, 819: 31, 820: 31, 821: 31, 822: 31, 823: 31, 824: 31, 825: 31, 826: 31, 827: 31, 828: 31, 829: 31, 830: 31, 831: 31, 832: 31, 833: 31})
STEP-2	Epoch: 20/200	classification_loss: 0.9543	gate_loss: 1.0133	step2_classification_accuracy: 72.7740	step_2_gate_accuracy: 68.8714
STEP-2	Epoch: 40/200	classification_loss: 0.7246	gate_loss: 0.6469	step2_classification_accuracy: 79.1444	step_2_gate_accuracy: 78.8659
STEP-2	Epoch: 60/200	classification_loss: 0.6207	gate_loss: 0.5239	step2_classification_accuracy: 81.5038	step_2_gate_accuracy: 82.4050
STEP-2	Epoch: 80/200	classification_loss: 0.5541	gate_loss: 0.4507	step2_classification_accuracy: 83.1051	step_2_gate_accuracy: 84.6136
STEP-2	Epoch: 100/200	classification_loss: 0.5139	gate_loss: 0.4092	step2_classification_accuracy: 84.2462	step_2_gate_accuracy: 85.7508
STEP-2	Epoch: 120/200	classification_loss: 0.4816	gate_loss: 0.3797	step2_classification_accuracy: 85.0159	step_2_gate_accuracy: 86.8492
STEP-2	Epoch: 140/200	classification_loss: 0.4439	gate_loss: 0.3455	step2_classification_accuracy: 85.8513	step_2_gate_accuracy: 87.7814
STEP-2	Epoch: 160/200	classification_loss: 0.4293	gate_loss: 0.3254	step2_classification_accuracy: 86.4857	step_2_gate_accuracy: 88.4660
STEP-2	Epoch: 180/200	classification_loss: 0.4151	gate_loss: 0.3135	step2_classification_accuracy: 86.7100	step_2_gate_accuracy: 88.8683
STEP-2	Epoch: 200/200	classification_loss: 0.4379	gate_loss: 0.3394	step2_classification_accuracy: 85.8977	step_2_gate_accuracy: 87.8433
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 65.4676	gate_accuracy: 74.8201
	Task-1	val_accuracy: 54.5455	gate_accuracy: 62.3377
	Task-2	val_accuracy: 68.0851	gate_accuracy: 60.6383
	Task-3	val_accuracy: 57.4468	gate_accuracy: 58.5106
	Task-4	val_accuracy: 70.8861	gate_accuracy: 69.6203
	Task-5	val_accuracy: 75.7143	gate_accuracy: 72.8571
	Task-6	val_accuracy: 83.1169	gate_accuracy: 74.0260
	Task-7	val_accuracy: 49.3671	gate_accuracy: 46.8354
	Task-8	val_accuracy: 64.6341	gate_accuracy: 62.1951
	Task-9	val_accuracy: 53.9474	gate_accuracy: 57.8947
	Task-10	val_accuracy: 84.3373	gate_accuracy: 81.9277
	Task-11	val_accuracy: 76.9231	gate_accuracy: 70.5128
	Task-12	val_accuracy: 68.9655	gate_accuracy: 64.3678
	Task-13	val_accuracy: 85.8696	gate_accuracy: 84.7826
	Task-14	val_accuracy: 66.1765	gate_accuracy: 60.2941
	Task-15	val_accuracy: 71.6418	gate_accuracy: 67.1642
	Task-16	val_accuracy: 66.2791	gate_accuracy: 65.1163
	Task-17	val_accuracy: 80.0000	gate_accuracy: 73.7500
	Task-18	val_accuracy: 75.5814	gate_accuracy: 68.6047
	Task-19	val_accuracy: 54.5455	gate_accuracy: 51.9481
	Task-20	val_accuracy: 78.0488	gate_accuracy: 68.2927
	Task-21	val_accuracy: 61.1111	gate_accuracy: 57.7778
	Task-22	val_accuracy: 77.6471	gate_accuracy: 74.1176
	Task-23	val_accuracy: 73.7500	gate_accuracy: 73.7500
	Task-24	val_accuracy: 43.6620	gate_accuracy: 52.1127
	Task-25	val_accuracy: 63.0952	gate_accuracy: 59.5238
	Task-26	val_accuracy: 62.0253	gate_accuracy: 59.4937
	Task-27	val_accuracy: 67.9487	gate_accuracy: 73.0769
	Task-28	val_accuracy: 45.6790	gate_accuracy: 53.0864
	Task-29	val_accuracy: 50.7042	gate_accuracy: 59.1549
	Task-30	val_accuracy: 85.2941	gate_accuracy: 77.9412
	Task-31	val_accuracy: 80.6818	gate_accuracy: 85.2273
	Task-32	val_accuracy: 70.6522	gate_accuracy: 69.5652
	Task-33	val_accuracy: 76.1364	gate_accuracy: 76.1364
	Task-34	val_accuracy: 49.4505	gate_accuracy: 58.2418
	Task-35	val_accuracy: 69.8925	gate_accuracy: 73.1183
	Task-36	val_accuracy: 64.7059	gate_accuracy: 75.0000
	Task-37	val_accuracy: 53.5714	gate_accuracy: 58.3333
	Task-38	val_accuracy: 73.8636	gate_accuracy: 78.4091
	Task-39	val_accuracy: 61.4458	gate_accuracy: 71.0843
	Task-40	val_accuracy: 77.6316	gate_accuracy: 85.5263
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.6791


[834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]
Polling GMM for: {834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853}
STEP-1	Epoch: 10/50	loss: 3.0349	step1_train_accuracy: 40.7534
STEP-1	Epoch: 20/50	loss: 1.1133	step1_train_accuracy: 80.4795
STEP-1	Epoch: 30/50	loss: 0.5572	step1_train_accuracy: 91.7808
STEP-1	Epoch: 40/50	loss: 0.3616	step1_train_accuracy: 93.1507
STEP-1	Epoch: 50/50	loss: 0.2566	step1_train_accuracy: 94.8630
FINISH STEP 1
Task-42	STARTING STEP 2
CLASS COUNTER: Counter({0: 30, 1: 30, 2: 30, 3: 30, 4: 30, 5: 30, 6: 30, 7: 30, 8: 30, 9: 30, 10: 30, 11: 30, 12: 30, 13: 30, 14: 30, 15: 30, 16: 30, 17: 30, 18: 30, 19: 30, 20: 30, 21: 30, 22: 30, 23: 30, 24: 30, 25: 30, 26: 30, 27: 30, 28: 30, 29: 30, 30: 30, 31: 30, 32: 30, 33: 30, 34: 30, 35: 30, 36: 30, 37: 30, 38: 30, 39: 30, 40: 30, 41: 30, 42: 30, 43: 30, 44: 30, 45: 30, 46: 30, 47: 30, 48: 30, 49: 30, 50: 30, 51: 30, 52: 30, 53: 30, 54: 30, 55: 30, 56: 30, 57: 30, 58: 30, 59: 30, 60: 30, 61: 30, 62: 30, 63: 30, 64: 30, 65: 30, 66: 30, 67: 30, 68: 30, 69: 30, 70: 30, 71: 30, 72: 30, 73: 30, 74: 30, 75: 30, 76: 30, 77: 30, 78: 30, 79: 30, 80: 30, 81: 30, 82: 30, 83: 30, 84: 30, 85: 30, 86: 30, 87: 30, 88: 30, 89: 30, 90: 30, 91: 30, 92: 30, 93: 30, 94: 30, 95: 30, 96: 30, 97: 30, 98: 30, 99: 30, 100: 30, 101: 30, 102: 30, 103: 30, 104: 30, 105: 30, 106: 30, 107: 30, 108: 30, 109: 30, 110: 30, 111: 30, 112: 30, 113: 30, 114: 30, 115: 30, 116: 30, 117: 30, 118: 30, 119: 30, 120: 30, 121: 30, 122: 30, 123: 30, 124: 30, 125: 30, 126: 30, 127: 30, 128: 30, 129: 30, 130: 30, 131: 30, 132: 30, 133: 30, 134: 30, 135: 30, 136: 30, 137: 30, 138: 30, 139: 30, 140: 30, 141: 30, 142: 30, 143: 30, 144: 30, 145: 30, 146: 30, 147: 30, 148: 30, 149: 30, 150: 30, 151: 30, 152: 30, 153: 30, 154: 30, 155: 30, 156: 30, 157: 30, 158: 30, 159: 30, 160: 30, 161: 30, 162: 30, 163: 30, 164: 30, 165: 30, 166: 30, 167: 30, 168: 30, 169: 30, 170: 30, 171: 30, 172: 30, 173: 30, 174: 30, 175: 30, 176: 30, 177: 30, 178: 30, 179: 30, 180: 30, 181: 30, 182: 30, 183: 30, 184: 30, 185: 30, 186: 30, 187: 30, 188: 30, 189: 30, 190: 30, 191: 30, 192: 30, 193: 30, 194: 30, 195: 30, 196: 30, 197: 30, 198: 30, 199: 30, 200: 30, 201: 30, 202: 30, 203: 30, 204: 30, 205: 30, 206: 30, 207: 30, 208: 30, 209: 30, 210: 30, 211: 30, 212: 30, 213: 30, 214: 30, 215: 30, 216: 30, 217: 30, 218: 30, 219: 30, 220: 30, 221: 30, 222: 30, 223: 30, 224: 30, 225: 30, 226: 30, 227: 30, 228: 30, 229: 30, 230: 30, 231: 30, 232: 30, 233: 30, 234: 30, 235: 30, 236: 30, 237: 30, 238: 30, 239: 30, 240: 30, 241: 30, 242: 30, 243: 30, 244: 30, 245: 30, 246: 30, 247: 30, 248: 30, 249: 30, 250: 30, 251: 30, 252: 30, 253: 30, 254: 30, 255: 30, 256: 30, 257: 30, 258: 30, 259: 30, 260: 30, 261: 30, 262: 30, 263: 30, 264: 30, 265: 30, 266: 30, 267: 30, 268: 30, 269: 30, 270: 30, 271: 30, 272: 30, 273: 30, 274: 30, 275: 30, 276: 30, 277: 30, 278: 30, 279: 30, 280: 30, 281: 30, 282: 30, 283: 30, 284: 30, 285: 30, 286: 30, 287: 30, 288: 30, 289: 30, 290: 30, 291: 30, 292: 30, 293: 30, 294: 30, 295: 30, 296: 30, 297: 30, 298: 30, 299: 30, 300: 30, 301: 30, 302: 30, 303: 30, 304: 30, 305: 30, 306: 30, 307: 30, 308: 30, 309: 30, 310: 30, 311: 30, 312: 30, 313: 30, 314: 30, 315: 30, 316: 30, 317: 30, 318: 30, 319: 30, 320: 30, 321: 30, 322: 30, 323: 30, 324: 30, 325: 30, 326: 30, 327: 30, 328: 30, 329: 30, 330: 30, 331: 30, 332: 30, 333: 30, 334: 30, 335: 30, 336: 30, 337: 30, 338: 30, 339: 30, 340: 30, 341: 30, 342: 30, 343: 30, 344: 30, 345: 30, 346: 30, 347: 30, 348: 30, 349: 30, 350: 30, 351: 30, 352: 30, 353: 30, 354: 30, 355: 30, 356: 30, 357: 30, 358: 30, 359: 30, 360: 30, 361: 30, 362: 30, 363: 30, 364: 30, 365: 30, 366: 30, 367: 30, 368: 30, 369: 30, 370: 30, 371: 30, 372: 30, 373: 30, 374: 30, 375: 30, 376: 30, 377: 30, 378: 30, 379: 30, 380: 30, 381: 30, 382: 30, 383: 30, 384: 30, 385: 30, 386: 30, 387: 30, 388: 30, 389: 30, 390: 30, 391: 30, 392: 30, 393: 30, 394: 30, 395: 30, 396: 30, 397: 30, 398: 30, 399: 30, 400: 30, 401: 30, 402: 30, 403: 30, 404: 30, 405: 30, 406: 30, 407: 30, 408: 30, 409: 30, 410: 30, 411: 30, 412: 30, 413: 30, 414: 30, 415: 30, 416: 30, 417: 30, 418: 30, 419: 30, 420: 30, 421: 30, 422: 30, 423: 30, 424: 30, 425: 30, 426: 30, 427: 30, 428: 30, 429: 30, 430: 30, 431: 30, 432: 30, 433: 30, 434: 30, 435: 30, 436: 30, 437: 30, 438: 30, 439: 30, 440: 30, 441: 30, 442: 30, 443: 30, 444: 30, 445: 30, 446: 30, 447: 30, 448: 30, 449: 30, 450: 30, 451: 30, 452: 30, 453: 30, 454: 30, 455: 30, 456: 30, 457: 30, 458: 30, 459: 30, 460: 30, 461: 30, 462: 30, 463: 30, 464: 30, 465: 30, 466: 30, 467: 30, 468: 30, 469: 30, 470: 30, 471: 30, 472: 30, 473: 30, 474: 30, 475: 30, 476: 30, 477: 30, 478: 30, 479: 30, 480: 30, 481: 30, 482: 30, 483: 30, 484: 30, 485: 30, 486: 30, 487: 30, 488: 30, 489: 30, 490: 30, 491: 30, 492: 30, 493: 30, 494: 30, 495: 30, 496: 30, 497: 30, 498: 30, 499: 30, 500: 30, 501: 30, 502: 30, 503: 30, 504: 30, 505: 30, 506: 30, 507: 30, 508: 30, 509: 30, 510: 30, 511: 30, 512: 30, 513: 30, 514: 30, 515: 30, 516: 30, 517: 30, 518: 30, 519: 30, 520: 30, 521: 30, 522: 30, 523: 30, 524: 30, 525: 30, 526: 30, 527: 30, 528: 30, 529: 30, 530: 30, 531: 30, 532: 30, 533: 30, 534: 30, 535: 30, 536: 30, 537: 30, 538: 30, 539: 30, 540: 30, 541: 30, 542: 30, 543: 30, 544: 30, 545: 30, 546: 30, 547: 30, 548: 30, 549: 30, 550: 30, 551: 30, 552: 30, 553: 30, 554: 30, 555: 30, 556: 30, 557: 30, 558: 30, 559: 30, 560: 30, 561: 30, 562: 30, 563: 30, 564: 30, 565: 30, 566: 30, 567: 30, 568: 30, 569: 30, 570: 30, 571: 30, 572: 30, 573: 30, 574: 30, 575: 30, 576: 30, 577: 30, 578: 30, 579: 30, 580: 30, 581: 30, 582: 30, 583: 30, 584: 30, 585: 30, 586: 30, 587: 30, 588: 30, 589: 30, 590: 30, 591: 30, 592: 30, 593: 30, 594: 30, 595: 30, 596: 30, 597: 30, 598: 30, 599: 30, 600: 30, 601: 30, 602: 30, 603: 30, 604: 30, 605: 30, 606: 30, 607: 30, 608: 30, 609: 30, 610: 30, 611: 30, 612: 30, 613: 30, 614: 30, 615: 30, 616: 30, 617: 30, 618: 30, 619: 30, 620: 30, 621: 30, 622: 30, 623: 30, 624: 30, 625: 30, 626: 30, 627: 30, 628: 30, 629: 30, 630: 30, 631: 30, 632: 30, 633: 30, 634: 30, 635: 30, 636: 30, 637: 30, 638: 30, 639: 30, 640: 30, 641: 30, 642: 30, 643: 30, 644: 30, 645: 30, 646: 30, 647: 30, 648: 30, 649: 30, 650: 30, 651: 30, 652: 30, 653: 30, 654: 30, 655: 30, 656: 30, 657: 30, 658: 30, 659: 30, 660: 30, 661: 30, 662: 30, 663: 30, 664: 30, 665: 30, 666: 30, 667: 30, 668: 30, 669: 30, 670: 30, 671: 30, 672: 30, 673: 30, 674: 30, 675: 30, 676: 30, 677: 30, 678: 30, 679: 30, 680: 30, 681: 30, 682: 30, 683: 30, 684: 30, 685: 30, 686: 30, 687: 30, 688: 30, 689: 30, 690: 30, 691: 30, 692: 30, 693: 30, 694: 30, 695: 30, 696: 30, 697: 30, 698: 30, 699: 30, 700: 30, 701: 30, 702: 30, 703: 30, 704: 30, 705: 30, 706: 30, 707: 30, 708: 30, 709: 30, 710: 30, 711: 30, 712: 30, 713: 30, 714: 30, 715: 30, 716: 30, 717: 30, 718: 30, 719: 30, 720: 30, 721: 30, 722: 30, 723: 30, 724: 30, 725: 30, 726: 30, 727: 30, 728: 30, 729: 30, 730: 30, 731: 30, 732: 30, 733: 30, 734: 30, 735: 30, 736: 30, 737: 30, 738: 30, 739: 30, 740: 30, 741: 30, 742: 30, 743: 30, 744: 30, 745: 30, 746: 30, 747: 30, 748: 30, 749: 30, 750: 30, 751: 30, 752: 30, 753: 30, 754: 30, 755: 30, 756: 30, 757: 30, 758: 30, 759: 30, 760: 30, 761: 30, 762: 30, 763: 30, 764: 30, 765: 30, 766: 30, 767: 30, 768: 30, 769: 30, 770: 30, 771: 30, 772: 30, 773: 30, 774: 30, 775: 30, 776: 30, 777: 30, 778: 30, 779: 30, 780: 30, 781: 30, 782: 30, 783: 30, 784: 30, 785: 30, 786: 30, 787: 30, 788: 30, 789: 30, 790: 30, 791: 30, 792: 30, 793: 30, 794: 30, 795: 30, 796: 30, 797: 30, 798: 30, 799: 30, 800: 30, 801: 30, 802: 30, 803: 30, 804: 30, 805: 30, 806: 30, 807: 30, 808: 30, 809: 30, 810: 30, 811: 30, 812: 30, 813: 30, 814: 30, 815: 30, 816: 30, 817: 30, 818: 30, 819: 30, 820: 30, 821: 30, 822: 30, 823: 30, 824: 30, 825: 30, 826: 30, 827: 30, 828: 30, 829: 30, 830: 30, 831: 30, 832: 30, 833: 30, 834: 30, 835: 30, 836: 30, 837: 30, 838: 30, 839: 30, 840: 30, 841: 30, 842: 30, 843: 30, 844: 30, 845: 30, 846: 30, 847: 30, 848: 30, 849: 30, 850: 30, 851: 30, 852: 30, 853: 30})
STEP-2	Epoch: 20/200	classification_loss: 0.9942	gate_loss: 1.0486	step2_classification_accuracy: 71.7057	step_2_gate_accuracy: 68.0367
STEP-2	Epoch: 40/200	classification_loss: 0.7650	gate_loss: 0.6837	step2_classification_accuracy: 77.9742	step_2_gate_accuracy: 77.8337
STEP-2	Epoch: 60/200	classification_loss: 0.6505	gate_loss: 0.5516	step2_classification_accuracy: 80.8587	step_2_gate_accuracy: 81.3544
STEP-2	Epoch: 80/200	classification_loss: 0.5788	gate_loss: 0.4725	step2_classification_accuracy: 82.4785	step_2_gate_accuracy: 84.0164
STEP-2	Epoch: 100/200	classification_loss: 0.5327	gate_loss: 0.4257	step2_classification_accuracy: 83.5909	step_2_gate_accuracy: 85.2108
STEP-2	Epoch: 120/200	classification_loss: 0.4931	gate_loss: 0.3865	step2_classification_accuracy: 84.6604	step_2_gate_accuracy: 86.4832
STEP-2	Epoch: 140/200	classification_loss: 0.4676	gate_loss: 0.3631	step2_classification_accuracy: 85.2342	step_2_gate_accuracy: 87.2756
STEP-2	Epoch: 160/200	classification_loss: 0.4451	gate_loss: 0.3395	step2_classification_accuracy: 85.8158	step_2_gate_accuracy: 88.0445
STEP-2	Epoch: 180/200	classification_loss: 0.4326	gate_loss: 0.3290	step2_classification_accuracy: 86.0695	step_2_gate_accuracy: 88.2865
STEP-2	Epoch: 200/200	classification_loss: 0.4185	gate_loss: 0.3138	step2_classification_accuracy: 86.5886	step_2_gate_accuracy: 88.9539
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 66.9065	gate_accuracy: 76.2590
	Task-1	val_accuracy: 58.4416	gate_accuracy: 67.5325
	Task-2	val_accuracy: 60.6383	gate_accuracy: 56.3830
	Task-3	val_accuracy: 64.8936	gate_accuracy: 65.9574
	Task-4	val_accuracy: 73.4177	gate_accuracy: 74.6835
	Task-5	val_accuracy: 72.8571	gate_accuracy: 64.2857
	Task-6	val_accuracy: 79.2208	gate_accuracy: 68.8312
	Task-7	val_accuracy: 55.6962	gate_accuracy: 45.5696
	Task-8	val_accuracy: 53.6585	gate_accuracy: 53.6585
	Task-9	val_accuracy: 56.5789	gate_accuracy: 57.8947
	Task-10	val_accuracy: 84.3373	gate_accuracy: 75.9036
	Task-11	val_accuracy: 67.9487	gate_accuracy: 64.1026
	Task-12	val_accuracy: 70.1149	gate_accuracy: 58.6207
	Task-13	val_accuracy: 79.3478	gate_accuracy: 76.0870
	Task-14	val_accuracy: 73.5294	gate_accuracy: 67.6471
	Task-15	val_accuracy: 65.6716	gate_accuracy: 61.1940
	Task-16	val_accuracy: 67.4419	gate_accuracy: 63.9535
	Task-17	val_accuracy: 72.5000	gate_accuracy: 63.7500
	Task-18	val_accuracy: 76.7442	gate_accuracy: 74.4186
	Task-19	val_accuracy: 57.1429	gate_accuracy: 53.2468
	Task-20	val_accuracy: 71.9512	gate_accuracy: 65.8537
	Task-21	val_accuracy: 61.1111	gate_accuracy: 57.7778
	Task-22	val_accuracy: 67.0588	gate_accuracy: 69.4118
	Task-23	val_accuracy: 72.5000	gate_accuracy: 77.5000
	Task-24	val_accuracy: 52.1127	gate_accuracy: 57.7465
	Task-25	val_accuracy: 77.3810	gate_accuracy: 79.7619
	Task-26	val_accuracy: 62.0253	gate_accuracy: 56.9620
	Task-27	val_accuracy: 62.8205	gate_accuracy: 60.2564
	Task-28	val_accuracy: 50.6173	gate_accuracy: 54.3210
	Task-29	val_accuracy: 49.2958	gate_accuracy: 54.9296
	Task-30	val_accuracy: 85.2941	gate_accuracy: 85.2941
	Task-31	val_accuracy: 73.8636	gate_accuracy: 79.5455
	Task-32	val_accuracy: 65.2174	gate_accuracy: 65.2174
	Task-33	val_accuracy: 76.1364	gate_accuracy: 79.5455
	Task-34	val_accuracy: 58.2418	gate_accuracy: 64.8352
	Task-35	val_accuracy: 62.3656	gate_accuracy: 72.0430
	Task-36	val_accuracy: 64.7059	gate_accuracy: 73.5294
	Task-37	val_accuracy: 53.5714	gate_accuracy: 52.3810
	Task-38	val_accuracy: 80.6818	gate_accuracy: 82.9545
	Task-39	val_accuracy: 65.0602	gate_accuracy: 72.2892
	Task-40	val_accuracy: 68.4211	gate_accuracy: 81.5789
	Task-41	val_accuracy: 61.6438	gate_accuracy: 69.8630
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 66.9746


[854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]
Polling GMM for: {854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873}
STEP-1	Epoch: 10/50	loss: 2.6256	step1_train_accuracy: 46.4832
STEP-1	Epoch: 20/50	loss: 0.9414	step1_train_accuracy: 76.7584
STEP-1	Epoch: 30/50	loss: 0.4944	step1_train_accuracy: 83.1804
STEP-1	Epoch: 40/50	loss: 0.3631	step1_train_accuracy: 87.4618
STEP-1	Epoch: 50/50	loss: 0.2975	step1_train_accuracy: 87.7676
FINISH STEP 1
Task-43	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33, 714: 33, 715: 33, 716: 33, 717: 33, 718: 33, 719: 33, 720: 33, 721: 33, 722: 33, 723: 33, 724: 33, 725: 33, 726: 33, 727: 33, 728: 33, 729: 33, 730: 33, 731: 33, 732: 33, 733: 33, 734: 33, 735: 33, 736: 33, 737: 33, 738: 33, 739: 33, 740: 33, 741: 33, 742: 33, 743: 33, 744: 33, 745: 33, 746: 33, 747: 33, 748: 33, 749: 33, 750: 33, 751: 33, 752: 33, 753: 33, 754: 33, 755: 33, 756: 33, 757: 33, 758: 33, 759: 33, 760: 33, 761: 33, 762: 33, 763: 33, 764: 33, 765: 33, 766: 33, 767: 33, 768: 33, 769: 33, 770: 33, 771: 33, 772: 33, 773: 33, 774: 33, 775: 33, 776: 33, 777: 33, 778: 33, 779: 33, 780: 33, 781: 33, 782: 33, 783: 33, 784: 33, 785: 33, 786: 33, 787: 33, 788: 33, 789: 33, 790: 33, 791: 33, 792: 33, 793: 33, 794: 33, 795: 33, 796: 33, 797: 33, 798: 33, 799: 33, 800: 33, 801: 33, 802: 33, 803: 33, 804: 33, 805: 33, 806: 33, 807: 33, 808: 33, 809: 33, 810: 33, 811: 33, 812: 33, 813: 33, 814: 33, 815: 33, 816: 33, 817: 33, 818: 33, 819: 33, 820: 33, 821: 33, 822: 33, 823: 33, 824: 33, 825: 33, 826: 33, 827: 33, 828: 33, 829: 33, 830: 33, 831: 33, 832: 33, 833: 33, 834: 33, 835: 33, 836: 33, 837: 33, 838: 33, 839: 33, 840: 33, 841: 33, 842: 33, 843: 33, 844: 33, 845: 33, 846: 33, 847: 33, 848: 33, 849: 33, 850: 33, 851: 33, 852: 33, 853: 33, 854: 33, 855: 33, 856: 33, 857: 33, 858: 33, 859: 33, 860: 33, 861: 33, 862: 33, 863: 33, 864: 33, 865: 33, 866: 33, 867: 33, 868: 33, 869: 33, 870: 33, 871: 33, 872: 33, 873: 33})
STEP-2	Epoch: 20/200	classification_loss: 0.9833	gate_loss: 1.0034	step2_classification_accuracy: 72.4880	step_2_gate_accuracy: 69.1838
STEP-2	Epoch: 40/200	classification_loss: 0.7578	gate_loss: 0.6699	step2_classification_accuracy: 78.2054	step_2_gate_accuracy: 78.0459
STEP-2	Epoch: 60/200	classification_loss: 0.6548	gate_loss: 0.5465	step2_classification_accuracy: 80.7642	step_2_gate_accuracy: 81.7766
STEP-2	Epoch: 80/200	classification_loss: 0.5903	gate_loss: 0.4795	step2_classification_accuracy: 82.0817	step_2_gate_accuracy: 83.5518
STEP-2	Epoch: 100/200	classification_loss: 0.5455	gate_loss: 0.4327	step2_classification_accuracy: 83.2189	step_2_gate_accuracy: 85.0635
STEP-2	Epoch: 120/200	classification_loss: 0.5296	gate_loss: 0.4217	step2_classification_accuracy: 83.3853	step_2_gate_accuracy: 85.2576
STEP-2	Epoch: 140/200	classification_loss: 1.7823	gate_loss: 0.9117	step2_classification_accuracy: 75.3138	step_2_gate_accuracy: 74.4609
STEP-2	Epoch: 160/200	classification_loss: 0.4569	gate_loss: 0.3491	step2_classification_accuracy: 85.4968	step_2_gate_accuracy: 87.7193
STEP-2	Epoch: 180/200	classification_loss: 0.4461	gate_loss: 0.3354	step2_classification_accuracy: 85.8332	step_2_gate_accuracy: 88.0452
STEP-2	Epoch: 200/200	classification_loss: 0.4296	gate_loss: 0.3246	step2_classification_accuracy: 86.0447	step_2_gate_accuracy: 88.3919
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 67.6259	gate_accuracy: 74.1007
	Task-1	val_accuracy: 55.8442	gate_accuracy: 66.2338
	Task-2	val_accuracy: 69.1489	gate_accuracy: 64.8936
	Task-3	val_accuracy: 73.4043	gate_accuracy: 70.2128
	Task-4	val_accuracy: 70.8861	gate_accuracy: 73.4177
	Task-5	val_accuracy: 64.2857	gate_accuracy: 64.2857
	Task-6	val_accuracy: 77.9221	gate_accuracy: 58.4416
	Task-7	val_accuracy: 54.4304	gate_accuracy: 48.1013
	Task-8	val_accuracy: 58.5366	gate_accuracy: 58.5366
	Task-9	val_accuracy: 59.2105	gate_accuracy: 60.5263
	Task-10	val_accuracy: 84.3373	gate_accuracy: 74.6988
	Task-11	val_accuracy: 71.7949	gate_accuracy: 62.8205
	Task-12	val_accuracy: 68.9655	gate_accuracy: 58.6207
	Task-13	val_accuracy: 78.2609	gate_accuracy: 72.8261
	Task-14	val_accuracy: 77.9412	gate_accuracy: 73.5294
	Task-15	val_accuracy: 71.6418	gate_accuracy: 65.6716
	Task-16	val_accuracy: 61.6279	gate_accuracy: 58.1395
	Task-17	val_accuracy: 68.7500	gate_accuracy: 62.5000
	Task-18	val_accuracy: 81.3953	gate_accuracy: 74.4186
	Task-19	val_accuracy: 58.4416	gate_accuracy: 59.7403
	Task-20	val_accuracy: 71.9512	gate_accuracy: 65.8537
	Task-21	val_accuracy: 65.5556	gate_accuracy: 64.4444
	Task-22	val_accuracy: 72.9412	gate_accuracy: 67.0588
	Task-23	val_accuracy: 72.5000	gate_accuracy: 71.2500
	Task-24	val_accuracy: 53.5211	gate_accuracy: 54.9296
	Task-25	val_accuracy: 70.2381	gate_accuracy: 75.0000
	Task-26	val_accuracy: 67.0886	gate_accuracy: 60.7595
	Task-27	val_accuracy: 62.8205	gate_accuracy: 64.1026
	Task-28	val_accuracy: 48.1481	gate_accuracy: 54.3210
	Task-29	val_accuracy: 47.8873	gate_accuracy: 54.9296
	Task-30	val_accuracy: 85.2941	gate_accuracy: 82.3529
	Task-31	val_accuracy: 77.2727	gate_accuracy: 79.5455
	Task-32	val_accuracy: 68.4783	gate_accuracy: 67.3913
	Task-33	val_accuracy: 68.1818	gate_accuracy: 70.4545
	Task-34	val_accuracy: 54.9451	gate_accuracy: 65.9341
	Task-35	val_accuracy: 74.1935	gate_accuracy: 78.4946
	Task-36	val_accuracy: 64.7059	gate_accuracy: 73.5294
	Task-37	val_accuracy: 58.3333	gate_accuracy: 63.0952
	Task-38	val_accuracy: 81.8182	gate_accuracy: 79.5455
	Task-39	val_accuracy: 69.8795	gate_accuracy: 75.9036
	Task-40	val_accuracy: 75.0000	gate_accuracy: 86.8421
	Task-41	val_accuracy: 58.9041	gate_accuracy: 64.3836
	Task-42	val_accuracy: 48.7805	gate_accuracy: 67.0732
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 67.3999


[874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]
Polling GMM for: {874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893}
STEP-1	Epoch: 10/50	loss: 2.2834	step1_train_accuracy: 58.5799
STEP-1	Epoch: 20/50	loss: 0.8797	step1_train_accuracy: 81.6568
STEP-1	Epoch: 30/50	loss: 0.5018	step1_train_accuracy: 93.1953
STEP-1	Epoch: 40/50	loss: 0.3464	step1_train_accuracy: 94.6746
STEP-1	Epoch: 50/50	loss: 0.2676	step1_train_accuracy: 95.8580
FINISH STEP 1
Task-44	STARTING STEP 2
CLASS COUNTER: Counter({0: 33, 1: 33, 2: 33, 3: 33, 4: 33, 5: 33, 6: 33, 7: 33, 8: 33, 9: 33, 10: 33, 11: 33, 12: 33, 13: 33, 14: 33, 15: 33, 16: 33, 17: 33, 18: 33, 19: 33, 20: 33, 21: 33, 22: 33, 23: 33, 24: 33, 25: 33, 26: 33, 27: 33, 28: 33, 29: 33, 30: 33, 31: 33, 32: 33, 33: 33, 34: 33, 35: 33, 36: 33, 37: 33, 38: 33, 39: 33, 40: 33, 41: 33, 42: 33, 43: 33, 44: 33, 45: 33, 46: 33, 47: 33, 48: 33, 49: 33, 50: 33, 51: 33, 52: 33, 53: 33, 54: 33, 55: 33, 56: 33, 57: 33, 58: 33, 59: 33, 60: 33, 61: 33, 62: 33, 63: 33, 64: 33, 65: 33, 66: 33, 67: 33, 68: 33, 69: 33, 70: 33, 71: 33, 72: 33, 73: 33, 74: 33, 75: 33, 76: 33, 77: 33, 78: 33, 79: 33, 80: 33, 81: 33, 82: 33, 83: 33, 84: 33, 85: 33, 86: 33, 87: 33, 88: 33, 89: 33, 90: 33, 91: 33, 92: 33, 93: 33, 94: 33, 95: 33, 96: 33, 97: 33, 98: 33, 99: 33, 100: 33, 101: 33, 102: 33, 103: 33, 104: 33, 105: 33, 106: 33, 107: 33, 108: 33, 109: 33, 110: 33, 111: 33, 112: 33, 113: 33, 114: 33, 115: 33, 116: 33, 117: 33, 118: 33, 119: 33, 120: 33, 121: 33, 122: 33, 123: 33, 124: 33, 125: 33, 126: 33, 127: 33, 128: 33, 129: 33, 130: 33, 131: 33, 132: 33, 133: 33, 134: 33, 135: 33, 136: 33, 137: 33, 138: 33, 139: 33, 140: 33, 141: 33, 142: 33, 143: 33, 144: 33, 145: 33, 146: 33, 147: 33, 148: 33, 149: 33, 150: 33, 151: 33, 152: 33, 153: 33, 154: 33, 155: 33, 156: 33, 157: 33, 158: 33, 159: 33, 160: 33, 161: 33, 162: 33, 163: 33, 164: 33, 165: 33, 166: 33, 167: 33, 168: 33, 169: 33, 170: 33, 171: 33, 172: 33, 173: 33, 174: 33, 175: 33, 176: 33, 177: 33, 178: 33, 179: 33, 180: 33, 181: 33, 182: 33, 183: 33, 184: 33, 185: 33, 186: 33, 187: 33, 188: 33, 189: 33, 190: 33, 191: 33, 192: 33, 193: 33, 194: 33, 195: 33, 196: 33, 197: 33, 198: 33, 199: 33, 200: 33, 201: 33, 202: 33, 203: 33, 204: 33, 205: 33, 206: 33, 207: 33, 208: 33, 209: 33, 210: 33, 211: 33, 212: 33, 213: 33, 214: 33, 215: 33, 216: 33, 217: 33, 218: 33, 219: 33, 220: 33, 221: 33, 222: 33, 223: 33, 224: 33, 225: 33, 226: 33, 227: 33, 228: 33, 229: 33, 230: 33, 231: 33, 232: 33, 233: 33, 234: 33, 235: 33, 236: 33, 237: 33, 238: 33, 239: 33, 240: 33, 241: 33, 242: 33, 243: 33, 244: 33, 245: 33, 246: 33, 247: 33, 248: 33, 249: 33, 250: 33, 251: 33, 252: 33, 253: 33, 254: 33, 255: 33, 256: 33, 257: 33, 258: 33, 259: 33, 260: 33, 261: 33, 262: 33, 263: 33, 264: 33, 265: 33, 266: 33, 267: 33, 268: 33, 269: 33, 270: 33, 271: 33, 272: 33, 273: 33, 274: 33, 275: 33, 276: 33, 277: 33, 278: 33, 279: 33, 280: 33, 281: 33, 282: 33, 283: 33, 284: 33, 285: 33, 286: 33, 287: 33, 288: 33, 289: 33, 290: 33, 291: 33, 292: 33, 293: 33, 294: 33, 295: 33, 296: 33, 297: 33, 298: 33, 299: 33, 300: 33, 301: 33, 302: 33, 303: 33, 304: 33, 305: 33, 306: 33, 307: 33, 308: 33, 309: 33, 310: 33, 311: 33, 312: 33, 313: 33, 314: 33, 315: 33, 316: 33, 317: 33, 318: 33, 319: 33, 320: 33, 321: 33, 322: 33, 323: 33, 324: 33, 325: 33, 326: 33, 327: 33, 328: 33, 329: 33, 330: 33, 331: 33, 332: 33, 333: 33, 334: 33, 335: 33, 336: 33, 337: 33, 338: 33, 339: 33, 340: 33, 341: 33, 342: 33, 343: 33, 344: 33, 345: 33, 346: 33, 347: 33, 348: 33, 349: 33, 350: 33, 351: 33, 352: 33, 353: 33, 354: 33, 355: 33, 356: 33, 357: 33, 358: 33, 359: 33, 360: 33, 361: 33, 362: 33, 363: 33, 364: 33, 365: 33, 366: 33, 367: 33, 368: 33, 369: 33, 370: 33, 371: 33, 372: 33, 373: 33, 374: 33, 375: 33, 376: 33, 377: 33, 378: 33, 379: 33, 380: 33, 381: 33, 382: 33, 383: 33, 384: 33, 385: 33, 386: 33, 387: 33, 388: 33, 389: 33, 390: 33, 391: 33, 392: 33, 393: 33, 394: 33, 395: 33, 396: 33, 397: 33, 398: 33, 399: 33, 400: 33, 401: 33, 402: 33, 403: 33, 404: 33, 405: 33, 406: 33, 407: 33, 408: 33, 409: 33, 410: 33, 411: 33, 412: 33, 413: 33, 414: 33, 415: 33, 416: 33, 417: 33, 418: 33, 419: 33, 420: 33, 421: 33, 422: 33, 423: 33, 424: 33, 425: 33, 426: 33, 427: 33, 428: 33, 429: 33, 430: 33, 431: 33, 432: 33, 433: 33, 434: 33, 435: 33, 436: 33, 437: 33, 438: 33, 439: 33, 440: 33, 441: 33, 442: 33, 443: 33, 444: 33, 445: 33, 446: 33, 447: 33, 448: 33, 449: 33, 450: 33, 451: 33, 452: 33, 453: 33, 454: 33, 455: 33, 456: 33, 457: 33, 458: 33, 459: 33, 460: 33, 461: 33, 462: 33, 463: 33, 464: 33, 465: 33, 466: 33, 467: 33, 468: 33, 469: 33, 470: 33, 471: 33, 472: 33, 473: 33, 474: 33, 475: 33, 476: 33, 477: 33, 478: 33, 479: 33, 480: 33, 481: 33, 482: 33, 483: 33, 484: 33, 485: 33, 486: 33, 487: 33, 488: 33, 489: 33, 490: 33, 491: 33, 492: 33, 493: 33, 494: 33, 495: 33, 496: 33, 497: 33, 498: 33, 499: 33, 500: 33, 501: 33, 502: 33, 503: 33, 504: 33, 505: 33, 506: 33, 507: 33, 508: 33, 509: 33, 510: 33, 511: 33, 512: 33, 513: 33, 514: 33, 515: 33, 516: 33, 517: 33, 518: 33, 519: 33, 520: 33, 521: 33, 522: 33, 523: 33, 524: 33, 525: 33, 526: 33, 527: 33, 528: 33, 529: 33, 530: 33, 531: 33, 532: 33, 533: 33, 534: 33, 535: 33, 536: 33, 537: 33, 538: 33, 539: 33, 540: 33, 541: 33, 542: 33, 543: 33, 544: 33, 545: 33, 546: 33, 547: 33, 548: 33, 549: 33, 550: 33, 551: 33, 552: 33, 553: 33, 554: 33, 555: 33, 556: 33, 557: 33, 558: 33, 559: 33, 560: 33, 561: 33, 562: 33, 563: 33, 564: 33, 565: 33, 566: 33, 567: 33, 568: 33, 569: 33, 570: 33, 571: 33, 572: 33, 573: 33, 574: 33, 575: 33, 576: 33, 577: 33, 578: 33, 579: 33, 580: 33, 581: 33, 582: 33, 583: 33, 584: 33, 585: 33, 586: 33, 587: 33, 588: 33, 589: 33, 590: 33, 591: 33, 592: 33, 593: 33, 594: 33, 595: 33, 596: 33, 597: 33, 598: 33, 599: 33, 600: 33, 601: 33, 602: 33, 603: 33, 604: 33, 605: 33, 606: 33, 607: 33, 608: 33, 609: 33, 610: 33, 611: 33, 612: 33, 613: 33, 614: 33, 615: 33, 616: 33, 617: 33, 618: 33, 619: 33, 620: 33, 621: 33, 622: 33, 623: 33, 624: 33, 625: 33, 626: 33, 627: 33, 628: 33, 629: 33, 630: 33, 631: 33, 632: 33, 633: 33, 634: 33, 635: 33, 636: 33, 637: 33, 638: 33, 639: 33, 640: 33, 641: 33, 642: 33, 643: 33, 644: 33, 645: 33, 646: 33, 647: 33, 648: 33, 649: 33, 650: 33, 651: 33, 652: 33, 653: 33, 654: 33, 655: 33, 656: 33, 657: 33, 658: 33, 659: 33, 660: 33, 661: 33, 662: 33, 663: 33, 664: 33, 665: 33, 666: 33, 667: 33, 668: 33, 669: 33, 670: 33, 671: 33, 672: 33, 673: 33, 674: 33, 675: 33, 676: 33, 677: 33, 678: 33, 679: 33, 680: 33, 681: 33, 682: 33, 683: 33, 684: 33, 685: 33, 686: 33, 687: 33, 688: 33, 689: 33, 690: 33, 691: 33, 692: 33, 693: 33, 694: 33, 695: 33, 696: 33, 697: 33, 698: 33, 699: 33, 700: 33, 701: 33, 702: 33, 703: 33, 704: 33, 705: 33, 706: 33, 707: 33, 708: 33, 709: 33, 710: 33, 711: 33, 712: 33, 713: 33, 714: 33, 715: 33, 716: 33, 717: 33, 718: 33, 719: 33, 720: 33, 721: 33, 722: 33, 723: 33, 724: 33, 725: 33, 726: 33, 727: 33, 728: 33, 729: 33, 730: 33, 731: 33, 732: 33, 733: 33, 734: 33, 735: 33, 736: 33, 737: 33, 738: 33, 739: 33, 740: 33, 741: 33, 742: 33, 743: 33, 744: 33, 745: 33, 746: 33, 747: 33, 748: 33, 749: 33, 750: 33, 751: 33, 752: 33, 753: 33, 754: 33, 755: 33, 756: 33, 757: 33, 758: 33, 759: 33, 760: 33, 761: 33, 762: 33, 763: 33, 764: 33, 765: 33, 766: 33, 767: 33, 768: 33, 769: 33, 770: 33, 771: 33, 772: 33, 773: 33, 774: 33, 775: 33, 776: 33, 777: 33, 778: 33, 779: 33, 780: 33, 781: 33, 782: 33, 783: 33, 784: 33, 785: 33, 786: 33, 787: 33, 788: 33, 789: 33, 790: 33, 791: 33, 792: 33, 793: 33, 794: 33, 795: 33, 796: 33, 797: 33, 798: 33, 799: 33, 800: 33, 801: 33, 802: 33, 803: 33, 804: 33, 805: 33, 806: 33, 807: 33, 808: 33, 809: 33, 810: 33, 811: 33, 812: 33, 813: 33, 814: 33, 815: 33, 816: 33, 817: 33, 818: 33, 819: 33, 820: 33, 821: 33, 822: 33, 823: 33, 824: 33, 825: 33, 826: 33, 827: 33, 828: 33, 829: 33, 830: 33, 831: 33, 832: 33, 833: 33, 834: 33, 835: 33, 836: 33, 837: 33, 838: 33, 839: 33, 840: 33, 841: 33, 842: 33, 843: 33, 844: 33, 845: 33, 846: 33, 847: 33, 848: 33, 849: 33, 850: 33, 851: 33, 852: 33, 853: 33, 854: 33, 855: 33, 856: 33, 857: 33, 858: 33, 859: 33, 860: 33, 861: 33, 862: 33, 863: 33, 864: 33, 865: 33, 866: 33, 867: 33, 868: 33, 869: 33, 870: 33, 871: 33, 872: 33, 873: 33, 874: 33, 875: 33, 876: 33, 877: 33, 878: 33, 879: 33, 880: 33, 881: 33, 882: 33, 883: 33, 884: 33, 885: 33, 886: 33, 887: 33, 888: 33, 889: 33, 890: 33, 891: 33, 892: 33, 893: 33})
STEP-2	Epoch: 20/200	classification_loss: 1.0032	gate_loss: 1.0292	step2_classification_accuracy: 71.8460	step_2_gate_accuracy: 68.7004
STEP-2	Epoch: 40/200	classification_loss: 0.7901	gate_loss: 0.6991	step2_classification_accuracy: 77.3032	step_2_gate_accuracy: 77.2558
STEP-2	Epoch: 60/200	classification_loss: 0.6807	gate_loss: 0.5705	step2_classification_accuracy: 79.7031	step_2_gate_accuracy: 81.0487
STEP-2	Epoch: 80/200	classification_loss: 0.6127	gate_loss: 0.5042	step2_classification_accuracy: 81.2996	step_2_gate_accuracy: 82.6825
STEP-2	Epoch: 100/200	classification_loss: 0.5663	gate_loss: 0.4520	step2_classification_accuracy: 82.5334	step_2_gate_accuracy: 84.2655
STEP-2	Epoch: 120/200	classification_loss: 0.5355	gate_loss: 0.4212	step2_classification_accuracy: 83.4079	step_2_gate_accuracy: 85.1773
STEP-2	Epoch: 140/200	classification_loss: 0.5203	gate_loss: 0.4076	step2_classification_accuracy: 83.5435	step_2_gate_accuracy: 85.5264
STEP-2	Epoch: 160/200	classification_loss: 0.4928	gate_loss: 0.3766	step2_classification_accuracy: 84.3977	step_2_gate_accuracy: 86.8009
STEP-2	Epoch: 180/200	classification_loss: 2.0134	gate_loss: 1.0740	step2_classification_accuracy: 73.6764	step_2_gate_accuracy: 72.3137
STEP-2	Epoch: 200/200	classification_loss: 0.4655	gate_loss: 0.3534	step2_classification_accuracy: 84.9095	step_2_gate_accuracy: 87.4517
FINISH STEP 2

VALIDATING MODEL
	Task-0	val_accuracy: 60.4317	gate_accuracy: 71.2230
	Task-1	val_accuracy: 50.6494	gate_accuracy: 63.6364
	Task-2	val_accuracy: 67.0213	gate_accuracy: 64.8936
	Task-3	val_accuracy: 62.7660	gate_accuracy: 68.0851
	Task-4	val_accuracy: 68.3544	gate_accuracy: 69.6203
	Task-5	val_accuracy: 71.4286	gate_accuracy: 70.0000
	Task-6	val_accuracy: 80.5195	gate_accuracy: 68.8312
	Task-7	val_accuracy: 50.6329	gate_accuracy: 37.9747
	Task-8	val_accuracy: 54.8780	gate_accuracy: 54.8780
	Task-9	val_accuracy: 60.5263	gate_accuracy: 63.1579
	Task-10	val_accuracy: 85.5422	gate_accuracy: 75.9036
	Task-11	val_accuracy: 71.7949	gate_accuracy: 67.9487
	Task-12	val_accuracy: 66.6667	gate_accuracy: 58.6207
	Task-13	val_accuracy: 80.4348	gate_accuracy: 78.2609
	Task-14	val_accuracy: 69.1176	gate_accuracy: 67.6471
	Task-15	val_accuracy: 67.1642	gate_accuracy: 68.6567
	Task-16	val_accuracy: 63.9535	gate_accuracy: 58.1395
	Task-17	val_accuracy: 68.7500	gate_accuracy: 56.2500
	Task-18	val_accuracy: 76.7442	gate_accuracy: 73.2558
	Task-19	val_accuracy: 58.4416	gate_accuracy: 53.2468
	Task-20	val_accuracy: 67.0732	gate_accuracy: 60.9756
	Task-21	val_accuracy: 63.3333	gate_accuracy: 56.6667
	Task-22	val_accuracy: 71.7647	gate_accuracy: 65.8824
	Task-23	val_accuracy: 72.5000	gate_accuracy: 71.2500
	Task-24	val_accuracy: 59.1549	gate_accuracy: 64.7887
	Task-25	val_accuracy: 76.1905	gate_accuracy: 77.3810
	Task-26	val_accuracy: 63.2911	gate_accuracy: 60.7595
	Task-27	val_accuracy: 58.9744	gate_accuracy: 57.6923
	Task-28	val_accuracy: 49.3827	gate_accuracy: 50.6173
	Task-29	val_accuracy: 42.2535	gate_accuracy: 50.7042
	Task-30	val_accuracy: 88.2353	gate_accuracy: 77.9412
	Task-31	val_accuracy: 77.2727	gate_accuracy: 80.6818
	Task-32	val_accuracy: 71.7391	gate_accuracy: 66.3043
	Task-33	val_accuracy: 70.4545	gate_accuracy: 76.1364
	Task-34	val_accuracy: 63.7363	gate_accuracy: 62.6374
	Task-35	val_accuracy: 75.2688	gate_accuracy: 78.4946
	Task-36	val_accuracy: 48.5294	gate_accuracy: 66.1765
	Task-37	val_accuracy: 53.5714	gate_accuracy: 55.9524
	Task-38	val_accuracy: 70.4545	gate_accuracy: 73.8636
	Task-39	val_accuracy: 71.0843	gate_accuracy: 73.4940
	Task-40	val_accuracy: 77.6316	gate_accuracy: 82.8947
	Task-41	val_accuracy: 54.7945	gate_accuracy: 53.4247
	Task-42	val_accuracy: 43.9024	gate_accuracy: 56.0976
	Task-43	val_accuracy: 44.7059	gate_accuracy: 60.0000
CALCULATING GATE ACCURACY ON VALIDATION DATA
	GATE_ACCURACY: 65.4641


DynamicExpert(
  (relu): ReLU()
  (bias_layers): ModuleList(
    (0): BiasLayer()
    (1): BiasLayer()
    (2): BiasLayer()
    (3): BiasLayer()
    (4): BiasLayer()
    (5): BiasLayer()
    (6): BiasLayer()
    (7): BiasLayer()
    (8): BiasLayer()
    (9): BiasLayer()
    (10): BiasLayer()
    (11): BiasLayer()
    (12): BiasLayer()
    (13): BiasLayer()
    (14): BiasLayer()
    (15): BiasLayer()
    (16): BiasLayer()
    (17): BiasLayer()
    (18): BiasLayer()
    (19): BiasLayer()
    (20): BiasLayer()
    (21): BiasLayer()
    (22): BiasLayer()
    (23): BiasLayer()
    (24): BiasLayer()
    (25): BiasLayer()
    (26): BiasLayer()
    (27): BiasLayer()
    (28): BiasLayer()
    (29): BiasLayer()
    (30): BiasLayer()
    (31): BiasLayer()
    (32): BiasLayer()
    (33): BiasLayer()
    (34): BiasLayer()
    (35): BiasLayer()
    (36): BiasLayer()
    (37): BiasLayer()
    (38): BiasLayer()
    (39): BiasLayer()
    (40): BiasLayer()
    (41): BiasLayer()
    (42): BiasLayer()
    (43): BiasLayer()
  )
  (gate): Sequential(
    (0): Linear(in_features=91, out_features=91, bias=True)
    (1): ReLU()
    (2): Linear(in_features=91, out_features=91, bias=True)
    (3): ReLU()
    (4): Linear(in_features=91, out_features=44, bias=True)
  )
  (experts): ModuleList(
    (0): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=34, bias=True)
      (mapper): Linear(in_features=34, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (1): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (2): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (3): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (4): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (5): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (6): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (7): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (8): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (9): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (10): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (11): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (12): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (13): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (14): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (15): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (16): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (17): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (18): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (19): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (20): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (21): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (22): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (23): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (24): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (25): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (26): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (27): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (28): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (29): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (30): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (31): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (32): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (33): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (34): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (35): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (36): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (37): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (38): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (39): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (40): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (41): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (42): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
    (43): Expert(
      (fc1): Linear(in_features=91, out_features=50, bias=True)
      (fc2): Linear(in_features=50, out_features=20, bias=True)
      (mapper): Linear(in_features=20, out_features=894, bias=False)
      (batchnorm): InstanceNorm1d(50, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
    )
  )
)
Execution time:
CPU time: 20:55:53	Wall time: 19:59:46
CPU time: 75353.57407455	Wall time: 71986.49966716766
FAA: 74.41697384200519
FF: 33.83169339652823

TRAINER.METRIC.ACCURACY
0: [93.5251798561151]
1: [91.36690647482014, 83.11688311688312]
2: [90.64748201438849, 83.11688311688312, 96.80851063829788]
3: [89.92805755395683, 76.62337662337663, 94.68085106382979, 85.1063829787234]
4: [90.64748201438849, 79.22077922077922, 87.2340425531915, 79.7872340425532, 91.13924050632912]
5: [86.33093525179856, 74.02597402597402, 87.2340425531915, 78.72340425531915, 88.60759493670885, 90.0]
6: [84.89208633093526, 74.02597402597402, 91.48936170212765, 79.7872340425532, 86.07594936708861, 88.57142857142857, 93.5064935064935]
7: [84.89208633093526, 70.12987012987013, 80.85106382978722, 71.27659574468085, 84.81012658227847, 88.57142857142857, 88.31168831168831, 74.68354430379746]
8: [84.89208633093526, 70.12987012987013, 85.1063829787234, 74.46808510638297, 87.34177215189874, 90.0, 92.20779220779221, 74.68354430379746, 82.92682926829268]
9: [80.57553956834532, 63.63636363636363, 85.1063829787234, 70.2127659574468, 81.0126582278481, 91.42857142857143, 87.01298701298701, 77.21518987341773, 81.70731707317073, 71.05263157894737]
10: [84.17266187050359, 66.23376623376623, 78.72340425531915, 73.40425531914893, 84.81012658227847, 85.71428571428571, 87.01298701298701, 67.08860759493672, 79.26829268292683, 77.63157894736842, 92.7710843373494]
11: [80.57553956834532, 61.038961038961034, 80.85106382978722, 73.40425531914893, 79.74683544303798, 84.28571428571429, 85.71428571428571, 70.88607594936708, 79.26829268292683, 69.73684210526315, 91.56626506024097, 85.8974358974359]
12: [77.6978417266187, 62.33766233766234, 85.1063829787234, 62.76595744680851, 83.54430379746836, 84.28571428571429, 90.9090909090909, 64.55696202531645, 76.82926829268293, 69.73684210526315, 90.36144578313254, 88.46153846153845, 81.60919540229885]
13: [76.97841726618705, 55.84415584415584, 79.7872340425532, 71.27659574468085, 82.27848101265823, 84.28571428571429, 90.9090909090909, 73.41772151898735, 75.60975609756098, 68.42105263157895, 92.7710843373494, 76.92307692307693, 80.45977011494253, 84.78260869565217]
14: [79.13669064748201, 59.74025974025974, 86.17021276595744, 69.14893617021278, 78.48101265822784, 82.85714285714286, 83.11688311688312, 62.0253164556962, 74.39024390243902, 61.8421052631579, 92.7710843373494, 82.05128205128204, 80.45977011494253, 86.95652173913044, 79.41176470588235]
15: [80.57553956834532, 50.649350649350644, 75.53191489361703, 65.95744680851064, 82.27848101265823, 82.85714285714286, 84.4155844155844, 65.82278481012658, 75.60975609756098, 68.42105263157895, 91.56626506024097, 76.92307692307693, 85.0574712643678, 84.78260869565217, 82.35294117647058, 73.13432835820896]
16: [81.29496402877699, 51.94805194805194, 78.72340425531915, 67.02127659574468, 77.21518987341773, 82.85714285714286, 83.11688311688312, 53.16455696202531, 79.26829268292683, 67.10526315789474, 89.1566265060241, 80.76923076923077, 79.3103448275862, 85.86956521739131, 77.94117647058823, 77.61194029850746, 75.5813953488372]
17: [78.41726618705036, 57.14285714285714, 74.46808510638297, 69.14893617021278, 78.48101265822784, 78.57142857142857, 83.11688311688312, 63.29113924050633, 74.39024390243902, 63.1578947368421, 95.18072289156626, 75.64102564102564, 80.45977011494253, 83.69565217391305, 85.29411764705883, 74.6268656716418, 68.6046511627907, 77.5]
18: [72.66187050359713, 59.74025974025974, 72.3404255319149, 70.2127659574468, 77.21518987341773, 80.0, 85.71428571428571, 58.22784810126582, 74.39024390243902, 64.47368421052632, 93.97590361445783, 79.48717948717949, 79.3103448275862, 85.86956521739131, 72.05882352941177, 73.13432835820896, 70.93023255813954, 78.75, 83.72093023255815]
19: [74.10071942446042, 49.35064935064935, 75.53191489361703, 67.02127659574468, 81.0126582278481, 77.14285714285715, 81.81818181818183, 59.49367088607595, 79.26829268292683, 60.526315789473685, 90.36144578313254, 75.64102564102564, 78.16091954022988, 85.86956521739131, 77.94117647058823, 73.13432835820896, 67.44186046511628, 76.25, 83.72093023255815, 63.63636363636363]
20: [79.13669064748201, 55.84415584415584, 70.2127659574468, 70.2127659574468, 79.74683544303798, 82.85714285714286, 79.22077922077922, 56.9620253164557, 71.95121951219512, 60.526315789473685, 91.56626506024097, 73.07692307692307, 82.75862068965517, 82.6086956521739, 75.0, 73.13432835820896, 74.4186046511628, 77.5, 84.88372093023256, 62.33766233766234, 80.48780487804879]
21: [70.50359712230215, 42.857142857142854, 73.40425531914893, 64.8936170212766, 74.68354430379746, 87.14285714285714, 87.01298701298701, 60.75949367088608, 59.756097560975604, 63.1578947368421, 91.56626506024097, 76.92307692307693, 78.16091954022988, 84.78260869565217, 77.94117647058823, 76.11940298507463, 74.4186046511628, 72.5, 83.72093023255815, 59.74025974025974, 75.60975609756098, 81.11111111111111]
22: [75.53956834532374, 55.84415584415584, 70.2127659574468, 70.2127659574468, 69.62025316455697, 80.0, 81.81818181818183, 58.22784810126582, 64.63414634146342, 65.78947368421053, 91.56626506024097, 76.92307692307693, 78.16091954022988, 84.78260869565217, 76.47058823529412, 73.13432835820896, 70.93023255813954, 76.25, 80.23255813953489, 58.44155844155844, 74.39024390243902, 68.88888888888889, 71.76470588235294]
23: [73.38129496402878, 51.94805194805194, 70.2127659574468, 63.829787234042556, 68.35443037974683, 77.14285714285715, 79.22077922077922, 54.43037974683544, 73.17073170731707, 56.57894736842105, 91.56626506024097, 79.48717948717949, 81.60919540229885, 80.43478260869566, 77.94117647058823, 77.61194029850746, 72.09302325581395, 76.25, 84.88372093023256, 58.44155844155844, 80.48780487804879, 67.77777777777779, 68.23529411764706, 70.0]
24: [71.22302158273382, 51.94805194805194, 71.27659574468085, 69.14893617021278, 70.88607594936708, 77.14285714285715, 79.22077922077922, 51.89873417721519, 67.07317073170732, 67.10526315789474, 91.56626506024097, 78.2051282051282, 74.71264367816092, 80.43478260869566, 82.35294117647058, 74.6268656716418, 70.93023255813954, 72.5, 86.04651162790698, 57.14285714285714, 75.60975609756098, 73.33333333333333, 75.29411764705883, 70.0, 59.154929577464785]
25: [76.2589928057554, 45.45454545454545, 76.59574468085107, 71.27659574468085, 77.21518987341773, 80.0, 80.51948051948052, 54.43037974683544, 62.19512195121951, 65.78947368421053, 89.1566265060241, 74.35897435897436, 78.16091954022988, 77.17391304347827, 82.35294117647058, 73.13432835820896, 74.4186046511628, 71.25, 81.3953488372093, 58.44155844155844, 80.48780487804879, 72.22222222222221, 69.41176470588235, 72.5, 59.154929577464785, 83.33333333333334]
26: [74.82014388489209, 54.54545454545454, 78.72340425531915, 68.08510638297872, 69.62025316455697, 84.28571428571429, 83.11688311688312, 54.43037974683544, 65.85365853658537, 68.42105263157895, 85.54216867469879, 76.92307692307693, 77.01149425287356, 83.69565217391305, 73.52941176470588, 74.6268656716418, 75.5813953488372, 80.0, 82.55813953488372, 51.94805194805194, 80.48780487804879, 72.22222222222221, 71.76470588235294, 71.25, 53.52112676056338, 79.76190476190477, 65.82278481012658]
27: [76.2589928057554, 51.94805194805194, 71.27659574468085, 64.8936170212766, 78.48101265822784, 80.0, 79.22077922077922, 51.89873417721519, 67.07317073170732, 76.31578947368422, 84.33734939759037, 74.35897435897436, 79.3103448275862, 78.26086956521739, 76.47058823529412, 70.1492537313433, 70.93023255813954, 75.0, 83.72093023255815, 58.44155844155844, 79.26829268292683, 70.0, 64.70588235294117, 72.5, 60.56338028169014, 83.33333333333334, 63.29113924050633, 58.97435897435898]
28: [70.50359712230215, 45.45454545454545, 75.53191489361703, 72.3404255319149, 73.41772151898735, 80.0, 74.02597402597402, 50.63291139240506, 62.19512195121951, 57.89473684210527, 87.95180722891565, 70.51282051282051, 72.41379310344827, 76.08695652173914, 70.58823529411765, 68.65671641791045, 73.25581395348837, 81.25, 83.72093023255815, 54.54545454545454, 75.60975609756098, 66.66666666666666, 69.41176470588235, 75.0, 57.74647887323944, 71.42857142857143, 63.29113924050633, 66.66666666666666, 43.20987654320987]
29: [73.38129496402878, 57.14285714285714, 68.08510638297872, 65.95744680851064, 68.35443037974683, 75.71428571428571, 81.81818181818183, 56.9620253164557, 63.41463414634146, 57.89473684210527, 89.1566265060241, 75.64102564102564, 60.91954022988506, 83.69565217391305, 73.52941176470588, 71.64179104477611, 72.09302325581395, 72.5, 84.88372093023256, 55.84415584415584, 76.82926829268293, 70.0, 68.23529411764706, 72.5, 53.52112676056338, 72.61904761904762, 64.55696202531645, 62.82051282051282, 49.382716049382715, 56.33802816901409]
30: [76.2589928057554, 50.649350649350644, 71.27659574468085, 64.8936170212766, 72.15189873417721, 82.85714285714286, 71.42857142857143, 51.89873417721519, 67.07317073170732, 60.526315789473685, 86.74698795180723, 71.7948717948718, 67.81609195402298, 77.17391304347827, 76.47058823529412, 65.67164179104478, 63.95348837209303, 68.75, 82.55813953488372, 63.63636363636363, 75.60975609756098, 73.33333333333333, 75.29411764705883, 67.5, 47.88732394366197, 75.0, 69.62025316455697, 66.66666666666666, 49.382716049382715, 50.70422535211267, 92.64705882352942]
31: [67.62589928057554, 55.84415584415584, 64.8936170212766, 67.02127659574468, 72.15189873417721, 75.71428571428571, 74.02597402597402, 51.89873417721519, 62.19512195121951, 53.94736842105263, 90.36144578313254, 71.7948717948718, 71.26436781609196, 85.86956521739131, 80.88235294117648, 70.1492537313433, 63.95348837209303, 72.5, 87.20930232558139, 59.74025974025974, 84.14634146341463, 63.33333333333333, 71.76470588235294, 70.0, 52.112676056338024, 79.76190476190477, 68.35443037974683, 67.94871794871796, 48.148148148148145, 49.29577464788733, 88.23529411764706, 77.27272727272727]
32: [71.22302158273382, 50.649350649350644, 74.46808510638297, 69.14893617021278, 69.62025316455697, 78.57142857142857, 79.22077922077922, 51.89873417721519, 62.19512195121951, 60.526315789473685, 89.1566265060241, 64.1025641025641, 67.81609195402298, 85.86956521739131, 76.47058823529412, 71.64179104477611, 74.4186046511628, 73.75, 81.3953488372093, 62.33766233766234, 80.48780487804879, 62.22222222222222, 75.29411764705883, 71.25, 49.29577464788733, 75.0, 68.35443037974683, 62.82051282051282, 56.79012345679012, 56.33802816901409, 88.23529411764706, 78.4090909090909, 69.56521739130434]
33: [74.10071942446042, 54.54545454545454, 74.46808510638297, 59.57446808510638, 73.41772151898735, 80.0, 72.72727272727273, 53.16455696202531, 63.41463414634146, 61.8421052631579, 85.54216867469879, 73.07692307692307, 66.66666666666666, 84.78260869565217, 75.0, 68.65671641791045, 72.09302325581395, 70.0, 77.90697674418605, 55.84415584415584, 70.73170731707317, 61.111111111111114, 75.29411764705883, 72.5, 60.56338028169014, 77.38095238095238, 62.0253164556962, 61.53846153846154, 48.148148148148145, 60.56338028169014, 86.76470588235294, 77.27272727272727, 77.17391304347827, 73.86363636363636]
34: [70.50359712230215, 53.246753246753244, 71.27659574468085, 59.57446808510638, 74.68354430379746, 80.0, 71.42857142857143, 62.0253164556962, 58.536585365853654, 63.1578947368421, 85.54216867469879, 74.35897435897436, 67.81609195402298, 83.69565217391305, 70.58823529411765, 73.13432835820896, 62.7906976744186, 66.25, 82.55813953488372, 55.84415584415584, 78.04878048780488, 67.77777777777779, 72.94117647058823, 73.75, 54.929577464788736, 75.0, 59.49367088607595, 61.53846153846154, 41.9753086419753, 49.29577464788733, 91.17647058823529, 78.4090909090909, 68.47826086956522, 71.5909090909091, 65.93406593406593]
35: [65.46762589928058, 53.246753246753244, 60.63829787234043, 65.95744680851064, 68.35443037974683, 68.57142857142857, 76.62337662337663, 55.69620253164557, 60.97560975609756, 63.1578947368421, 83.13253012048193, 80.76923076923077, 70.11494252873564, 80.43478260869566, 76.47058823529412, 73.13432835820896, 60.46511627906976, 77.5, 79.06976744186046, 54.54545454545454, 74.39024390243902, 64.44444444444444, 68.23529411764706, 67.5, 56.33802816901409, 76.19047619047619, 65.82278481012658, 66.66666666666666, 46.913580246913575, 46.478873239436616, 89.70588235294117, 81.81818181818183, 68.47826086956522, 72.72727272727273, 63.73626373626373, 69.89247311827957]
36: [69.06474820143885, 53.246753246753244, 60.63829787234043, 65.95744680851064, 70.88607594936708, 75.71428571428571, 76.62337662337663, 53.16455696202531, 67.07317073170732, 57.89473684210527, 90.36144578313254, 78.2051282051282, 71.26436781609196, 78.26086956521739, 64.70588235294117, 65.67164179104478, 68.6046511627907, 73.75, 84.88372093023256, 54.54545454545454, 80.48780487804879, 56.666666666666664, 65.88235294117646, 71.25, 54.929577464788736, 77.38095238095238, 64.55696202531645, 57.692307692307686, 49.382716049382715, 45.07042253521127, 88.23529411764706, 73.86363636363636, 66.30434782608695, 71.5909090909091, 60.43956043956044, 68.81720430107528, 60.29411764705882]
37: [69.06474820143885, 59.74025974025974, 71.27659574468085, 65.95744680851064, 73.41772151898735, 71.42857142857143, 80.51948051948052, 53.16455696202531, 64.63414634146342, 61.8421052631579, 90.36144578313254, 65.38461538461539, 67.81609195402298, 72.82608695652173, 75.0, 70.1492537313433, 73.25581395348837, 72.5, 82.55813953488372, 61.038961038961034, 80.48780487804879, 63.33333333333333, 68.23529411764706, 72.5, 57.74647887323944, 79.76190476190477, 62.0253164556962, 62.82051282051282, 46.913580246913575, 54.929577464788736, 88.23529411764706, 78.4090909090909, 70.65217391304348, 78.4090909090909, 64.83516483516483, 68.81720430107528, 55.88235294117647, 58.333333333333336]
38: [74.10071942446042, 45.45454545454545, 73.40425531914893, 61.702127659574465, 70.88607594936708, 74.28571428571429, 75.32467532467533, 55.69620253164557, 59.756097560975604, 56.57894736842105, 84.33734939759037, 61.53846153846154, 66.66666666666666, 84.78260869565217, 69.11764705882352, 68.65671641791045, 63.95348837209303, 70.0, 82.55813953488372, 55.84415584415584, 74.39024390243902, 62.22222222222222, 70.58823529411765, 68.75, 50.70422535211267, 76.19047619047619, 64.55696202531645, 57.692307692307686, 49.382716049382715, 54.929577464788736, 82.35294117647058, 81.81818181818183, 79.34782608695652, 76.13636363636364, 54.94505494505495, 73.11827956989248, 66.17647058823529, 52.38095238095239, 75.0]
39: [59.71223021582733, 53.246753246753244, 72.3404255319149, 62.76595744680851, 73.41772151898735, 80.0, 75.32467532467533, 56.9620253164557, 59.756097560975604, 61.8421052631579, 86.74698795180723, 75.64102564102564, 64.36781609195403, 76.08695652173914, 70.58823529411765, 74.6268656716418, 76.74418604651163, 75.0, 76.74418604651163, 55.84415584415584, 80.48780487804879, 60.0, 65.88235294117646, 72.5, 49.29577464788733, 70.23809523809523, 67.08860759493672, 67.94871794871796, 50.617283950617285, 45.07042253521127, 88.23529411764706, 79.54545454545455, 71.73913043478261, 75.0, 48.35164835164835, 73.11827956989248, 51.470588235294116, 58.333333333333336, 70.45454545454545, 68.67469879518072]
40: [65.46762589928058, 54.54545454545454, 68.08510638297872, 57.446808510638306, 70.88607594936708, 75.71428571428571, 83.11688311688312, 49.36708860759494, 64.63414634146342, 53.94736842105263, 84.33734939759037, 76.92307692307693, 68.96551724137932, 85.86956521739131, 66.17647058823529, 71.64179104477611, 66.27906976744185, 80.0, 75.5813953488372, 54.54545454545454, 78.04878048780488, 61.111111111111114, 77.64705882352942, 73.75, 43.66197183098591, 63.095238095238095, 62.0253164556962, 67.94871794871796, 45.67901234567901, 50.70422535211267, 85.29411764705883, 80.68181818181817, 70.65217391304348, 76.13636363636364, 49.45054945054945, 69.89247311827957, 64.70588235294117, 53.57142857142857, 73.86363636363636, 61.44578313253012, 77.63157894736842]
41: [66.90647482014388, 58.44155844155844, 60.63829787234043, 64.8936170212766, 73.41772151898735, 72.85714285714285, 79.22077922077922, 55.69620253164557, 53.65853658536586, 56.57894736842105, 84.33734939759037, 67.94871794871796, 70.11494252873564, 79.34782608695652, 73.52941176470588, 65.67164179104478, 67.44186046511628, 72.5, 76.74418604651163, 57.14285714285714, 71.95121951219512, 61.111111111111114, 67.05882352941175, 72.5, 52.112676056338024, 77.38095238095238, 62.0253164556962, 62.82051282051282, 50.617283950617285, 49.29577464788733, 85.29411764705883, 73.86363636363636, 65.21739130434783, 76.13636363636364, 58.24175824175825, 62.365591397849464, 64.70588235294117, 53.57142857142857, 80.68181818181817, 65.06024096385542, 68.42105263157895, 61.64383561643836]
42: [67.62589928057554, 55.84415584415584, 69.14893617021278, 73.40425531914893, 70.88607594936708, 64.28571428571429, 77.92207792207793, 54.43037974683544, 58.536585365853654, 59.210526315789465, 84.33734939759037, 71.7948717948718, 68.96551724137932, 78.26086956521739, 77.94117647058823, 71.64179104477611, 61.627906976744185, 68.75, 81.3953488372093, 58.44155844155844, 71.95121951219512, 65.55555555555556, 72.94117647058823, 72.5, 53.52112676056338, 70.23809523809523, 67.08860759493672, 62.82051282051282, 48.148148148148145, 47.88732394366197, 85.29411764705883, 77.27272727272727, 68.47826086956522, 68.18181818181817, 54.94505494505495, 74.19354838709677, 64.70588235294117, 58.333333333333336, 81.81818181818183, 69.87951807228916, 75.0, 58.9041095890411, 48.78048780487805]
43: [60.431654676258994, 50.649350649350644, 67.02127659574468, 62.76595744680851, 68.35443037974683, 71.42857142857143, 80.51948051948052, 50.63291139240506, 54.87804878048781, 60.526315789473685, 85.54216867469879, 71.7948717948718, 66.66666666666666, 80.43478260869566, 69.11764705882352, 67.16417910447761, 63.95348837209303, 68.75, 76.74418604651163, 58.44155844155844, 67.07317073170732, 63.33333333333333, 71.76470588235294, 72.5, 59.154929577464785, 76.19047619047619, 63.29113924050633, 58.97435897435898, 49.382716049382715, 42.25352112676056, 88.23529411764706, 77.27272727272727, 71.73913043478261, 70.45454545454545, 63.73626373626373, 75.26881720430107, 48.529411764705884, 53.57142857142857, 70.45454545454545, 71.08433734939759, 77.63157894736842, 54.794520547945204, 43.90243902439025, 44.70588235294118]

=====RUNNING ON TEST SET=====
CALCULATING TEST ACCURACY PER TASK
	TASK-0	CLASSES: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29 30 31 32 33]	test_accuracy: 56.6138
	TASK-1	CLASSES: [34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53]	test_accuracy: 52.3810
	TASK-2	CLASSES: [54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73]	test_accuracy: 61.6000
	TASK-3	CLASSES: [74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93]	test_accuracy: 69.3548
	TASK-4	CLASSES: [ 94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111
 112 113]	test_accuracy: 73.1481
	TASK-5	CLASSES: [114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131
 132 133]	test_accuracy: 72.1649
	TASK-6	CLASSES: [134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151
 152 153]	test_accuracy: 73.5849
	TASK-7	CLASSES: [154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171
 172 173]	test_accuracy: 58.8785
	TASK-8	CLASSES: [174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191
 192 193]	test_accuracy: 68.1818
	TASK-9	CLASSES: [194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211
 212 213]	test_accuracy: 62.5000
	TASK-10	CLASSES: [214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231
 232 233]	test_accuracy: 82.3009
	TASK-11	CLASSES: [234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251
 252 253]	test_accuracy: 74.7664
	TASK-12	CLASSES: [254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271
 272 273]	test_accuracy: 74.1379
	TASK-13	CLASSES: [274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291
 292 293]	test_accuracy: 80.4878
	TASK-14	CLASSES: [294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311
 312 313]	test_accuracy: 63.1579
	TASK-15	CLASSES: [314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331
 332 333]	test_accuracy: 61.7021
	TASK-16	CLASSES: [334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351
 352 353]	test_accuracy: 69.5652
	TASK-17	CLASSES: [354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371
 372 373]	test_accuracy: 63.3028
	TASK-18	CLASSES: [374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391
 392 393]	test_accuracy: 69.8276
	TASK-19	CLASSES: [394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411
 412 413]	test_accuracy: 57.1429
	TASK-20	CLASSES: [414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431
 432 433]	test_accuracy: 68.4685
	TASK-21	CLASSES: [434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451
 452 453]	test_accuracy: 57.9832
	TASK-22	CLASSES: [454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471
 472 473]	test_accuracy: 71.9298
	TASK-23	CLASSES: [474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491
 492 493]	test_accuracy: 76.8519
	TASK-24	CLASSES: [494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511
 512 513]	test_accuracy: 63.2653
	TASK-25	CLASSES: [514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531
 532 533]	test_accuracy: 74.3363
	TASK-26	CLASSES: [534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551
 552 553]	test_accuracy: 64.4860
	TASK-27	CLASSES: [554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571
 572 573]	test_accuracy: 53.2710
	TASK-28	CLASSES: [574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591
 592 593]	test_accuracy: 57.7982
	TASK-29	CLASSES: [594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611
 612 613]	test_accuracy: 58.5859
	TASK-30	CLASSES: [614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631
 632 633]	test_accuracy: 74.7368
	TASK-31	CLASSES: [634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651
 652 653]	test_accuracy: 79.4872
	TASK-32	CLASSES: [654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671
 672 673]	test_accuracy: 70.7317
	TASK-33	CLASSES: [674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691
 692 693]	test_accuracy: 65.2542
	TASK-34	CLASSES: [694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711
 712 713]	test_accuracy: 56.1983
	TASK-35	CLASSES: [714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731
 732 733]	test_accuracy: 79.6748
	TASK-36	CLASSES: [734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751
 752 753]	test_accuracy: 58.9474
	TASK-37	CLASSES: [754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771
 772 773]	test_accuracy: 52.6316
	TASK-38	CLASSES: [774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791
 792 793]	test_accuracy: 67.7966
	TASK-39	CLASSES: [794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811
 812 813]	test_accuracy: 65.1786
	TASK-40	CLASSES: [814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831
 832 833]	test_accuracy: 73.0769
	TASK-41	CLASSES: [834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851
 852 853]	test_accuracy: 53.4653
	TASK-42	CLASSES: [854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871
 872 873]	test_accuracy: 51.8182
	TASK-43	CLASSES: [874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891
 892 893]	test_accuracy: 48.6726

====================

f1_score(micro): 65.6701240593858
f1_score(macro): 62.55923535521345
Classification report:
              precision    recall  f1-score   support

           0       1.00      0.50      0.67         4
           1       0.73      0.89      0.80         9
           2       1.00      0.60      0.75         5
           3       1.00      0.50      0.67         4
           4       0.60      0.75      0.67         4
           5       0.33      0.20      0.25         5
           6       1.00      0.00      0.00         4
           7       1.00      0.80      0.89         5
           8       0.75      0.75      0.75         4
           9       0.89      0.89      0.89         9
          10       1.00      0.00      0.00         4
          11       1.00      0.75      0.86         4
          12       1.00      0.60      0.75         5
          13       1.00      0.40      0.57         5
          14       0.80      1.00      0.89         4
          15       1.00      0.78      0.88         9
          16       0.00      0.00      0.00         9
          17       1.00      0.00      0.00         4
          18       0.25      0.25      0.25         4
          19       1.00      0.50      0.67         4
          20       1.00      0.40      0.57         5
          21       0.50      0.25      0.33         4
          22       0.00      0.00      0.00         4
          23       0.71      1.00      0.83         5
          24       1.00      0.67      0.80         9
          25       1.00      0.00      0.00         4
          26       1.00      1.00      1.00         4
          27       1.00      0.25      0.40         4
          28       0.62      0.56      0.59         9
          29       0.89      0.89      0.89         9
          30       0.67      0.67      0.67         9
          31       1.00      1.00      1.00         4
          32       0.67      0.40      0.50         5
          33       1.00      0.78      0.88         9
          34       0.00      0.00      0.00         5
          35       1.00      0.00      0.00         4
          36       1.00      0.75      0.86         4
          37       0.50      0.25      0.33         4
          38       1.00      0.50      0.67         4
          39       0.60      0.60      0.60         5
          40       0.58      0.78      0.67         9
          41       0.50      0.22      0.31         9
          42       1.00      1.00      1.00         4
          43       1.00      0.00      0.00         4
          44       1.00      0.00      0.00         5
          45       0.83      1.00      0.91         5
          46       1.00      0.75      0.86         4
          47       0.20      0.25      0.22         4
          48       1.00      0.00      0.00         4
          49       0.64      0.78      0.70         9
          50       1.00      0.60      0.75         5
          51       0.80      1.00      0.89         4
          52       0.82      1.00      0.90         9
          53       0.05      0.25      0.08         4
          54       0.75      1.00      0.86         9
          55       0.70      0.78      0.74         9
          56       0.33      0.25      0.29         4
          57       1.00      0.40      0.57         5
          58       1.00      0.00      0.00         4
          59       1.00      1.00      1.00         4
          60       0.67      0.50      0.57         4
          61       0.57      1.00      0.73         4
          62       0.50      0.44      0.47         9
          63       1.00      0.20      0.33         5
          64       0.00      0.00      0.00         4
          65       0.67      0.80      0.73         5
          66       1.00      0.75      0.86         4
          67       0.89      0.89      0.89         9
          68       0.90      1.00      0.95         9
          69       0.00      0.00      0.00         4
          70       0.75      0.50      0.60         6
          71       0.50      0.33      0.40         9
          72       1.00      0.89      0.94         9
          73       0.45      0.56      0.50         9
          74       0.67      1.00      0.80         4
          75       1.00      0.75      0.86         4
          76       0.08      0.25      0.12         4
          77       0.14      0.25      0.18         4
          78       1.00      1.00      1.00         9
          79       0.67      1.00      0.80         4
          80       0.12      0.22      0.15         9
          81       0.62      0.56      0.59         9
          82       0.80      0.89      0.84         9
          83       1.00      0.89      0.94         9
          84       0.50      0.75      0.60         4
          85       0.80      1.00      0.89         4
          86       0.75      0.60      0.67         5
          87       0.88      0.78      0.82         9
          88       0.50      0.20      0.29         5
          89       0.78      0.78      0.78         9
          90       0.00      0.00      0.00         4
          91       0.75      0.60      0.67         5
          92       0.67      0.80      0.73         5
          93       1.00      1.00      1.00         9
          94       0.00      0.00      0.00         4
          95       1.00      0.00      0.00         4
          96       1.00      0.75      0.86         4
          97       0.60      0.75      0.67         4
          98       0.90      1.00      0.95         9
          99       0.67      0.50      0.57         4
         100       1.00      0.75      0.86         4
         101       0.62      0.89      0.73         9
         102       0.75      0.75      0.75         4
         103       0.62      0.56      0.59         9
         104       0.50      0.75      0.60         4
         105       0.75      0.75      0.75         4
         106       1.00      0.75      0.86         4
         107       1.00      1.00      1.00         4
         108       0.57      0.89      0.70         9
         109       1.00      1.00      1.00         5
         110       1.00      0.67      0.80         9
         111       1.00      1.00      1.00         5
         112       0.50      0.40      0.44         5
         113       0.80      1.00      0.89         4
         114       1.00      0.75      0.86         4
         115       1.00      1.00      1.00         4
         116       1.00      0.75      0.86         4
         117       0.50      0.50      0.50         4
         118       1.00      0.75      0.86         4
         119       1.00      0.75      0.86         4
         120       0.64      1.00      0.78         9
         121       1.00      1.00      1.00         4
         122       0.80      0.89      0.84         9
         123       0.00      0.00      0.00         4
         124       1.00      1.00      1.00         4
         125       0.54      0.78      0.64         9
         126       0.75      0.75      0.75         4
         127       0.00      0.00      0.00         4
         128       0.80      1.00      0.89         4
         129       0.00      0.00      0.00         4
         130       1.00      1.00      1.00         5
         131       0.75      0.75      0.75         4
         132       1.00      0.00      0.00         4
         133       0.71      1.00      0.83         5
         134       0.43      0.60      0.50         5
         135       0.69      1.00      0.82         9
         136       0.57      0.44      0.50         9
         137       1.00      0.25      0.40         4
         138       0.60      0.60      0.60         5
         139       1.00      0.60      0.75         5
         140       1.00      0.25      0.40         4
         141       0.90      1.00      0.95         9
         142       1.00      0.75      0.86         4
         143       0.62      0.71      0.67         7
         144       1.00      0.75      0.86         4
         145       1.00      0.75      0.86         4
         146       1.00      1.00      1.00         4
         147       0.10      0.50      0.17         4
         148       0.80      1.00      0.89         4
         149       1.00      1.00      1.00         4
         150       0.89      0.89      0.89         9
         151       1.00      1.00      1.00         4
         152       1.00      0.25      0.40         4
         153       1.00      1.00      1.00         4
         154       0.89      0.89      0.89         9
         155       0.00      0.00      0.00         4
         156       0.75      0.75      0.75         4
         157       0.80      1.00      0.89         4
         158       0.75      0.75      0.75         4
         159       0.75      0.75      0.75         4
         160       1.00      1.00      1.00         5
         161       0.57      1.00      0.73         4
         162       1.00      0.00      0.00         4
         163       0.12      0.11      0.12         9
         164       1.00      0.75      0.86         4
         165       0.80      1.00      0.89         4
         166       1.00      0.00      0.00         4
         167       0.00      0.00      0.00         4
         168       0.62      0.89      0.73         9
         169       0.33      0.22      0.27         9
         170       1.00      1.00      1.00         5
         171       1.00      1.00      1.00         4
         172       1.00      0.00      0.00         4
         173       0.75      0.67      0.71         9
         174       1.00      1.00      1.00         5
         175       1.00      0.75      0.86         4
         176       0.50      1.00      0.67         4
         177       1.00      0.00      0.00         4
         178       0.80      1.00      0.89         4
         179       0.62      1.00      0.77         5
         180       0.69      1.00      0.82         9
         181       0.75      0.75      0.75         4
         182       1.00      0.75      0.86         4
         183       0.67      0.50      0.57         4
         184       1.00      1.00      1.00         9
         185       1.00      1.00      1.00         5
         186       1.00      1.00      1.00         5
         187       1.00      0.75      0.86         4
         188       0.75      0.75      0.75         4
         189       0.43      0.33      0.38         9
         190       0.00      0.00      0.00         4
         191       0.44      0.44      0.44         9
         192       0.60      0.60      0.60         5
         193       0.40      0.22      0.29         9
         194       1.00      0.75      0.86         8
         195       0.33      0.80      0.47         5
         196       1.00      0.75      0.86         4
         197       0.38      0.75      0.50         4
         198       0.00      0.00      0.00         4
         199       0.83      1.00      0.91         5
         200       0.00      0.00      0.00         9
         201       0.78      0.78      0.78         9
         202       0.50      0.40      0.44         5
         203       0.00      0.00      0.00         4
         204       1.00      0.80      0.89         5
         205       0.67      1.00      0.80         4
         206       0.71      1.00      0.83         5
         207       1.00      1.00      1.00         9
         208       0.33      0.75      0.46         4
         209       0.00      0.00      0.00         4
         210       1.00      1.00      1.00         4
         211       0.67      1.00      0.80         4
         212       0.00      0.00      0.00         4
         213       1.00      0.50      0.67         4
         214       1.00      0.80      0.89         5
         215       0.40      0.67      0.50         9
         216       0.80      1.00      0.89         4
         217       1.00      1.00      1.00         4
         218       1.00      1.00      1.00         4
         219       1.00      1.00      1.00         4
         220       0.86      0.67      0.75         9
         221       1.00      1.00      1.00         4
         222       0.67      0.50      0.57         4
         223       0.86      0.67      0.75         9
         224       1.00      0.75      0.86         4
         225       1.00      1.00      1.00         5
         226       1.00      1.00      1.00         4
         227       0.89      0.89      0.89         9
         228       1.00      0.80      0.89         5
         229       0.00      0.00      0.00         4
         230       0.89      0.89      0.89         9
         231       0.82      1.00      0.90         9
         232       1.00      1.00      1.00         4
         233       1.00      1.00      1.00         4
         234       1.00      0.75      0.86         4
         235       0.62      0.89      0.73         9
         236       0.56      1.00      0.71         5
         237       1.00      1.00      1.00         4
         238       0.83      0.56      0.67         9
         239       1.00      0.25      0.40         4
         240       0.00      0.00      0.00         4
         241       0.33      0.50      0.40         4
         242       0.80      1.00      0.89         4
         243       0.64      0.78      0.70         9
         244       0.00      0.00      0.00         4
         245       0.36      1.00      0.53         4
         246       1.00      1.00      1.00         4
         247       0.50      1.00      0.67         5
         248       1.00      1.00      1.00         4
         249       0.82      1.00      0.90         9
         250       0.75      0.75      0.75         4
         251       0.88      0.78      0.82         9
         252       0.75      0.75      0.75         4
         253       1.00      0.50      0.67         4
         254       0.75      0.75      0.75         4
         255       0.89      0.89      0.89         9
         256       0.50      0.50      0.50         4
         257       1.00      1.00      1.00         9
         258       0.00      0.00      0.00         4
         259       0.67      0.50      0.57         4
         260       1.00      1.00      1.00         4
         261       0.35      0.78      0.48         9
         262       0.75      1.00      0.86         9
         263       0.70      0.78      0.74         9
         264       0.00      0.00      0.00         4
         265       0.09      0.25      0.13         4
         266       0.67      1.00      0.80         4
         267       0.58      0.78      0.67         9
         268       1.00      0.75      0.86         4
         269       1.00      1.00      1.00         4
         270       0.71      1.00      0.83         5
         271       0.88      0.78      0.82         9
         272       1.00      1.00      1.00         4
         273       0.00      0.00      0.00         4
         274       0.75      1.00      0.86         9
         275       0.62      1.00      0.77         5
         276       1.00      0.75      0.86         4
         277       0.75      0.75      0.75         4
         278       0.75      0.38      0.50         8
         279       0.00      0.00      0.00         4
         280       1.00      1.00      1.00         4
         281       1.00      0.50      0.67         4
         282       0.80      1.00      0.89         4
         283       1.00      0.89      0.94         9
         284       0.80      0.89      0.84         9
         285       0.67      0.89      0.76         9
         286       1.00      0.80      0.89         5
         287       0.71      1.00      0.83         5
         288       0.56      1.00      0.72         9
         289       1.00      0.50      0.67         4
         290       0.50      0.80      0.62         5
         291       0.83      0.56      0.67         9
         292       0.67      1.00      0.80         4
         293       0.75      1.00      0.86         9
         294       0.75      0.75      0.75         4
         295       0.80      0.80      0.80         5
         296       0.83      1.00      0.91         5
         297       0.80      1.00      0.89         4
         298       0.00      0.00      0.00         4
         299       0.67      0.50      0.57         4
         300       0.43      0.33      0.38         9
         301       0.80      1.00      0.89         4
         302       0.40      0.50      0.44         4
         303       0.00      0.00      0.00         4
         304       0.67      0.50      0.57         4
         305       0.80      1.00      0.89         4
         306       1.00      0.80      0.89         5
         307       0.57      1.00      0.73         4
         308       0.67      0.50      0.57         4
         309       1.00      1.00      1.00         9
         310       0.67      0.40      0.50         5
         311       0.50      0.20      0.29         5
         312       0.29      0.50      0.36         4
         313       0.50      0.75      0.60         4
         314       0.80      1.00      0.89         4
         315       0.08      0.11      0.09         9
         316       0.30      0.75      0.43         4
         317       0.60      1.00      0.75         9
         318       0.60      0.60      0.60         5
         319       1.00      1.00      1.00         5
         320       0.00      0.00      0.00         4
         321       1.00      0.75      0.86         4
         322       1.00      1.00      1.00         4
         323       1.00      0.75      0.86         4
         324       0.40      0.50      0.44         4
         325       0.60      0.75      0.67         4
         326       0.43      0.60      0.50         5
         327       1.00      1.00      1.00         4
         328       1.00      0.80      0.89         5
         329       0.00      0.00      0.00         4
         330       0.00      0.00      0.00         4
         331       1.00      0.00      0.00         4
         332       0.75      0.75      0.75         4
         333       1.00      1.00      1.00         4
         334       0.80      0.80      0.80         5
         335       0.60      0.75      0.67         4
         336       0.13      0.50      0.21         4
         337       0.00      0.00      0.00         4
         338       1.00      1.00      1.00         4
         339       1.00      0.75      0.86         4
         340       1.00      0.00      0.00         4
         341       0.20      0.20      0.20         5
         342       0.89      0.89      0.89         9
         343       0.82      1.00      0.90         9
         344       1.00      1.00      1.00         4
         345       0.88      0.78      0.82         9
         346       1.00      0.78      0.88         9
         347       0.55      0.86      0.67         7
         348       1.00      0.56      0.71         9
         349       0.67      0.50      0.57         4
         350       0.25      0.75      0.38         4
         351       0.00      0.00      0.00         4
         352       1.00      1.00      1.00         4
         353       0.89      0.89      0.89         9
         354       0.50      0.25      0.33         4
         355       0.83      1.00      0.91         5
         356       1.00      0.60      0.75         5
         357       0.83      1.00      0.91         5
         358       0.67      0.22      0.33         9
         359       0.60      0.67      0.63         9
         360       0.71      0.56      0.63         9
         361       1.00      0.44      0.62         9
         362       0.80      1.00      0.89         4
         363       0.00      0.00      0.00         4
         364       0.50      0.50      0.50         4
         365       0.67      0.50      0.57         4
         366       1.00      1.00      1.00         4
         367       1.00      1.00      1.00         4
         368       0.00      0.00      0.00         4
         369       0.67      0.89      0.76         9
         370       0.83      1.00      0.91         5
         371       0.80      1.00      0.89         4
         372       0.67      0.50      0.57         4
         373       0.60      0.75      0.67         4
         374       1.00      0.89      0.94         9
         375       0.25      0.20      0.22         5
         376       0.71      0.56      0.63         9
         377       0.90      1.00      0.95         9
         378       0.80      0.80      0.80         5
         379       0.50      0.75      0.60         4
         380       1.00      0.75      0.86         4
         381       1.00      0.75      0.86         4
         382       1.00      0.80      0.89         5
         383       1.00      0.60      0.75         5
         384       0.78      0.78      0.78         9
         385       1.00      1.00      1.00         4
         386       1.00      1.00      1.00         4
         387       0.00      0.00      0.00         4
         388       0.75      0.67      0.71         9
         389       0.60      0.75      0.67         4
         390       1.00      0.25      0.40         4
         391       0.67      0.80      0.73         5
         392       1.00      0.20      0.33         5
         393       1.00      0.89      0.94         9
         394       0.70      0.78      0.74         9
         395       0.60      0.75      0.67         4
         396       0.75      0.75      0.75         4
         397       0.67      0.50      0.57         4
         398       0.00      0.00      0.00         4
         399       0.00      0.00      0.00         4
         400       1.00      0.50      0.67         4
         401       0.00      0.00      0.00         4
         402       0.41      0.78      0.54         9
         403       0.88      0.78      0.82         9
         404       0.04      0.11      0.06         9
         405       1.00      0.50      0.67         4
         406       1.00      1.00      1.00         4
         407       1.00      0.75      0.86         4
         408       0.75      0.75      0.75         4
         409       0.82      1.00      0.90         9
         410       1.00      0.00      0.00         4
         411       0.00      0.00      0.00         4
         412       0.60      0.75      0.67         4
         413       1.00      1.00      1.00         4
         414       0.00      0.00      0.00         9
         415       0.73      0.89      0.80         9
         416       1.00      1.00      1.00         5
         417       1.00      1.00      1.00         9
         418       0.75      0.75      0.75         4
         419       0.50      0.25      0.33         4
         420       0.88      0.78      0.82         9
         421       0.00      0.00      0.00         4
         422       1.00      1.00      1.00         4
         423       1.00      1.00      1.00         4
         424       1.00      0.60      0.75         5
         425       0.50      0.25      0.33         4
         426       1.00      0.80      0.89         5
         427       1.00      0.40      0.57         5
         428       0.75      0.75      0.75         4
         429       1.00      1.00      1.00         4
         430       1.00      0.40      0.57         5
         431       1.00      1.00      1.00         9
         432       0.80      0.80      0.80         5
         433       1.00      0.75      0.86         4
         434       0.50      0.67      0.57         9
         435       0.75      0.67      0.71         9
         436       0.50      0.40      0.44         5
         437       1.00      0.89      0.94         9
         438       0.00      0.00      0.00         9
         439       1.00      0.25      0.40         4
         440       0.60      0.75      0.67         4
         441       1.00      0.75      0.86         4
         442       0.78      0.78      0.78         9
         443       1.00      0.75      0.86         4
         444       0.83      1.00      0.91         5
         445       0.00      0.00      0.00         4
         446       0.80      0.80      0.80         5
         447       0.33      0.20      0.25         5
         448       0.00      0.00      0.00         4
         449       1.00      0.50      0.67         4
         450       1.00      0.25      0.40         4
         451       0.21      0.44      0.29         9
         452       1.00      1.00      1.00         9
         453       1.00      1.00      1.00         4
         454       0.75      0.75      0.75         4
         455       0.50      0.40      0.44         5
         456       1.00      1.00      1.00         4
         457       0.54      0.78      0.64         9
         458       0.50      0.25      0.33         4
         459       1.00      1.00      1.00         4
         460       0.71      0.56      0.63         9
         461       0.90      1.00      0.95         9
         462       0.80      1.00      0.89         4
         463       1.00      0.75      0.86         4
         464       1.00      1.00      1.00         5
         465       0.00      0.00      0.00         4
         466       0.80      1.00      0.89         4
         467       0.62      0.89      0.73         9
         468       0.71      0.56      0.63         9
         469       0.80      1.00      0.89         4
         470       0.57      0.80      0.67         5
         471       1.00      0.20      0.33         5
         472       0.67      0.50      0.57         4
         473       0.78      0.78      0.78         9
         474       0.78      0.78      0.78         9
         475       0.50      0.50      0.50         4
         476       0.89      0.89      0.89         9
         477       0.83      1.00      0.91         5
         478       1.00      1.00      1.00         4
         479       0.00      0.00      0.00         4
         480       1.00      0.80      0.89         5
         481       1.00      0.89      0.94         9
         482       0.60      0.75      0.67         4
         483       1.00      0.78      0.88         9
         484       0.80      1.00      0.89         4
         485       0.80      0.80      0.80         5
         486       1.00      0.00      0.00         4
         487       0.75      0.75      0.75         4
         488       0.75      0.75      0.75         4
         489       0.75      0.75      0.75         4
         490       1.00      1.00      1.00         4
         491       0.80      1.00      0.89         4
         492       0.12      0.25      0.17         4
         493       0.82      1.00      0.90         9
         494       0.00      0.00      0.00         4
         495       0.25      0.25      0.25         4
         496       0.43      0.60      0.50         5
         497       0.67      0.50      0.57         4
         498       0.40      0.67      0.50         9
         499       0.80      0.89      0.84         9
         500       0.39      0.78      0.52         9
         501       0.57      1.00      0.73         4
         502       0.80      1.00      0.89         4
         503       1.00      1.00      1.00         5
         504       1.00      1.00      1.00         4
         505       0.40      0.80      0.53         5
         506       0.00      0.00      0.00         4
         507       1.00      0.75      0.86         4
         508       0.11      0.25      0.15         4
         509       0.67      1.00      0.80         4
         510       0.00      0.00      0.00         4
         511       0.20      0.25      0.22         4
         512       0.33      0.25      0.29         4
         513       1.00      1.00      1.00         4
         514       1.00      0.50      0.67         4
         515       0.86      0.67      0.75         9
         516       0.60      0.60      0.60         5
         517       0.57      1.00      0.73         4
         518       0.80      0.80      0.80         5
         519       0.50      0.78      0.61         9
         520       1.00      1.00      1.00         4
         521       0.75      0.75      0.75         4
         522       0.62      0.89      0.73         9
         523       1.00      0.75      0.86         4
         524       1.00      1.00      1.00         5
         525       0.80      0.89      0.84         9
         526       1.00      1.00      1.00         4
         527       0.00      0.00      0.00         4
         528       0.00      0.00      0.00         4
         529       1.00      1.00      1.00         4
         530       0.36      0.44      0.40         9
         531       0.33      0.50      0.40         4
         532       1.00      1.00      1.00         4
         533       0.75      1.00      0.86         9
         534       1.00      0.89      0.94         9
         535       0.54      0.78      0.64         9
         536       0.00      0.00      0.00         4
         537       1.00      0.00      0.00         4
         538       0.64      0.78      0.70         9
         539       0.00      0.00      0.00         9
         540       0.75      0.75      0.75         4
         541       0.75      0.75      0.75         4
         542       1.00      0.50      0.67         4
         543       0.80      0.80      0.80         5
         544       0.70      0.78      0.74         9
         545       1.00      0.75      0.86         4
         546       0.71      1.00      0.83         5
         547       0.67      1.00      0.80         4
         548       1.00      1.00      1.00         4
         549       0.20      0.50      0.29         4
         550       0.67      0.50      0.57         4
         551       0.00      0.00      0.00         4
         552       0.67      1.00      0.80         4
         553       0.80      1.00      0.89         4
         554       0.40      0.50      0.44         4
         555       1.00      0.00      0.00         4
         556       0.57      1.00      0.73         4
         557       0.20      0.25      0.22         4
         558       0.00      0.00      0.00         4
         559       0.89      0.89      0.89         9
         560       0.75      0.75      0.75         4
         561       0.83      1.00      0.91         5
         562       0.90      1.00      0.95         9
         563       0.00      0.00      0.00         9
         564       1.00      0.00      0.00         4
         565       0.00      0.00      0.00         4
         566       1.00      0.60      0.75         5
         567       0.00      0.00      0.00         4
         568       1.00      0.50      0.67         4
         569       1.00      0.75      0.86         4
         570       0.90      1.00      0.95         9
         571       0.00      0.00      0.00         4
         572       0.75      0.67      0.71         9
         573       0.40      0.50      0.44         4
         574       0.00      0.00      0.00         9
         575       0.80      1.00      0.89         4
         576       0.80      0.80      0.80         5
         577       0.80      1.00      0.89         4
         578       0.67      1.00      0.80         4
         579       0.80      1.00      0.89         4
         580       0.60      0.60      0.60         5
         581       0.00      0.00      0.00         9
         582       0.00      0.00      0.00         4
         583       0.80      1.00      0.89         4
         584       1.00      1.00      1.00         4
         585       0.29      0.40      0.33         5
         586       0.80      0.80      0.80         5
         587       0.14      0.50      0.22         4
         588       0.89      0.89      0.89         9
         589       0.00      0.00      0.00         4
         590       0.50      0.25      0.33         4
         591       0.62      0.89      0.73         9
         592       0.25      0.25      0.25         4
         593       0.67      0.67      0.67         9
         594       0.00      0.00      0.00         4
         595       1.00      0.75      0.86         4
         596       0.60      0.67      0.63         9
         597       1.00      0.00      0.00         5
         598       0.20      0.20      0.20         5
         599       1.00      0.75      0.86         4
         600       0.67      1.00      0.80         4
         601       0.67      0.50      0.57         4
         602       1.00      1.00      1.00         5
         603       0.80      1.00      0.89         4
         604       1.00      0.50      0.67         4
         605       0.40      0.50      0.44         4
         606       1.00      0.75      0.86         4
         607       1.00      0.75      0.86         4
         608       0.80      1.00      0.89         4
         609       0.88      0.78      0.82         9
         610       1.00      0.00      0.00         4
         611       1.00      0.00      0.00         4
         612       0.60      0.60      0.60         5
         613       0.75      0.67      0.71         9
         614       0.60      0.75      0.67         4
         615       0.80      1.00      0.89         4
         616       0.57      0.80      0.67         5
         617       1.00      0.75      0.86         4
         618       0.50      0.50      0.50         4
         619       0.00      0.00      0.00         4
         620       1.00      1.00      1.00         5
         621       1.00      0.50      0.67         4
         622       1.00      0.89      0.94         9
         623       1.00      0.75      0.86         4
         624       0.80      1.00      0.89         4
         625       1.00      0.25      0.40         4
         626       1.00      1.00      1.00         5
         627       0.80      1.00      0.89         4
         628       1.00      0.40      0.57         5
         629       1.00      0.89      0.94         9
         630       0.67      0.50      0.57         4
         631       1.00      1.00      1.00         4
         632       1.00      0.50      0.67         4
         633       0.83      1.00      0.91         5
         634       0.80      1.00      0.89         4
         635       1.00      0.00      0.00         4
         636       1.00      1.00      1.00         4
         637       0.33      0.50      0.40         4
         638       0.80      0.89      0.84         9
         639       0.89      0.89      0.89         9
         640       1.00      0.88      0.93         8
         641       0.33      0.75      0.46         4
         642       0.40      0.80      0.53         5
         643       0.89      0.89      0.89         9
         644       0.57      0.89      0.70         9
         645       0.45      1.00      0.62         5
         646       0.57      1.00      0.73         4
         647       0.00      0.00      0.00         4
         648       0.78      0.78      0.78         9
         649       0.80      1.00      0.89         4
         650       0.43      0.75      0.55         4
         651       1.00      0.60      0.75         5
         652       0.50      1.00      0.67         4
         653       0.88      0.78      0.82         9
         654       0.67      0.50      0.57         4
         655       1.00      0.75      0.86         4
         656       0.75      0.33      0.46         9
         657       1.00      1.00      1.00         4
         658       0.50      0.75      0.60         4
         659       1.00      0.56      0.71         9
         660       1.00      0.80      0.89         5
         661       0.69      1.00      0.82         9
         662       1.00      1.00      1.00         9
         663       1.00      0.75      0.86         4
         664       0.82      1.00      0.90         9
         665       0.83      1.00      0.91         5
         666       0.67      1.00      0.80         4
         667       1.00      1.00      1.00         4
         668       0.50      0.44      0.47         9
         669       0.43      0.60      0.50         5
         670       0.83      0.56      0.67         9
         671       0.50      0.75      0.60         4
         672       0.67      0.44      0.53         9
         673       0.17      0.25      0.20         4
         674       1.00      0.78      0.88         9
         675       1.00      0.78      0.88         9
         676       0.50      0.25      0.33         4
         677       0.80      1.00      0.89         4
         678       1.00      0.80      0.89         5
         679       0.00      0.00      0.00         4
         680       1.00      0.00      0.00         4
         681       1.00      0.00      0.00         4
         682       1.00      1.00      1.00         9
         683       0.90      1.00      0.95         9
         684       1.00      0.67      0.80         9
         685       0.00      0.00      0.00         4
         686       0.67      0.80      0.73         5
         687       1.00      0.50      0.67         4
         688       0.80      1.00      0.89         4
         689       0.88      0.78      0.82         9
         690       0.29      0.50      0.36         4
         691       0.60      0.67      0.63         9
         692       1.00      0.00      0.00         4
         693       1.00      1.00      1.00         5
         694       1.00      0.50      0.67         4
         695       0.29      0.80      0.42         5
         696       1.00      0.25      0.40         4
         697       0.60      0.75      0.67         4
         698       0.88      0.78      0.82         9
         699       0.71      0.56      0.63         9
         700       1.00      0.00      0.00         4
         701       0.11      0.11      0.11         9
         702       1.00      0.67      0.80         9
         703       1.00      0.50      0.67         4
         704       0.50      0.56      0.53         9
         705       1.00      0.00      0.00         4
         706       1.00      0.78      0.88         9
         707       1.00      0.75      0.86         4
         708       0.75      0.33      0.46         9
         709       0.50      1.00      0.67         4
         710       0.80      0.89      0.84         9
         711       0.43      0.75      0.55         4
         712       0.50      0.25      0.33         4
         713       0.50      0.75      0.60         4
         714       0.44      0.80      0.57         5
         715       0.50      0.50      0.50         4
         716       1.00      1.00      1.00         4
         717       0.50      0.89      0.64         9
         718       1.00      1.00      1.00         9
         719       0.75      1.00      0.86         9
         720       0.25      0.25      0.25         4
         721       0.86      0.67      0.75         9
         722       0.33      0.25      0.29         4
         723       0.23      0.33      0.27         9
         724       0.60      0.60      0.60         5
         725       0.62      0.89      0.73         9
         726       1.00      0.75      0.86         4
         727       0.82      1.00      0.90         9
         728       0.60      0.75      0.67         4
         729       0.43      0.75      0.55         4
         730       1.00      1.00      1.00         4
         731       0.62      1.00      0.77         5
         732       0.80      1.00      0.89         4
         733       0.69      1.00      0.82         9
         734       1.00      1.00      1.00         4
         735       0.33      0.25      0.29         4
         736       0.00      0.00      0.00         4
         737       0.75      0.75      0.75         4
         738       1.00      1.00      1.00         4
         739       0.80      1.00      0.89         4
         740       0.80      1.00      0.89         4
         741       1.00      1.00      1.00         4
         742       0.89      0.89      0.89         9
         743       1.00      0.50      0.67         4
         744       0.50      0.50      0.50         4
         745       1.00      0.25      0.40         4
         746       0.00      0.00      0.00         4
         747       0.50      0.25      0.33         4
         748       0.67      1.00      0.80         4
         749       0.71      0.56      0.63         9
         750       0.50      0.25      0.33         4
         751       0.55      0.67      0.60         9
         752       0.33      0.25      0.29         4
         753       0.33      0.25      0.29         4
         754       1.00      0.75      0.86         4
         755       1.00      0.75      0.86         4
         756       0.86      0.67      0.75         9
         757       1.00      0.20      0.33         5
         758       1.00      1.00      1.00         9
         759       0.00      0.00      0.00         4
         760       0.80      1.00      0.89         4
         761       1.00      0.80      0.89         5
         762       1.00      1.00      1.00         4
         763       1.00      0.00      0.00         4
         764       1.00      0.40      0.57         5
         765       1.00      1.00      1.00         4
         766       0.00      0.00      0.00         4
         767       0.00      0.00      0.00         4
         768       0.75      0.75      0.75         4
         769       1.00      0.60      0.75         5
         770       1.00      0.89      0.94         9
         771       0.50      0.56      0.53         9
         772       0.50      0.11      0.18         9
         773       0.00      0.00      0.00         9
         774       0.00      0.00      0.00         4
         775       0.67      1.00      0.80         4
         776       1.00      0.80      0.89         5
         777       1.00      0.00      0.00         4
         778       0.00      0.00      0.00         4
         779       1.00      0.78      0.88         9
         780       0.50      0.75      0.60         4
         781       0.86      0.67      0.75         9
         782       0.88      0.78      0.82         9
         783       0.80      1.00      0.89         4
         784       1.00      1.00      1.00         9
         785       0.80      0.89      0.84         9
         786       0.80      1.00      0.89         4
         787       1.00      0.40      0.57         5
         788       1.00      1.00      1.00         5
         789       1.00      0.50      0.67         4
         790       1.00      0.22      0.36         9
         791       0.50      0.50      0.50         4
         792       0.67      0.89      0.76         9
         793       0.75      0.75      0.75         4
         794       1.00      0.00      0.00         4
         795       0.89      0.89      0.89         9
         796       0.00      0.00      0.00         9
         797       1.00      0.00      0.00         4
         798       0.67      0.80      0.73         5
         799       1.00      0.25      0.40         4
         800       1.00      0.89      0.94         9
         801       1.00      0.80      0.89         5
         802       1.00      0.75      0.86         4
         803       1.00      0.75      0.86         4
         804       0.75      0.75      0.75         4
         805       0.60      1.00      0.75         9
         806       0.14      0.25      0.18         4
         807       0.90      1.00      0.95         9
         808       1.00      0.75      0.86         4
         809       1.00      0.78      0.88         9
         810       1.00      0.00      0.00         4
         811       1.00      1.00      1.00         4
         812       0.67      0.50      0.57         4
         813       0.67      1.00      0.80         4
         814       1.00      1.00      1.00         4
         815       0.00      0.00      0.00         5
         816       1.00      1.00      1.00         5
         817       0.89      0.89      0.89         9
         818       1.00      1.00      1.00         4
         819       1.00      0.75      0.86         4
         820       0.89      0.89      0.89         9
         821       1.00      1.00      1.00         4
         822       1.00      1.00      1.00         5
         823       0.50      0.50      0.50         4
         824       1.00      1.00      1.00         4
         825       1.00      0.80      0.89         5
         826       1.00      0.75      0.86         4
         827       1.00      0.00      0.00         4
         828       0.75      1.00      0.86         9
         829       0.64      0.78      0.70         9
         830       1.00      0.25      0.40         4
         831       0.75      0.75      0.75         4
         832       0.00      0.00      0.00         4
         833       0.50      0.50      0.50         4
         834       1.00      0.20      0.33         5
         835       0.00      0.00      0.00         9
         836       0.00      0.00      0.00         4
         837       1.00      0.40      0.57         5
         838       1.00      1.00      1.00         5
         839       0.80      1.00      0.89         4
         840       1.00      0.75      0.86         4
         841       0.40      0.50      0.44         4
         842       1.00      0.50      0.67         4
         843       1.00      0.60      0.75         5
         844       1.00      0.25      0.40         4
         845       1.00      0.00      0.00         4
         846       1.00      0.78      0.88         9
         847       1.00      1.00      1.00         5
         848       1.00      0.80      0.89         5
         849       0.88      0.78      0.82         9
         850       0.50      0.75      0.60         4
         851       0.00      0.00      0.00         4
         852       1.00      0.25      0.40         4
         853       1.00      1.00      1.00         4
         854       0.89      0.89      0.89         9
         855       0.17      0.11      0.13         9
         856       1.00      1.00      1.00         4
         857       1.00      0.00      0.00         4
         858       0.67      1.00      0.80         4
         859       0.75      0.75      0.75         4
         860       0.00      0.00      0.00         9
         861       0.67      1.00      0.80         4
         862       1.00      1.00      1.00         4
         863       0.43      0.60      0.50         5
         864       0.67      0.50      0.57         4
         865       1.00      0.67      0.80         9
         866       1.00      0.25      0.40         4
         867       1.00      0.00      0.00         4
         868       0.75      0.60      0.67         5
         869       0.00      0.00      0.00         4
         870       0.83      0.56      0.67         9
         871       1.00      0.25      0.40         4
         872       0.67      0.50      0.57         4
         873       0.67      0.86      0.75         7
         874       1.00      0.20      0.33         5
         875       1.00      0.25      0.40         4
         876       0.70      0.78      0.74         9
         877       1.00      0.25      0.40         4
         878       1.00      0.75      0.86         4
         879       0.00      0.00      0.00         4
         880       0.75      0.75      0.75         4
         881       0.73      0.89      0.80         9
         882       1.00      1.00      1.00         4
         883       1.00      0.44      0.62         9
         884       0.67      0.40      0.50         5
         885       1.00      0.60      0.75         5
         886       1.00      0.00      0.00         4
         887       1.00      0.67      0.80         9
         888       0.00      0.00      0.00         4
         889       1.00      0.89      0.94         9
         890       0.20      0.25      0.22         4
         891       0.00      0.00      0.00         4
         892       0.00      0.00      0.00         9
         893       1.00      0.75      0.86         4

    accuracy                           0.66      4917
   macro avg       0.71      0.63      0.63      4917
weighted avg       0.71      0.66      0.65      4917

